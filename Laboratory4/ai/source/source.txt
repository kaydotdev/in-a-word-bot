Diff. Eq.

The following
content is provided under a Creative
Commons license. Your support will help MIT
OpenCourseWare continue to offer high-quality
educational resources for free. To make a donation or
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: OK. Let's start. So last time, we started
with kinetic theory. And we will focus for
gas systems mostly. And the question
that we would like to think about and answer
somehow is the following. You start with a gas
that is initially confined to one chamber. And you can calculate all of
its thermodynamic properties. You open at time 0 a
hole, allowing the gas to escape into a second
initially empty chamber. And after some time,
the whole system will come to a new
equilibrium position. It's a pretty reversible thing. You can do this experiment
many, many times. And you will always get
roughly the same amount of time for the situation to
start from one equilibrium and reach another equilibrium. So how do we describe that? It's slightly beyond what
we did in thermodynamics, because we want to go
from one equilibrium state to another equilibrium state. Now, we said, OK, we know
the equations of motion that governs the particles
that are described by this. So here we can say that we
have, let's say, N particles. They have their own
momenta and coordinates. And we know that these
momenta and coordinates evolve in time, governed
by some Hamiltonian, which is a function of all of
these momenta and coordinates. OK? Fine. How do we go from
a situation which describes a whole
bunch of things and coordinates that
are changing with time to some microscopic description
of macroscopic variables going from one equilibrium
state to another? So our first attempt
in that direction was to say, well, I could start
with many, many, many examples of the same situation. Each one of them would
correspond to a different trajectory of p's and q's. And so what we can
do is to construct some kind of an ensemble
average, or ensemble density, first, which is what we did. We can say that the very, very
many examples of this situation that I can have will
correspond to different points at some particular
instant of time, occupying this
6N-dimensional phase space, out of which I can construct
some kind of a density in phase space. But then I realize that since
each one of these trajectories is evolving according
to this Hamiltonian, this density could potentially
be a function of time. And we described the
equation for the evolution of that density with time. And we could write it in
this form-- V rho by dt is the Poisson bracket of
the Hamiltonian with rho. And this Poisson
bracket was defined as a sum over all
of your coordinates. Oops. We had d rho by d vector qi,
dot product with dH by dpi minus the other [? way. ?] So there are essentially
three N terms-- well, actually, six N terms, but three
N pairs of terms in this sum. I can either use
some index alpha running from one to
three N, or indicate them as contributions of things that
come from individual particles and then use this notation
with three vectors. So this is essentially
a combination of sum of three terms. OK? So I hope that
somehow this equation can describe the evolution
that goes on over here. And ultimately, when I wait
sufficiently long time, I will reach a situation
where d rho by dt does not change anymore. And I will find some density
that is invariant on time. I'll call that rho equilibrium. And so we saw that we could have
our rho equilibrium, which then should have zero Poisson bracket
with H to be a function of H and any other
conserved quantity. OK? So in principle,
we sort of thought of a way of describing
how the system will evolve in equilibrium--
towards an equilibrium. And indeed, we will find that in
statistical mechanics later on, we are going to rely heavily
on these descriptions of equilibrium
densities, which are governed by only
the Hamiltonian. And if there are any other
conserved quantities, typically there are not. So this is the general
form that we'll ultimately be using a lot. But we started
with a description of evolution of
these coordinates in time, which is
time-reversible. And we hope to end up
with a situation that is analogous to what we
describe thermodynamically, we describe something that
goes to some particular state, and basically
stays there, as far as the macroscopic
description is concerned. So did we somehow manage to just
look at something else, which is this density, and
achieve this transition from reversibility
to irreversibility? And the answer is no. This equation-- also,
if you find, indeed, a rho that goes from here to
here, you can set t to minus t and get a rho that goes
from back here to here. It has the same
time-reversibility. So somehow, we have to
find another solution. And the solution that we
will gradually build upon relies more on
physics rather than the rigorous mathematical
nature of this thing. Physically, we know
that this happens. All of us have seen this. So somehow, we should be able
to use physical approximation and assumptions that
are compatible with what is observed. And if, during that,
we have to sort of make mathematical
approximations, so be it. In fact, we have to make
mathematical approximations, because otherwise, there is
this very strong reversibility condition. So let's see how we are about
to proceed, thinking more in terms of physics. Now, the information
that I have over here, either in this
description of ensemble or in this description
in terms of trajectories, is just enormous. I can't think of any
physical process which would need to keep track of the
joined positions of coordinates and momenta of 10
to the 23 particles, and know them with infinite
precision, et cetera. Things that I make
observations with and I say, we see this,
well, what do we see? We see that something has some
kind of a density over here. It has some kind of pressure. Maybe there is, in the
middle of this process, some flow of gas particles. We can talk about the
velocity of those things. And let's say even
when we are sort of being very non-equilibrium
and thinking about velocity of those particles going from
one side to the other side, we really don't care which
particle among the 10 to the 23 is at some instant
of time contributing to the velocity of the
gas that is swishing past. So clearly, any physical
observable that we make is something that
does not require all of those degrees of freedom. So let's just construct
some of those things that we may find
useful and see how we would describe
evolutions of them. I mean, the most useful
thing, indeed, is the density. So what I could do
is I can actually construct a
one-particle density. So I want to look at
some position in space at some time t and
ask whether or not there are particles there. I don't care which
one of the particles. Actually, let's put in a
little bit more information; also, keep track of whether,
at some instant of time, I see at this
location particles. And these particles,
I will also ask which direction they are moving. Maybe I'm also going
to think about the case where, in the intermediate,
I am flowing from one side to another. And keeping track of both
of these is important. I said I don't care which
one of these particles is contributing. So that means that I have
to sum over all particles and ask whether
or not, at time t, there is a particle at
location q with momentum p. So these delta functions are
supposed to enforce that. And again, I'm not thinking
about an individual trajectory, because I think, more or
less, all of the trajectories are going to behave
the same way. And so what I will do
is I take an ensemble average of this quantity. OK? So what does that mean? To do an ensemble
average, we said I have to integrate the
density function, rho, which depends on all
of the coordinates. So I have coordinate
for particle one, coordinate-- particle and
momentum, particle and momentum for particle two,
for particle three, all the way to
particle number N. And this is something
that depends on time. That's where the time
dependence comes from. And again, let's think
about the time dependence where 0 is you
lift the partition and you allow things to
go from one to the other. So there is a non-equilibrium
process that you are following. What I need to do is I have
to multiply this density with the function that I
have to consider the average. Well, the function being a
delta function, it's very nice. The first term in the sum,
we'll set simply q1 equals to q. And it will set p1
equals to p when I do the integration
over p1 and q1. But then I'm left to have to do
the integration over particle two, particle three,
particle four, and so forth. OK? But this is only the
first term in the sum. Then I have to write the
sum for particle number two, particle number
three, et cetera. But all of the particles,
as far as I'm concerned, are behaving the same way. So I just need to multiply
this by a factor of N. Now, this is something that
actually we encountered before. Recall that this rho is a
joint probability distribution. It's a joint
probability distribution of all N particles, their
locations, and momentum. And we gave a name to taking
a joint probability density function, which this
is, and integrating over some subset of variables. The answer is an unconditional
probability distribution. So essentially, the
end of this story is if I integrate
over coordinates of two all the way to N, I will
get an unconditional result pertaining to the first
set of coordinate. So this is the same thing
up to a factor of N-- the unconditional probability
that I will call rho one. Actually, I should
have called this f1. So this is something
that is defined to be the one particle density. I don't know why this name stuck
to it, because this one, that is up to a factor of
N different from it, is our usual unconditional
probability for one particle. So this is rho of p1 being
p, q1 being q at time. OK? So essentially, what I've said
is I have a joint probability. I'm really interested
in one of the particles, so I just integrate
over all the others. And this is kind of
not very elegant, but that's somehow
the way that it has appeared in the literature. The entity that I
have on the right is the properly
normalized probability. Once I multiply by N, it's a
quantity this is called f1. And it's called a density
and used very much in the literature. OK? And that's the thing that
I think will be useful. Essentially, all of
the things that I know about observations
of a system, such as its density,
its velocity, et cetera, I should be able
to get from this. Sometimes we need a little
bit more information, so I allow for the
possibility that I may also need to focus on a
two-particle density, where I keep information pertaining
to two particles at time t and I integrate over everybody
else that I'm not interested. So I introduce an integration
over coordinates number two-- sorry, number
three all the way to N to not have to repeat this
combination all the time. The 6N-dimensional
phase space for particle i I will indicate by dVi. So I take the full N particle
density, if you like, integrate over
everybody except 2. And up to a normalization
of NN minus 1, this is called a
two-particle density. And again, absent
this normalization, this is simply the
unconditional probability that I would construct out
of this joint probability [INAUDIBLE]. And just for some mathematical
purposes, the generalization of this to S particles,
we will call fs. So this depends on p1
through qs at time t. And this is going to
be N factorial divided by N minus S
factorial in general. And they join the unconditional
probability that depends on S coordinates p1
through qs at time t. OK? You'll see why I need
these higher ones although, in reality, all my
interest is on this first one, because practically
everything that I need to know about this initial
experiment that I set up and how a gas expands I
should be able to extract from this one-particle density. OK? So how do I calculate this? Well, what I would
need to do is to look at the time variation
of fs with t to calculate the time dependence
of any one of these quantities. Ultimately, I want to have
an equation for df1 by dt. But let's write the general one. So the general one is up
to, again, this N factorial, N minus s factorial
and integral over coordinates that I'm not
interested, of d rho by dt. OK? So I just take the time
derivative inside the integral, because I know how
d rho by dt evolves. And so this is going to be
simply the Poisson bracket of H and rho. And I would proceed from there. Actually, to proceed
further and in order to be able to say things
that are related to physics, I need to say something
about the Hamiltonian. I can't do so in the
most general case. So let's write the
kind of Hamiltonian that we are interested
and describes the gas-- so the Hamiltonian for
the gas in this room. One term is simply
the kinetic energy. Another term is
that gas particles are confined by some potential. The potential could be as easy
as the walls of this room. Or it could be some
more general potential that could include gravity,
whatever else you want. So let's include
that possibility. So these are so-called
one-body terms, because they pertain to the
coordinates of one particle. And then there are
two-body terms. So for example, I could look
at all pairs of particles. Let's say i not equal to j
to avoid self-interaction. V of qi minus qj. So certainly, two particles
in this gas in this room, when they get close
enough, they certainly can pass through each other. They each have a size. But even when they are
a few times their sizes, they start to feel
some interaction, which causes them to collide. Yes. AUDIENCE: The whole
series of arguments that we're developing,
do these only hold for time-independent potentials? PROFESSOR: Yes. Although, it is not that
difficult to sort of go through all of
these arguments and see where the corresponding
modifications are going to be. But certainly, the
very first thing that we are using, which is
this one, we kind of implicitly assume the time-independent
Hamiltonian. So you have to start changing
things from that point. OK? So in principle, you could
have three-body and higher body terms. But essentially, most
of the relevant physics is captured by this. Even for things
like plasmas, this would be a reasonably
good approximation, with the Coulomb
interaction appearing here. OK? Now, what I note is that what
I have to calculate over here is a Poisson bracket. And then I have to integrate
that Poisson bracket. Poisson bracket
involves a whole bunch of derivatives over
all set of quantities. And I realize that when I'm
integrating over derivatives, there are
simplifications that can take place, such as
integration by parts. But that only will take
place when the derivative is one of the variables
that is being integrated. Now, this whole process has
separated out arguments of rho, as far as this
expression is concerned, into two sets-- one set, or the
set that is appearing out here, the first s ones that don't
undergo the integration, and then the remainder that
do undergo the integration. So this is going to be relevant
when we do our manipulations. And therefore, it is useful
to rewrite this Hamiltonian in terms of three entities. The first one that I call
H sub s-- so if you like, this is an end
particle Hamiltonian. I can write an H sub s, which
is just exactly the same thing, except that it
applies to coordinates that I'm not integrating over. And to sort of
make a distinction, I will label them by N. And
it includes the interaction among those particles. And I can similarly
write something that pertains to coordinates
that I am integrating over. I will label them by j and k. So this is s plus 1 to N, pj
squared over 2m plus u of qj plus 1/2 sum over j and
k, V of qj minus qk. OK? So everything that involves one
set of coordinates, everything that involves the other
set of coordinates. So what is left are terms
that [? copy ?] one set of coordinates to another. So N running from 1 to s,
j running from s plus 1 to N, V between qm and qj. OK? So let me rewrite this
equation rather than in terms of-- f in terms of the
probabilities' rhos. So the difference
only is that I don't have to include this
factor out front. So I have dVi. I have the d rho by dt, which
is the commutator of H with rho, which I have been writing
as Hs plus HN minus s. I'm changing the way I write s. Sorry. Plus H prime and rho. OK? So there is a bit
of mathematics to be performed to analyze this. There are three terms
that I will label a, b, and c, which are
the three Poisson brackets that I
have to evaluate. So the contribution
that I call a is the integral
over coordinates s plus 1 to N, of the Poisson
bracket of Hs with rho. Now, the Poisson bracket
is given up here. It is a sum that involves N
terms over all N particles. But since, if I'm
evaluating it for Hs and Hs will only give nonzero
derivatives with respect to the coordinates
that are present in it, this sum of N terms
actually becomes simply a sum of small N terms. So I will get a sum over
N running from 1 to s. These are the only terms. And I will get the things that
I have for d rho by dpN-- sorry, rho by dqN, dHs by dpN minus
d rho by dpN, dHs by dqN. OK? Did I make a mistake? AUDIENCE: Isn't it the Poisson
bracket of rho H, not H rho? PROFESSOR: Rho. Oh, yes. So I have to put
a minus sign here. Right. Good. Thank you. OK. Now, note that these
derivatives and operations that involve Hs involve
coordinates that are not appearing in the
integration process. So I can take
these entities that do not depend on
variables that are part of the integration outside the
integration, which means that I can then write the
result as being an exchange of the order
of the Poisson bracket and the integration. So I would essentially have the
integration only appear here, which is the same
thing as Hs and rho s. This should be rho s. This is the definition of rho s,
which is the same thing as rho. OK. What does it mean physically? So it's actually much
easier to tell you what it means physically
than to do the math. So what we saw was
happening was that if I have a Hamiltonian that
describes N particles, then for that Hamiltonian,
the corresponding density satisfies this
Liouville equation. D rho by dt is HN rho. And this was a consequence of
this divergence-less character of the flow that we
have in this space, that the equations that
we write down over here for p dot and q
dot in terms of H had this character that
the divergence was 0. OK? Now, this is true if I have
any number of particles. So if I focus simply
on s of the particles, and they are governed
by this Hamiltonian, and I don't have anything
else in the universe, as far as this
Hamiltonian is concerned, I should have the analog
of a Liouville equation. So the term that I have obtained
over there from this first term is simply stating
that d rho s by dt, if there was no other
interaction with anybody else, would simply satisfy the
corresponding Liouville equation for s particles. And because of that, we
also expect and anticipate-- and I'll show that
mathematically-- that the next term
in the series, that is the Poisson bracket
of N minus s and rho, should be 0, because as
far as this s particles that I'm focusing on
and how they evolve, they really don't care about
what all the other particles are doing if they
are not [INAUDIBLE]. So anything interesting
should ultimately come from this third term. But let's actually go and do the
calculation for the second term to show that this anticipation
that the answer should be 0 does hold up and why. So for the second term, I need
to calculate a similar Poisson bracket, except that
this second Poisson bracket involves H of N minus s. And H of N minus s, when
I put in the full sum, will only get contribution from
terms that start from s plus 1. So the same way that
that started from N, this contribution starts
from s plus 1 to N. And actually, I can just write
the whole thing as above, d rho by d qj dotted by d HN minus
s by dpj, plus d rho by-- no, this is the rho. d rho by d pj
dotted by dHN minus s by dqj. So now I have a totally
different situation from the previous case,
because the previous case, the derivatives were over
things I was not integrating. I could take outside
the integral. Now all of the
derivatives involve things that I'm integrating over. Now, when that happens, then
you do integration by parts. So what you do is
you take rho outside and let the derivative
act on everything else. OK? So what do we end up with if
we do integration by parts? I will get surface terms. Surface terms are essentially
rho-evaluated when the coordinates are at
infinity or at the edge of your space, where rho is 0. So there is no surface term. There is an overall
change in sign, so I will get a product
i running from s plus 1 to N, dVi. Now the rho comes outside. And the derivative acts on
everything that is left. So the first term will
give me a second derivative of HN minus s, with respect
to p, with respect to q. And the second term will be
essentially the opposite way of doing the derivative. And these two are,
of course, the same. And the answer is 0. OK? So we do expect that
the evolution of all the other particles
should not affect the subset that
we are looking at. And that's worn out also. So the only thing
that potentially will be relevant and exciting
is the last term, number c. So let's take a look at that. So here, I have to do an
integration over variables that I am not interested. And then I need now,
however, to do a full Poisson bracket of a whole
bunch of terms, because now the terms
that I'm looking at have coordinates from both sets. So I have to be a
little bit careful. So let me just make sure that
I follow the notes that I have here and don't
make mistakes. OK. So this H prime
involves two sums. So I will write the first
sum, N running from 1 to s. And then I have the second
sum, j running from s plus 1 to N. What do I need? I need the-- OK. Let's do it the following way. So what I have to do
for the Poisson bracket is a sum that involves
all coordinates. So let's just write
this whole expression. But first, for
coordinates 1 through s. So I have a sum N
running from 1 to s. And then I will
write the term that corresponds to
coordinates s plus 1 to m. For the first set of
coordinates, what do I have? I have d rho by dqn. And then I have
d H prime by dpn. So I didn't write
H prime explicitly. I'm just breaking
the sum over here. And then I have sum j
running from s plus 1 to N, d rho by dqj times
d H prime by dpj. And again, my H prime
is this entity over here that [? copies ?]
coordinates from both sets. OK. First thing is I claim that one
of these two sets of sums is 0. You tell me which. AUDIENCE: The first. PROFESSOR: Why first? AUDIENCE: Because H prime is
independent of p [? dot. ?] PROFESSOR: That's true. OK. That's very good. And then it sort of brings
up a very important question, which is, I forgot to
write two more terms. [LAUGHTER] Running to s of d rho by dpn,
d H prime by dqn minus sum j s plus 1 to N of d rho by
dpj dot dH prime by dqj. So indeed, both answers
now were correct. Somebody said that
the first term is a 0, because H prime
does not depend on pn. And somebody over here
said that this term is 0. And maybe they can explain why. AUDIENCE: [INAUDIBLE]. PROFESSOR: Same
reason as up here. That is, I can do
integration by parts. AUDIENCE: [INAUDIBLE]. PROFESSOR: To get rid of this
term plus this term together. So it's actually
by itself is not 0. But if I do
integration by parts, I will have-- actually,
even by itself, it is 0, because I would have d
by dqj, d by pj, H prime. And H prime, you cannot
have a double derivative pj. So each one of them,
actually, by itself is 0. But in general, they would
also cancel each other through their single process. Yes. AUDIENCE: Do you have the sign
of [? dH ?] of [? H prime? ?] PROFESSOR: Did I have
the sign incorrect? Yes. Because for some
reason or other, I keep reading from here, which
is rho and H. So let's do this. OK? AUDIENCE: Excuse me. PROFESSOR: Yes? AUDIENCE: [INAUDIBLE]. PROFESSOR: OK. Yes, it is different. Yes. So what I said, if I had
a more general Hamiltonian that also depended
on momentum, then this term would, by itself
not 0, but would cancel, be the corresponding
term from here. But the way that I
have for H prime, indeed, each term by
itself would be 0. OK. So hopefully-- then
what do we have? So actually, let's keep the
sign correct and do this, because I need this right
sign for the one term that is preserved. So what does that say? It is a sum, n
running from 1 to s. OK? I have d H prime by dqn. And I have this integration. I have the integration i running
from s plus 1 to N dV of i. I have d rho by
dpn dot producted with d H prime by dqN. d H prime by dqn I can
calculate from here, is a sum over terms j
running from s plus 1 to N of V of qn minus qj. All right. AUDIENCE: Question. PROFESSOR: Yes. AUDIENCE: Why aren't you
differentiating [? me ?] if you're
differentiating H prime? PROFESSOR: d by dqj. All right? AUDIENCE: Where is the qn? PROFESSOR: d by dqn. Thank you. Right. Because always, pn of
qn would go together. Thank you. OK. All right. So we have to slog
through these derivations. And then I'll give you
the physical meaning. So I can rearrange this. Let's see what's happening here. I have here a sum
over particles that are not listed on
the left-hand side. So when I wrote
this d rho by dt, I had listed coordinates
going from p1 through qs that were s coordinates
that were listed. If you like, you can think
of them as s particles. Now, this sum involves
the remaining particles. What is this? Up to a sign. This is the force that
is exerted by particle j from the list of
particles that I'm not interested on one of the
particles on the list that I am interested. OK? Now, I expect that at the end
of the day, all of the particles that I am not interested
I can treat equivalently, like everything
that we had before, like how I got this factor
of N or N minus 1 over there. I expect that all of these will
give me the same result, which is proportional to the number
of these particles, which is N minus s. OK? And then I can focus on just
one of the terms in this sum. Let's say the term
that corresponds to j, being s plus 1. Now, having done that,
I have to be careful. I can do separately
the integration over the volume of this one
coordinate that I'm keeping, V of s plus 1. And what do I have here? I have the force that
exerted on particle number N by the particle that
is labelled s plus 1. And this force is dot
producted with a gradient along the momentum in direction
of particle N of its density. Actually, this is the
density of all particles. This is the rho that
corresponds to the joint. But I had here s
plus 1 integrations. One of them I wrote
down explicitly. All the others I do over here. Of the density. So basically, I change the
order of the derivative and the integrations over
the variables not involved in the remainder. And the reason I
did that, of course, is that then this
is my rho s plus 1. OK? So what we have at
the end of the day is that if I take the time
variation of an s particle density, I will get one
term that I expected, which is if those s particles
were interacting only with themselves, I would
write the Liouville equation that would be
appropriate to them. But because of the
collisions that I can have with particles
that are not over here, suddenly, the momenta that
I'm looking at could change. And because of that, I
have a correction term here that really describes
the collisions. It says here that
these s particles were following the
trajectory that was governed by the
Hamiltonian that was peculiar to the s particles. But suddenly, one of them had
a collision with somebody else. So which one of them? Well, any one of them. So I could get a contribution
from any one of the s particles that is listed over here, having
a collision with somebody else. How do I describe
the effect of that? I have to do an integration over
where this new particle that I am colliding with could be. I have to specify both where the
particle is that I am colliding with, as well as its momentum. So that's this. Then I need to know the
force that this particle is exerting on me. So that's the V of qs plus
1 minus qN divided by dqN. This is the force that is
exerted by this particle that I don't see on myself. Then I have to multiply this,
or a dot product of this, with d by dpN, because what
happens in the process, because of this force, the
momentum of the N particle is changing. The variation of
that is captured through looking at
the density that has all of these particles in
addition to this new particle that I am colliding with. But, of course, I am not really
interested in the coordinate of this new particle,
so I integrate over it. There are N minus
s such particles. So I really have to put
a factor of N minus s here for all of
potential collisions. And so that's the equation. Again, it is more common,
rather than to write the equation for rho, to
write the equation for f. And the f's and the
rhos where simply related by these factors of
N factorial over N minus 1 s factorial. And the outcome of that
is that the equation for f simply does not have this
additional factor of N minus s, because that disappears
in the ratio of rho of s plus 1 and rho s. And it becomes a sum over
N running from 1 to s. Integral over coordinates and
momenta of a particle s plus 1. The force exerted
by particle s plus 1 on particle N used to vary the
momentum of the N particle. And the whole thing would depend
on the density that includes, in addition to the s
particles that I had before, the new particle that
I am colliding with. OK? So there is a set
of equations that relates the different densities
and how they evolve in time. The evolution of f1, which is
the thing that I am interested, will have, on the
right-hand side, something that involves f2. The evolution of
f2 will involve f3. And this whole thing is
called a BBGKY hierarchy, after people whose names
I have in the notes. [SOFT LAUGHTER] But again, what have
we learned beyond what we had in the original case? And originally,
we had an equation that was governing a function
in 6N-dimensional space, which we really don't need. So we tried our
best to avoid that. We said that all of the physics
is in one particle, maybe two particle densities. Let's calculate the evolution
of one-particle and two-particle densities. Maybe they will tell us about
this non-equilibrium situation that we set up. But we see that the time
evolution of the first particle density requires
two-particle density. Two-particle density requires
three-particle densities. So we sort of made this
ladder, which ultimately will [? terminate ?] at the
Nth particle densities. And so we have not
really gained much. So we have to now look at these
equations a little bit more and try to inject more physics. So let's write down the
first two terms explicitly. So what I will do is I will
take this Poisson bracket of H and f, to the left-hand
side, and use the Hamiltonian that we have over here
to write the terms. So the equation that we
have for f1-- and I'm going to write it as a
whole bunch of derivatives acting on f1. f1 is a function of p1 q1 t. And essentially, what
Liouville's theorem says is that as you move
along the trajectory, the total derivative is
0, because the expansion of the flows is incompressible. So what does that mean? It means that d by dt, which
is this argument, plus q1 dot times d by dq1 plus
p1 dot by d by dp1. So here, I should write
p1 dot and q1 dot. In the absence of
everything else is 0. Then, of course, for q1
dot, we use the momentum that we would get out of this. q1 dot is momentum
divided by mass. So that's the velocity. And p1 dot, changing
momentum, is the force, is minus dH by dq1. So this is minus d of this
one particle potential divided by dq1 dotted by [? h ?] p1. So if you were asked to think
about one particle in a box, then you know its
equation of motion. If you have many,
many realizations of that particle in a box,
you can construct a density. Each one of the elements
of the trajectory, you know how they
evolve according to [? Newton's ?] equation. And you can see how the
density would evolve. It would evolve
according to this. I would have said, equal to 0. But I can't set it to 0 if I'm
really thinking about a gas, because my particle
can come and collide with a second
particle in the gas. The second particle
can be anywhere. And what it will do is that
it will exert a force, which would be like this,
on particle one. And this force will
change the momentum. So my variation of
the momentum will not come only from the
external force, but also from the
force that is coming from some other
particle in the medium. So that's where
this d by dp really gets not only the
external force but also the force from somebody else. But then I need to know
where this other particle is, given that I know where
my first particle is. So I have to include here a
two-particle density which depends on p1 as
well as q2 at time t. OK. Fine. Now you say, OK, let's write
down-- I need to know f2. Let's write down
the equation for f2. So I will write it more rapidly. I have p1 over m, d by dq1. I have p2 over m, d by dq2. I will have dU by dq1, d by dp1. I will have dU by dq2, d by dp2. I will have also a term from
the collision between q1 and q2. And once it will change the
momentum of the first particle, but it will change the
momentum of the second particle in the opposite direction. So I will put the
two of them together. So this is all of the terms that
I would get from H2, Poisson bracket with density acting
on the two-particle density. And the answer would be
0 if the two particles were the only thing in the box. But there's also
other particles. So there can be
interactions and collisions with a third particle. And for that, I
would need to know, let's actually try
to simplify notation. This is the force that is
exerted from two to one. So I will have here the
force from three to one dotted by d by dp1. Right. And the force that is
exerted from three to two dotted by d by dp2 acting on
a three-particle density that involves everything up to three. And now let's write
the third one. [LAUGHTER] So that, I will leave
to next lecture. But anyway, so this
is the structure. Now, this is the
point at which we would like to inject some
physics into the problem. So what we are going to do is to
estimate the various terms that are appearing in this
equation to see whether there is some approximation
that we can make to make the equations more
treatable and handle-able. All right? So let's try to look
at the case of a gas-- let's say the gas in this room. A typical thing
that is happening in the particles of
the gas in this room is that they are zipping around. Their velocity is of
the order-- again, just order of magnitude,
hundreds of meters per second. OK? So we are going to, again,
be very sort of limited in what we are
trying to describe. There is this experiment. Gas expands into a chamber. In room temperature, typical
velocities are of this order. Now we are going to
use that to estimate the magnitude of the
various terms that are appearing in this equation. Now, the whole thing
about this equation is variation with time. So the entity that
we are looking at in all of these
brackets is this d by dt, which means that the various
terms in this differential equation, apart
from d by dt, have to have dimensions
of inverse time. So we are going to
try to characterize what those inverse times are. So what are the typical
magnitudes of various terms? If I look at the
first equation, I said what that first
equation describes. That first equation describes
for you a particle in a box. We've forgotten about
everything else. So if I have a
particle in a box, what is the characteristic time? It has to be set by the
size of the box, given that I am moving
with that velocity. So there is a
timescale that I would call extrinsic in the
sense that it is not really a property of the gas. It will be different if
I make the box bigger. There's a timescale
over which I would go from one side of the box
to another side of the box. So this is kind of
a timescale that is related to the term
that knows something about the box, which
is dU by dq, d by dp. I would say that if I were to
assign some typical magnitude to this type of
term, I would say that it is related to having
to traverse a distance that is of the order of
the size of the box, given the velocity
that I have specified. This is an inverse timescale. Right? And let's sort of
imagine that I have an-- and I will call this
timescale 1 over tau U, because it is sort of
determined by my external U. Let's say we have
a typical size that is of the order of millimeter. If I make it larger,
it will be larger. So actually, let's say we
have 10 to the minus 3 meters. Actually, let's make it bigger. Let's make it of the order
of 10 to the minus 1 meter. Kind of reasonable-sized box. Then you would say
that this 1 over tau c is of the order of 10 to the 2
divided by 10 to the minus 1, which is of the order of 1,000. Basically, it
takes a millisecond to traverse a box
that is a fraction of a meter with
these velocities. OK? You say fine. That is the kind
of timescale that I have in the first equation
that I have in my hierarchy. And that kind of
term is certainly also present in the second
equation for the hierarchy. If I have two particles,
maybe these two particles are orbiting each
other, et cetera. Still, their center of
mass would move, typically, with this velocity. And it would take
this amount of time to go across the
size of the box. But there is another
timescale inside there that I would call intrinsic,
which involves dV/dq, d by dp. Now, if I was to see what
the characteristic magnitude of this term is, it would
have to be V divided by a lens scale that
characterizes the potential. And the potential, let's
say, is of the order of atomic size or
molecular size. Let's call it d. So this is an atomic
size-- or molecular size. More correctly, really, it's
the range of the interaction that you have between particles. And typical values of
these [? numbers ?] are of the order of 10
angstroms, or angstroms, or whatever. Let's say 10 to the
minus 10 meters. Sorry. The first one I would
like to call tau U. This second time,
that I will call 1 over tau c for collisions,
is going to be the ratio of 10 to the 2 to 10 to the minus 10. It's of the order of 10
to the 12 in both seconds. OK? So you can see that this term is
much, much larger in magnitude than the term that was
governing the first equation. OK? And roughly, what you
expect in a situation such as this-- let's imagine,
rather than shooting particles from here, you are
shooting bullets. And then the bullets
would come and basically have some kind of
trajectory, et cetera. The characteristic time
for a single one of them would be basically
something that is related to the
size of the box. How long does it take a bullet
to go over the size of the box? But if two of these
bullets happen to come together and
collide, then there's a very short period
of time over which they would go in
different directions. And the momenta
would get displaced from what they were before. And that timescale is
of the order of this. But in the situation
that I set up, this particular
time is too rapid. There is another
more important time, which is, how long
do I have to wait for two of these particles,
or two of these bullets, to come and hit each other? So it's not the duration of the
collision that is irrelevant, but how long it would be for
me to find another particle to collide with. And actually, that is what
is governed by the terms that I have on the other side. Because the terms on the
other side, what they say is I have to find another particle. So if I look at the terms that
I have on the right-hand side and try to construct
a characteristic time out of them, I have to compare
the probability that I will have, or the density that
I will have for s plus 1, integrated over some
volume, over which the force between these
particles is non-zero. And then in order to
construct a timescale for it, I know that the d by dt on
the left-hand side acts on fs. On the right-hand
side, I have fs plus 1. So again, just if I want
to construct dimensionally, it's a ratio that
involves s plus 1 to s. OK? So I have to do an
additional integration over a volume in phase space
over which two particles can have substantial interactions. Because that's where
this [? dv ?] by dq would be non-zero, provided
that there is a density for s plus 1 particle
compared to s particles. If you think about
it, that means that I have to look
at the typical density or particles times d cubed for
these additional operations multiplied by this collision
time that I had before. OK? And this whole thing I will
call 1 over tau collision. And another way of getting
the same result is as follows. This is typically how you get
collision times by pictorially. You say that I have something
that can interact over some characteristic size d. It moves in space
with velocity v so that if I wait a time
that I will call tau, within that time,
I'm essentially sweeping a volume of space that
has volume d squared v tau. So my cross section, if my
dimension is d, is d squared. I sweep in the other
direction by [? aman ?] d tau. And how many particles
will I encounter? Well, if I know
the density, which is the number of
particles per unit volume, I have to multiply this by n. So how far do I have
to go until I hit 1? I'll call that tau x. Then my formula
for tau x would be 1 over nvd squared, which
is exactly what I have here. 1 over tau x is
nd squared v. OK? So in order to compare
the terms that I have on the right-hand
side with the terms on the left-hand
side, I notice that I need to know something
about nd cubed. So nd cubed tells you
if I have a particle here and this particle has
a range of interactions that I call d, how many
other particles fall within that range
of interaction? Now, for the gas in this room,
the range of the interaction is of the order of the
size of the molecule. It is very small. And the distance between
molecules in this room is far apart. And indeed, you can estimate
that for gas, nd cubed has to be of the order
of 10 to the minus 4. And how do I know that? Because if I were to take all of
the gas particles in this room and put them together so
that they are touching, then I would have a liquid. And the density of,
say, typical liquid is of the order of 10,000 times
larger than the density of air. So basically, it has to
be a number of this order. OK? So fine. Let's look at our equations. So what I find is that
in this equation for f2, on the left-hand
side, I have a term that has magnitude that is
very large-- 1 over tau c. Whereas the term on
the right-hand side, in terms of magnitude,
is something like this. And in fact, this will be
true for every equation in the hierarchy. So maybe if I am in the
limit where nd cubed is much, much less than 1-- such as
the gas in this room, which is called the dilute limit-- I
can ignore the right-hand side. I can set the
right-hand side to 0. OK? Now, I can't do that
for the first equation, because the first
equation is really the only equation in the
hierarchy that does not have the collision term
on the left-hand side. Right? And so for the first equation,
I really need to keep that. And actually, it goes
back to all of the story that we've had over here. Remember that we said
this rho equilibrium has to be a function of H
and conserved quantities. Suppose I go to my
Hamiltonian and I ignore all of these
interactions, which is what I would
have done if I just look at the first term over
here and set the collision term on the
right-hand side to 0. What would happen then? Then clearly, for each
one of the particles, I have-- let's say it's energy
is going to be conserved. Maybe the magnitude
of its momentum is going to be conserved in
the appropriate geometry. And so there will
be a huge number of individual
conserved quantities that I would have
to put over there. Indeed, if I sort of
go back to the picture that I was drawing over
here, if I ignore collisions between the particles, then
the bullets that I send will always be following
a trajectory such as this forever, because momentum
will be conserved. You will always-- I mean,
except up to reflection. And say the
magnitude of velocity would be always
following the same thing. OK? So however, if there is
a collision between two of the particles-- so the
particles that come in here, they have different velocities,
they will hit each other. The moment they hit
each other, they go off different directions. And after a certain
number of hits, then I will lose all
of the regularity of what I had in the beginning. And so essentially,
this second term on the right-hand
with the collisions is the thing that
is necessary for me to ensure that my gas does come
to equilibrium in the sense that their momenta get
distributed and reversed. I really need to
keep track of that. And also, you can see that
the timescales for which this kind of
equilibration takes place has to do with this
collision time. But as far as this term
is concerned, for the gas or for that system of bullets,
it doesn't really matter. Because for this term
to have been important, it would have been
necessary for something interesting to physically occur
should three particles come together simultaneously. And if I complete
the-- say that never in the history of this
system three particles will come together, they do
come together in reality. It's not that big a difference. It's only a factor of
10 to the 4 difference between the right-hand side
and the left-hand side. But still, even if
they didn't, there was nothing about
equilibration of the gas that would be missed by this. So it's a perfectly reasonable
approximation and assumption, therefore, for us
to drop this term. And we'll see that although
that is physically motivated, it actually doesn't resolve
this question of irreversibility yet, because that's also
potentially a system that you could set up. You just eliminate all of
the three-body interactions from the problem. Still, you could have
a very reversible set of conditions and
deterministic process that you could reverse in time. But still, it's
sort of allows us to have something that
is more manageable, which is what we will
be looking at next. Before I go to what
is next, I also mentioned that there is
one other limit where one can do things,
which is when, within the range of
interaction of one particle, there are many other particles. So you are in the dense limit,
nd cubed greater than 1. This does not happen for a
liquid, because for a liquid, the range does not allow many
particles to come within it. But it happens
for a plasma where you have long-range
Coulomb interaction. And within the range
of Coulomb interaction, you could have many
other interactions. And so that limit you will
explore in the problem set leads to a different description
of approach to equilibrium. It's called the Vlasov equation. What we are going to proceed
with now will lead to something else, which is called--
in the dilute limit, it will get the
Boltzmann equation. OK? So let's see what we have. So currently, we
achieved something. We want to describe
properties of a few particles in the system-- densities
that describe only, say, one particle by itself
if one was not enough. But I can terminate
the equations. And with one-particle density
and two-particle density, I should have an
appropriate description of how the system evolves. Let's think about
it one more time. So what is happening here? There is the
one-particle description that tells you how the
density for one particle, or an ensemble, the probability
for one particle, its position and momentum evolves. But it requires
knowledge of what would happen with a
second particle present. But the equations that we
have for the density that involves two particles is
simply a description of things that you would do if you had
deterministic trajectories. There is nothing else
on the right-hand side. So basically, all you
need to do is in order to determine this, is to have
full knowledge of what happens if two particles come
together, collide together, go away, all kinds of things. So if you have those
trajectories for two particles, you can, in principle,
build this density. It's still not an easy task. But in principle,
one could do that. And so this is the
description of f2. And we expect f2 to
describe processes in which, over a very
rapid timescale, say, momenta gets shifted from one
direction to another direction. But then there is something
about the overall behavior that should follow, more or less, f1. Again, what do I mean? What I mean is the following,
that if I open this box, there is what you would observe. The density would kind
of gush through here. And so you can
have a description for how the density, let's
say in coordinate space, would be evolving as
a function of time. If I ask how does the
two-particle prescription evolve, well, the two-particle
prescription, part of it is what's the
probability that I have a particle here and
a particle there? And if the two particles, if
the separations are far apart, you would be justified to
say that that is roughly the product of the probabilities
that I have something here and something there. When you become very close
to each other, however, over the range of
interactions and collisions, that will have to be modified,
because at those descriptions from here, you
would have to worry about the collisions, and the
exchange of momenta, et cetera. So in that sense, part of f2
is simply following f1 slowly. And part of f1 captures all of
the collisions that you have. In fact, that part of f2 that
captures in the collisions, we would like to simplify
as much as possible. And that's the next
task that we do. So what I need to do
is to somehow express the f2 that appears
in the first equation while solving this equation
that is the second one. I will write the answer
that we will eventually deal with and
explain it next time. So the ultimate result would
be that the left-hand side, we will have the terms
that we have currently. f1. On the right-hand
side, what we find is that I need to
integrate over all momenta of a second particle
and something that is like a distance to the
target-- one term that is the flux of
incoming particles. And then we would have f2
after collision minus f2 before collision. And this is really
the Boltzmann equation after one more approximation,
where we replace f2 with f1. f1. But what all of that
means symbolically and what it is we'll have
to explain next time.

Algorithms

The following
content is provided under a Creative
Commons license. Your support will help MIT
OpenCourseWare continue to offer high quality
educational resources for free. To make a donation or
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. MEHRAN KARDAR: You
decide at first look at the our simple system,
which was the ideal gas. And imagine that we
have this gas contained in a box of volume V that
contains N particles. And it's completely isolated
from the rest of the universe. So you can know the amount
of energy that it has. So the macroscopic
description of the system consists of these three
numbers, E, V, and N. And this was the characteristics
of a microcanonical ensemble in which there was no
exchange of heat or work. And therefore,
energy was conserved. And our task was somehow to
characterize the probability to find the system
in some microstate. Now if you have N
particles in the system, there is at the microscopic
level, some microstate that consists of a description of
all of the positions of momenta. And there is
Hamiltonian that governs how that microstate evolves
as a function of time. And for the case of ideal gas,
the particles don't interact. So the Hamiltonian can be
written as the sum of n terms that describe essentially
the energy of the end particle composed of
its kinetic energy. And the term that is really
just confining the particle is in this box. And so the volume of
the box is contained. Let's say that in
this potential, that it's zero inside the
box and infinity outside. And we said, OK, so given that
I know what the energy, volume, number of particles
are, what's the chance that I will find the system
in some particular microstate? And the answer was
that, obviously, you will have to put zero if the
particles are outside box. Or if the energy, which is
really just the kinetic energy, does not much the energy that
we know is in the system, we sum over i of P
i squared over 2m is not equal to the
energy of the system. Otherwise, we say that if
the microstate corresponds to exactly the right
amount of energy, then I have no reason to exclude it. And just like saying
that the dice can have six possible
faces, you would assign all of those possible
phases equal probability. I will give all of
the microstates that don't conflict
with the conditions that I have set out
the same probability. I will call that
probability 1 over some overall constant omega. And so this is one otherwise. So then the next
statement is well what is this number omega
that you have put it? And how do we determine it? Well, we know that this
P is a probability so that if I were to integrate
over the entirety of the face space of this probability,
the answer should be 1. So that means this
omega, which is a function of these parameters
that I set out from the outside to describe the
microstate, should be obtained by integrating over
all q and p of this collection of 1s and 0s that
I have out here. So I think this box of
1s and 0s, put it here, and I integrate. So what do I get? Well, the integration
over the q's is easy. The places that I get 1 are
when the q's are inside the box. So each one of them will
give me a factor of V. And there are N of them. So I would get V to the N. The integrations over
momenta essentially have to do with seeing whether
or not the condition sum over i Pi squared over 2m
equals to E is satisfied or not. So this I can write
also as sum over i P i squared equals to 2mE,
which I can write as R squared. And essentially, in
this momentum space, I have to make sure that the
sum of the components of all of the momenta squared
add up to this R. squared, which as we discussed
last time, is the surface of hypershpere
in 3N dimensions of radius R, which is square root of 2mE. So I have to integrate
over all of these momenta. And most of the
time I will get 0, except when I heat the
surface of this sphere. There's kind of a little
bit of singularity here because you have a
probability that there's 0, except at the very sharp
interval, and then 0 again. So it's kind of like a delta
function, which is maybe a little bit hard to deal with. So sometimes we
will generalize this by adding a little
bit be delta E here. So let's say that the energy
does not have to be exactly E, but E minus plus
a little bit, so that when we look at
this surface in three n dimensional space--
let's say this was two dimensional space--
rather than having to deal with an
exact boundary, we have kind of smoothed that
out into an interval that has some kind of a thickness
R, that presumably is related to this delta E
that I put up there. Turns out it doesn't
really make any difference. The reason it doesn't
make any difference I will tell you shortly. But now when I'm integrating
over all of these P's-- so there's P. There's
another P. This could be P1. This could be P2. And there are
different components. I then get 0, except
when I hit this interval around the surface
of this hypersphere. So what do I get as a result
of the integration over this 3N dimensional space? I will get the volume
of this element, which is composed of the
surface area, which has some kind of a solid
angle in 3N dimensions. The radius raised to
the power of dimension minus 1, because it's a surface. And then if I want to
really include a delta R to make it into a volume, this
would be the appropriate volume of this interval
in momentum space. Yes. AUDIENCE: Just to
clarify, you're asserting that
there's no potential inside the [INAUDIBLE] that
comes from the hard walls. MEHRAN KARDAR: Correct. We can elaborate
on that later on. But for the description of the
ideal gas without potential, like in the box, I have
said that potential to be just 0 infinity. OK? OK, so fine. So this is the description. There was one
thing that I needed to tell you, which is the d,
dimension, of solid angle, which is 2pi to the
d over 2 divided by d over 2 minus 1
factorial So again, in two dimensions, such as the
picture that I drew over here, the circumference of a
circle would be 2 pi r. So this s sub 2--
right there'd be 2 pi-- and you can show that
it is 2 pi divided by 0 factorial, which is 1. In three dimensions it should
give you 4 pi r squared. Kind of looks
strange because you get 2 pi to the 3/2 divided
by 1/2 half factorial. But the 1/2 factorial
is in fact root 2 over pi-- root pi over 2. And so this will work out fine. Again, the definition of
this factorial in general is through the gamma
function and an integral that we saw already. And factorial is the integral
0 to infinity, dx, x to the n into the minus x. Now, the thing is that
this is a quantity that for large values of dimension
grows exponentially v E d. So what I claim is that if I
take the log of this surface area and take the limit that
d is much larger than 1, the quantity that I
will get-- well, let's take the log of this. I will get log of 2. I will get d over 2 log
pi and minus the log of this large factorial. And the log of the factorial
I will use Sterling's formula. I will ignore in
that large limit the difference between d
over 2 and d over 2 minus 1. Or actually I guess
at the beginning I may even write it d over 2
minus 1 log of d over 2 minus 1 plus d over 2 minus 1. Now if I'm in this
limit of large d again, I can ignore the 1s. And I can ignore the log 2
with respect to this d over 2. And so the answer in this
limit is in fact proportional to d over 2. And I have the log. I have pi. I have this d over 2 that
will carry in the denominator. And then this d over 2 times 1
I can write as d over 2 log e. And so this is
the answer we get. So you can see that the
answer is exponentially large if I were to again write s of d. S of d grows like
an exponential in d. OK, so what do I
conclude from that? I conclude that s over Kd--
and we said that the entropy, we can regard as the entropy of
this probability distribution. So that's going to give
me the log of this omega. And log off this omega,
we'll get a factor from this v to the n. So I will get N
log V. I will get a factor from log of S of 3N. I figured out what that
log was in the limit of large dimensions. So I essentially have 3N over 2
because my d is now roughly 3N. It's in fact exactly 3N, sorry. I have the log of 2 pi e. For d I have 3N. And then I actually
have also from here a 3N log R, which I can write
as 3N over 2 log of R squared. And my R squared is 2mE. The figure I have here. And then you say, OK,
we added this delta R. But now you can
see that I can also ignore this delta R, because
everything else that I have in this expression is
something that grows radially with N. What's the worse
that I can do for delta R? I could make delta R even as big
as the entirety of this volume. And then the
typical volume would be of the order of the energy--
sorry, the typical value of R would be like the
square root of energy. So here I would
have to put this log of the square root
of the energy. And log of a square roots
of an extensive quantity is much less than the
extensive quantity. I can ignore it. And actually this reminds me,
that some 35 years ago when I was taking this course,
from Professor Felix Villiers, he said that he
had gone to lunch. And he had gotten to this
very beautiful, large orange. And he was excited. And he opened up the
orange, and it was all skin. And there was just a
little bit in the middle. He was saying it is like this. It's all in the surface. So if Professor Villiers had
an orange in 3N dimension, he would have exponentially
hard time extracting an orange. So this is our formula for
the entropy of this gas. Essentially the
extensive parts, n log v and something that
depends on n log E. And that's really all
we need to figure out all of the thermodynamic
properties, because we said that
we can construct-- that's in thermodynamics--
dE is TdS minus PdV plus YdN in the case of a gas. And so we can
rearrange that to dS be dE over T plus P over
T dV minus Y over T dN. And the first thing
that we see is by taking the derivative of S
with respect to the quantities that we have
established, E, V, and N, we should be able to read
off appropriate quantities. And in particular,
let's say 1 over T would be dS by dE
of constant v and n. S will be proportional to kB. And then they dependents
of this object on E only appears on
this log E. Except that there's a factor
of 3N over 2 out front. And the derivative of log E
with respect to E is 1 over E. So I can certainly
immediately rearrange this and to get that
the energy is 3/2 N k T in this system of ideal point
particles in three dimensions. And then the
pressure, P over T, is the S by dV at constant e and n. And it's again, kB. The only dependence on V
is through this N log V. So I will get a factor of N over
V, which I can rearrange to PV is N kB T by the ideal gas law. And in principle,
the next step would be to calculate the
chemical potential. But we will leave that
for the time being for reasons that
will become apparent. Now, one thing to note is that
what you have postulated here, right at the beginning, is
much, much, more information than what we extracted here
about thermodynamic properties. It's a statement about a
joint probability distribution in this six N
dimensional face space. So it has huge amount
of information. Just to show you part
of it, let's take a note at the following. What it is a probability
as a function of all coordinates and momenta
across your system. But let me ask a
specific question. I can ask what's the probability
that some particular particle-- say particle number
one-- has a momentum P1. It's the only
question that I care to ask about this huge
amount of degrees of freedom that are encoded in P of mu. And so what do I do if I
don't really care about all of the other degrees of freedom
is I will integrate them. So I don't really care where
particle number one is located. I didn't ask where
it is in the box. I don't really care where part
because numbers two through N are located or which
momenta they have. So I integrate over
all of those things of the full joint
probability, which depends on the entirety
of the face space. Fine, you say, OK. This joint probability actually
has a very simple form. It is 1 over this
omega E, V, and N, multiplying either 1 or 0. So I have to integrate
over all of these q1's, all of these qi P i. Of 1 over omega or 0
over omega, this delta like function that
we put in a box up there-- so this is
this delta function that says that the particular
should be inside the box. And the sum of
the momenta should be on the surface
of this hypershpere. Now, let's do
these integrations. Let's do it here. I may need space. The integration over
Q1 is very simple. It will give me a factor of V. I have this omega E, V,
N, in the denominator. And I claim that the numerator
is simply the following. Omega E minus P1 squared
over 2m V N minus 1. Why? Because what I need to do over
here in terms of integrations is pretty much what I would
have to integrate over here that gave rise to that surface
and all of those factors with one exception. First of all, I integrated
over one particle already, so the coordinate momenta here
that I'm integrating pertains to the remaining N minus 1. Hence, the omega
pertains to N minus 1. It's still in the
same box of volume V. So V, the other
argument, is the same. But the energy is changed. Why? Because I told you
how much momentum I want the first
particle to carry. So given the knowledge
that I'm looking at the probability of the first
particle having momentum P1, then I know that the
remainder of the energy should be shared among the
momenta of all the remaining N minus 1 particles. So I have already calculated
these omegas up here. All I need to do is to
substitute them over here. And I will get this probability. So first of all, let's check
that the volume part cancels. I have one factor a volume here. Each of my omegas is in fact
proportional to V to the N. So the denominator
has V to the N. The numerator has a
V to the N minus 1. And all of the V's
would cancel out. So the interesting thing really
comes from these solid angle and radius parts. The solid angle is a ratio of--
let's write the denominator. It's easier. It is 2 pi to the 3N over
2 divided by 3N over 2 minus one factorial. The numerator would be 2
pi to the 3 N and minus 1 over 2 divided by 3 N minus
1 over 2 minus 1 factorial. And then I have these
ratio of the radii. In the denominator I have 2mE to
the power of 3 N minus 1 over 2 minus 1. So minus 3N-- it is
3N minus 1 over 2. Same thing that we have
been calculating so far. And in the numerator it is 2m
E minus P1 squared over 2m. So I will factor out the E.
I have 1 minus P1 squared over 2m E. The whole
thing raised to something that is 3 N minus
1 minus 1 over 2. Now, the most
important part of this is the fact that the dependence
on P1 appears as follows. I have this factor of 1
minus P1 squared over 2m E. That's the one place
that P1, the momentum of the particle that
I'm interested appears. And it raised to
the huge problem, which is of the
order of 3N over 2. It is likely less. But it really doesn't
make any difference whether I write 3N over 2, 3N
over minus 1 over 2, et cetera. Really, ultimately,
what I will have is 1 minus the very small
number, because presumably the energy of one
part they can is less than the energy of the
entirety of the particle. So this is something that is
order of 1 out of N raised to something that is
order of N. So that's where an exponentiation
will come into play. And then there's a whole
bunch of other factors that if I don't make any
mistake I can try to write down. There is the 2s
certainly cancel when I look at the factors of pi. The denominator with
respect to the numerator has an additional
factor of pi to the 3/2. In fact, I will
have a whole bunch of things that are raised
to the powers of 3/2. I also have this
2mE that compared to the 2mE that
comes out front has an additional factor of 3/2. So let's put all
of them together. 2 pi mE raised to
the power of 3/2. And then I have the ratio
of these factorials. And again, the factorial that
I have in the denominator has one and a half times
more or 3/2 times more than what is in the numerator. Roughly it is something
like the ratio of 3 N over 2
factorial divided by 3 N minus 1 over 2 factorial. And I claim that, say,
N factorial compared to N minus 1 factorial is
larger by a factor of N. If I go between N factorial
N minus 2 factorial is a factor that is
roughly N squared. Now this does not shift either
by 1 or by 2, but by 1 and 1/2. And if you go through
Sterling formula, et cetera, you can convince yourself that
this is roughly 3 N over 2 to the power of 1 and 1/2-- 3/2. And so once you do all of your
arrangements, what do you get? 1 minus a small quantity
raised to a huge power, that's the definition
of the exponential. So I get exponential of
minus P1 squared over 2m. And the factor that
multiplies it is E. And then I have 3N over 2. And again, if I have
not made any mistake and I'm careful with all of
the other factors that remain, I have here 2 pi m
E. And this E also gets multiplied by the
inverse of 3N over 2. So I will have this
replaced by 2E over 3N. So statement number
one, this assignment of probabilities according
to just throwing the dice and saying that everything
that has the same right energy is equally likely is
equivalent to looking at one of the
particles and stating that the momentum of that part
again is Gaussian distributed. Secondly, you can check that
this combination, 2E divided by 3N is the same thing as kT. So essentially
this you could also if you want to
replace 1 over kT. And you would get the
more familiar kind of Maxwell type of
distribution for the momentum of a single particle
in an ideal gas. And again, since
everything that we did was consistent with the
laws of probability, if we did not mix up the
orders of N, et cetera, the answer should be
properly normalized. And indeed, you can check that
this is the three dimensional normalization that you
would require for this gas. So the statement of saying
that everything is allowed is equally likely is a
huge statement in space of possible configurations. On the one hand, it gives
you macroscopic information. On the other hand, it
retains a huge amount of microscopic information. The parts of it
that are relevant, you can try to
extract this here. OK? So those were the successes. Question is why didn't
I calculate for you this u over T? It is because this
expression as we wrote down has a glaring problem
with it, which in order to make it explicit, we will
look at mixing entropies. So the idea is
this is as follows. Let's imagine that we
start with two gases. Initially, I have N1 particles
of one type in volume 1. And I have N2 particles of
another type in volume 2. And for simplicity I will
assume that both of them are of the same temperature. So this is my initial state. And then I remove the partition. And I come up with
this situation where the particles are mixed. So the particles of type
1 could be either way. Particles of type 2
could be in either place. And let's say I have a box
of some toxic gas here. And I remove the lid. And it will get
mixed in the room. It's certainly an
irreversible situation where is an increase of
entropy i associated with that. And we can calculate
that increase of entropy, because we know what the
expression for entropy is. So what we have to do is to
compare the entropy initially. So this is the initial entropy. And I calculate
everything in units of kB so I don't have to write
kB all over the place. For particle number
one, what do I have? I have N1 log V1. And then I have a contribution,
which is 3 N 1 over 2. But I notice that whatever
appears here is really only a function of
E over N. E over N is really only a
function of temperature. So this is something that I can
call a sigma of T over here. And the contribution of box 2
is N2 log V plus 3N 2 over 2. This-- huh-- let's
say that they are-- we ignore the
difference in masses. You could potentially have
here sigma 1, sigma 2. It really doesn't
make any difference. The final state,
what do we have? Essentially, the one
thing that changed is that the N1 particles now
are occupying the box of volume V. So if call V to the
V1 plus V2, what we have is that we have N1 log
of V plus N2 log of V. My claim is that all
of these other factors really stay the same. Because essentially what is
happening in these expressions are various ratios of E over N.
And by stating that initially I had the things at
the same temperature, what I had effectively
stated was that E1 over N1 is the same thing as E2 over N2. I guess in the ideal
gas case this E over N is the same thing as 3/2 kT. But if I have a
ratio such as this, that is also the same as E1
plus E2 divided by N1 plus N2. This is a simple manipulation
of fractions that I can make. And E1 plus E2 over N1 plus N2,
by the same kinds of arguments, would give me the
final temperature. So what I have to compute is
that the final temperature is the same thing as
the initial temperature. Essentially, in this
mixing of the ideal gases, temperature does not change. So basically, these
factors of sigma are the same before and after. And so when we calculate
the increase in entropy, Sf minus Si, really
the contribution that you care about comes
from these volume factors. And really the statement is
that in one particle currently are occupying a
volume of size V, whereas previously
they were in V1. And similarly for
the N2 particles. And if you have more of these
particles, more of these boxes, you could see how the
general expression for the mixing entropy goes. And so that's fine. V is certainly
greater than V1 or V2. Each of these logs gives
you a positive contribution. There's an increase in
entropy as we expect. Now, there is the following
difficulty however. What if the gases are
identical-- are the same? We definitely have to do this
if I take a box of methane here and I open it, we all know
that something has happened. There is an irreversible
process that has occured. But if the box--
I have essentially taken the air in this
room, put it in this box, whether I open the lid
or not open the lid, it doesn't make any difference. There is no
additional work that I have to do in order to
close or open the lid. Is no there no increase of
entropy one way or the other. Whereas if I look
at this expression, this expression only
depends on the final volume and the initial
volumes, and says that there should an
increase in entropy when we know that
there shouldn't be. And of course, the
resolution for that is something like this. That if I look at my two boxes--
and I said maybe one of them is a box that contains methane. Let's call it A.
And the other is the room that contains the air. Now this situation
where all of the methane is in the box and the oxygen
freely floating in the room is certainly different
from a configuration where I exchange these two
and the methane is here and the oxygen
went into the box. They're different
configurations. You can definitely
tell them apart. Whereas if I do the same
thing, but the box and outside contain the same entity, and
the same entity is, let's say, oxygen, then how can you tell
apart these two configurations? And so the meaning of-- yes. AUDIENCE: Are you
thinking quantum mechanically or classically. Classically we can
tell them apart, right? MEHRAN KARDAR:
This is currently I am making a
macroscopic statement. Now when I get to the
distinction of microstates we have to-- so I was
very careful in saying whether or not you
could tell apart whether it is methane or oxygen. So this was a very
macroscopic statement as to whether or not you
can distinguish this circumstance versus
that circumstance. So as far as our senses of
this macroscopic process is concerned, these two cases
have to be treated differently. Now, what we have calculated
here for these factors are some volume of phase space. And where in the
evening you might say that following
this procedure you counted these as
two distinct cases. In this case, these
were two distinct cases. But here, you can't
really tell them apart. So if you can't
tell them apart, you shouldn't call them
two distinct cases. You have over counted phase
space by a factor of two here. And here, I just looked
at two particles. If I have N
particles, I have over counted the phase space
of identical particles by all possible permutations of
n objects, it is n factorial. So there is an over
counting of phase space or configurations of
N identical particles by a factor of N factorial. I.e., when we said that particle
number one can be anywhere in the box, particle
number two can be anywhere in the box, all the way
to particle number n, well, in fact, I can't
tell which each is which. If I can't tell which
particle is which, I have to divide by the number
of permutations and factors. Now, as somebody was
asking the question, as you were asking the
question, classically, if I write a
computer program that looks at the trajectories
of N particles in the gas in this room,
classically, your computer would always know the particle
that started over here after many collisions
or whatever is the particle that
ended up somewhere else. So if you ask the
computer, the computer can certainly distinguish
these classical trajectories. And then it is kind of
strange to say that, well, I have to divide by N factorial
because all of these are identical. Again, classically
these particles are following
specific trajectories. And you know where in
phase space they are. Whereas quantum mechanically,
you can't tell that apart. So quantum mechanically, as
we will describe later, rather than classical statistical
mechanics-- when we do quantum statistical
mechanics-- if you have identical particles,
you have to write down of a wave function that
is either symmetric or anti-symmetric under the
exchange of particles. And when we do eventually
the calculations for these factors of
1 over N factorial will emerge very naturally. So I think different people
have different perspectives. My own perspective is
that this factor really is due to the quantum
origin of identity. And classically, you have to
sort of fudge it and put it over there. But some people say that really
it's a matter of measurements. And if you can't really tell
A and B sufficiently apart, then you don't know. I always go back
to the computer. And say, well, the
computer can tell. But it's kind of
immaterial at this stage. It's obvious that for all
practical purposes for things that are identical you have
to divide by this factor. So what happens if you
divide by that factor? So I have changed all
of my calculations now. So when I do the log of--
previously I had V to the N. And it gave me N log
V. Now, I have log of V to the N divided by N factorial. So I will get my Sterling's
approximation additional factor of minus N log N plus N, which
I can sort of absorb here in this fashion. Now you say, well, having
done that, you have to first of all show me that you
fixed the case of this change in entropy for
identical particles, but also you should show me
that the previous case where we know there has to be an
increase in entropy just because of the gas being
different that that is not changed because of this
modification that you make. So let's check that. So for distinct gases, what
would be the generalization of this form Sf minus
Si divided by kV? Well, what happens here? In the case of the final object,
I have to divide N1 log of V. But that V really becomes
V divided by N1, because in the volume of size
V, I have N1 oxygen that I can't tell apart. So I divide by the N1
factorial for the oxygens. And then I have N2 methanes
that I can't tell apart in that volume, so I divide by
essentially N2 factorial that goes over there. The initial change is over
here I would have N1 log of V1 over N1. And here I would have
had N2 log of V2 over N2. So every one of these
expressions that was previously log V, and I had four
of them, gets changed. But they get change precisely in
a manner that this N1 log of N1 here cancels this N1
log of and N1 here. This N2 log of N2 here cancels
this N2 log of N2 here. So the delta S that I get
is precisely the same thing as I had before. I will get N1 log of V over
V1 plus N2 log of V over V2. So this division,
because the oxygens were identical to
themselves and methanes were identical to
themselves, does not change the mixing entropy
of oxygen and nitrogen. But let's say that both
gases are the same. They're both oxygen. Then what happens? Now, in the final
state, I have a box. It has a N1 plus N2 particles
that are all oxygen. I can't tell them apart. So the contribution
from the phase space would be N1 plus N2 log of the
volume divided by N1 plus N2 factorial. That ultimately will give me
a factor of N1 plus N2 here. The initial entropy
is exactly the one that I calculated before. For the line above,
I have N1 log of V1 over N1 minus N2
log of V2 over N2. Now certainly, I still expect
to see some mixing entropy if I have a box of oxygen
that is at very low pressure and is very dilute,
and I open it into this room, which is
at much higher pressure and is much more dense. So really, the
case where I don't expect to see any
change in entropy is when the two boxes
have the same density. And hence, when I mix
them, I would also have exactly the same density. And you can see that, therefore,
all of these factors that are in the log are of the
inverse of the same density. And there's N1 plus N2
of them that's positive. And N1 plus N2 of
them that is negative. So the answer in
this case, as long as I try to mix identical
particles of the same density, if I include this
correction to the phase space of identical
particles, the answer will be [? 0. ?] Yes? AUDIENCE: Question,
[INAUDIBLE] in terms of the revolution
of the [INAUDIBLE] there is no
transition [INAUDIBLE] so that your temporary, and say
like, oxygen and nitrogen can catch a molecule, put it
in a [? aspertometer. ?] and have different isotopes. You can take like closed
isotopes of oxygen and still tell them apart. But this is like
their continuous way of choosing a pair
of gases which would be arbitrarily
closed in atomic mass. MEHRAN KARDAR: So,
as I said, there are alternative explanations
that I've heard. And that's precisely
one of them. And my counter is that
what we are putting here is the volume of phase space. And to me that has a
very specific meaning. That is there's a set of
coordinates and momenta that are moving according
to Hamiltonian trajectories. And in principle, there
is a computer nature that is following
these trajectories, or I can actually put
them on the computer. And then no matter
how long I run and they're identical
oxygen molecules, I start with number one
here, numbers two here. The computer will say that this
is the trajectory of number one and this is the
trajectory of numbers two. So unless I change my
definition of phase space and how I am calculating
things, I run into this paradox. So what you're saying
is forget about that. It's just can tell isotopes
apart or something like that. And I'm saying that that's fine. That's perspective,
but it has nothing to do with phase space counting. OK? Fine, now, why didn't
I calculate this? It was also for the
same reason, because we expect to have quantities
that are extensive and quantities
that are intensive. And therefore, if I
were to, for example, calculate this object,
that it should be something that is intensive. Now the problem is that if I
take a derivative with respect N, I have log V.
And log V is clearly something that does not
grow proportionately to size but grows proportionately
to size logarithmically. So if I make volume
twice as big, I will get an additional factor
of log 2 here contribution to the chemical potential. And that does not make sense. But when I do this identity,
then this V becomes V over N. And then everything
becomes nicely intensive. So if I allowed now to
replace this V over N, then I can calculate V over T
as dS by dN at constant E and V. And so then
essentially I will get to drop the factor of log N that
comes in front, so I will get kT log of V over
N. And then I would have 3/2 log of something, which
I can put together as 4 pi N E over 3N raised
to the 3/2 power. And you can see that
there were these E's from Sterling's
approximation up there that got dropped here, because
you can also take derivative with respect to the
N's that are inside. And you can check that the
function of derivatives with respects to the
N's that are inside is precisely to get
rid of those factors. OK? Now, there is still
one other thing that is not wrong, but
kind of like jarring about the expressions
that they've had so far in that right
from the beginning, I said that you can certainly
calculate entropies out of probabilities as minus
log of P average if you like. But it makes sense
only if you're dealing with discrete
variables, because when you're dealing with continual
variables and you have a probability density. And the probability
density depends on the units of measurement. And if you were to change
measurement from meters to centimeters or
something else, then there will be changes
in the probability densities, which would then modify the
various factors over here. And that's really also
is reflected ultimately in the fact that these
combinations of terms that I have written
here have dimensions. And it is kind
of, again, jarring to have expressions inside the
logarithm or in the exponential that our not dimensionless. So it would be good
if we had some way of making all of
these dimensionless. And you say, well,
really the origin of it is all the way back
here, when I was calculating volumes
in phase space. And volumes in phase
space have dimensions. And that dimensions of
pq raised to the 3N power really survives all
the way down here. So I can say, OK, I
choose some quantity as a reference that has the
right dimensions of the product of p and q, which is an action. And I divide all
of my measurements by that reference unit, so
that, for example, here I have 3N factors of this. Or let's say each
one of them is 3. I divide by some quantity
that has units of action . And then I will be set. So basically, the units of this
h is the product of p and q. Now, at this point
we have no way of choosing some h as
opposed to another h. And so by adding that factor,
we can make things look nicer. But then things are undefined
after this factor of h. When we do quantum
mechanics, another thing that quantum mechanics
does is to provide us with precisely [? age ?] of
Planck's constant as a measure of these kinds of integrations. So when we eventually
you go to calculate, say, the ideal gas or any
other mechanic system that involves p and q
in quantum mechanics, then the phase space
becomes discretized. You would have-- The
appropriate description would have energies that are
discretized corresponding to various other discretization
that are eventually the equivalent to dividing
by this Planck's constant. Ultimately, I will
have additionally a factor of h squared
appearing here. And it will make everything
nicely that much [? less. ?] None of these other quantities
that I mentioned calculated would be affected by this. So essentially,
what I'm saying is that you are going to
use a measure for phase space of identical particles. Previously we had a product,
d cubed Pi, d cubed Qi. This is what we were
integrating and requiring that this integration
will give us [INAUDIBLE]. Now, we will change this to
divide by this N factorial, if the particles are identical. And we divide by h
to the 3N because of the number of pairs of
pq that appear in this. The justification to come when
we ultimately do quantum study. Any questions? So I said that this
prescription when we look at a system
at complete isolation, and therefore, specify
fully its energy is the microcanonical
ensemble, as opposed to the canonical
ensemble, whereas the set of microscopic parameters
that you identified with your system, you replace
the energy with temperature. So in general,
let's say there will be some bunch of
displacements, x, that give you the work content to the system. Just like we fixed over there
the volume and the number of particles, let's say that
all of the work parameters, such as x
microscopically, we will fix in this canonical ensemble. So however, the
ensemble is one in which the energy is not specified. And so how do I
imagine that I can maintain a system
at temperature T? Well, if this room is at
some particular temperature, I assume that smaller objects
that I put in this room will come to the
same temperature. So the general prescription
for beginning something other than temperature T is
to put it in contact with something that
is much bigger. So let's call this
to be a reservoir. And we put our system, which
we assume to be smaller, in contact with it. And we allow it to exchange
heat with the reservoir. Now I did this way of
managing the system to come to a
temperature T, which is the characteristic
of a big reservoir. Imagine that you have a lake. And you put your gas or
something else inside the lake. And it will equilibrate to
the temperature of the lake. I will assume that
the two of them, the system and the
reservoir, just for the purpose of my being
able to do some computation, are isolated from the
rest of the universe, so that the system plus
reservoir is microcanonical. And the sum total of their
energies is sum E total. So now this system still
is something like a gas. It's a has a huge number of
potential degrees of freedom. And these potential number
of degrees of freedom can be captured through the
microstate of the system, u sub s. And similarly, the water
particle in the lake have their own state. An there's some
microstate that describes the positions and the momenta
of all of the particles that are in the lake. Yes? AUDIENCE: When you're writing
the set of particles used to describe it, why
don't you write N? Since it said the number
of particles in the system is not fixed. MEHRAN KARDAR: Yes, so I
did want to [INAUDIBLE] but in principle I could add N.
I wanted to be kind of general. If you like X,
[? it ?] is allowed to include chemical
work type of an X. So what do I know? I know that there is also if
I want to describe microstates and their revolution,
I need to specify that there's a
Hamiltonian that governs the evolution of
these microstates. And presumably
there's a Hamiltonian that describes the evolution
of the reservoir microstate. And so presumably the
allowed microstates are ones in which
E total is made up of the energy of the system plus
the energy of the reservoir. So because the whole thing
is the microcanonical, I can assign a probability,
a joint probability, to finding some particular
mu s, mu r combination, just like we were
doing over there. You would say that
essentially this is a combination
of these 1s and 0s. So it is 0 if H of--
again, for simplicity I drop the s on the
system-- H of mu s plus H reservoir of mu reservoir
is not equal to E total. And it is 1 over some omega
of reservoir in the system otherwise. So this is just again,
throwing the dice, saying that it has so many
possible configurations, given that I know what
the total energy is. All the ones that are consistent
with that are allowed. Which is to say, I don't
really care about the lake. All I care is about to
the states of my gas, and say, OK, no problem, if
I have the joint probability distribution just
like I did over here, I get rid of all of
the degrees of freedom that I'm not interested in. So if I'm interested only
in the states of the system, I sum over or integrate
over-- so this would be a sum. This would be an
integration, whatever-- of the joint probability
distribution. Now actually follow the
steps that I had over here when we were looking at the
momentum of a gas particle. I say that what
I have over here, this probability see
is this 1 over omega R, S. This is a function
that is either 1 or 0. And then I have so sum
over all configurations of the reservoir. But given that I said what
the microstate of the system is, then I know that the
reservoir has to take energy in total minus the amount
the microstates has taken. And I'm summing over all
of the microstates that are consistent with
the requirement that the energy in
the reservoir is E total minus H of microstate. So what that is that
the omega that I have for the reservoir--
and I don't know what it is, but whatever it is,
evaluated at the total energy minus the energy that is
taken outside the microstate. So again, exactly the reason
why this became E minus Pi squared over 2. This becomes E total
minus H of mu S. Except that I don't know either
what this is or what this is. Actually, I don't really
even care about this because all of the H dependents
on microstate dependents is in the numerator. So I write that as
proportional to exponential. And the log of omega
is the entropy. So I have the entropy of the
[INAUDIBLE] in units of kB, evaluated at the argument that
this E total minus H of mu S. So my statement is
that when I look at the entropy of the
reservoir as a function of E total minus the energy
that is taken out by the system, my
construction I assume that I'm putting a small
volume of gas in contact with a huge lake. So this total energy is
overwhelmingly larger than the amount of energy
that the system can occupy. So I can make a Taylor
expansion of this quantity and say that this is S R of E
total minus the derivative of S with respect to its energy. So the derivative
of the S reservoir with respect to the energy
of the reservoir times H of the microstate and
presumably higher order terms that are negligible. Now the next thing that is
important about the reservoir is you have this huge lake. Let's say it's exactly at some
temperature of 30 degrees. And you take some small
amount of energy from it to put in the system. The temperature of the
lake should not change. So that's the definition
of the reservoir. It's a system that is so big
that for the range of energies that we are considering,
this S by dE is 1 over the temperature that
characterizes the reservoir. So just like here,
but eventually the answer that we
got was something like the energy of the
particle divided by kT. Once I exponentiate, I
find that the probability to find the system in some
microstate is proportional to E to the minus of the energy of
that microstate divided by kT. And of course, there's
a bunch of other things that I have to eventually
put into a normalization that will cause. So in the canonical
prescription you sort of replace this throwing of
the dice and saying that everything is equivalent
to saying that well, each microstates can have
some particular energy. And the probabilities
are partitioned according to the Boltzmann weights
of these energies. And clearly this quantity,
Z, the normalization is obtained by integrating
over the entire space of microstates, or
summing over them if they are discrete of this
factor of E to the minus beta H of mu S. And we'll use
this notation beta 1 over kT sometimes for simplicity. Now, the thing is that
thermodynamically, we said that you can choose
any set of parameters, as long as they are
independent, to describe the macroscopic equilibrium
state of the system. So what we did in the
microcanonical ensemble is we specified a number of
things, such as energy. And we derived the other
things, such as temperature. So here, in the
canonical ensemble, we have stated what the
temperature of the system is. Well, then what happened? On one hand, maybe we have
to worry because energy is constantly being
exchanged with the reservoir. And so the energy of the system
does not have a specific value. There's a probability for it. So probability of
system having energy epsilon-- it doesn't
have a fixed energy. There is a probability
that it should have energy. And this probability,
let's say we indicate with P epsilon given that
we know what temperature is. Well, on one hand we
have this factor of E to the minus epsilon over kT. That comes from the [INAUDIBLE]. But there isn't a single
state that has that energy. There's a whole
bunch of other states of the system that
have that energy. So as I scan the
microstates, there will be a huge number of them,
omega of epsilon in number, that have this right energy. And so that's the
probability of the energy. And I can write this
as E to the minus 1 over kT that I've called beta. I have epsilon. And then the log of
omega that I take in the numerator
is S divided by Kb. I can take that Kb here and
write this as T S of epsilon. And so this kind of
should remind you of something like a free energy. But it tells you is
that this probability to have some particular energy
is some kind of [? a form. ?] Now note that again for
something like a gas or whatever, we expect typical
values of both the energy and entropy to be
quantities that are proportional to
the size of the system. As the size of the system
becomes exponentially large, we would expect that
this probability would be one of those things
that has portions that let's say are exponentially
larger than any other portion. There will be a factor of
E to the minus N something that will really peak up,
let's say, the extremum and make the extremum
overwhelmingly more likely than other places. Let's try to quantify
that a little bit better. Once we have a
probability, we can also start calculating averages. So let's define what the
average energy of the system is. The average energy
of the system is obtained by summing
over all microstates. The energy of that
microstate, the probability of that microstate, which
is E to the minus beta H microstate divided by the
partition function, which is the sum-- OK,
the normalization, which we will call the
partition function, which is the sum over all
of these microstates. Now this is something
that we've already seen. If I look at this expression
in the denominator that we call Z and has a
name, which is the partition function, then it's
certainly a function of beta. If I take a derivative of Z with
respect to beta, what happens I'll bring down a
factor of H over here. So the numerator up to
a sine is the derivative of Z with respect to beta. And the denominator is 1 over
Z. And so this is none other than minus the log Z
with respect to beta. So OK, fine, so the mean
value of this probability is given by some
expression such as this. Well, you can see that if I
were to repeat this process and rather than
taking one derivative, I will take n derivatives
and then divide by 1 over Z. Each time I do that, I will
bring down a factor of H. So this is going to give me
the average of H to the N. The end moment of this
probability distribution of energy is obtainable
by this procedure. So now you recognize, oh, I've
seen things like such as this. So clearly this
partition function is something that
generates the moments by taking subsequent
derivatives. I can generate different
moments of this distribution. But then there
was something else that maybe this
should remind you, which is that if there's
a quantity that generates moments, then its log
generates cumulants. So you would say,
OK, the nth cumulant should be obtainable up to this
factor of minus 1 to the n, as the nth derivative with
respect to the beta of logs. And it's very easy
to check that indeed if I were to take
two derivatives, I will get the
expectation value of H squared minus the average
of H squared, et cetera. But the point is that
clearly this to log Z is, again, something
that is extensive. Another way of getting
the normalization-- I guess I forgot to put
this 1 over Z here. So now it is a perfectly
normalized object. So another way to get
z would be to look at the normalization
of the probability. I could integrate over epsilon
this factor of E to the minus beta epsilon minus
T S of epsilon. And that would give
me Z. Now, again the quantities that appear
in the exponent, energy-- entropy, their
difference, free energy-- are quantities
that are extensive. So this Z is going
to be dominated again by where this peak is. And therefore, log of Z
will be proportional to log of what we have over here. And it be an extensive quantity. So ultimately, my statement
is that this log of Z is something that is order of N. So we are, again,
kind of reminiscent of the central limit theorem. In a situation where we have
a probability distribution, at large N, in which
all of the cumulants are proportional to N. The
mean is proportional to N. The variance is proportional
to N. All of the cumulants are proportional
to N, which means that essentially the extent of
the fluctuations that you have over here are going to go the
order of the square root of N. So the bridge, the thing
that again allows us, while we have in principle in
the expression that we have said, a variable
energy for the system. In fact, in the limit of
things becoming extensive, I know where that energy
is, up to fluctuations or up to uncertainty
that is only of the order of
square root of N. And so the relative uncertainty
will vanish as the N goes to infinity, limit
is approached. So although again
we have something that is in principle
probabilistic, again, in the
thermodynamic sense we can identify uniquely an
energy for our system as, let's say, the mean value
or the most likely value. They're all the same thing
of the order of 1 over N. And again, to be more
precise, the variance is clearly the second derivative
of log Z. 1 derivative of a log Z is going to give
me the energy. So this is going to
be d by d beta up to a minus sign of the
energy or the expectation value of the Hamiltonian,
which we identified as the energy of the system. The derivative with
respect to beta, I can write as kB T squared. The derivative of energy
with respect to T, everything here we are doing
that conditions of no work. So the variance is
in fact kB T squared, the heat capacity of the system. So the extent that these
fluctuations squared is kB T squared times the
heat capacity the system. OK, so next time
what we will do is we will calculate the
results for the ideal gas. First thing, the
canonical ensemble to show that we get exactly
the same macroscopic and microscopic descriptions. And then we look
at other ensembles. And that will
conclude the segment that we have on
statistical mechanics of non-interacting systems.

CS

The following content is
provided under a Creative Commons license. Your support will help
MIT OpenCourseWare continue to offer high quality
educational resources for free. To make a donation or
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. ERIK DEMAINE: All right,
let's get started. Today, we have another cool
graph algorithm or problem. Actually, we'll
have two algorithms. The problem is called
minimum spanning tree. You can probably guess from the
title what it's trying to do. We'll see two
algorithms for doing it. Both of them are in the category
of greedy algorithms, which is something we've
seen a couple of times already in 6.046,
starting with lecture 1. This is the definition of
greedy algorithm from lecture 1, roughly. The idea is to always make
greedy choices, meaning the choice is locally best. For right now, it seems
like a good thing to do, but maybe in the future
it will screw you over. And if you have a
correct greedy algorithm, you prove that it
won't screw you over. So it's sort of like
Cookie Monster here, always locally seems like a good
idea to eat another cookie, but maybe it'll bite
you in the future. So today we will embrace
our inner Cookie Monster and eat as many-- eat
the largest cookie first, would be the standard
algorithm for Cookie Monster. I don't know if you learned
that in Sesame Street, but-- all right. So what's the problem? Minimum spanning tree. Can anyone tell
me what a tree is? Formally, not the outside thing. In graph land. Acyclic graph, close. Connected acyclic graph, good. That's important. This is 604.2 stuff. OK, so how about
a spanning tree? Sorry? AUDIENCE: It contains
all the vertices. ERIK DEMAINE: It contains
all the vertices. Yeah. So let me go over here. Spanning means it contains all
the vertices, so implicit here, I guess, is subtree or subgraph. You're given a graph. You want a spanning
tree of that graph. It's going to be a tree
that lives inside the graph. So we're going to take
some of the edges of G, make a tree out of them, make
a connected acyclic graph. And that tree should hit
all the vertices in G. So this is going to be a subset
of the edges, or subgraph. Those edges should form a tree. And, I'll say, hit
all vertices of G. OK, if I just said they
should form a tree, then I could say, well,
I'll take no edges, and here's a tree
with one vertex. That's not very interesting. You want a vertex--
you want, basically, the vertex set of the tree
to be the same as the vertex set of the graph. That's the spanning property. But you still want
it to be a tree, so you want it to be connected
and you want it to be acyclic. Now if G is disconnected,
this is impossible. And for that, you could
define a spanning forest to be like a maximal
thing like this, but we'll focus on the case
here as G is connected. That's the interesting case. And so we can get
a spanning tree. All right? So what is this minimum
spanning tree problem? Minimum spanning tree. We're given a weighted
graph, just like last time, with shortest paths. We have an edge weight function
W giving me a real number, say, for every edge. And we want to find a spanning
tree of minimum total weight. So I'm going to define
the weight of a tree T to be the sum over
all edges in T, because I'm viewing a spanning
tree as a set of edges, of the weight of that edge. OK, so pretty much
what you would expect. Minimum weight spanning tree. It's a relatively
simple problem, but it's not so easy
to find an algorithm. You need to prove a lot to
make sure that you really find the right tree. I guess the really
naive algorithm here would be to try
all spanning trees, compute the weight
of each spanning tree and return the minimum. That sounds reasonable. That's correct. But it's bad, because-- n to
the fourth, that would be nice. It's larger than that. Maybe not so obvious, but
it can be exponential. Here's a graph where the
number of spanning trees is exponential. This is a complete
bipartite graph with two vertices on one side
and n vertices on the other, and so you can-- let's
say we put these two edges into the spanning tree. And now, for each
of these vertices, we can choose whether it
connects to the left vertex or the right vertix. It can only do one, but it could
do either one independently. So maybe this guy
chooses the left one, this one chooses the right one. This one chooses the
left one, and so on. If I have n vertices down here,
I have 2 to the n different spanning trees. So there can be an
exponential number. So that algorithm
is not so good. Exponential bad. Polynomial good. So today, we're going to
get a polynomial algorithm. In fact, we will get an almost
linear time algorithm as fast as Dijkstra's algorithm. But we can't use
Dijkstra's algorithm, there's no shortest paths here. Plus, one of the algorithms will
actually look pretty similar. Two lectures ago, the
dynamic programming lecture, we saw an example where
we tried to do greedy, and it gave the
wrong answer, and so we fell back on
dynamic programming. Today, we're going to try
to do dynamic programming, it's going to fail, and we're
going to fall back on greedy. It's like the reverse. But the way it's
going to fail is we're going to get
exponential time initially, and then greedy will let
us get polynomial time. This is actually a bit unusual. I would say more typically,
dynamic programming can solve anything,
but, you know, with n to the seventh running
time, something slow. And then you apply
greedy, and you get down to like n or n log
n running time. So that's more common. But today, we're going
to go from exponential down to polynomial. And that's pretty nice. Cool. So let me tell you a little
bit about greedy algorithm theory, so to speak. This is from the textbook. If your problem can be
solved by greedy algorithm, usually you can
prove two properties about that algorithm. One of them is called
optimal substructure. And the other is called
the greedy choice property. Optimal substructure
should be familiar idea because it's essentially
an encapsulation of dynamic programming. Greedy algorithms
are, in some sense, a special form of
dynamic programming. So this is saying
something like, if you can solve
subproblems optimally, smaller subproblems, or
whatever, then you can solve your original problem. And this may happen
recursively, whatever. That's essentially
what makes a recurrence work for dynamic programming. And with dynamic programming,
for this to be possible, we need to guess some
feature of the solution. For example, in
minimum spanning tree, maybe you guess one of the edges
that's in the right answer. And then, once you do
that, you can reduce it to some other subproblems. And if you can solve
those subproblems, you combine them and
get an optimal solution to your original thing. So this is a familiar property. I don't usually think of it this
way for dynamic programming, but that is essentially what
we're doing via guessing. But with greedy algorithms,
we're not going to guess. We're just going to be greedy. Eat the largest cookie. And so that's the
greedy choice property. This says that eating
the largest cookie is actually a good thing to do. If we keep making
locally optimal choices, will end up with a
globally optimal solution. No tummy ache. This is something you wouldn't
expect to be true in general, but it's going to be true
for minimum spanning tree. And it's true for a
handful of other problems. You'll see a bunch more
in recitation tomorrow. This is sort of general
theory, but I'm actually going to have a theorem like
this for minimum spanning tree and a theorem like this
for minimum spanning tree. This is the prototype, but most
of today is all about minimum spanning tree. And for minimum spanning
tree, neither of these is very obvious. So I'm just going to
show you these theorems. They're fairly easy to prove,
in fact, but finding them is probably the tricky part. Actually, I guess optimal
substructure is probably the least intuitive or the
least obvious greedy choice. You're probably
already thinking, what are good greedy choices? Minimum weight edge seems
like a good starting point, which we will get to. But there's even
a stronger version of that, which we will prove. And first, optimal substructure. So here, I'm going to think
like a dynamic program. Let's suppose that we know an
edge that's in our solution. Suppose we know
an edge that lives in a minimum spanning tree. We could guess that. We're not going
to, but we could. Either way, let's just
suppose than an edge e-- I should mention, I
guess I didn't say, this graph is undirected. A minimum spanning tree
doesn't quite make sense with directed graphs. There are other
versions of the problem but here, the graph
is undirected. So probably, I should write
this as a unordered set, u, v. And there are possibly
many minimum spanning trees. There could be many solutions
with the same weight. For example, if all of
these edges have weight 1, all of these trees
are actually minimum. If all the edges have weight
1, every spanning tree is minimum, because every
spanning tree has exactly n minus 1 edges. But let's suppose we
know an edge that's guaranteed to be in some minimum
spanning tree, at least one. What I would like to do is take
this, so let me draw a picture. I have a graph. We've identified some edge
in the graph, e, that lives in some minimum spanning tree. I'm going to draw some kind
of tree structure here. OK. The wiggly lines are the tree. There are some
other edges in here, which I don't want to draw too
many of them because it's ugly. Those are other
edges in the graph. Who knows where they are? They could be all
sorts of things. OK? But I've highlighted the
graph in a particular way. Because the minimum
spanning tree is a tree, if I delete
e from the tree, then I get two components. Every edge I remove--
I'm minimally connected. So if I delete an edge, I
disconnect into two parts, so I've drawn that as the left
circle and the right circle. It's just a general way
to think about a tree. Now there are other unused
edges in this picture, who knows where they live? OK? What I would like to do is
somehow simplify this graph and get a smaller problem,
say a graph with fewer edges. Any suggestions
on how to do that? I don't actually know where
all these white edges are, but what I'd like to do is--
I'm supposing I know where e is, and that's an edge in my
minimum spanning tree. So how could I get rid of it? Yeah. AUDIENCE: Find
the minimum weight spanning tree of the two edges. ERIK DEMAINE: I'd like
to divide and conquer. Maybe find the
minimum weight over here, minimum weight over here. Of course, I don't know
which nodes are in what side. So that's a little trickier. But what do I do but E itself? Let's start with that. Yeah. AUDIENCE: You remove it? ERIK DEMAINE: You
could remove it. That's a good idea. Doesn't work, but worth
a Frisbee nonetheless. If I delete this
edge, one problem is maybe none of these red
edges exist and then my graph is disconnected. Well, maybe that's
actually a good case. That probably would
be a good case. Then I know how to
divide and conquer. I just look at the
connected components. In general, if I
delete the edge, and I have these red
edges, then I maybe find a minimum spanning
tree on what remains. Maybe I'll end up including
one of these edges. Maybe this edge ends up
in the spanning tree, and then I can't put E in. So it's a little awkward. Yeah? AUDIENCE: Can you merge
the two nodes into one? Merge the two nodes into one. Yes. Purple Frisbee. Impressive. This is what we call
contracting the edge. It just means merge
the endpoints. Merge u and v. So I will draw
a new version of the graph. So this was u and v before. You've got to put
the label inside. And now we have a new
vertex here, which is uv. Or you can think it
as the set u, v. We won't really need to
keep track of names. And whatever edges
you had over here, you're going to have over here. OK? Just collapse u and v.
The edge e disappears. And one other thing can happen. Let me-- go over here. We could end up with duplicate
edges by this process. So for example, suppose
we have u and v, and they have a common neighbor. Might have many common
neighbors, who knows. Add some other edges,
uncommon neighbors. When I merge, I'd
like to just have a single edge to that vertex and
a single edge to that vertex. And what I'm going
to do is, if I have some weights on these
edges, let's say a and b, and c and d, I'm just
going to take the minimum. Because what I'm about to do is
compute a minimum spanning tree in this graph. And if I take the minimum
spanning tree here, and I had multiple
edges-- one weight a, one weight b-- do you think I
would choose the larger weight edge? It does-- they're
exactly the same edge, but one is higher weight. There's no point in keeping
the higher weight one, so I'm just going to throw
away the higher weight one. Take them in. So this is a particular form
of edge contraction and graphs. And I claim it's a good
thing to do, in the sense that if I can find a
minimum spanning tree in this new graph--
this is usually called a G slash e, slash
instead of negative, to remove e. I'm contracting e. So this is G slash e. This is G. If I can find a
minimum spanning tree in G slash e, I claim I can find
one in the original graph G just by adding the edge e. So I'm going to
say if G prime is a minimum spanning
tree, of G slash e, then T prime union e is a
minimum spanning tree of G. So overall, you
can think of this as a recurrence in
a dynamic program, and let me write down
that dynamic program. It won't be very
good dynamic program, but it's a starting point. This is conceptually
what we want to do. We're trying to guess
an edge e that's in a minimum spanning tree. Then we're going to
contract that edge. Then we're going to recurse,
find the minimum spanning tree on what remains, and then we
find the minimum spanning tree. Then we want to
decontract the edge, put it back, put the
graph back the way it was. And then add e to the
minimum spanning tree. And what this lemma
tells us, is that this is a correct algorithm. If you're lucky-- and we're
going to force luckiness by trying all edges-- but if
we start with an edge that is guaranteed to be in some
minimum spanning tree, call it a safe edge, and
we contract, and we find a minimum spanning
tree on what remains, then we can put e
back in at the end, and we'll get a minimum spanning
tree of the original graph. So this gives us correctness
of this algorithm. Now, this algorithm's
bad, again, from a complexity standpoint. The running time is
going to be exponential. The number of sub problems we
might have to consider here is all subsets of edges. There's no particular way--
because at every step, we're guessing an arbitrary
edge in the graph, there's no structure. Like, we can't say well,
it's the first k edges, or some substring of edges. It's just going to be
some subset of edges. There's exponentially
many subsets, 2 to the e, so this is exponential. But we're going to
make a polynomial by removing the guessing. This is actually a
really good prototype for a greedy algorithm. If instead of guessing,
trying all edges, if we could find a
good edge to choose that's guaranteed to be in
a minimum spanning tree, then we could actually
follow this procedure, and this would be like
an iterative algorithm. If you-- you don't
guess-- you correctly choose a good-- you
take the biggest cookie, you contract it, and then
you repeat that process over and over, that
would be a prototype for a greedy algorithm and
that's what's going to work. There's different ways to
choose this greedy edge, and we're going to get
two different algorithms accordingly. But that's where we're going. First, I should prove
this claim, cause, you know, where did edge
contraction come from? Why does it work? It's not too hard to prove. Let's do it. Question? Oh. All right. I should be able to do
this without looking. So-- Proof of optimal substructure. So we're given a lot. We're told that e belongs
to a minimize spanning tree. Let's give that
spanning tree a name. Say we have a minimum spanning
tree T star, which contains e. So we're assuming that
exists, then we contract e. And then we're
given T prime, which is a minimum spanning
tree of G slash e. And then we want to
analyze this thing. So I want to claim
that this thing is a minimum spanning
tree, in other words, that the weight of
that spanning tree is equal to the weight
of this spanning tree, because this one is minimum. This is a minimum spanning
of G. And this is also supposed to be a minimum
spanning tree of G. OK. Sounds easy, right? I'm going to cheat, sorry. I see. Right. Duh. Easy, once you know how. So what we're going to do is
think about contracting e. OK, we already
know we're supposed to be thinking about
contracting e in the graph. Let's look at how it changes
that given minimum spanning tree. So we have T star,
minimum spanning tree of the whole graph, and
then I'm going to contract e. What I mean is, if
that edge happens to be in the spanning
tree-- it is, actually. We assumed that e is in there. So I'm basically removing,
I'm just deleting that edge, maybe I should call it minus e. Then that should be a
spanning tree of G slash e. So when I contract
the edge in the graph, if I throw away the edge
from this spanning tree, I should still have
a spanning tree, and I don't know
whether it's minimum. Probably, it is, but we
won't prove that right now. I claim it's still
a spanning tree. What would that take? It still hits all the vertices,
because if I removed the edge, things would not be
connected together. But this edge was in
the spanning tree, and then I fused those
two vertices together, so whatever spanning-- I
mean, whatever was connected before is still connected. Contraction generally
preserves connectivity. If these things were already
connected directly by an edge when I contract, I still
have a connected structure, so I'm still hitting
all the vertices. And also, the number of
edges is still exactly right. Before, I had n minus 1 edges. Afterwards, I'll still
have n minus 1 edges, because I removed one edge
and I removed one vertex, in terms of the count. So that proves that it's
still a spanning tree, using properties of trees. Cool. So that means the minimum
spanning tree, this thing, T prime, the minimum
spanning tree of G slash e, has a smaller weight
than this one. Because this is a
spanning tree, the minimum is smaller than
all spanning trees. So we know the
weight of T prime is less than or equal to the
weight of T star minus e. Cool. And now we want to know about
this thing, the weight of T prime plus e. Well, that's just the weight of
T prime plus the weight of e, because the weight
of a tree is just the sum of the
weights of the edges. So this is less
than or equal to w of T star minus e plus e, which
is just the weight of T star. So we proved that the weight
of our proposed spanning tree is less than or equal to
the weight of the minimum spanning tree in G, and
therefore, T prime union e actually is a
minimum spanning tree. OK? This is really easy. It actually implies that
all of these inequalities have to be
equalities, because we started with something minimum. Clear? That's the easier half. The More interesting property is
going to be this greedy choice property. This is sort of where the
action is for greedy algorithms, and this is usually
the heart of proving greedy algorithms are correct. We don't yet have
a greedy algorithm, but we're thinking about it. We need some way to
intelligently choose an edge e, and I'm going to give
you a whole bunch of ways to intelligently
choose an edge e. So here's a really
powerful lemma, and we're going to make it
even stronger in a moment. So I'm going to introduce
the notion of a cut, that's going to be a similar
picture to what I had before. I'm going to look at
some set of vertices. S here is a subset
of the vertices, and that leaves in the
graph, everything else. This would be V minus
S. OK, so there's some vertices over here,
some vertices over here, there's some edges
that are purely inside one side of the cut. And then what I'm
interested in are the edges that cross the cut. OK, whatever they look
like, these edges. If an edge has one vertex in
V and one vertex not in V, I call that edge
a crossing edge. OK, so let's suppose that e is
a least-weight edge crossing the cut. So let's say, let me be
specific, if e is uv, then I want one of the
endpoints, let's u, to be in S, and I want the other
one to be not in S, so it's in capital
V minus S. And that would be a crossing edge, and
among all the crossing edges, I want to take one
of minimum weight. There might be many,
but pick any one. Then I claim that edge is
in a minimum spanning tree. This is our golden
ticket, right? If we can guarantee an edge is
in the minimum spanning tree, then we plug that in here. Instead of guessing, we'll
just take that edge-- we know it's in a
minimum spanning tree-- and then we'll contract it
and repeat this process. So the tricky part-- I mean, it
is true that the minimum weight edge is in a minimum spanning
tree, I'll give that away. But the question is,
what you do then? And I guess you
contract and repeat but, that will be
Kruskal's algorithm. But this is, in some
sense, a more general tool that will let us identify
edges that are guaranteed to be in the minimum
spanning tree, even after we've already
identified some edges as being in the minimum spanning tree,
so it's a little more powerful. Let's prove this claim. This is where things
get particularly cool. And this is where we're
going to use something called a c and paste argument. And if you are ever
trying to prove a greedy algorithm
correct, the first thing that should come to your
mind is cut and paste. This is almost universally how
you prove greedy algorithms to be correct, which is, suppose
you have some optimal solution which doesn't have
the property you want, like that it includes e here. And then you modify it,
usually by cutting out one part of the solution and
pasting in a different part, like e, and prove that you
still have an optimal solution, and therefore, there
is an optimal solution. There is an MST that has
the property you want. OK, so we're going to do that
by starting from an arbitrary minimum spanning tree. So let T star be a minimum
spanning tree of G, and if the edge e is
in there, we're done. So presumably, e is not in
that minimum spanning tree. We're going to modify
T star to include e. So again, let me draw the cut. There's S and V minus
S. We have some edge e which crosses the
cut, goes from u to v, that's not in the
minimum spanning tree. Let's say in blue, I draw
the minimum spanning tree. So you know, the
minimum spanning tree connects everything
together here. I claim it's got to have some
edges that cross the cut, because if it has no
edges that cross the cut, it doesn't connect vertices over
here with vertices over here. So it may not use e, but some
of the edges must cross the cut. So here's a possible
minimum spanning tree. It happens to have sort of
two components over here in S, maybe. Who knows? But there's got to be at least
one edge the crosses over. In fact, the minimum
spanning tree, T star, has to connect vertex
u to vertex v, somehow. It doesn't use e, but there's
got to be-- it's a tree, so in fact, there has to
be a unique path from u to v in the minimum
spanning tree. And now u is in S, v is not in
S. So if you look at that path, for a while, you
might stay in S, but eventually you
have to leave S, which means there has to be an
edge like this one, which I'll call it e prime,
which transitions from S to V minus S. So there must be an edge e prime
in the minimum spanning tree that crosses the cut, because
u and v are connected by a path and that path starts in S,
ends not in S, so it's got to transition at least once. It might transition many
times, but there has to be at least one such edge. And now what I'm going
to do is cut and paste. I'm going to remove e
prime and add an e instead. So I'm going to look at T
star minus e prime plus e. I claim that is a
minimum spanning tree. First I want to claim, this is
maybe the more annoying part, that it is a spanning tree. This is more of a
graph theory thing. I guess one comforting
thing is that you've preserved the number of
edges, so it should still be if you get one
property, you get the other, because I remove
one edge, add in one edge, I'm still going to
have n minus 1 edges. The worry, I guess, is that
things become disconnected when you do that, but
that's essentially not going to happen
because if I think of removing e prime, again, that
disconnects the tree into two parts. And I know, by this path, that
one part contains this vertex, another part
contains this vertex, and I know that this
vertex is connected to u and this vertex is connected
to v. Maybe I should call this u prime and v prime. I know u and u prime
are connected by a path. I know v and v prime
are connected by a path. But I know that by
deleting e prime, u prime and v prime are not
connected to each other. Therefore, u and v are not
connected to each other, after removing e prime. So when I add in e, I newly
connect u and v again, and so everything's
connected back together. I have exactly the
right number of edges. Therefore, I'm a spanning tree. So that's the graph
three theory part. Now the interesting part
from a greedy algorithm is to prove to this is minimum,
that the weight is not too big. So let's do that over here. So I have the weight of T
star minus e plus-- minus e prime plus e. By linearity, this
is just the weight of T star minus the weight e
prime plus the weight of e. And now we're going
to use this property, we haven't that yet, e is a
least-weight edge crossing the cut. So e prime crosses
the cut, so does e, but e is the smallest
possible weight you could have crossing the cut. That means that-- I'll put that
over here-- the weight of e is less than or equal
to the weight of e prime, because e prime is
a particular edge crossing the cut, e was the
smallest weight of them. So that tells us
something about this. Signs are so difficult.
I think that means that this is negative or zero. So this should be less than
or equal to w of T star, and that's what I
want, because that says the weight of this
spanning tree is less than or equal to the optimum
weight, the minimum weight. So that means, actually,
this must be minimum. So what I've done is I've
constructed a new minimum spanning tree. It's just as good as T star,
but now it includes my edge e, and that's what I
wanted to prove. There is a minimum
spanning tree that contains e, provided e
is the minimum weight edge crossing a cut. So that proves this
greedy choice property. And I'm going to observe one
extra feature of this proof, which is that-- so
we cut and paste, in the sense that we removed
one thing, which was e prime, and we added a
different thing, e. And a useful feature is that
the things that we change only are edges that cross the cut. So we only, let's say, modified
edges that cross the cut. I'm going to use that later. We removed one edge that crossed
the cut, and we put in the one that we wanted. OK so far? There's a bunch of lemmas. Now we actually get to do
algorithms using these lemmas. We'll start with maybe the
less obvious algorithm, but it's nice because it's
very much like Dijkstra. It follows very closely
to the Dijkstra model. And then we'll get to
the one that we've all been thinking about, which
was choose a minimum weight edge, contract, and repeat. That doesn't-- well, that does
work, but the obvious way is, maybe, slow. We want to do it in
near linear time. Let's start with the
Dijkstra-like algorithm. This is Prim's algorithm. Maybe I'll start by
writing down the algorithm. It's a little long. In general, the idea-- we want
to apply this greedy choice property. To apply the greedy
choice property, you need to choose a cut. With Prim, we're going to start
out with an obvious cut, which is a single vertex. If we have a single
vertex S, and we say that is our set
capital S, then you know, there's some images
coming out of it. There's basically S
versus everyone else. That's a cut. And so I could take
the minimum weight edge coming out of that
cut and put that in my minimum spanning tree. So when I do that, I put it
in my minimum spanning tree because I know it's in
some minimum spanning tree. Now, I'm going to make capital
S grow a little bit to include that vertex, and repeat. That's actually also a
very natural algorithm. Start with a tiny s and just
keep growing it one by one. At each stage use this lemma to
guarantee the edge I'm adding is still in the
minimum spanning tree. So to make that work
out, we're always going to need to choose the
minimum weight edge that's coming out of the cut. And we'll do that
using a priority queue, just like we do in Dijkstra. So for every vertex
that's in V minus S, we're going to have that
vertex in the priority queue. And the question is, what is
the key value of that node stored in the priority queue? So the invariant I'm going
to have is that the key of v is the minimum of the
weights of the edges that cross the cut into
v. So for vertex v, I want to look at
the-- I'm not going to compute this every time,
I'm only going to maintain it. I want the minimum weight
of an edge that starts in S and goes to v, which is not in
S because v in Q-- Q only stores vertices that are not in
S-- I want the key value to be that minimum
weight so if I choose the overall minimum
vertex, that gives me the edge of minimum weight
that crosses the cut. OK? I've sort of divided this
minimum vertex by vertex. For every vertex
over here, I'm going to say, what's the minimum
incoming weight from somebody over here? What's the minimum
incoming weight from someone over here to there? To here? Take the minimum
of those things. And of course, the
min of all those will be the min of
all those edges. OK, that's how I'm
dividing things up. And this will be easier
to maintain, but let me first initialize everything. OK, I guess we're going
to actually initialize with S being the empty set,
so Q will store everybody, except I'm going to get
things started by setting for particular vertex little s. I'm going to set
its key to zero. It doesn't matter
who little s is. That's just your start vertex. Just pick one vertex
and set its key to zero. That will force it
to be chosen first because for everyone else,
for v not equal to S, I'm going to set
the key to infinity, because we haven't yet seen
any edges that go in there, but we'll change
that in a moment. OK, so that was the
initialization, now we're going to do a loop. We're going to keep going
until the Q is empty, because when the Q is empty,
that means S is everybody, and at that point, we'll
have a spanning tree on the whole graph, and
it better be minimum. OK, and we're going to
do that by extracting the minimum from
our priority Q. When we remove Q-- we remove
vertex u from the queue Q, this means that we're
adding u to S. OK, by taking it out of Q,
that means it enters S, by the invariant at the top. So now we need to
update this invariant, that all the key
values are correct. As soon as we move
a vertex into S, now there are new edges we have
to consider from S to not S, and we do that just by looking
at all of the neighbors of u. I haven't written
this in a long time, but this is how it's
usually written in Dijkstra, except in Dijkstra, these
are the outgoing edges from u and v are the neighbors. Here, it's an undirected
graph, so these are all of the neighbors v of u. This as an adjacency list. OK, so we're looking at u,
which has just been added to S, and we're looking at the edges. We want to look at the edge
as they go to V minus S, only those ones. And then for those vertices v,
we need to update their keys, because it used to just
count all of these edges that went from the rest of S to
v. And now we have a new edge uv that v needs to consider,
because u just got added to S. So the first thing I'm
going say is if v in in Q. So we're just going to store
a Boolean for every vertex about whether it's
in the queue, and so when I extract it
from the queue, I just set that
Boolean to false. Being in the queue is the
same as being not in S, this is what Q represents. So Q is over here, kind of. So if we're in the queue,
same as saying v is not in S, then we're going
to do a check which lets us compute the minimum. This is going to look a
lot like a relaxation. Sorry. A couple things
going on because I want to compute
not just the value of the minimum spanning
tree, I actually want to find the
minimum spanning tree, so I'm going to store
parent pointers. But this is just
basically taking a min. I say, if the
weight of this edge is smaller than what's
currently in the key, then update the key, because the
key is supposed to be the min. OK, that's all we need to do to
maintain this invariant, this for loop. After the for loop, this
property will be restored, v dot key will be that minimum. And furthermore, we kept track
of where the minimums came from, so when you end
up extracting a vertex, you've already figured
out which edge you added to put that into the set. So in fact, u
already had a parent, this would be u
dot parent, and we want to add that edge into
the minimum spanning tree when we add u to S. Overall, let
me write why this is happening. At the end of the algorithm,
for every vertex v, we want the v dot parent. And that will be our
minimum spanning tree. Those are the edges that form
the minimum spanning tree. Let's prove that this works. Actually, let's do an example. We've done enough
proofs for a while. Let's do it over here. I need a little break. Examples are fun, though easy
to make mistakes, so correct me if you see me making a mistake. And let me draw a graph. OK, weights. 14, 3, 8, 5, 6, 12, 7, 9, 15. 10. OK. Colors. So I want to start
at this vertex just because I know it
does an interesting thing, or it's a nice example. Here's my weighted
undirected graph. I want to compute
minimum spanning tree. I'm going to start
with a capital S being-- well actually, I start
with capital S being nothing, and all of the weights--
all of the key values are initially infinity. So I'm going to write
the key values in blue. So initially everything is
infinity for every vertex, except for S the value is zero. So all of these things
are in my priority queue, and so when I extract from the
queue, I of course get S. OK, that's the point of that set up. So that's when I draw the red
circle containing little s. The red circle here is
supposed to be capital S. So at this point, I've
added capital S-- little s to capital S, and then I look
at all of the neighbors v of S. And I make sure that they are
outside of S. In this case, they all are. All three neighbors, these
three guys, are not in S. And then I look at the
weights of the edges. Here I have a weight 7 edge. That's smaller than
infinity, so I'm going to cross out
infinity and write 7. And 15 is smaller
than infinity, so I'm going to cross out
infinity and write 15. And 10, surprise, is
smaller than infinity. So I'm going to cross
out infinity rate 10. So now I've updated the key
values for those three nodes. I should mention in
the priority queue, to do that, that is a
decrease-key operation. This thing here
is a decrease-key. You need to update the
priority queue to say, hey look, the key of
this node changed. And so you're going to have
to move it around in the heap, or whatever. Just like Dijkstra,
same thing happens. OK, so I've decreased the
key of those three nodes. Now I do another iteration. I look at all of the
key values stored. The smallest one is 7, because
this node's no longer in there. So I'm going to add
this node to capital S. So capital S is going to
grow to include that node. I've extracted it
from the queue. And now I look at all the
neighbors of that node. So, for example,
here's a neighbor. 9 is less than
infinity, so I write 9. Here's a neighbor. 12 is less than
infinity, so I write 12. 5 is less than
infinity, so I write 5. Here's a neighbor,
but s is in big S, so we're not going
to touch that edge. I'm not going to touch s. OK? I will end up looking at every
edge twice, so no big deal. Right now, who's smallest? 5, I think. It's the smallest blue key. So we're going to
add 5 to the set. Sorry, add this
vertex to the set S, and then look at all of the
outgoing edges from here. So 6 is actually less
than 12, so this edge is better than that one was. Then, what's that, an 8? 8 Is less than 10. 14 is definitely
less than infinity. And we look at this
edge, but that edge stays inside the red set,
so we forget about it. Next smallest value is 6. So 6, we add this guy in. We look at the edges
from that vertex, but actually nothing happens
because all those vertices are inside capital S, so we
don't care about those edges. Next one is 8, so we'll
add in this vertex. And there's only one edge that
leaves the cut, so that's 3, and 3 is indeed better than 14. So never mind. Stop. So good, now I think
the smallest key is 3. Notice smallest key is
smaller than anything we've seen before, other
than 0, but that's OK. I'll just add it in,
and there's no edges leaving the cut from there. And then over here,
we have 9 and 15. So first we'll add 9. There's no edges there. Then we add 15. OK, now s is everything. We're done. Q is empty. Where's the minimal
spanning tree? I forgot to draw it. Luckily, all of the edges
here have different numbers as labels. So when I have a 3
here, what I mean is, include 3 in the
minimum spanning tree, the edge that was labeled 3. OK, so this will be a
minimum spanning tree edge. 5 will be a minimum
spanning tree edge. These are actually
the parent pointers. 6 will be a minimum
spanning tree edge. 7, 9, 15, and 8. Every vertex except
the starting one will have a parent, which means
we'll have exactly n minus 1 edges, that's a good sign. And in fact, this will be
a minimum spanning tree. That's the claim, because
every time we grew the circle to include a bigger
thing, we were guaranteed that this edge was
in the minimum spanning tree by applying this
property with that cut. Let me just write that down. OK, to prove
correctness, you need to prove an invariant that this
key, the key of every vertex, always remains this minimum. So this is an invariant. You should prove
that by induction. I won't prove it here. But we have another invariant,
a more interesting one from an MST
perspective, you know, it's just a sort of algorithm
implementation detail, that the tree T sub S,
within S is always contained in a minimum spanning
tree of G. So over here, we have this way of computing
minimum spanning tree for all vertices v,
but what I'd like to do is just look
at v that's currently in S. By the end, that
will be the whole thing, but if I look at v in S, and I
always look at the edge from v to v dot parent, that
gives me this tree TS. I claim it will be contained
in a minimum spanning tree of the entire graph,
proof by induction. So by induction, let's assume--
induction hypothesis will be that, let's say there is
a minimum spanning tree T star, which contains
T sub S, and then what the algorithm does, is it
repeatedly grows S by adding this vertex u to S. So let's
suppose that it adds u to S. So I'm actually going to look
at the edge that it adds. So we have S and V minus S, and
we do this thing, like we just saw, of growing by one. We add one new vertex
over here to S, and that vertex has a parent
edge, has a parent pointer. So this edge, I'm
going to call e. So we're adding some vertex u
that we extract at the minimum, and we also added an
edge e to this TS, because we grew S by 1. OK, when I do that,
all I do is say, look, greedy choice
property guarantees there's a minimum spanning
tree that contains e. Because we extracted
the min from the queue, and the key values are this,
as I was arguing before, that is the minimum overall
edge that crosses the cut. e is a minimum weight
edge that crosses the cut, and so by greedy
choice property, there is some minimum
spanning tree that contains e. But actually, I need that
the minimum spanning tree not only contains e, but also
contains all the other spanning tree edges that we had
already said were in T star. OK, so here's where I'm going
to use the stronger property. I can modify T star to
include e and T sub S. So we already assumed that T
star includes T sub S. I just don't want to break that. And if you remember the proof
of this greedy choice property, we said, well all we need to do
is remove one edge that crosses the cut and replace it with e. So here what I'm saying
is there's some edge, yeah, maybe there's some
edge over here in T star that we had to remove,
and then we put e in. And then we get a minimum
spanning tree again, T star prime. OK, this edge that I remove
cannot be one of the TS edges because the TS edges
are all inside S. So because I'm only removing
an edge that crosses the cut, I'm not disturbing TS. TS will remain inside T star,
but then I get the new property that e is inside T star, and so
I prove this invariant holds. OK? I keep changing T star, but I
always preserve the property that all of the spanning
tree edges that are inside S are contained in some minimum
spanning tree of G. Maybe I'll add in some for emphasis. Cool? So that's how we use
the greedy choice property to get correctness
of Prim's algorithm. What's the running time
of Prim's algorithm? Same as Dijkstra, good answer. I guess it depends what
priority queue you use, but whatever priority queue you
use, it's the same as Dijkstra. And so in particular, if we
use Fibonacci heaps, which, again, we're not covering, we
get V log V plus E. In general, for every edge, we have
to do a decrease-key. Actually, for every edge we do
two decrease-key operations, potentially, if
you think about it. But this for loop over
the adjacency, the cost of this stuff is constant. The cost of this is the
degree of the vertex u. And so we're basically
doing the sum of the degrees of
the vertices, which is the number of edges times 2. That's the handshaking lemma. So for every edge,
we're potentially doing one decrease-key
operation, and with Fibonacci heaps,
that's constant time. But we're also doing V extract
mins those cost log V time, cause the size of
the queue is at most V, and so that is actually
the right running time. Just like Dijkstra, so
easy formula to remember. All right, let's do one more
algorithm, Kruskal's algorithm. Kruskal's algorithm is a
little bit weirder from the S perspective, I guess. We'll see what cuts
we're using in a moment, but it's based
around this idea of, well, the globally
minimum weight edge is the minimum weight edge
for all cuts that cross it, or for all cuts that it crosses. The globally minimum weight edge
is going to be a valid choice, and so, by this
theorem, you pick some S that partitions
the endpoints of e, therefore e is in a
minimum spanning tree. So let's choose that one
first, and then repeat. Conceptually, what we want to
do is that DP idea of contract the vertex, sorry,
contract the edge and then find the minimum
weight edge that remains. But the way I'm going to phrase
it doesn't explicitly contract, although implicitly,
it's doing that. And there's a catch. The catch is suppose I've
picked some edges out to be in my minimum spanning tree. Suppose this was
the minimum weight and this was the next minimum,
next minimum, next minimum, next minimum. Suppose that the next
lar-- at this point, after contracting those
edges, the minimum weight edge is this one. Do I want to put this edge
in my minimum spanning tree? No. That would add a cycle. Cycles are bad. This is the tricky
part of this algorithm. I have to keep track of
whether I should actually add an edge, in other
words, whether this vertex and this vertex have already
been connected to each other. And it turns out you've
already seen a data structure to do that. This is what I call
union-find and the textbook calls it disjoint-set
data structure. So it's in recitation. Recitation 3. So I want to maintain
for my MST so far, so I'm adding edges
one at a time. And I have some tree-- well,
it's actually a forest, but I'm still
going to call it T, and I'm going to maintain it
in a union-find structure, disjoint-set set data structure. Remember, this had three
operations, make set, union, and find set. Tell me given an item which
set does it belong to? We're going to use
that, the sets are going to be the connected components. So after I've added these edges,
these guys, these vertices here, will form one
connected component, and, you know,
everybody else will just be in its own
separate component. So to get started, I'm not going
to have any edges in my tree, and so every vertex is in
its own connected component. So I represent that by calling
make-set v for all vertices. So every vertex lives in
its own singleton set. OK, now I'd like to do the
minimum weight edge, and then the next minimum weight edge,
and the next minimum weight edge. That's also known
as sorting, so I'm going to sort E by
weight, increasing weight, so I get to start with
the minimum weight edge. So now I'm going to do a
for-loop over the edges, increasing order by weight. Now I want to know--
I have an edge, it's basically the minimum
weight edge among the edges that remain, and so I want to
know whether I should add it. I'm going to add it provided
the endpoints of the edge are not in the same
connected component. How can I find out
whether two vertices are in the same connected
component, given this setup? Yeah? AUDIENCE: Call find-set
twice and then-- ERIK DEMAINE: Call
find-set twice and see whether they're equal, exactly. Good answer. So if you find-set of u
is from find-set of v, find-set just returns
some identifier. We don't really care
what it is, as long as it returns the same
thing for the same set. So if u and v are in the
same set, in other words, they're in the same
connected component, then find-set will return
the same thing for both. But provided they're
not equal, then we can add this
edge into our tree. So we add e to the
set T, and then we have to represent
the fact that we just merged the connected
components of u and v, and we do that
with a union call. And if you're ever
wondering what the heck do we use union-find
for, this is the answer. The union-find data structure
was invented in order to implement Kruskal's
algorithm faster, OK? In fact, a lot of
data structures come from graph algorithms. The reason Fibonacci
heaps were invented was because there was
Dijkstra's algorithm and we wanted it to run fast. So same deal here, you just
saw it in the reverse order. First you saw union-find. Now, union-find, you
know you can solve v in alpha of n time, the
inverse Ackermann function, super, super tiny, slow growing
function, smaller than log log log log log log log. Really small. But we have this sorting,
which is kind of annoying. So the overall
running time-- we'll worry about correctness
in a moment. We have to sort--
to sort E by weight. So I'll just call
that's sort of E. Then we have to do some unions. I guess for every edge,
potentially, we do a union. I'll just write E times alpha
of v. And then we have to do, well, we also have to
find-sets, but same deal. So find-set and union
cost alpha amortized, so the total cost for
doing this for all edges is going to be the number
of edges times alpha, and then there's like plus v,
I guess, but that's smaller. That's a connected graph. So other than the sorting time,
this algorithm is really good. It's faster. But if you're sorting
by an n log n algorithm, this is not so great. That's how it goes. I think you can reduce this to
sorting just v things, instead of E things, with a
little bit of effort, like doing a select operation. But when this algorithm
is really good is if your weights are integers. If You have weights, let's
say weight of e is 0 or 1 or, say, n to the c,
for some constant c, then I can use rate x
sort, linear time sorting, and then this will
be linear time, and I'm only paying
E times alpha. So if you have
reasonably small weights, Kruskal's algorithm is better. Otherwise, I guess you
prefer Prim's algorithm. But either away. I actually used a variation
of this algorithm recently. If you want to generate
a random spanning tree, then you can use exactly
the same algorithm. You pick a random manage that
you haven't picked already, you see, can I add this
edge with this test? If you can, add it and repeat. That will give you a
random spanning tree. It will generate all spanning
trees uniform leap likely. So that's a fun fact,
useful thing for union-find. Let me tell you briefly
about correctness. Again, we proved correctness
with an invariant. Claim that at all
times the tree T of edges that
we've picked so far is contained in some minimum
spanning tree, T star. T start is going to
change, but I always want the edges I've chosen to be
inside a minimum spanning tree. Again, we can prove
this by induction. So assume by induction
that this is true so far, and then suppose that
we're adding an edge here. So we're converting T into
T prime, which is T union e. By the data structural
setup, I know that the endpoints
of e, u, and v are in different
connected components. In general, what my
picture looks like, is I have some various
connected components, maybe there's a single
vertex, whatever. I've built a minimum
spanning tree for each one. I built some tree,
and I actually know that these trees are
contained in one global minimum spanning tree. OK, and now we're
looking at an edge that goes from some vertex u
in one connected component to some vertex v in a
different connected component. This is our edge e. That's our setup. Because the union-find
data structure maintains connected
components, that's another invariant to prove. We're considering adding
this edge, which connects two different connected components. So I want to use the greedy
choice property with some S. What should S be? I want e to cross a cut,
so what's a good cut? Yeah? AUDIENCE: The
connected component of u and then everything else. ERIK DEMAINE:
Connected component of u and everything else? AUDIENCE: Yeah. ERIK DEMAINE: That
would work, which is also the opposite of the
connected component containing v. There are many
choices that work. I could take basically
this cut, which is the connected component
of you with everything else versus the
connected component of v. I could take this cut, which
is the connected component of u only versus everybody else. Either of those will work. Good. Good curve, all right. So let's say S equals the
connected component of u, or connected component of v.
e crosses that, all right? Because it goes from u to
v, and u is on one side, v is on the other side. I wanted to include an
entire connected component because when I apply the
greedy choice property, I modify T star,
and I don't want to modify, I don't want to
delete any of these edges that are already in my
connected components, that I've already put in there. But if I choose my
cut to just be this, I know that the edge
that I potentially remove will cross this
cut, which means it goes between connected
components, which means I haven't added that yet to T. So when I apply this
greedy choice property, I'm not deleting anything
from T. Everything that was in T is still in T star. So that tells me that T prime
is contained in T star prime. The new T star that I get
when I apply the cut and paste argument, I modify
T star potentially by removing one edge
and putting e in. And the edge that
I remove was not already in T, which means
I preserve this part, but I also get that
my new edge e is in the minimum spanning tree. And so that's how you prove
by induction that at all times the edges that you've
chosen so far are in T star. Actually, to apply the
greedy choice property, I need not only that e is cut--
sorry, that e crosses the cut, I also need that e
is the minimum weight edge crossing the cut. That's a little more
argument to prove. The rough idea is
that if you forget about the edges we've
already dealt with, e is the globally
minimum weight edge. OK, but what about the edges
we've already dealt with? Some of them are in the tree. The edges that are in
these-- that are in T, those obviously don't cross the cut. That's how we designed the cut. The cup was designed
not to cross, not two separate any of
these connected components. So all the edges that we've
added to T, those are OK. They're not related to the
edges that cross this cut. But we may have already
considered some lower weight edges that we didn't add to T.
If we didn't add an edge to T, that means actually they
were in the same set, which means also those are-- I'm going
to use my other color, blue. Those are extra
edges in here that are inside a
connected component, have smaller weight
than e, but they're inside the connected component. So again, they're not crossed. So they don't cross
the cut, rather. So e is basically the
first edge that we're considering that
crosses this cut, because otherwise we would have
added that other edge first. So here, we have to do sort
of the greedy argument again, considering edges
by weight and e is going to be the
first edge that crosses this
particular cut, which is this connected component
versus everyone else. So e has to be the minimum
weight edge crossing the cut, so the greedy choice
property applies. So we can put e in the
minimum spanning tree, and this algorithm is correct. OK? So we've used that lemma
a zillion times by now. That's minimum spanning
tree and nearly linear time.

Calculus

Well let's get started. Thanks for coming despite the rain, but at least we can feel lucky that the sun rose
today cuz we have a lot more to do and it would be hard to do if
the sun stopped rising. So, okay, so
we were talking about MGFs last time. We've done all the theory that we need for
MGFs but I'm not sure that the intuition is clear enough yet
and there are some important examples. So I just want to start with
a few examples of MGFs. We already have all the theorems. Okay, especially,
how do we work with MGFs for some of the most important distributions
such as exponential, normal, and Poisson? Just to show you how
the MGFs are useful for some of those famous distributions, okay. So let's start with the exponential. And [COUGH] we talked before about,
So this is the Expo MGF. We talked before about the fact that if we
have exponential lambda we can always find a constant to multiply by
to make it exponential one. So let's just start with the exponential
one case cuz that's simpler, hat is lambda equals one. Let x be exponential one, and suppose that we wanna find find MGF and
find the moments, okay. Find moments. And this will really show you why it's
called the moment generating function. That doesn't actually, I didn't actually talk about where
did the word moment come from? It comes from physics. Those of you who've done
moment of inertia and stuff, there's actually a pretty strong analogy
between variance and moment of inertia. That doesn't answer the question for where
did the word moment come from in physics, but you can ask the physicist that. But it came into statistics via physics
because of this analogy with moment of inertia. Anyway, so we have an exponential,
okay, and so let's find the MGF. Well by lotus that's
a pretty easy calculation. M(t), remember just by definition it's
just the expected value of (e to the tx). And this is a perfectly
valid thing to write down. This (e tx), that's just some random,
we're taking it's expectation, and then we're viewing
this as a function of t. And I pointed out last time, t is a dummy
variable, so I could of just as well said M(s) = expected (e to the sx), or whatever
you wanna call it that doesn't clash. [COUGH] The interpretation
is just that this is a very, very useful bookkeeping device for
keeping track of moments, and it's another way to describe distribution,
rather than a CDF or a PDF. Okay, so let's just compute this thing. Well this is an easy lotus problem
cuz by lotus we can just immediately say this is the integral 0 to infinity,
e to the tx, e to the -x dx. All right,
that's just immediate from lotus, combine the two exponentials,
so that's e to the -x(1-t) dx. So that's just an easy integral, right. So that integral, well actually one way
to do it is just to do the integral. Another way to do this integral is to
recognize this as another exponential PDF with a different parameter and
put it in the normalizing constant. And you'll get 1 over 1- t,
and this is for t less than 1. If t is bigger than 1,
we have some problems here. Cuz if you let t be 2 for example,
then that would 1- 2 is -1. You'd get e to the +x,
which would blow up. But as long as t is less than 1,
this will be okay. Exponential decay, not exponential growth. So we have to assume t is less than 1. But that's okay, cuz we talked last time
about the fact that we wanted to have some interval, I called it -a to
a on which this is finite. So in this this case it's finite
everywhere to the left of 1, right. So in particular it could take
some interval like say -1 to 1 open interval on which it's finite. So this is a perfectly valid MGF. Okay, so
now we wanna get the moments, right. So from what I said last time,
we could take this thing 1 over 1-t and start taking derivatives,
so, and plug in 0. So would be true that M'(0)
would be the mean and M''(0) would be the second moment. And once we have this and
this we could easily get the variance. We already talked about the mean and the
variance of the exponentials, so you could do this and check that it agrees with
what we did earlier through lotus, okay. And then third moment would be the third derivative evaluated at 0,
and so on, right. So we could do that, but that's kind of annoying in the sense that
you have to keep taking derivatives. Now for this function, taking a bunch
of derivatives is not too bad, okay. But it's still a much better way to do
this, Is to recognize the pattern, right. A lot of this is about
pattern recognition, okay. Where have we seen 1 over 1-t before? Geometric series, right. We keep using the Taylor series for
e to the x and geometric series over and over again. It can go in both directions, right. You can have the geometric series result
and write it as a geometric series or you can have a geometric series and
simplify it to this. Anytime you see one over
one minus something, you should be thinking that may have
something to do with the geometric series. That may be a useful interpretation,
it may not, but at least the idea should pop into your
mind just cuz you see this pattern, okay. So if we do that we get 1 over 1-t
equals just a geometric series, a sum t to the n, n=0 to infinity. And this is valid for t greater than,
for absolute value of t less than 1. That's when this converges. By writing it this way,
we don't actually have to do derivatives. We're just looking at this series,
okay, and then we're just gonna
read off the moments. So the only thing we have to be
careful about is the n factorial. Because I said with the MGF,
you take the Taylor expansion and the moment is whatever is in front
of t to the n over n factorial. I don't see an n factorial here,
but that's no problem, right. We just multiply and divide by n factorial,
cuz we need the n factorial there. So I'll multiply by n factorial
t to the n over n factorial. Now this matches exactly the pattern
that we talked about last time, about what ever's in front of the t to the
n over n factorial, that's the nth moment. So that's the nth moment. So we immediately know now that E(x
to the n)=n factorial for all n. So instead of taking derivatives over and over again, we simultaneously
get all the moments of x, okay. So that's nice, right. Didn't need to take any derivatives. So, by the way, that's kind of like
the coolest thing about MGFs is the fact that if you, just in general,
not necessarily for this example. If you wanna find the moments
of some distribution by lotus, you would think you have to integrate,
right. You want e of x to the n so you're going
to integrate x to the n times the PDF. That may be an incredibly
difficult integral. But the MGFs, once you have the MGF,
we're taking derivatives not integrals. So it's pretty surprising
to me at least that you can do derivatives of the MGF rather
than the integrals of powers of X. Derivatives are much easier usually than
integrals, so that can save a lot of work. So let's just quickly see what
happens if it's exponential lambda, where lambda is not necessarily 1. So now let's let Y be exponential lambda. And then, let's just convert it,
just to see how to apply this. Convert it, well, exponential, so we talked before about the fact that
if you multiply or divide by lambda, it may be hard to remember, should you
multiply by lambda or divide lambda? But there's an easy way to see that. Let's just let X =, Lambda Y. So I need to multiply by lambda
rather than dividing because we know the exponential lambda
has mean 1 over lambda. So if we multiply by lambda
now this has a mean 1. And we show that this is,
in fact, exponential of 1. So we've converted it to this case. In other words, Y = X over lambda,
and we can take nth powers. So now we immediately have to
moment of the nth moment of Y. Expected value Y to be n = expected
value of X to the n which is n!, divided by lambda to the n. Okay, so I didn't do any calculus here. I only used the geometric series. We could have directly done something
similar to this for Y, but I think it's easier working with the exponential
one and then converting it back. Similarly, at the end of last time, we derived the MGF of the standard normal,
okay? Now if you want any
normal mu sigma squared, then you just write a mu plus sigma z,
right? Then you can get its MGF very easily. So a lot of times it's easier to
work with the standard normal. Okay, so speaking of standard normal,
let's actually get the normal moments now. We already know the odd moments. So the problem is let Z be standard
normal, And find all its moments. Okay? We already know that
the first moment is 0 and the second moment is 1 cuz
it's mean 0 variance 1. We already know that
the odd moments are all 0. That's just by symmetry, we mentioned
that fact before but you should check for yourself that that makes sense,
that to practice the symmetry. Because if you write down
the integral using LOTUS, you would be integrating an odd
function symmetrically about 0 so the negative area cancels
the positive area. So don't need to do any work to get this,
just use symmetry. Even moments, though,
that seems pretty hard. And we already know E of Z squared, and
we did that by doing some integrations. Now if we want E of Z to the forth,
if we use LOTUS, you're gonna have to integrate Z to
the fourth times the normal PDF. How do you do that integral? I don't know, I mean, you can try doing
some substitutions, you can try doing integration by parts and you can easily
spend a couple hours doing that integral. And it's possible to do it, but
it's not easy, it'll be a lot of work. And that would just be the fourth moment,
and then you'll say well, what about the sixth moment? What about the eighth moment, right? So that's not a very
efficient way to do things, it's doing a lot of
nasty looking integrals. Okay, so let's use the MGF instead. The MGF that we derived last time is the function M of t = e
to the t squared over 2. So that at least gives us an approach to
getting the moments that doesn't involve having to figure out how to
do these integrals, okay? It's something more straightforward. Like for derivatives, we have the chain
rule, the product rule and so on. There's no chance that you can't do this
derivative if you know your chain rule and product rule, and stuff like that. Whereas for integration, you may just
not know how to do the integral, okay? So we could take the derivative of this,
use the chain rule. And we're gonna get a t that comes out
in front because of the chain rule. And then we take the second derivative, because then there's gonna be a t out
there after the first derivative. Then we will have to use the product rule,
okay. And then we take another derivative,
then we have 2 terms and then terms start multiplying and
get more and more terms to deal with. And it'll get more and more tedious and
ugly, the more derivatives we took. It's still something that you can do. It's pretty mechanical, but it's tedious,
and we wanna avoid tedious stuff, okay. So here's a much better
way to think about it. Over there with the exponential, I emphasized just the pattern
recognition geometric series. Let's apply the same thinking again. Pattern recognition,
this is e to a power, okay? Unlike the geometric series,
the Taylor series for e to the x converges everywhere. So I can immediately just write
down the Taylor series for this, without taking any derivatives. This is just the sum of t squared
over 2 to the n over n!, right. Because the Taylor series for e to the x
is valid everywhere, so in particular, I can plug in t squared over 2, okay? So this is a much,
much better way to do it, than to start taking derivatives of this. So let's simplify this, this is the sum,
notice that we're only gonna get even powers of t, which makes sense
because this is an even function. So it's gonna be t to the 2n, And, there's a 1 over 2 to the n in
the denominator and there's an n!. Okay, so that's what it is. Now, same as over there,
we just have to read off the moments. The only thing you have to be careful
about is the fact that there's a 2n here in the exponent, and there's an n!, there. So there's kind of a mismatch right now,
okay? We want the 2n moment because 2n
is just an arbitrary even number. Okay, we want the 2n moment,
so the 2n moment, we want the coefficient
t to the 2n over 2n!. We don't have a 2n!. Well that's okay, just put in 2n!. As long as we multiply by that, it's okay. So I just multiplied and divided by 2n!,
that immediately tells use the answer. The expected value of Z to the 2n, so
that's just an arbitrary even moment, we already have the odd moments, Is just
the coefficient of t to the 2n over 2n!. That's everything that's left. That's 2n!, over 2 to the n n!. And let's just check whether this
makes sense in the cases we know. If n = 1, N = 1, this is 2!, over 2 times 2, when n = 1, so 2 times 1. Okay, so So
that's 2 divided by 2 times 1 is 1. So E(Z-squared) = 1, and that's what
we expected because the variance is 1. And let's just do a couple more, n = 2, so then get the fourth moment,
z to the fourth. n = 2, four factorial is 24 divided by 8, 24 divide by 8 is 3,
so the 4th moment is 3. And the next one E(Z to the 6th) is gonna be 3.5 = 15. And you'll see,
alright as 1 times 3 times 5. You'll see the pattern as, if it's 1,
1 times 3, 1 times 3 times 5, 1 times 3 times 5 times 7 and so on. And this is not the first time
that we've seen these numbers, or at least if you've done the strategic
practice problems going way back. That was the number of ways to break two
n people into end to end partnerships and there's a story problem there. We could either write it this way,
or as a product of odd numbers. So kind of a surprising fact,
or at least I found it really surprising that the same expression
comes up for even moments of the normal. As it's the same number as breaking
up people into partnerships and counting number of ways to do that. And I thought that was kind of mysterious,
it turns out that it's not a coincidence. But there's this kind of a very
deep combinatorial explanation for that which I can't get into but
there is a reason for that. Anyway, that gives us all the moments
of the normal distribution now without doing any calculus. So that's nice, okay? So one more MGF problem,
we haven't talked it yet in class, the MGF of the Poisson distribution,
so let's do that. So you know Poisson, we know that
the mean is, the Poisson lambda, we know it has mean lambda and variance lambda,
but we haven't computed any other moments. But mainly for the Poisson,
I wanted to show you the other, like I said last time that there are three
reasons why the MGF is important. And those examples to illustrate
the fact why it's a moment generating, cuz we generated all the moments. But for the Poisson I wanna show
you the other important reasons. So let's let x be Poisson lambda and
find its MGF again. Well, let's just use LOTUS,
the expected value of E(e to tx) = the sum, so Poisson takes non-negative integer values
so I'll just say k equals 0 infinity. e to the tk, all right it's just LOTUS,
e to the mn times the Poisson pmf, e to the minus lambda,
lambda to the k over k factorial. Okay, looks like a kind of ugly sum. But actually you'll find that this sum is
an example on the math review handout, so I was planning for in advance. But you don't have to memorize that or
anything, this is just another example of pattern
recognition dealing with a series. It looks a little ugly
when you first see it but this is actually easy once you're
familiar with the pattern, right? So either the minus lambda comes
out cause that's just a constant, look at what's left inside. We have, this is either the t to the k and
this is lambda to the k, so together that's lambda e
to the t to the k, right? So all that's left is the sum of
something to the k over k factorial. That's just the the Taylor series for
e to the x again. So this is very easy once you've mastered
the Taylor series for e to the x. So and just immediately write that down,
that is the Taylor Series for e to the x evaluated at x
= lambda e to the t, okay? So we can simplify that a little bit,
it's e to the lambda e to the t- 1. So that's the Poisson MGF and it's valid for all values of t
because the series converges for all t. Okay, so that's the Poisson MGF. So one thing we could do with it
is to start taking derivatives or trying to expand it or
whatever to get the moments. But I'm not doing this example
because I want do moments, I want to show you the other
applications of MGF. So now let's let Y be the Poisson of mu. So we have two Poisson's now,
not one Poisson. And suppose that X and Y are independent. And the problem is find
the distribution of X + Y. So we wanna study the sum of
two independent Poissons. Okay, so that's called a convolution, and
we'll come back to convolutions later on in the semester, but
you know in general it can be nasty. But I pointed out last time that for MGFs you can just multiply the MGFs,
that's easy. Whereas, doing a sum or
an integral could be pretty nasty, okay? So all we have to do is multiply the MGFs. That is I'm just going to take
the MGF of X times the MGF of Y. So here's the MGF of X,
e to the lambda, e to the t minus 1. MGF of Y is going to be the same thing, except that the parameter is now
called mu instead of lambda. So that's gonna be e to the mu, e to
the t- 1 =, and let's just simplify it. That's e to the lambda + mu,
factor that out, e to the t- 1. That immediately tells us that
X + Y is Poisson lambda + mu. Because of the fact,
we didn't prove this theorem, as I said that's a really
difficult theorem. But that is a theorem that this is
the Poisson lambda plus mu MGF. There's no other distribution
that has the same MGF. So this is, therefore,
the only possibility. By the way, it was obvious that the mean
had to be lambda + mu, by linearity. So this, we already knew. The interesting part is that it's Poisson, the sum of independent
Poissons is still Poisson. Most distributions don't
have such a nice property. Like you'll add independent
versions of them, and usually you get some other family. Here it's still within the Poisson
family of distributions, okay? So that's a very,
very nice property of the Poisson. And a common mistake with this, is to ignore the assumption that X and
Y are independent. So to justify being able to be just
multiply the MGFs we need X and Y to be independent. So just to see a quick counter example,
if they're not independent. If X and Y are dependent,
well the most extreme case of dependence that I can
think of is when X = Y. Okay, so let's just see why
this doesn't work when X = Y. Well obviously if X = Y, then X + Y is 2X. And that's not Poisson. Why is that not Poisson? Yeah,
&gt;&gt; [INAUDIBLE]. &gt;&gt; Okay, so that's a good way
to think of it with the MGF, if we take the MGF of this thing
you're gonna get 2 in there. And what you actually gonna have, you are gonna take the selected value of
e to the 2tx, so you've replaced t by 2t. So you'd get 2t up there and
that doesn't look like a Poisson, now so that's close to a proof but it's a little
more complicated than I was thinking of. And you would still deed to say like could
there be some miracle of algebra that would reduce that back down. It's not true right? If you put a 2 there
it's not of this form. But still, what if you just didn't
think of the brilliant algebraic way to simplify it down. Yeah.
&gt;&gt; [INAUDIBLE] &gt;&gt; Yeah that's the simplest way to see it. What she just said was
that this thing is even. So that's one good way to see it. A Poisson has to take on any
possibles non-negative integer value. This thing is always an even number,
so it couldn't possibly be a Poisson. That's the simplest way to think about it, is just looking at one
of the possible values. Another way to see it, would be to compute
the variance, the mean and variance. So the expected value of x plus y,
which is 2x, would be 2 lambda. So if it were Poisson, it would have to
be Poisson 2 lambda cuz that's the mean. But the variance of 2x is 4 lambda
cuz the 2 comes out squared. For a Poisson the mean
always equals the variance. For this thing the variance is double. Intuitively that should make sense because
you're adding the same thing to itself. That increases the variance compared
to if you added independent things, then you might expect if one
thing happens to be very large, then the other thing might offset it,
right? But if you're adding the same thing
to itself and it happens to be large, then you're adding the same
large thing twice, okay? I've seen similar mistakes, cause
this is like an easy counter example. I've seen some more mistakes since
that one time, many many times where maybe we have something
like a sum of x1 plus x2 plus x3. And a student just then
replaces them all in their iid, and a student replaces them all by
x plus x plus x, and then get 3x. But X is not independent of itself, and you'll end up with
the same mistake as this. So I wanna mention that counterexample,
okay? So and there's other ways to see it,
too, but we just talked about three reasons why
this was not Poisson when using the MGF, one by looking at the possible values,
one by looking at the mean and variance. So hopefully you're convinced
now that that's not Poisson. Okay, so next major topic in this
course is joint distributions. That is, something we dealt
with a little bit before but just like bringing it in as its
own topic in its own right. So joint distributions just means, how do we work with the distribution
of more than one random variable, okay? So that's why everything in this
course is cumulative, right? Because if you don't fully
understand the CDF of one random variable then it's going to be really
hard to understand the joint CDF of more than one random variable, okay? So joint distributions, we already talk about independence
versus dependence right? If you have independent random variables
joint distribution just means multiply the individual CDFs of the individuals
PDFs and it's pretty straightforward. Remember the slogan independent
means multiple, okay? But in general we need to have
some tools and notation and so on for
dealing with dependent random variables. Maybe just two of them or
maybe a million of them, okay? So we're gonna talk about
joint distributions. And I think the best way to
start is in the simplest case, where we have two random variables. And let's even say there are two
binary random variables. So we can think of this in
terms of two by two tables. And this may seem really, really simple. I hope it seems pretty simple, cuz then if
you understand this simple case really, really well, it'll give you a lot of
intuition for the more complicated case. Okay, so I'll start with a simple one where x and y are Bernoullis. Possibly dependent, possibly independent,
and possibly the same p, possibly different p's. I'm not saying they're both
Bernoulli-p with the same p. Okay, then we can think of this
in terms of two by two tables. So we could draw an example
light like this with a table and we could just tabulate
values where here is x = 0, x = 1, and y = 0, y = 1, okay? Okay and then to specify the joint
distribution all we have to do is put in four numbers here that
are non-negative and add up to 1, right? Any four numbers you want as long
as they are non-negative and add up to 1 that will be
a valid joint distribution. So remember for your know PMFs to be valid
a PMF just non-negative adds up to 1? Completely analogous it's just now in two
dimensions instead of one dimension okay? So we can just make up four
numbers that add up to one. I guess we can talk about some
of the general definitions here. So this is for this specific case. But let's also talk
about the general case. So if we have x and y, first of all, they're joint CDF. It's completely analogous
to the individual CDF. So the joint CDF is the function
of two variables now. F(x,y) = the probability X less than or
equal to x, Y less than or equal to Y. Similarly we have a joint
PMF in the discrete case. Which would just be the probability that
X equals little x, Y equals little y, all right. So we just add this part. That's the PMF. The joint PMF means we're considering
both of them together, okay? Now, in the case where they're
independent if x and y are independent. That means that this joint PMF is the
product, P of X = x times P of Y equals y. So we need, so
that's called the joint CDF. Joint PMF. And now, so this is when we're
considering them together, right? Because it's comma within the same P. Right, it's considering them jointly. Okay if they're independent, that's equivalent to independence
is you can split this up. Okay, so now there's another concept
that we need called marginal The marginal distribution,
marginal just means take them separately. So the marginal distribution for
x would be a probably x less than or equal x is called the marginal
distribution of x. Similarly marginal PMF would
just be just this part, okay? So therefore in words,
we could say that marginal Independence means that
the joint distribution, the joint CDF is the product
of the marginal CDFs. Okay, and similarly we have, we can continue this over here,
we have the notion of a joint PDF I'm doing kind of discrete and continuous together, because
they're analogous to each other and they're analogous to
the one dimensional case. So a join PDF, which we might
write as little f(x, y) such that, so this would be the continuous
case in two dimensions. What does it mean to be a joint PDF? Just as like in the one dimensional case, the PDF is what you integrate
to get a probability. Two dimensional case, same thing. If we wanna know what's
the probability that x and y are in some set,
let's say x,y is in some set B, where B is some region in the plane. Maybe it's a rectangle,
maybe it's a circle or something. Just imagine some area in the plane. Then what we do is integrate
over that region f of x, ydxdy. So that's the first time that we've
written down a double integral here, but as far as what we're concerned, for
the most part, double integrals, for this course, the double integrals,
we're not gonna need to do a lot of them. And when we do normally we can just
think of it as one single integral and another single integral so
just do two integrals. But the intuition should be clear, right? The PDF is what you integrate
to get a probability. So it's completely analogous. And so independence means that We've already talked about this before,
I'm just using new terminology for it. Independence means that the joint x and
y are independent. If and only if, The joint CDF is the product of the marginal CDFs. So, I'll call that, just for emphasis, it would be confusing to use the same
letter F here without any clarification. This is the marginal CDF of x. This is the marginal cd F of x,
this is the marginal cd F of y, this is the joint cd F, okay? So it says that instead of having to do
some kind of complicated joint thing, I can just find the probability of this
event times the probability of this event. So that's the definition of independence. But we've seen over and over again that
usually it's easier to use PDFs or PMFs. So it's equivalent,
it's not too difficult. It's a little bit tedious but
with some algebra, we can show that it's the same thing as saying the joint
pmf is the product of the marginal pmfs. That's in the discrete case. And in the continuous case,
that the joint PDF, is the product of the marginal PDFs. And I wanna emphasize that this
has to be for all x and y. Not just for x and
y that make this thing positive. You have to pay attention
to the zeros also. We'll see an example like that later. So for all real x and y,
we can't restrict it. All right, so coming back to this
little example, we can make up any four numbers we want as long as they're
non-negative and add up to one. So I made up 4 numbers,
just for the sake of example. Two-sixth, one-sixth,two-sixth, one-sixth. So I made up a simple little example here,
and I could ask the question,
are x and y independent? And to answer that we need to say well two ways we can think about it. One would be so I so I wrote this in terms
of, you know, joint CDFs, joint PMF. We could also write
something like conditional. That is independence means you
don't have the distribution of y given that x equals something. It doesn't actually depend on that x part. So it's the same as
the unconditional distribution. Okay, so, well anyway, so each number in this table is one
of the joint probabilities, right? So two-sixth is the probability that x and
y are both zero, one-sixth is the probability that
they're both one, and so on, okay? So to check that they're independent
from the definition, well, what that means is we first need to
find the marginal distributions and then check that this is true. Okay, now to get from
the marginal to the joint. Here's just quickly how do we
get marginal distributions? Getting marginals is actually pretty
easy from the joint distribution. Because Let's just do
the discrete case first. If we wanna know the marginal
distribution of x as the marginal PMF, then just by the action of probability, all we have to do is add up
the different possibilities for y. So that the sum of all y P of X = x,
Y = y, okay? Because just the axiom
of probability right? That we're adding up just
joint cases the union is this. You can also write it as a conditional. You can also think of this as the law of
probability, and write given Y equals y, times P of Y equals y,
it would be the same thing. Okay, that just says add up, X = x, but
Y could be anything so we sum over Y. That's called marginalizing over Y,
that we're just summing up. We start with this thing that's
a function of x and y, sum over all y, then we just get a function of x. And in the continuous case, let's get
the marginal so that's the discrete case. And the continuous case, let's say we want
the marginal distribution of y, similarly, you can get the marginal
distribution of y, I'm not gonna write the same thing again. If you want the marginal PDF? So this is the marginal PDF of y. Marginal, this means viewing it. On its own, as its own thing, right? Then all we have to do is integrate
completely analogous to this. Integrate the joint density, f of x,y, (x, y), integrate over all x. That's just the continuous analog of that. Here we're summing over all values. I swapped the x and
y here just for variety, here we are summing overall values of y. Here we're integrating overall values
of x, the joint density, okay? So, you can go in that direction, this is getting marginal distributions
from joint distributions. You can't go in the other direction. If we only know the marginal distributions
that doesn't tell us anything about how x and y are related to each other, right? So you can't go any other way. But you can go from the joint
distributions to the marginal distribution. So for this example,
let's get the marginal distributions. So what's the probability that y equals 0? Well, obviously, we're just adding
this case plus this case, right? Cuz those are the two
cases where y equals 0. So we add those two cases,
we get four-sixths. Add these two cases, we get two six's. And for the other way around if we
want the X = 0, just add this case and this case and
you get three six's or one half. This one plus this one, 3 / 6. And by the way,
one thing you have to be careful about, is the terminology in economics and
statistics is very different. And when you take an econ class you
always hear about marginal revenue and marginal cost and things like that. And usually in like, AP Econ,
then they don't want to use calculus, and so they explain everything is incremental,
if you do one more unit of something, then what happens? And then later when you actually
see what's going on with calculus, you realize that in Econ,
marginal means derivative and in statistics, marginal means integrate,
so it's completely opposite meaning and I don't know
where the Econ term came from but you can see here where
the statistics term came from. Cuz it's called marginal cuz we
write these numbers in the margins. So that's a marginal distribution. So, once you understand
this two by two table, you basically have the key
intuition into joint distributions. In this case,
here they are independent in this example. To check that they're independent
There are other ways to do it, but just to check it by the definition, what independence means is that
to compute any of these entries. Let's say 2/6ths Asl I need to do
is find the probability that X = 0 x the probability that y = 0 so
I'd multiple 3/6ths times 4/6ths. Which is 1/2 times 2/3 is 1/3 which
is this, so if you get this number I can multiply this times this and so
on so you check this four numbers. So each of these joint probabilities
is obtained by just multiplying two marginal probabilities. So that means they are independent. Or as you can make up your own examples,
if you just here is kind of an extreme example
it doesn't have to be this extreme. But I can pick whatever numbers I want
as long as they're nonnegative and add up to 1. For example, I just made one up here
where these nonnegative add up to 1. So this is a perfectly
valid joint distribution. But you can see right away that this
0 means that it's not gonna be true, that if you multiply,
you can't obtain it that way cuz if you do the marginal thing again,
1/2, 1/2, and this is 1/4, 3/4, and you multiply 1/4 x 1/2,
you don't get 0. So this one would be dependent. This one is dependent,
you can make up your own examples. It doesn't have to have a 0
in it to make it dependent, that was just an easy,
extreme case to see what's going on. Okay, so this is a simple two dimensional
discrete example to think about. Let's also do one simple
continuous example just to have some intuition on
what this all means. So the simplest way to start is I
think at the uniform distribution. What is uniform in two dimensions mean? So let's consider as an example what if we have uniform on the square that's all x y such that x and y are both between 0 and 1. So we just have this square here. We can draw our coordinates, and
have a square here where this is 1 and this is 1, okay? So we have this square, and we want a distribution that's
uniform over this square, so. Remember, in the one-dimensional case, uniform meant that the PDF was
constant on some interval, okay? So the analogous concept would be, we want
a PDF, which is gonna be a joint PDF, and we want it to be
constant on that square. And 0 outside the square, right? So, that just captures the notion
of being a completely random point. As we're picking a random point, x comma
y, we want a completely random point in the square, so we want the density
to be constant all over that square. So 0 outside, let's find the joint PDF. Well, the joint PDF, therefore from
what I just said is some constant c, if x and y are both between 0 and
1, and 0 otherwise. Now in one dimension, if you integrate
the number of one over some interval, you get the length of the interval. In two dimensions if you
integrate the constant one over some region you get
the area of the region so if we integrate this thing we get
the area, so the integral is area, So C = 1/area would normalize it, which = 1 because the area
of that square is 1. So the joint PDF would just be 1
inside here and 0 outside, and if you want the marginal distributions,
then just integrate out the, integrate this Dx or integrate this Dy,
you'll get 1 so marginally, X and y are independent uniform,
which is pretty intuitive uniform 01. Which is kind of intuitive right because
it just says if you pick a random point in the square in the x coordinates
uniform, the y coordinate is uniform. So that's pretty straightforward,
that's an example of independence. But I want to contrast that
with an example of dependence, where instead of a square,
let's use a circle. So, suppose we want uniform in
the circle I'll say disc for clarity. A circle might just mean a circle
we want everything inside. So on the disc, x squared plus y
squared less than or equal to 1. Okay, so let's see what that looks like so
we just draw a circle. Sorry, it doesn't look like
a very good circle, but pretend that that's a perfect
circle centered at 0 of radius 1. And we wanna be uniform in here, okay? We wanna write down what the joint PDF,
what are the marginal PDF's, okay? So first of all for
the Joint PDF by the same kinda reasoning. It's just because its uniform that that
means another way to say uniform is that the probability of some region must
be proportional to its area, right. So now in one dimension I said
probability is proportional to length for uniform distribution. Here probability is proportional to area,
so because of that the normalizing constant has to be 1 over the area of
the circle, Pi r squared, so that's Pi. So a joint PDF is 1 over pi
inside the circle and 0 outside. And a common mistake with this kind
of thing is to then think that that says that they're independent
because that's just a constant so it doesn't It looks like I can factor 1
over pi as constant times a constant. It's just a constant but they're not
independent because of this constraint. They're actually very dependent
because for example, if x is 0, then y could be anywhere from -1 to 1. But if x is close to one, then y has to
be in some tiny little interval, right? So, if we fix x to be here, then y
could be between here and here, right? So the values depend on where, that is knowing x constrains
the possible values of y. That says that they're not independent. So here x and y are dependent. And in fact,
we can show that given that x equals x Then, we can actually say, what can y be? Y has to be between square root of
1 minus x squared and minus that. Because x squared plus y squared
are less than or equal to 1. So this depends on x,
this is the constraint. So we might guess that Y is
uniform between here and here. That is if X is here,
then we know it's between here and here, but could be anywhere, right? So a good guess would be uniform, but next time we'll do an integral
to show that for practice. But you can see right
now they're dependent. Okay, so see you on Friday.

Calculus

This time, we started solving
differential equations. This is the third
lecture of the term, and I have yet to solve
a single differential equation in this class. Well, that will be
rectified from now until the end of the term. So, once you learn
separation of variables, which is the most elementary
method there is, the single, I think the single
most important equation is the one that's called the
first order linear equation, both because it
occurs frequently in models because it's solvable,
and-- I think that's enough. If you drop the
course after today you will still have learned
those two important methods: separation of variables, and
first order linear equations. So, what does such an
equation look like? Well, I'll write it in there. There are several
ways of writing it, but I think the
most basic is this. I'm going to use x as
the independent variable because that's what
your book does. But in the
applications, it's often t, time, that is the
independent variable. And, I'll try to give you
examples which show that. So, the equation
looks like this. I'll find some
function of x times y prime plus some other
function of x times y is equal to yet
another function of x. Obviously, the x doesn't
have the same status here that y does, so y is
extremely limited in how it can appear in the equation. But, x can be pretty much
arbitrary in those places. So, that's the equation
we are talking about, and I'll put it up. This is the first version of
it, and we'll call them purple. Now, why is that called
the linear equation? The word linear is a
very heavily used word in mathematics, science,
and engineering. For the moment, the
best simple answer is because it's linear in y
and y prime, the variables y and y prime. Well, y prime is not a variable. Well, you will learn,
in a certain sense, it helps to think of it as
one, not right now perhaps, but think of it as linear. The most closely analogous thing
would be a linear equation, a real linear equation, the
kind you studied in high school, which would look like this. It would have two
variables, and, I guess, constant coefficients, equal c. Now, that's a linear equation. And that's the sense in
which this is linear. It's linear in y
prime and y, which are the analogs of the
variables y1 and y2. A little bit of terminology,
if c is equal to zero, it's called homogeneous, the
same way this equation is called homogeneous, as you know
from 18.02, if the right hand side is zero. So, c of x I should
write here, but I won't. That's called homogeneous. Now, this is a common
form for the equation, but it's not what it's
called standard form. The standard form for the
equation, and since this is going to be a prime course
of confusion, which is probably completely correct, but a
prime source of confusion is what I meant. The standard linear
form, and I'll underline linear is the
first co efficient of y prime is taken to be one. So, you can always convert
that to a standard form by simply dividing
through by it. And if I do that, the equation
will look like y prime plus, now, it's common to not call
it b anymore, the coefficient, because it's really b over a. And, therefore, it's
common to adopt, yet, a new letter for it. And, the standard one
that many people use is p. How about the right hand side? We needed a letter
for that, too. It's c over a, but
we'll call it q. So, when I talk about
the standard linear form for a linear first
order equation, it's absolutely that
that I'm talking about. Now, you immediately
see that there is a potential
for confusion here because what did I call the
standard form for a first order equation? So, I'm going to say, not this. The standard first order
form, what would that be? Well, it would be y prime
equals, and everything else on the left hand side. So, it would be y prime. And now, if I turn this into
the standard first order form, it would be negative
p of x y plus q of x. But, of course, nobody
would write negative p of x. So, now, I explicitly
want to say that this is a form which I will
never use for this equation, although half the
books of the world do. In short, this poor little
first order equation belongs to two ethnic groups. It's both a first
order equation, and therefore, its standard
form should be written this way, but it's also a linear equation,
and therefore its standard form should be used this way. Well, it has to decide,
and I have decided for it. It is, above all, a
linear equation, not just a first order equation. And, in this course, this will
always be the standard form. Now, well, what on
earth is the difference? If you don't do it that
way, the difference is entirely in the sin(p). But, if you get the sign
of p wrong in the answers, it is just a disaster
from that point on. A trivial little change
of sign in the answer produces solutions and
functions which have totally different behavior. And, you are going to be
really lost in this course. So, maybe I should
draw a line through it to indicate, please
don't pay any attention to this whatsoever, except that
we are not going to do that. Okay, well, what's so
important about this equation? Well, number one, it
can always be solved. That's a very, very big thing
in differential equations. But, it's also
the equation which arises in a variety of models. Now, I'm just going
to list a few of them. All of them I think you will
need either in part one or part two of problem sets over these
first couple of problem sets, or second and third maybe. But, of them, I'm going to put
at the very top of the list of what I'll call here,
I'll give it two names: the temperature
diffusion model, well, it would be better to call it
temperature concentration by analogy, temperature
concentration model. There's the mixing model,
which is hardly less important. In other words, it's
almost as important. You have that in
your problem set. And then, there are other,
slightly less important models. There is the model
of radioactive decay. There's the model of a bank
interest, bank account, various motion models, you
know, Newton's Law type problems if you can figure
out a way of getting rid of the second derivative,
some motion problems. A classic example is the motion
of a rocket being fired off, etc., etc., etc. Now, today I have
to pick a model. And, the one I'm going to
pick is this temperature concentration model. So, this is going
to be today's model. Tomorrow's model
in the recitation, I'm asking the recitations
to, among other things, make sure they do a mixing
problem, A) to show you how to do it, and B) because
it's on the problem sets. That's not a good reason,
but it's not a bad one. The others are
either in part one or we will take them
up later in the term. This is not going to be the only
lecture on the linear equation. There will be another one
next week of equal importance. But, we can't do
everything today. So, let's talk about the
temperature concentration model, except I'm going
to change its name. I'm going to change its name to
the conduction diffusion model. I'll put conduction over
there, and diffusion over here, let's say, since, as you
will see, the similarities, they are practically
the same model. All that's changed
from one to the other is the name of the ideas. In one case, you
call it temperature, and the other, you should
call it concentration. But, the actual mathematics
isn't identical. So, let's begin with conduction. All right, so, I need a
simple physical situation that I'm modeling. So, imagine a tank
of some liquid. Water will do as
well as anything. And, in the inside
is a suspended, somehow, is a chamber. A metal cube will
do, and let's suppose that its walls are partly
insulated, not so much that no heat can get through. There is no such thing
as perfect insulation anyway, except maybe an
absolute perfect vacuum. Now, inside, so here on
the outside is liquid. Okay, on the inside
is, what I'm interested in is the temperature
of this thing. I'll call that T. Now, that's
different from the temperature of the external water bath. So, I'll call that T
sub e, T for temperature measured in Celsius, let's say,
for the sake of definiteness. But, this is the
external temperature. So, I'll indicate it with an e. Now, what is the model? Well, in other words, how do I
set up a differential equation to model the situation? Well, it's based on a physical
law, which I think you know, you've had simple
examples like this, the so called Newton's
Law of cooling, -- -- which says that
the rate of change, the temperature of the heat goes
from the outside to the inside by conduction only. Heat, of course, can
travel in various ways, by convection, by conduction,
as here, or by radiation, are the three most common. Of these, I only want one,
namely transmission of heat by conduction. And, that's the way it's
probably a little better to call it the conduction model,
rather than the temperature model, which might involve
other ways for the heat to be traveling. So, dt, the
independent variable, is not going to be x,
as it was over there. It's going to be t for time. So, maybe I should write
that down. t equals time. Capital T equals temperature
in degrees Celsius. So, you can put in the
degrees Celsius if you want. So, it's proportional to
the temperature difference between these two. Now, how shall I
write the difference? Write it this way because if you
don't you will be in trouble. Now, why do I write it that way? Well, I write it
that way because I want this constant to be
positive, a positive constant. In general, any constant, so,
parameters which are physical, have some physical
significance, one always wants to arrange the equation so
that they are positive numbers, the way people normally
think of these things. This is called the conductivity. The conductivity of what? Well, I don't
know, of the system of the situation, the
conductivity of the wall, or the wall if the metal
were just by itself. At any rate, it's a constant. It's thought of as a constant. And, why positive, well, because
if the external temperature is bigger than the
internal temperature, I expect T to rise, the
internal temperature to rise. That means dT / dt, its
slope, should be positive. So, in other words, if
Te is bigger than T, I expect this number
to be positive. And, that tells you that k
must be a positive constant. If I had turned it the
other way, expressed the difference in
the reverse order, K would then be negative,
have to be negative in order that this turn out to be
positive in that situation I described. And, since nobody wants
negative values of k, you have to write the
equation in this form rather than the
other way around. So, there's our
differential equation. It will probably have
an initial condition. So, it could be the temperature
at the starting time should be some given number, T zero. But, the condition could
be given in other ways. One can ask, what's
the temperature as time goes to infinity, for example? There are different ways of
getting that initial condition. Okay, that's the
conduction model. What would the
diffusion model be? The diffusion model,
mathematically, would be, word for word, the same. The only difference
is that now, what I imagine is I'll draw
the picture the same way, except now I'm going to put,
label the inside not with a T but with a C, C
for concentration. It's in an external
water bath, let's say. So, there is an
external concentration. And, what I'm talking
about is some chemical, let's say salt will do
as well as anything. So, C is equal to salt
concentration inside, and Ce would be the salt
concentration outside, outside in the water bath. Now, I imagine some mechanism,
so this is a salt solution. That's a salt solution. And, I imagine some mechanism
by which the salt can diffuse, it's a diffusion model
now, diffuse from here into the air or possibly
out the other way. And that's usually done
by vaguely referring to the outside as a
semi permeable membrane, semi permeable, so
that the salt will have a little hard time
getting through but permeable, so that it won't be
blocked completely. So, there's a membrane. You write the semi
permeable membrane outside, outside the inside. Well, I give up. You know, membrane somewhere. Sorry, membrane wall. How's that? Now, what's the equation? Well, the equation is
the same, except it's called the diffusion equation. I don't think Newton
got his name on this. The diffusion equation says
that the rate at which the salt diffuses across
the membrane, which is the same up to a constant
as the rate at which the concentration inside
changes, is some constant, usually called k still, okay. Do I contradict? Okay, let's keep calling it k1. Now it's different,
times Ce minus C. And, for the same
reason as before, if the external
concentration is bigger than the internal concentration,
we expect salt to flow in. That will make C rise. It will make this
positive, and therefore, we want k to be positive,
just k1 to be positive for the same reason
it had to be positive before. So, in each case, the model
that I'm talking about is the differential equation. So, maybe I should, let's
put that, make that clear. Or, I would say that this first
order differential equation models this physical
situation, and the same thing is true on the other
side over here. This is the diffusion
equation, and this is the conduction equation. Now, if you are in any doubt
about the power of differential equations, the point is,
when I talk about this thing, I don't have to say which
of these I'm following. I'll use neutral
variables like Y and X to solve these equations. But, with a single
stroke, I will be handling those situations together. And, that's the
power of the method. Now, you obviously must
be wondering, look, these look very, very special. He said he was going to talk
about the first, general first order equation. But, these look
rather special to me. Well, not too special. How should we write it? Suppose I write, let's take
the temperature equation just to have something definite. Notice that it's in a form
corresponding to Newton's Law. But it is not in the
standard linear form. Let's put it in
standard linear form, so at least you could see
that it's a linear equation. So, if I put it
in standard form, it's going to look like
DTDTD little t plus KT is equal to K times TE. Now, compare that with
the general, the way the general equation
is supposed to look, the yellow box over there,
the standard linear form. How are they going to compare? Well, this is a pretty
general function. This is general. This is a general function
of T because I can make the external temperature. I could suppose it behaves in
anyway I like, steadily rising, decaying exponentially,
maybe oscillating back and forth for some reason. The only way in which
it's not general is that this K is a constant. So, I will ask you
to be generous. Let's imagine the conductivity
is changing over time. So, this is usually
constant, but there's no law which says it has to be. How could a conductivity
change over time? Well, we could
suppose that this wall was made of slowly congealing
Jell O, for instance. It starts out as liquid,
and then it gets solid. And, Jell O doesn't
transmit heat, I believe, quite as well as
liquid does, as a liquid would. Is Jell O a solid or liquid? I don't know. Let's forget about that. So, with this understanding,
so let's say not necessarily here, but not necessarily, I
can think of this, therefore, by allowing K to vary with time. And the external temperature
to vary with time. I can think of it as a
general, linear equation. So, these models
are not special. They are fairly general. Well, I did promise you I
would solve an equation, and that this lecture, I still
have not solved any equations. OK, time to stop
temporizing and solve. So, I'm going to, in order
not to play favorites with these two models,
I'll go back to, and to get you used to thinking
of the variables all the time, that is, you know, be eclectic
switching from one variable to another according to
which particular lecture you happened to be sitting in. So, let's take our equation in
the form, Y prime plus P of XY, the general form using
the old variables equals Q of X. Solve me. Well, there are different ways
of describing the solution process. No matter how you
do it, it amounts to the same amount of
work and there is always a trick involved
at each one of them since you can't suppress
a trick by doing the problem some other way. The way I'm going to do
it, I think, is the best. That's why I'm giving it to you. It's the easiest to remember. It leads to the
least work, but I have colleagues who would
fight with me about that point. So, since they are not
here to fight with me I am free to do whatever I like. One of the main
reasons for doing it the way I'm going
to do is because I want you to get what our word
into your consciousness, two words, integrating factor. I'm going to solve this equation
by finding and integrating factor of the form U of X.
What's an integrating factor? Well, I'll show you not by
writing an elaborate definition on the board, but showing
you what its function is. It's a certain
function, U of X, I don't know what it is, but
here's what I wanted to do. I want to multiply, I'm going
to drop the X's a just so that the thing looks
less complicated. So, what I want to do is
multiply this equation through by U of X. That's why it's called
a factor because you're going to multiply
everything through by it. So, it's going to look like
UY prime plus PUY equals QU, and now, so far,
it's just a factor. What makes it an
integrating factor is that this, after I do
that, I want this to turn out to be the derivative of
something with respect to X. You see the
motivation for that. If this turns out to be the
derivative of something, because I've chosen
U so cleverly, then I will be able to solve
the equation immediately just by integrating
this with respect to X, and integrating that
with respect to X. You just, then, integrate
both sides with respect to X, and the equation is solved. Now, the only question is,
what should I choose for U? Well, if you think of
the product formula, there might be many
things to try here. But there's only one
reasonable thing to try. Try to pick U so that it's
the derivative of U times Y. See how reasonable that is? If I use the product
rule on this, the first term is
U times Y prime. The second term would
be U prime times Y. Well, I've got the Y there. So, this will work. It works if, what's the
condition that you must satisfy in order for that to be true? Well, it must be that after
it to the differentiation, U prime turns out
to be P times U. So, is it clear? This is something we want to be
equal to, and the thing I will try to do it is by
choosing U in such a way that this equality
will take place. And then I will be able
to solve the equation. And so, here's what my
U prime must satisfy. Hey, we can solve that. But please don't
forget that P is P of X. It's a function of X. So, if you separate variables,
I'm going to do this. So, what is it, DU over
U equals P of X times DX. If I integrate that,
so, separate variables, integrate, and you're going
to get DU over U integrates to the be the log of
U, and the other side integrates to be the
integral of P of X DX. Now, you can put an
arbitrary constant there, or you can think of
it as already implied by the indefinite integral. Well, that doesn't tell
us, yet, what U is. What should U be? Notice, I don't have to find
every possible U, which works. All I'm looking for is one. All I want is a single view
which satisfies that equation. Well, U equals the integral,
E to the integral of PDX. That's not too
beautiful looking, but by differential
equations, things can get so complicated
that in a week or two, you will think of this as
an extremely simple formula. So, there is a formula for
our integrating factor. We found it. We will always be able to
write an integrating factor. Don't worry about the arbitrary
constant because you only need one such U. So: no arbitrary constant
since only one U needed. And, that's the
solution, the way we solve the linear equation. OK, let's take over,
and actually do it. I think it would be
better to summarize it as a clear cut method. So, let's do that. So, what's our method? It's the method for solving
Y prime plus PY equals Q. Well, the first place, make sure
it's in standard linear form. If it isn't, you must
put it in that form. Notice, the formula for the
integrating factor, the formula for the integrating
factor involves P, the integral of PDX. So, you'd better
get the right P. Otherwise, you are sunk. OK, so put it in
standard linear form. That way, you will have
the right P. Notice that if you wrote
it in that form, and all you remembered
was E to the integral PDX, the P would have the wrong sign. If you're going to write, that
P should have a negative sign there. So, do it this way,
and no other way. Otherwise, you will get
confused and get wrong signs. And, as I say, that will
produce wrong answers, and not just slightly wrong
answers, but disastrously wrong answers from the point
of view of the modeling if you really want answers
to physical problems. So, here's a
standard linear form. Then, find the
integrating factor. So, calculate E to the integral,
PDX, the integrating factor, and that multiply both,
I'm putting this as both, underlined that as many times
as you have room in your notes. Multiply both sides by this
integrating factor by E to the integral PDX. And then, integrate. OK, let's take a simple example. Suppose we started with
the equation XY prime minus Y equals, I had X2,
X3, something like that, X3, I think, yeah, X2. OK, what's the
first thing to do? Put it in standard form. So, step zero will be to
write it as Y prime minus one over X times Y equals X2. Let's do the work first, and
then I'll talk about mistakes. Well, we now calculate
the integrating factor. So, I would do it in steps. You can integrate negative
one over X, right? That integrates to
minus log X. So, the integrating factor is E
to the integral of this, DX. So, it's E to the
negative log X. Now, in real life, that's
not the way to leave that. What is E to the negative log X? Well, think of it as E to
the log X to the minus one. Or, in other words, it is
E to the log X is X. So, it's one over X. So, the integrating
factor is one over X. OK, multiply both sides
by the integrating factor. Both sides of what? Both sides of this: the equation
written in standard form, and both sides. So, it's going to be one over
XY prime minus one over X2 Y is equal to X2 times one over
X, which is simply X. Now, if you have done
the work correctly, you should be able, now,
to integrate the left hand side directly. So, I'm going to
write it this way. I always recommend that you
put it as extra step, well, put it as an extra
step the reason for using that
integrating factor, in other words, that the left
hand side is supposed to be, now, one over X times Y prime. I always put it that
because there's always a chance you made a mistake
or forgot something. Look at it, mentally
differentiated using the product rule just
to check that, in fact, it turns out to be the same
as the left hand side. So, what do we get? One over X times Y prime
plus Y times the derivative of one over X, which indeed
is negative one over X2. And now, finally, that's 3A,
continue, do the integration. So, you're going
to get, let's see if we can do it all on one
board, one over X times Y is equal to X plus a
constant, X, sorry, X2 over two plus a constant. And, the final step
will be, therefore, now I want to isolate Y by itself. So, Y will be equal to
multiply through by X. X3 over two plus C times X.
And, that's the solution. OK, let's do one a little
slightly more complicated. Let's try this one. Now, my equation
is going to be one, I'll still keep two, Y
and X, as the variables. I'll use T and F
for a minute or two. One plus cosine X,
so, I'm not going to give you this one in
standard form either. It's a trick question. Y prime minus sine X times Y is
equal to anything reasonable, I guess. I think X, 2X, make
it more exciting. OK, now, I think
I should warn you where the mistakes are just so
that you can make all of them. So, this is mistake number one. You don't put it
in standard form. Mistake number two: generally
people can do step one fine. Mistake number two is, this
is my most common mistake, so I'm very sensitive to it. But that doesn't
mean if you make it, you'll get any sympathy from me. I don't give sympathy to myself. You are so intense,
so happy at having found the integrating
factor, you forget to multiply Q by the
integrating factor also. You just handle the left
hand side of the equation, if you forget about
the right hand side. So, the emphasis on the
both here is the right hand, please include the Q. Please include the
right hand side. Any other mistakes? Well, nothing that
I can think of. Well, maybe only,
anyway, we are not going to make any mistakes
the rest of this lecture. So, what do we do? We write this in standard form. So, it's going to look
like Y prime minus sine X, sine X divided by one
plus cosine X times Y equals, my heart
sinks because I know I'm supposed to integrate
something like this. And, boy, that's going
to give me problems. Well, not yet. With the integrating factor? The integrating
factor is, well, we want to calculate the
integral of negative sine X over one plus cosine. That's the integral of PDX. And, after that, we
have to exponentiate it. Well, can you do this? Yeah, but if you stare
at it a little while, you can see that the top is
the derivative of the bottom. That is great. That means it integrates
to be the log of one plus cosine X. Is that
right, one over one plus cosine X times the
derivative of this, which is negative cosine X. Therefore,
the integrating factor is E to that. In other words, it
is one plus cosine X. Therefore, so this
was step zero. Step one, we found the
integrating factor. And now, step two, we multiply
through the integrating factor. And what do we get? We multiply through the
standard for equation by the integrating factor, if
you do that, what you get is, well, Y prime gets the
coefficient one plus cosine X, Y prime minus sign X equals 2X. Oh, dear. Well, I hope somebody
would giggle at this point. What's giggle able about it? Well, that all this was
totally wasted work. It's called spinning
your wheels. No, it's not
spinning your wheels. It's doing what
you're supposed to do, and finding out that you
wasted the entire time doing what you were supposed to do. Well, in other words,
that net effect of this is to end up with the same
equation we started with. But, what is the point? The point of having
done all this was because now the left
hand side is exactly the derivative of something,
and the left hand side should be the derivative of what? Well, it should be
the derivative of one plus cosine X
times Y, all prime. Now, you can check that
that's in fact the case. It's one plus cosine X,
Y prime, plus minus sine X, the derivative of
this side times Y. So, if you had thought, in
looking at the equation, to say to yourself, this
is a derivative of that, maybe I'll just check
right away to see if it's the derivative of
one plus cosine X sine. You would have saved that work. Well, you don't have to
be brilliant or clever, or anything like that. You can follow your
nose, and it's just, I want to give you a positive
experience in solving linear equations,
not too negative. Anyway, so we got to this point. So, now this is 2X, and
now we are ready to solve the equation, which is the
solution now will be one plus cosine X times Y is equal
to X2 plus a constant, and so Y is equal to X2 divided
by X2 plus a constant divided by one plus cosine X. Suppose
I have given you an initial condition, which I didn't. But, suppose the initial
condition said that Y of zero were one, for instance. Then, the solution would
be, so, this is an if, I'm throwing in at the end just
to make it a little bit more of a problem, how
would I put, then I could evaluate the constant
by using the initial condition. What would it be? This would be, on the
left hand side, one, on the right hand side
would be C over two. So, I would get one
equals C over two. Is that correct? Cosine of zero is one,
so that's two down below. Therefore, C is equal to
two, and that would then complete the solution. We would be X2 plus two
over one plus cosine X. Now, you can do this
in general, of course, and get a general formula. And, we will have occasion
to use that next week. But for now, why
don't we concentrate on the most interesting
case, namely that of the most
linear equation, with constant
coefficient, that is, so let's look at
the linear equation with constant coefficient,
because that's the one that most closely models
the conduction and diffusion equations. So, what I'm interested in, is
since this is the, of them all, probably it's the
most important case is the one where P
is a constant because of its application to that. And, many of the other, the
bank account, for example, all of those will use
a constant coefficient. So, how is the
thing going to look? Well, I will use the cooling. Let's use the temperature
model, for example. The temperature
model, the equation will be DTDT plus
KT is equal to. Now, notice on the right hand
side, this is a common error. You don't put TE. You have to put KTE because
that's what the equation says. If you think units, you
won't have any trouble. Units have to be compatible on
both sides of a differential equation. And therefore, whatever the
units were for capital KT, I'd have to have the same
units on the right hand side, which indicates I cannot have KT
on the left of the differential equation, and just
T on the right, and expect the units
to be compatible. That's not possible. So, that's a good
way of remembering that if you're modeling
temperature or concentration, you have to have
the K on both sides. OK, let's do, now, a lot of this
we are going to do in our head now because this
is really too easy. What's the integrating factor? Well, the integrating factor is
going to be the integral of K, the coefficient now is just K. P is a constant, K, and if
I integrate KDT, I get KT, and I exponentiate that. So, the integrating
factor is E to the KT. I multiply through both sides,
multiply by E to the KT, and what's the
resulting equation? Well, it's going to be , I'll
write it in the compact form. It's going to be E to the
KT times T, all prime. The differentiation is now,
of course, with respect to the time. And, that's equal
to KTE, whatever that is, times E to the KT. This is a function
of T, of course, the function of little
time, sorry, little T time. OK, and now, finally, we
are going to integrate. What's the answer? Well, it is E to the, so, are we
going to get E to the KT times T is, sorry, K little t, K
times time times the temperature is equal to the integral of KTE. I'll put the fact that
it's a function of T inside just to remind
you, E to the KT, and now I'll put the
arbitrary constant. Let's put in the arbitrary
constant explicitly. So, what will T be? OK, T will look
like this, finally. It will be E to the negative KT. That's on the outside. Then, you will integrate. Of course, the difficulty
of doing this integral depends entirely upon how this
external temperature varies. But anyways, it's going to be
K times that function, which I haven't specified,
E to the KT plus C times E to the negative KT. Now, some people, many, in
fact, that almost always, in the engineering
literature, almost never write indefinite integrals
because an indefinite integral is indefinite. In other words, this covers
not just one function, but a whole multitude
of functions which differ from each other
by an arbitrary constant. So, in a formula like this,
there's a certain vagueness, and it's further
compounded by the fact that I don't know whether the
arbitrary constant is here. I seem to have put it explicitly
on the outside the way you're used to
doing from calculus. Many people, therefore,
prefer, and I think you should
learn this, to do what is done in the very first
section of the notes called definite integral solutions. If there's an initial
condition saying that the internal
temperature at time zero is some given value,
what they like to do is make this thing definite
by integrating here from zero to T, and making
this a dummy variable. You see, what that
does is it gives you a particular
function, whereas, I'm sorry I didn't put in
the DT one minus two. What it does is that
when time is zero, all this automatically
disappears, and the arbitrary constant
will then be, it's T. So, in other words, C
times this, which is one, is that equal to [T?]. In other words, if
I make this zero, that I can write C as equal to
this arbitrary starting value. Now, when you do this,
the essential thing, and we're going to come
back to this next week, but right away,
because K is positive, I want to emphasize that so much
at the beginning of the period, I want to conclude by showing
you what its significance is. This part disappears
because K is positive. The conductivity is positive. This part disappears
as T goes to zero. This goes to zero as
T goes to infinity. So, this is a
solution that remains. This, therefore, is called
the steady state solution, the thing which the
temperature behaves like, as T goes to infinity. This is called the transient. because it disappears
as T goes to infinity. It depends on the
initial condition, but it disappears,
which shows you, then, in the long run
for this type of problem the initial condition
makes no difference. The function behaves always the
same way as T goes to infinity.

Data Structures

The following content is
provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer
high quality educational resources for free.
To make a donation or to view additional materials from
hundreds of MIT courses, visit MIT OpenCourseWare at
ocw.mit.edu. OK, so last time we've seen the
curl of the vector field with components M and N.
We defined that to be N sub x minus M sub y.
And, we said this measures how far that vector field is from
being conservative. If the curl is zero,
and if the field is defined everywhere, then it's going to
be conservative. And so, when I take the line
integral along a closed curve, I don't have to compute it.
I notes going to be zero. But now, let's say that I have
a general vector field. So, the curl will not be zero.
And, I still want to compute the line integral along a closed
curve. Well, I could compute it
directly or there's another way. And that's what we are going to
see today. So, say that I have a closed
curve, C, and I want to find the work.
So, there's two options. One is direct calculation,
and the other one is Green's theorem.
So, Green's theorem is another way to avoid calculating line
integrals if we don't want to. OK, so what does it say?
It says if C is a closed curve enclosing a region R in the
plane, and I have to insist C should go counterclockwise.
And, if I have a vector field that's defined and
differentiable everywhere not only on the curve,
C, which is what I need to define the line integral,
but also on the region inside. Then -- -- the line integral
for the work done along C is actually equal to a double
integral over the region inside of curl F dA.
OK, so that's the conclusion. And, if you want me to write it
in coordinates, maybe I should do that.
So, the line integral in terms of the components,
that's the integral of M dx plus N dy.
And, the curl is (Nx-My)dA. OK, so that's the other way to
state it. So, that's a really strange
statement if you think about it because the left-hand side is a
line integral. OK, so the way we compute it is
we take this expression Mdx Ndy and we parameterize the curve.
We express x and y in terms of some variable,
t, maybe, or whatever you want to call it.
And then, you'll do a one variable integral over t.
This right-hand side here, it's a double integral,
dA. So, we do it the way that we
learn how to couple of weeks ago.
You take your region, you slice it in the x direction
or in the y direction, and you integrate dx dy after
setting up the bounds carefully, or maybe in polar coordinates r
dr d theta. But, see, the way you compute
these things is completely different.
This one on the left-hand side lives only on the curve,
while the right-hand side lives everywhere in this region
inside. So, here, x and y are related,
they live on the curve. Here, x and y are independent.
There just are some bounds between them.
And, of course, what you're integrating is
different. It's a line integral for work.
Here, it's a double integral of some function of x and y.
So, it's a very perplexing statement at first.
But, it's a very powerful tool. So, we're going to try to see
how it works concretely, what it says,
what are the consequences, how we could convince ourselves
that, yes, this works, and so on.
That's going to be the topic for today.
Any questions about the statement first?
No? OK, yeah, one remark, sorry.
So, here, it stays counterclockwise.
What if I have a curve that goes clockwise?
Well, you could just take the negative, and integrate
counterclockwise. Why does the theorem choose
counterclockwise over clockwise? How doesn't know that it's
counterclockwise rather than clockwise?
Well, the answer is basically in our convention for curl.
See, we've said curl is Nx minus My, and not the other way
around. And, that's a convention as
well. So, somehow,
the two conventions match with each other.
That's the best answer I can give you.
So, if you met somebody from a different planet,
they might have Green's theorem with the opposite conventions,
with curves going clockwise, and the curl defined the other
way around. Probably if you met an alien,
I'm not sure if you would be discussing Green's theorem
first, but just in case. OK, so that being said,
there is a warning here which is that this is only for closed
curves. OK, so if I give you a curve
that's not closed, and I tell you,
well, compute the line integral, then you have to do it
by hand. You have to parameterize the
curve. Or, if you really don't like
that line integral, you could close the path by
adding some other line integral to it,
and then compute using Green's theorem.
But, you can't use Green's theorem directly if the curve is
not closed. OK, so let's do a quick example.
So, let's say that I give you C, the circle of radius one,
centered at the point (2,0). So, it's out here.
That's my curve, C. And, let's say that I do it
counterclockwise so that it will match with the statement of the
theorem. And, let's say that I want you
to compute the line integral along C of ye^(-x) dx plus (one
half of x squared minus e^(-x)) dy.
And, that's a kind of sadistic example, but maybe I'll ask you
to do that. So, how would you do it
directly? Well, to do it directly you
would have to parameterize this curve.
So that would probably involve setting x equals two plus cosine
theta y equals sine theta. But, I'm using as parameter of
the angle around the circle, it's like the unit circle,
the usual ones that shifted by two in the x direction.
And then, I would set dx equals minus sine theta d theta.
I would set dy equals cosine theta d theta.
And, I will substitute, and I will integrate from zero
to 2pi. And, I would probably run into
a bit of trouble because I would have these e to the minus x,
which would give me something that I really don't want to
integrate. So, instead of doing that,
which looks pretty much doomed, instead, I'm going to use
Green's theorem. So, using Green's theorem,
the way we'll do it is I will, instead, compute a double
integral. So, I will -- -- compute the
double integral over the region inside of curl F dA.
So, I should say probably what F was.
So, let's call this M. Let's call this N.
And, then I will actually just choose the form coordinates,
(Nx minus My) dA. And, what is R here?
Well, R is the disk in here. OK, so, of course,
it might not be that pleasant because we'll also have to set
up this double integral. And, for that,
we'll have to figure out a way to slice this region nicely.
We could do it dx dy. We could do it dy dx.
Or, maybe we will want to actually make a change of
variables to first shift this to the origin,
you know, change x to x minus two and then switch to polar
coordinates. Well, let's see what happens
later. OK, so what is, so this is R.
So, what is N sub x? Well, N sub x is x plus e to
the minus x minus, what is M sub y,
e to the minus x, OK?
This is Nx. This is My dA.
Well, it seems to simplify a bit.
I will just get double integral over R of x dA,
which looks certainly a lot more pleasant.
Of course, I made up the example in that way so that it
simplifies when you use Green's theorem.
But, you know, it gives you an example where
you can turn are really hard line integral into an easier
double integral. Now, how do we compute that
double integral? Well, so one way would be to
set it up. Or, let's actually be a bit
smarter and observe that this is actually the area of the region
R, times the x coordinate of its center of mass.
If I look at the definition of the center of mass,
it's the average value of x. So, it's one over the area
times the double integral of x dA, well, possibly with the
density, but here I'm thinking uniform density one.
And, now, I think I know just by looking at the picture where
the center of mass of this circle will be,
right? I mean, it would be right in
the middle. So, that is two,
if you want, by symmetry.
And, the area of the guy is just pi because it's a disk of
radius one. So, I will just get 2pi.
I mean, of course, if you didn't see that,
then you can also compute that double integral directly.
It's a nice exercise. But see, here,
using geometry helps you to actually streamline the
calculation. OK, any questions?
Yes? OK, yes, let me just repeat the
last part. So, I said we had to compute
the double integral of x dA over this region here,
which is a disk of radius one, centered at,
this point is (2,0). So, instead of setting up the
integral with bounds and integrating dx dy or dy dx or in
polar coordinates, I'm just going to say, well,
let's remember the definition of a center of mass.
It's the average value of a function, x in the region.
So, it's one over the area of origin times the double integral
of x dA. If you look,
again, at the definition of x bar, it's one over area of
double integral x dA. Well, maybe if there's a
density, then it's one over mass times double integral of x
density dA. But, if density is one,
then it just becomes this. So, switching the area,
moving the area to the other side,
I'll get double integral of x dA is the area of origin times
the x coordinate of the center of mass.
The area of origin is pi because it's a unit disk.
And, the center of mass is the center of a disk.
So, its x bar is two, and I get 2 pi.
OK, that I didn't actually have to do this in my example today,
but of course that would be good review.
It will remind you of center of mass and all that.
OK, any other questions? No?
OK, so let's see, now that we've seen how to use
it practice, how to avoid calculating the line integral if
we don't want to. Let's try to convince ourselves
that this theorem makes sense. OK, so, well,
let's start with an easy case where we should be able to know
the answer to both sides. So let's look at the special
case. Let's look at the case where
curl F is zero. Then, well, we'd like to
conclude that F is conservative. That's what we said.
Well let's see what happens. So, Green's theorem says that
if I have a closed curve, then the line integral of F is
equal to the double integral of curl on the region inside.
And, if the curl is zero, then I will be integrating
zero. I will get zero.
OK, so this is actually how you prove that if your vector field
has curve zero, then it's conservative. OK, so in particular,
if you have a vector field that's defined everywhere the
plane, then you take any closed curve.
Well, you will get that the line integral will be zero.
Straightly speaking, that will only work here if the
curve goes counterclockwise. But otherwise,
just look at the various loops that it makes,
and orient each of them counterclockwise and sum things
together. So let me state that again. So,
OK, so a consequence of Green's
theorem is that if F is defined everywhere in the plane -- --
and the curl of F is zero everywhere,
then F is conservative. And so, this actually is the
input we needed to justify our criterion.
The test that we saw last time saying,
well, to check if something is a gradient field if it's
conservative, we just have to compute the
curl and check whether it's zero.
OK, so how do we prove that now carefully?
Well, you just take a closed curve in the plane.
You switch the orientation if needed so it becomes
counterclockwise. And then you look at the region
inside. And then you know that the line
integral inside will be equal to the double integral of curl,
which is the double integral of zero.
Therefore, that's zero. But see, OK,
so now let's say that we try to do that for the vector field
that was on your problems that was not defined at the origin.
So if you've done the problem sets and found the same answers
that I did, then you will have found that this vector field had
curve zero everywhere. But still it wasn't
conservative because if you went around the unit circle,
then you got a line integral that was 2pi.
Or, if you compared the two halves, you got different
answers for two parts that go from the same point to the same
point. So, it fails this property but
that's because it's not defined everywhere.
So, what goes wrong with this argument?
Well, if I take the vector field that was in the problem
set, and if I do things, say that I look at the unit
circle. That's a closed curve.
So, I would like to use Green's theorem.
Green's theorem would tell me the line integral along this
loop is equal to the double integral of curl over this
region here, the unit disk. And, of course the curl is
zero, well, except at the origin.
At the origin, the vector field is not
defined. You cannot take the
derivatives, and the curl is not defined.
And somehow that messes things up.
You cannot apply Green's theorem to the vector field.
So, you cannot apply Green's theorem to the vector field on
problem set eight problem two when C encloses the origin.
And so, that's why this guy, even though it has curl zero,
is not conservative. There's no contradiction.
And somehow, you have to imagine that,
well, the curl here is really not defined.
But somehow it becomes infinite so that when you do the double
integral, you actually get 2 pi instead of zero.
I mean, that doesn't make any sense, of course,
but that's one way to think about it.
OK, any questions? Yes?
Well, though actually it's not defined because the curl is zero
everywhere else. So, if a curl was well defined
at the origin, you would try to,
then, take the double integral. no matter what value you put
for a function, if you have a function that's
zero everywhere except at the origin,
and some other value at the origin,
the integral is still zero. So, it's worse than that.
It's not only that you can't compute it, it's that is not
defined. OK, anyway, that's like a
slightly pathological example. Yes?
Well, we wouldn't be able to because the curl is not defined
at the origin. So, you can actually integrate
it. OK, so that's the problem.
I mean, if you try to integrate, we've said everywhere
where it's defined, the curl is zero.
So, what you would be integrating would be zero.
But, that doesn't work because at the origin it's not defined.
Yes? Ah, so if you take a curve that
makes a figure 8, then indeed my proof over there
is false. So, I kind of tricked you.
It's not actually correct. So, if the curve does a figure
8, then what you do is you would actually cut it into its two
halves. And for each of them,
you will apply Green's theorem. And then, you'd still get,
if a curl is zero then this line integral is zero.
That one is also zero. So this one is zero.
OK, small details that you don't really need to worry too
much about, but indeed if you want to be
careful with details then my proof is not quite complete.
But the computation is still true.
Let's move on. So, I want to tell you how to
prove Green's theorem because it's such a strange formula that
where can it come from possibly? I mean,
so let me remind you first of all the statement we want to
prove is that the line integral along a closed curve of Mdx plus
Ndy is equal to the double integral over the region inside
of (Nx minus My)dA. And, let's simplify our lives a
bit by proving easier statements.
So actually, the first observation will
actually prove something easier, namely, that the line integral,
let's see, of Mdx along a closed curve is
equal to the double integral over the region inside of minus
M sub y dA. OK, so that's the special case
where N is zero, where you have only an x
component for your vector field. Now, why is that good enough?
Well, the claim is if I can prove this, I claim you will be
able to do the same thing to prove the other case where there
is only the y component. And then, if the other
together, you will get the general case.
So, let me explain. OK, so a similar argument which
I will not do, to save time,
will show, so actually it's just the same thing but
switching the roles of x and y, that if I integrate along a
closed curve N dy, then I'll get the double
integral of N sub x dA. And so, now if I have proved
these two formulas separately, then if you sum them together
will get the correct statement. Let me write it.
We get Green's theorem. OK, so we've simplified our
task a little bit. We'll just be trying to prove
the case where there's only an x component.
So, let's do it. Well, we have another problem
which is the region that we are looking at, the curve that we're
looking at might be very complicated.
If I give you, let's say I give you,
I don't know, a curve that does something
like this. Well, it will be kind of tricky
to set up a double integral over the region inside.
So maybe we first want to look at curves that are simpler,
that will actually allow us to set up the double integral
easily. So, the second observation,
so that was the first observation.
The second observation is that we can decompose R into simpler
regions. So what do I mean by that?
Well, let's say that I have a region and I'm going to cut it
into two. So, I'll have R1 and R2.
And then, of course, I need to have the curves that
go around them. So, I had my initial curve,
C, was going around everybody. They have curves C1 that goes
around R1, and C2 goes around R2.
OK, so, what I would like to say is if
we can prove that the statement is true, so let's see,
for C1 and also for C2 -- -- then I claim we can prove the
statement for C. How do we do that?
Well, we just add these two equalities together.
OK, why does that work? There's something fishy going
on because C1 and C2 have this piece here in the middle.
That's not there in C. So, if you add the line
integral along C1 and C2, you get these unwanted pieces.
But, the good news is actually you go twice through that edge
in the middle. See, it appears once in C1
going up, and once in C2 going down.
So, in fact, when you will do the work,
when you will sum the work, you will add these two guys
together. They will cancel.
OK, so the line integral along C will be, then,
it will be the sum of the line integrals on C1 and C2.
And, that will equal, therefore, the double integral
over R1 plus the double integral over R2, which is the double
integral over R of negative My. OK and the reason for this
equality here is because we go twice through the inner part.
What do I want to say? Along the boundary between R1
and R2 -- -- with opposite orientations.
So, the extra things cancel out. OK, so that means I just need
to look at smaller pieces if that makes my life easier.
So, now, will make my life easy? Well, let's say that I have a
curve like that. Well, I guess I should really
draw a pumpkin or something like that because it would be more
seasonal. But, well, I don't really know
how to draw a pumpkin. OK, so what I will do is I will
cut this into smaller regions for which I have a well-defined
lower and upper boundary so that I will be able to set up a
double integral, dy dx, easily.
So, a region like this I will actually cut it here and here
into five smaller pieces so that each small piece will let me set
up the double integral, dy dx.
OK, so we'll cut R in to what I will call vertically simple --
-- regions. So, what's a vertically simple
region? That's a region that's given by
looking at x between a and b for some values of a and b.
And, for each value of x, y is between some function of x
and some other function of x. OK, so for example,
this guy is vertically simple. See, x runs from this value of
x to that value of x. And, for each x,
y goes between this value to that value.
And, same with each of these. OK, so now we are down to the
main step that we have to do, which is to prove this identity
if C is, sorry, if -- -- if R is vertically
simple -- -- and C is the boundary of R going
counterclockwise. OK, so let's look at how we
would do it. So, we said vertically simple
region looks like x goes between a and b, and y goes between two
values that are given by functions of x.
OK, so this is y equals f2 of x. This is y equals f1 of x.
This is a. This is b.
Our region is this thing in here.
So, let's compute both sides. And, when I say compute,
of course we will not get numbers because we don't know
what M is. We don't know what f1 and f2
are. But, I claim we should be able
to simplify things a bit. So, let's start with the line
integral. How do I compute the line
integral along the curve that goes all around here?
Well, it looks like there will be four pieces.
OK, so we actually have four things to compute,
C1, C2, C3, and C4. OK?
Well, let's start with C1. So, if we integrate on C1 Mdx,
how do we do that? Well, we know that on C1,
y is given by a function of x. So, we can just get rid of y
and express everything in terms of x.
OK, so, we know y is f1 of x, and x goes from a to b.
So, that will be the integral from a to b of,
well, I have to take the function, M.
And so, M depends normally on x and y.
Maybe I should put x and y here. And then, I will plug y equals
f1 of x dx. And, then I have a single
variable integral. And that's what I have to
compute. Of course, I cannot compute it
here because I don't know what this is.
So, it has to stay this way. OK, next one.
The integral along C2, well, let's think for a second.
On C2, x equals b. It's constant.
So, dx is zero, and you would integrate,
actually, above a variable, y.
But, well, we don't have a y component.
See, this is the reason why we made the first observation.
We got rid of the other term because it's simplifies our life
here. So, we just get zero.
OK, just looking quickly ahead, there's another one that would
be zero as well, right?
Which one? Yeah, C4.
This one gives me zero. What about C3?
Well, C3 will look a lot like C1.
So, we're going to use the same kind of thing that we did with
C. OK, so along C3,
well, let's see, so on C3, y is a function of x,
again. And so we are using as our
variable x, but now x goes down from b to a.
So, it will be the integral from b to a of M of (x and f2 of
x) dx. Or, if you prefer,
that's negative integral from a to b of M of (x and f2 of x) dx.
OK, so now if I sum all these pieces together,
I get that the line integral along the closed curve is the
integral from a to b of M(x1f1 of x) dx minus the integral from
a to b of M(x1f2 of x) dx. So, that's the left hand side.
Next, I should try to look at my double integral and see if I
can make it equal to that. So, let's look at the other
guy, double integral over R of negative MydA.
Well, first, I'll take the minus sign out.
It will make my life a little bit easier.
And second, so I said I will try to set this up in the way
that's the most efficient. And, my choice of this kind of
region means that it's easier to set up dy dx,
right? So, if I set it up dy dx,
then I know for a given value of x, y goes from f1 of x to f2
of x. And, x goes from a to b, right?
Is that OK with everyone? OK, so now if I compute the
inner integral, well, what do I get if I get
partial M partial y with respect to y?
I'll get M back, OK? So -- So, I will get M at the
point x f2 of x minus M at the point x f1 of x.
And so, this becomes the integral from a to b.
I guess that was a minus sign, of M of (x1f2 of x) minus M of
(x1f1 of x) dx. And so, that's the same as up
there. And so, that's the end of the
proof because we've checked that for this special case,
when we have only an x component and a vertically
simple region, things work.
Then, we can remove the assumption that things are
vertically simple using this second observation.
We can just glue the various pieces together,
and prove it for any region. Then, we do same thing with the
y component. That's the first observation.
When we add things together, we get Green's theorem in its
full generality. OK, so let me finish with a
cool example. So, there's one place in real
life where Green's theorem used to be extremely useful.
I say used to because computers have actually made that
obsolete. But, so let me show you a
picture of this device. This is called a planimeter.
And what it does is it measures areas.
So, it used to be that when you were an experimental scientist,
you would run your chemical or biological experiment or
whatever. And, you would have all of
these recording devices. And, the data would go,
well, not onto a floppy disk or hard disk or whatever because
you didn't have those at the time.
You didn't have a computer in your lab.
They would go onto a piece of graph paper.
So, you would have your graph paper, and you would have some
curve on it. And, very often,
you wanted to know, what's the total amount of
product that you have synthesized, or whatever the
question might be. It might relate with the area
under your curve. So, you'd say, oh, it's easy.
Let's just integrate, except you don't have a
function. You can put that into
calculator. The next thing you could do is,
well, let's count the little squares.
But, if you've seen a piece of graph paper, that's kind of
time-consuming. So, people invented these
things called planimeters. It's something where there is a
really heavy thing based at one corner, and there's a lot of
dials and gauges and everything. And, there's one arm that you
move. And so, what you do is you take
the moving arm and you just slide it all around your curve.
And, you look at one of the dials.
And, suddenly what comes, as you go around,
it gives you complete garbage. But when you come back here,
that dial suddenly gives you the value of the area of this
region. So, how does it work?
This gadget never knows about the region inside because you
don't take it all over here. You only take it along the
curve. So, what it does actually is it
computes a line integral. OK, so it has this system of
wheels and everything that compute for you the line
integral along C of, well, it depends on the model.
But some of them compute the line integral of x dy.
Some of them compute different line integrals.
But, they compute some line integral, OK?
And, now, if you apply Green's theorem, you see that when you
have a counterclockwise curve, this will be just the area of
the region inside. And so, that's how it works.
I mean, of course, now you use a computer and it
does the sums. Yes?
That costs several thousand dollars, possibly more.
So, that's why I didn't bring one.

Probability

We've known for several videos
now that the dot product of two nonzero vectors, a and b,
is equal to the length of vector a times the length of
vector b times the cosine of the angle between them. Let me draw a and b just
to make it clear. If that's my vector a and
that's my vector b right there, the angle between
them is this angle. And we defined it in this way. And actually, if you ever want
to solve-- if you have two vectors and you want to solve
for that angle, and I've never done this before explicitly. And I thought, well, I might
as well do it right now. You could just solve
for your theta. So it would be a dot b divided
by the lengths of your two vectors multiplied by each
other is equal to the cosine of theta. And then to solve for theta
you would have to take the inverse cosine of both sides,
or the arc cosine of both sides, and you would get theta
is equal to arc cosine of a dot b over the magnitudes or
the lengths of the products of, or the lengths of each of
those vectors, multiplied by each other. So if I give you two arbitrary
vectors-- and the neat thing about it is, this might be
pretty straightforward. If I just drew something in two
dimensions right here, you could just take your protractor
out and measure this angle. But if a and b each have a
hundred components, it becomes hard to visualize the
idea of an angle between the two vectors. But you don't need to visualize
them anymore because you just have to calculate
this thing right here. You just have to calculate
this value right there. And then go to your calculator
and then type in arc cosine, or the inverse cosine that are
the equivalent functions, and it'll give you an angle. And that, by definition, is the
angle between those two vectors, which is a
very neat concept. And then you can start
addressing issues of perpendicularity and
whatever else. This was a bit of a tangent. But the other outcome that I
painstakingly proved to you in the previous video was that
the length of the cross product of two vectors is equal
to-- it's a very similar expression. It's equal to the product of the
two vectors' lengths, so the length of a times the length
of b times the sine of the angle between them. Times the sign of the
angle between them. So it's the same angle. So what I want to do is take
these two ideas and this was a bit of a diversion there just
to kind of show you how to solve for theta because I
realized I've never done that for you before. But what I want to do is I want
to take this expression up here and this expression up
here and see if we can develop an intuition, at least in R3
because right now we've only defined our cross product. Or the cross product of two
vectors is only defined in R3. Let's take these two ideas in
R3 and see if we can develop an intuition. And I've done a very similar
video in the physics playlist where I compare the dot product
to the cross product. Now, if I'm talking about--
let me redraw my vectors. So the length of a--
so let me draw a. b, I want to do it
bigger than that. So let me do it like that. So that is my vector b. So this is b. That is a. What is the length of a times
the length of b times the cosine of the angle? So let me do that right there. So this is the angle. So the length of a if I were to
draw these vectors is this length right here. This is the length of a. It's this distance right
here, the way I've drawn this vector. So this is, literally, the
length of vector a. And I'm doing it in R3 or maybe
a version of it that I can fit onto my little
blackboard right here. So it'll just be the length
of this line right there. And then the length of
b is the length of that line right there. So that is the length of b. Let me rewrite this
thing up here. Let me write it as b, the length
of b times the length-- and I want to be careful. I don't want to do the dot there
because you'll think it's a dot product. Times a cosine of theta. All I did is I rearranged
this thing here. It's the same thing
as a dot b. Well what is a times the
cosine of theta? Let's get out our basic
trigonometry tools-- SOH CAH TOA. Cosine of theta is equal to
adjacent over hypotenuse. So if I drop, if I create a
right triangle here, and let me introduce some new colors
just to ease the monotony. If I drop a right triangle
right here and I create a right triangle right there, and
this is theta, than what is the cosine of theta? It's equal to this. Let me do it another color. It's equal to this,
the adjacent side. It's equal to this little
magenta thing. Not all of b, just this
part that goes up to my right angle. That's my adjacent. I want to do it a little
bit bigger. It's equal to the adjacent
side over the hypotenuse. So let me write this down. So cosine of theta is equal to
this little adjacent side. I'm just going to write
it like that. Is equal to this adjacent side
over the hypotenuse. But what is the hypotenuse? It is the length of vector a. It's this. That's my hypotenuse
right there. So my hypotenuse is the
length of vector a. And so if I multiply both sides
by the length of vector a I get the length of vector a
times the cosine of theta is equal to the adjacent side. I'll do that in magenta. So this expression right here,
which was just a dot b can be rewritten as-- I just told you
that the length of vector a times cosine of theta is equal
to this little magenta adjacent side. So this is equal to
the adjacent side. So you can view a dot b as being
equal to the length of vector b-- that length-- times
that adjacent side. And you're saying, Sal, what
does that do for me? Well what it tells you
is you're multiplying essentially, the length of
vector b times the amount of vector a that's going in the
same direction as vector b. You can kind of view this as
the shadow of vector a. And I'll talk about projections
in the future. And I'll more formally define
them, but if the word projection helps you, just
think of that word. If you have a light that shines
down from above, this adjacent side is kind of like
the shadow of a onto vector b. And you can imagine, if these
two vectors-- if our two vectors looked more like this,
if they were really going in the same direction. Let's say that's vector a and
that's vector b, then the adjacent side that I care about
is going to-- they're going to have a lot
more in common. The part of a that is going in
the same direction of b will be a lot larger. So this will have a larger
dot product. Because the dot product is
essentially saying, how much of those vectors are going
in the same direction? But it's just a number, so it
will just be this adjacent side times the length of b. And what if I had vectors that
are pretty perpendicular to each other? So what if I had two vectors
that were like this? What if my vector a looked
like that and my vector b looked like that? Well now the adjacent, the way
I define it here, if I had to make a right triangle
like that, the adjacent side's very small. So you're dot product, even
though a is still a reasonably large vector, is now much
smaller because a and b have very little commonality
in the same direction. And you can do it
the other way. You could draw this down like
that and you could do the adjacent the other way, but it
doesn't matter because these a's and b's are arbitrary. So the take away is the fact
that a dot b is equal to the lengths of each of those times
the cosine of theta. To me it says that the dot
product tells me how much are my vectors moving together? Or the product of the
part of the vectors that are moving together. Product of the lengths of the
vectors that are moving together or in the
same direction. You could view this adjacent
side here as the part of a that's going in the
direction of b. That's the part of a that's
going in the direction of b. So you're multiplying
that times b itself. So that's what the
dot product is. How much are two things going
in the same direction. And notice, when two things are
orthogonal or when they're perpendicular-- when a dot b is
equal to 0, we say they're perpendicular. And that makes complete sense
based on this kind of intuition of what the dot
product is doing. Because that means that
they're perfectly perpendicular. So that's b and that's a. And so the adjacent part of a,
if I had to draw a right trianlge, it would come
straight down. And if I were to say the
projection of a and I haven't draw that. Or if I put a light shining down
from above and I'd say what's the shadow of a onto b? You'd get nothing. You'd get 0. This arrow has no width, even
though I've drawn it to have some width. It has no width. So you would have
a 0 down here. The part of a that goes in
the same direction as b. No part of this vector
goes in the same direction as this vector. So you're going to have this 0
kind of adjacent side times b, so you're going to get
something that's 0. So hopefully that makes
a little sense. Now let's think about
the cross product. The cross product tells us well,
the length of a cross b, I painstakingly showed, you is
equal to the length of a times the length of b times the sin
of the angle between them. So let me do the same example. Let me draw my two vectors. That's my vector a and
this is my vector b. And now sin-- SOH CAH TOA. So sin of theta, let
me write that. Sin of theta-- SOH CAH TOA-- is
equal to opposite over the hypotenuse. So if I were to draw a little
right triangle here, so if I were to draw a perpendicular
right there, this is theta. What is the sin of theta equal
to in this context? The sin of theta is
equal to what? It's equal to this
side over here. Let me call that just
the opposite. It's equal to the opposite
side over the hypotenuse. So the hypotenuse is
the length of this vector a right there. It's the length of
this vector a. So the hypotenuse is the length
over my vector a. So if I multiply both sides of
this by my length of vector a, I get the length of vector a
times the sin of theta is equal to the opposite side. So if we rearrange this a little
bit, I can rewrite this as equal to-- I'm just
going to swap them. I have to do the dot
product as well. This is equal to b, the length
of vector b, times the length of vector a sin of theta. Well this thing is just the
opposite side as I've defined it right here. So this right here is just
the opposite side, this side right there. So when we're taking the cross
product, we're essentially multiplying the length of vector
b times the part of a that's going perpendicular
to b. This opposite side is the
part of a that's going perpendicular to b. So they're kind of
opposite ideas. The dot product, you're
multiplying the part of a that's going in the same
direction as b with b. While when you're taking the
cross product, you're multiplying the part of
a that's going in the perpendicular direction to
b with the length of b. It's a measure, especially when
you take the length of this, it's a measure of
how perpendicular these two guys are. And this is, it's a measure of
how much do they move in the same direction? And let's just look at
a couple of examples. So if you take two
right triangles. So if that's a and that's-- or
if you take two vectors that are perpendicular to each other,
the length of a cross b is going to be equal to-- if we
just use this formula right there-- the length of a
times the length of b. And what's the sin
of 90 degrees? It's 1. So in this case you kind of have
maximized the length of your cross product. This is as high as it can go. Because sin of theta, it's
a maximum value. Sin of theta is always less
than or equal to 1. So this is as good as you're
ever going to get. This is the highest possible
value when you have perfectly perpendicular vectors. Now, when is-- actually, just to
kind of go back to make the same point here. When do you get the maximum
value for your cosine of-- for your dot product? Well, it's when your two
vectors are collinear. If my vector a looks like that
and my vector b is essentially another vector that's going
in the same direction, then theta is 0. There's no angle between them. And then you have a dot b is
equal to the magnitude or the length of vector a times the
length of vector b times the cosine of the angle
between them. The cosine of the angle between
them, the cosine of that angle is 0. Or the angle is 0, so the
cosine of that is 1. So when you have two vectors
that go exactly in the same direction or they're collinear,
you kind of maximize your dot product. You maximize your cross product
when they're perfectly perpendicular to each other. And just to make the analogy
clear, when they're perpendicular to each other
you've minimized-- or at least the magnitude of your
dot product. You can get negative dot
products, but the absolute size of your dot product, the
absolute value of your dot product is minimized
when they're perpendicular to each other. Similarly, if you were to take
two vectors that are collinear and they're moving in the same
direction, so if that's vector a, and then I have vector b that
just is another vector that I want to draw them
on top of each other. But I think you get the idea. Let's say vector
b is like that. Then theta is 0. You can't even see it. It's been squeezed out. I've just brought these two
things on top of each other. And then the cross product in
this situation, a cross b is equal to-- well, the length of
both of these things times the sin of theta. Sin of 0 is 0. So it's just 0. So two collinear vectors, the
magnitude of their cross product is 0. But the magnitude of their dot
product, the a dot b, is going to be maximized. It's going to be as high
as you can get. It's going to be the length of
a times the length of b. Now the opposite scenario
is right here. When they're perpendicular to
each other, the cross product is maximized because it's
measuring on how much of the vectors-- how much of the
perpendicular part of a is-- multiplying that times
the length of b. And then when you have two
orthogonal vectors, your dot product is minimized,
or the absolute value of your dot product. So a dot b in this case,
is equal to 0. Anyway, I wanted to make all
of this clear because sometimes you kind of get into
the formulas and the definitions and you lose the
intuition about what are all of these ideas really for? And actually, before I move on,
let me just make another kind of idea about what the
cross product can be interpreted as. Because a cross product tends
to give people more trouble. That's my a and that's my b. What if I wanted to figure
out the area of this parallelogram? If I were to shift a and have
that there and if I were to shift b and draw a line parallel
to b, and if I wanted to figure out the area of this
parallelogram right there, how would I do it just using
regular geometry? Well I would drop a
perpendicular right there. This is perpendicular and this
length is h for height. Then the area of this, the area
of the parallelogram is just equal to the length of my
base, which is just the length of vector b times my height. But what is my height? Let me just draw a little
theta there. Let me do a green theta,
it's more visible. So theta. So we know already that the sin
of this theta is equal to the opposite over
the hypotenuse. So it's equal to the height
over the hypotenuse. The hypotenuse is just the
length of vector a. So it's just the length
of vector a. Or we could just solve for
height and we'd get the height is equal to the length
of vector a times the sin of theta. So I can rewrite this here. I can replace it with that and
I get the area of this parallelogram is equal to the
length of vector b times the length of vector a sin theta. Well this is just the length
of the cross product of the two vectors, a cross b. This is the same thing. I mean you can rearrange
the a and the b. So we now have another way of
thinking about what the cross product is. The cross product of two
vectors, or at least the magnitude or the length of the
cross product of two vectors-- obviously, the cross product
you're going to get a third vector. But the length of that third
vector is equal to the area of the parallelogram that's defined
or that's kind of-- that you can create from
those two vectors. Anyway, hopefully you found this
a little bit intuitive and it'll give you a little bit
more of kind of a sense of what the dot product and cross
product are all about.

Probability

Today, once again,
a day of solving no differential equations
whatsoever. The topic is a special kind of
differential equation, which occurs a lot.
It's one in which the right-hand side doesn't have any
independent variable in it. Now, since I'm going to use as
the independent variable, t for time, maybe it would be
better to write the left-hand side to let you know,
since you won't be able to figure out any other way what it
is, dy dt. We will write it this time.
dy dt is equal to, and the point is that there is
no t on the right hand side. So, there's no t.
There's a name for such an equation.
Now, some people call it time independent.
The only problem with that is that sometimes the independent
variable is a time. It's something else.
We need a generic word for there being no independent
variable on the right-hand side. So, the word that's used for
that is autonomous. So, that means no independent
variable on the right-hand side. It's a function of y alone,
the dependent variable. Now, your first reaction should
be, oh, well, big deal.
Big deal. If there's no t on the right
hand side, then we can solve this by separating variables.
So, why has he been talking about it in the first place?
So, I admit that. We can separate variables,
and what I'm going to talk about today is how to get useful
information out of the equation about how its solutions look
without solving the equation. The reason for wanting to do
that is, A, it's fast. It gives you a lot of insight,
and the actual solution, I'll illustrate one,
in the first place, take you quite a while.
You may not be able to actually do the integrations,
the required and separation of variables to get an explicit
solution, or it might simply not be worth the effort of doing if
you only want certain kinds of separations about the solution.
So, the thing is, the problem is,
therefore, to get qualitative information about the solutions
without actually solving -- Without actually having to
solve the equation. Now, to do that,
let's take a quick look at how the direction fields of such an
equation, after all, it's the direction field is our
principal tool for getting qualitative information about
solutions without actually solving.
So, how does the direction field look?
Well, think about it for just a second, and you will see that
every horizontal line is an isocline.
So, the horizontal lines, what are their equations?
This is the t axis. And, here's the y axis.
The horizontal lines have the formula y equals a constant.
Let's make it y equals a y zero for different values of the
constant y zero. Those are the horizontal lines.
And, the point is they are isoclines.
Why? Well, because along any one of
these horizontal lines, I'll draw one in,
what are the slopes of the line elements?
The slopes are dy / dt is equal to f of y zero,
but that's a constant because there's no t to change as you
move in the horizontal direction.
The slope is a constant. So, if I draw in that isocline,
I guess I've forgotten, as our convention,
isoclines are in dashed lines, but if you have color,
you are allowed to put them in living yellow.
Well, I guess I could make them solid, in that case.
I don't have to make a dash. Then, all the line elements,
you put them in at will because they will all have,
they are all the same, and they have slope,
that, f of y0. And, similarly down here,
they'll have some other slope. This one will have some other
slope. Whatever, this is the y zero,
the value of it, and whatever that happens to
be. I'll put it one more.
That's the x-axis. I can use the x-axis.
That's an isocline, too.
Now, what do you deduce about how the solutions must look?
Well, let's draw one solution. Suppose one solution looks like
this. Well, that's an integral curve,
in other words. Its graph is a solution.
Now, as I slide along, these slope elements stay
exactly the same, I can slide this curb along
horizontally, and it will still be an
integral curve everywhere. So, in other words,
they integral curves are invariant under translation for
an equation of this type. They all look exactly the same,
and you get them all by taking one, and just pushing it along.
Well, that's so simple it's almost uninteresting,
except in that these equations occur a lot in practice.
They are often hard to integrate directly.
And, therefore, it's important to be able to
get information about them. Now, how does one do that?
There's one critical idea, and that is the notion of a
critical point. These equations have what are
called critical points. And, what it is is very simple.
There are three ways of looking at it: critical point,
y zero; what does it mean for y0 to be
a critical point? It means, another way of saying
it is that it should be a zero of the right-hand side.
So, if I ask you to find the critical points for the
equation, what you will do is solve the equation f of y equals
zero. Now, what's interesting about
them? Well, for a critical point,
what would be the slope of the line element along,
if this is at a critical level, if that's a critical point?
Look at that isocline. What's the slope of the line
elements along it? It is zero.
And therefore, for these guys,
these are, in other words, our solution curves.
But let's prove it formally. So, there are three ways of
saying it. y zero is a critical point. It's a zero of the right-hand
side, or, y equals y0 is a solution to the equation.
Now, that's perfectly easy to verify.
If y zero makes this right-hand side zero,
it's certainly also y equals y0 makes the left-hand
side zero because you're differentiating a constant.
So, the reasoning, if you want reasoning,
is proof. Maybe we can make one line out
of a proof. To say that it's a solution,
what does it mean to say that it's a solution?
It means to say that when you plug it in, plug in this
constant function, y0, the dy0 dt is equal to f of
y0. Is that true?
Yeah. Both sides are zero.
It's true. Now, y0 is not a number.
Well, it is. It's a number on this side,
but on this side, what I mean is a constant
function whose constant value is y zero, this function, and its derivatives are zero
because it has slope zero everywhere.
So, this guy is a constant function, has slope zero.
This is a number which makes the right-hand side zero.
Well, that's nice. So, in other words,
what we found are, by finding these critical
points, solving that equation, we found all the horizontal
solutions. But, what's so good about
those? Surely, they must be the most
interesting solutions there are. Well, think of how the picture
goes. Let's draw in one of those
horizontal solutions. So, here's a horizontal
solution. That's a solution.
So, this is my y0. That's the height at which it
is. And, I'm assuming that f of y0
equals zero. So, that's a solution.
Now, the significance of that is, because it's a solution,
in other words, it's an integral curve,
remember what's true about integral curves.
Other curves are not allowed to cross them.
And therefore, these things are the absolute
barriers. So, for example,
suppose I have two of them is y0, and let's say here's another
one, another constant solution. I want to know what the curves
in between those can do. Well, I do know that whatever
those red curves do, the other integral curves,
they cannot cross this, and they cannot cross that.
And, you must be able to translate them along each other
without ever having two of them intersect.
Now, that really limits their behavior, but I'm going to nail
it down even more. So, other curves can't cross
these. Other integral curves can't
cross these yellow curves, these yellow lines,
these horizontal lines. But, I'm going to show you
more, and namely, so what I'm after is deciding,
without solving the equation, what those curves must look
like in between. Now, the way to do that is you
draw, so if we want to make steps, everybody likes steps,
okay, so step one is going to be, find these.
Find the critical points. And, you're going to do that by
solving this equation, finding out where it's zero.
Once you have done that, you are going to draw the graph
of f of y. And, the interest is going to
be, where is it positive? Where is it negative?
You've already found where it's zero.
Everywhere else, therefore, it must be either
positive or negative. Now, once you have found that
out, why am I interested in that?
Well, because dy / dt is equal to f of y, right? That's what the differential
equation says. Therefore, if this,
for example, is positive,
that means this must be positive.
It means that y must be increasing.
It means the solution must be increasing.
Where it's negative, the solution will be
decreasing. And, that tells me how it's
behaving in between these yellow lines, or on top of them,
or on the bottom. Now, at this point,
I'm going to stop, or not stop,
I mean, I'm going to stop talking generally.
And everything in the rest of the period will be done by
examples which will get increasingly complicated,
not terribly complicated by the end.
But, let's do one that's super simple to begin with.
Sorry, I shouldn't say that because some of you may be
baffled even by here because after all I'm going to be doing
the analysis not in the usual way, but by using new ideas.
That's the way you make progress.
All right, so, let's do our bank account.
So, y is money in the bank account.
r is the interest rate. Let's assume it's a continuous
interest rate. All banks nowadays pay interest
continuously, the continuous interest rate.
So, if that's all there is, and money is growing,
you know the differential equation says that the rate at
which it grows is equal to r, the interest rate times a
principle, the amount that's in the bank at that time.
So, that's the differential equation that governs that.
Now, that's, of course, the solution is
simply an exponential curve. There's nothing more to say
about it. Now, let's make it more
interesting. Let's suppose there is a shifty
teller at the bank, and your money is being
embezzled from your account at a constant rate.
So, let's let w equal, or maybe e, but e has so many
other uses in mathematics, w is relatively unused,
w is the rate of embezzlement, thought of as continuous.
So, every day a little bit of money is sneaked out of your
account because you are not paying any attention to it.
You're off skiing somewhere, and not noticing what's
happening to your bank account. So, since it's the rate,
the time rate of embezzlement, I simply subtract it from this.
It's not w times y because the embezzler isn't stealing a
certain fraction of your account.
It's simply stealing a certain number of dollars every day,
the same number of dollars being withdrawn for the count.
Okay, now, of course, you could solve this.
This separates variables immediately.
You get the answer, and there's no problem with
that. Let's analyze the behavior of
the solutions without solving the equation by using these two
points. So, I want to analyze this
equation using the method of critical points.
So, the first thing I should do is, so, here's our equation,
is find the critical points. Notice it's an autonomous
equation all right, because there's no t on the
right-hand side. Okay, so, the critical points,
well, that's where ry minus w equals zero.
In other words, there's only one critical
point, and that occurs when y is equal to w over r.
So, that's the only critical point.
Now, I want to know what's happening to the solution.
So, in other words, if I plot, I can write away,
of course, negative values aren't of particularly
interesting here, there is definitely a
horizontal solution, and it has the value,
it's at the height, w over r.
That's a solution. The question is,
what do the other solutions look like?
Now, watch how I make the analysis because I'm going to
use two now. So, this is step one,
then step two. What do I do? Well, 
I'm going to graph f of y. Well, f of y is ry minus w. What does that look like? Okay, so, here is the y-axis.
Notice the y-axis is going horizontally because what I'm
interested in is the graph of this function.
What do I call the other axis? I'm going to use the same
terminology that is used on the little visual that describes
this. And, that's dy.
You could call this other axis the f of y axis.
That's not a good name for it. You could call it the dy / dt
axis because it's, so to speak,
the other variable. That's not great either.
But, worst of all would be introducing yet another letter
for which we would have no use whatsoever.
So, let's think of it. We are plotting,
now, the graph of f of y. f of y is this function, ry minus w.
Well, that's a line. Its intercept is down here at
w, and so the graph looks something like this.
It's a line. This is the line,
ry minus w. It has slope r.
Well, what am I going to get out of that line?
Just exactly this. What am I interested in about
that line? Nothing other than where is it
above the axis, and where is it below?
This function is positive over here, and therefore,
I'm going to indicate that symbolically,
this, by putting an arrow here. The meeting of this arrow is
that y of t is increasing.
See where it's the right-hand side of that last board?
y of t is increasing when f of y is positive.
f of y is positive here, and therefore,
to the right of this point, it's increasing.
Here to the left of it, f of y is negative,
and therefore over here it's going to be decreasing.
What point is this, in fact?
Well, that's where it crosses the axis.
That's exactly the critical point, w over r.
Therefore, what this says is that a solution,
once it's bigger than y over r, it increases,
and it increases faster and faster because this function is
higher and higher. And, that represents the rate
of change. So, in other words,
once the solution, let's say a solution starts
over here at time zero. So, this is the t axis.
And, here is the y axis. So, now, I'm plotting
solutions. If it starts at t equals zero,
above this line, that is, starts with the value
w over r, which is bigger than zero,
a value bigger than w over r, then it increases,
and increases faster and faster.
If it starts below that, it decreases and decreases
faster and faster. Now, in fact,
I only have to draw two of those because what do all the
others look like? They are translations.
All the other curves look exactly like those.
They are just translations of them.
This guy, if I start closer, it's still going to decrease.
Well, that's supposed to be a translation.
Maybe it is. So, these guys look like that.
Let's do just a tiny bit more interpretation of that.
Well, I think I better leave it there because we've got harder
things to do, and I want to make sure we've
got time for it. Sorry.
Okay, next example, a logistic equation.
Some of you have already solved this in recitation,
and some of you haven't. This is a population equation.
This is the one that section 7.1 and section 1.7 is most
heavily concerned with, this particular equation.
The derivation of it is a little vague.
It's an equation which describes how population
increases. And one minute,
the population behavior of some population, --
-- let's call it, y is the only thing I know to
call anything today, but of course your book uses
capital P for population, to get you used to different
variables. Now, the basic population
equation runs dy / dt. There's a certain growth rate.
Let's call it k y. So, k is what's called the
growth rate. It's actually,
sometimes it's talked about in terms of birthrate.
But, it's the net birth rate. It's the rate at which people,
or bacteria, or whatever are being born
minus the rate at which they are dying.
So, it's a net birthrate. But, let's just call it the
growth rate. Now, if this is the equation,
we can think of this, if k is constant,
that's what's called simple population growth.
And you are all familiar with that.
Logistical growth allows for slightly more complex
situations. Logistic growth says that
calling k a constant is unrealistic because the Earth is
not filled entirely with people. What stops it from having
unlimited growth? Well, the fact that the
resources, the food, the organism has to live on
gets depleted. And, in other words,
the growth rate declines as y increases.
As the population increases, one expects the growth rate to
decline because resources are being used up,
and they are not indefinitely available.
Well, in other words, we should replace k by a
function with this behavior. What's the simplest function
that declines as y increases? The simplest choice,
and if you are ignorant about what else to do,
stick with the simplest, at least you won't work any
harder than you have to, would be to take k equal to the
simplest declining function of y there is, which is simply a
linear function, A minus BY.
So, if I use that as the choice of the declining growth rate,
the new equation is dy / dt equals, here's my new k.
The y stays the same, so the equation becomes a minus
by, the quantity times y, or in other words, ay minus b y squared. This equation is what's called
the logistic equation. It has many applications,
not just to population growth. It's applied to the spread of
disease, the spread of a rumor, the spread of many things.
Yeah, a couple pieces of chalk here. Okay, now, those of you who
have solved it know that the explicit solution involves,
well, you separate variables, but you will have to use
partial fractions, ugh, I hope you love partial
fractions. You're going to need them later
in the term. But, I could avoid them now by
not solving the equation explicitly.
But anyway, you get a solution, which I was going to write on
the board for you, but you could look it up in
your book. It's unpleasant enough looking
to make you feel that there must be an easier way at least to get
the basic information out. Okay, let's see if we can get
the basic information out. What are the critical points?
Well, this is pretty easy. A, I want to set the right-hand
side equal to zero. So, I'm going to solve the
equation. I can factor out a y.
It's going to be y times a minus by equals zero. And therefore,
the critical points are where y equals zero. That's one. And, the other factor is when
this factor is zero, and that happens when y is
equal to a over b. So, there are my two critical
points. Okay, what does,
let's start drawing pictures of solutions.
Let's put it in those right away.
Okay, the critical point, zero, gives me a solution that
looks like this. And, the critical point,
a over b, those are positive numbers.
So, that's somewhere up here. So, those are two solutions,
constant solutions. In other words,
if the population by dumb luck started at zero,
it would stay at zero for all time.
That's not terribly surprising. But, it's a little less obvious
that if it starts at that magic number, a over b,
it will also stay at that magic number for all time without
moving up or down or away from it.
Now, the question is, therefore, what happens in
between? So, for the in between,
I'm going to make that same analysis that I made before.
And, it's really not very hard. Look, so here's my dy/dt-axis.
I'll call that y prime, okay?
And, here's the y-axis. So, I'm now doing step two.
This was step one. Okay, the function that I want
to graph is this one, ay minus b y squared,
or in factor form, y times a minus by. Now, this function,
we know, has a zero. It has a zero here,
and it has a zero at the point a over b.
At these two critical points, it has a zero.
What is it doing in between? Well, in between,
it's a parabola. It's a quadratic function.
It's a parabola. Does it go up or does it go
down? Well, when y is very large,
it's very negative. That means it must be a
downward-opening parabola. And therefore,
this curve looks like this. So, I'm interested in knowing,
where is it positive, and where is it negative?
Well, it's positive, here, for this range of values
of y. Since it's positive there,
it will be increasing there. Here, it's negative,
and therefore it will be decreasing.
Here, it's negative, and therefore,
dy / dt will be negative also, and therefore the function,
y, will be decreasing here. So, how do these other
solutions look? Well, we can put them in.
I'll put them in in white, okay, because this has got to
last until the end of the term. So, how are they doing?
They are increasing between the two curves.
They are not allowed to cross either of these yellow curves.
But, they are always increasing.
Well, if they're always increasing, they must start here
and increase, and not allowed to cross.
It must do something like that. This must be a translation of
it. In other words,
the curves must look like that. Those are supposed to be
translations of each other. I know they aren't,
but use your imaginations. But what's happening above?
So in other words, if I start with a population
anywhere bigger than zero but less than a over b,
it increases asymptotically to the level a over b.
What happens if I start above that?
Well, then it decreases to it because, this way,
for the values of y bigger than a over b,
it decreases as time increases. So, these guys up here are
doing this. And, how about the ones below
the axis? Well, they have no physical
significance. But let's put them in anyway.
Whether they doing? They are decreasing away from
zero. So, these guys don't mean
anything physically, but mathematically they exist.
Their solutions, they're going down like that.
Now, you notice from this picture that there are,
even though both of these are constant solutions,
they have dramatically different behavior.
This one, this solution, is the one that all other
solutions try to approach as time goes to infinity.
This one, the solution zero, is repulsive,
as it were. Any solution that starts near
zero, if it starts at zero, of course, it stays there for
all time, but if it starts just a little bit above zero,
it increases to a over b. This is called a stable solution because everybody tries
to get closer and closer to it. This is called,
zero is also a constant solution, but this is an
unstable solution. And now, usually,
solution is too general a word. I think it's better to call it
a stable critical point, and an unstable critical point.
But, of course, it also corresponds to a
solution. So, critical points are not all
the same. Some are stable,
and some are unstable. And, you can see which is which
just by looking at this picture. If the arrows point towards
them, you've got a stable critical point.
If it arrows point away from them, you've got an unstable
critical point. Now, there is a third
possibility. Okay, I think we'd better
address it because otherwise you're going to sit there
wondering, hey, what did he do?
Suppose it looks like this. Suppose it were just tangent.
Well, this is the picture of that curve, the pink curve.
What would the arrows look like then?
What would the arrows look like then?
Well, since they are positive, it's always positive,
the arrow goes like this. And then on the side,
it also goes in the same direction.
So, is this critical point stable or unstable?
It's stable if you approach it from the left.
So, how, in fact, do the curves,
how would the corresponding curves look?
Well, there's our long-term solution.
This corresponds to that point. Let's call this a,
and then this will be the value, a.
If I start below it, I rise to it.
If I start above it, I increase.
So, if I start above it, I do this.
Well, now, that's stable on one side, and unstable on the other.
And, that's indicated by saying it's semi-stable.
That's a brilliant word. I wonder how long it to do
think that one up, semi-stable critical point:
stable on one side, unstable on the other depending
on whether you start below it. And, of course,
it could be reversed if I had drawn the picture the other way.
I could have approached it from the top, and left it from below.
You get the idea of the behavior.
Okay, let's now take, I'm going to soup up this
logistic equation just a little bit more.
So, let's talk about the logistic equation.
But, I'm going to add to it harvesting, with harvesting.
So, this is a very late 20th century concept.
So, we imagine, for example,
a bunch of formerly free range Atlantic salmon penned in one of
these huge factory farms off the coast of Maine or someplace.
They've made salmon much cheaper than it used to be,
but at a certain cost to the salmon, and possibly to our
environment. So, what happens?
Well, the salmon grow, and grow, and do what salmon
do. And, they are harvested.
That's a word somewhere in the category of ethnic cleansing in
my opinion. But, it's, again,
a very 20th-century word. I think it was Hitler who
discovered that, that all you had to do was call
something by a sanitary name, and no matter how horrible it
was, good bourgeois people would accept it.
So, the harvesting, which means,
of course, picking them up and killing them,
and putting them in cans and stuff like that,
okay, so what's the equation? I'm going to assume that the
harvest is at a constant time rate.
In other words, it's not a certain fraction of
all the salmon that are being caught each day and canned.
The factory has a certain capacity, so,
400 pounds of salmon each day are pulled out and canned.
So, it's a constant time rate. That means that the equation is
now going to be dy/dt is equal to, well, salmon grow
logistically. ay minus b y squared, 
so, that part of the equation is the same.
But, I need a term to take care of this constant harvesting
rate, and that will be h. Let's call it h,
not h times y. Then, I would be harvesting a
certain fraction of all the salmon there,
which is not what I'm doing. Okay: our equation.
Now, I want to analyze what the critical points of this look
like. Now, this is a little more
subtle because there's now a new parameter, there.
And, what I want to see is how that varies with the new
parameter. The best thing to do is,
I mean, the thing not to do is make this equal to zero,
fiddle around with the quadratic formula,
get some massive expression, and then spend the next half
hour scratching your head trying to figure out what it means,
and what information you are supposed to be getting out of
it. Draw pictures instead.
Draw pictures. If h is zero,
that's the smallest harvesting rate I could have.
The picture looks like our old one.
So, if h is zero, the picture looks like,
what color did I, okay, pink.
Yellow. Yellow is the cheapest,
but I can't find it. Okay, yellow is commercially
available. These are precious.
All right, purple if it's okay, purple.
So, this is the one, our original one corresponding
to h equals zero. Or, in other words,
it's the equation ay minus b y squared. h is zero. Now, if I want to find,
I now want to increase the value of h, well,
if I increase the value of h, in other words,
harvest more and more, what's happening?
Well, I simply lower this function by h.
So, if I lower h somewhat, it will come to here.
So, this is some value, ay minus b y squared minus h1, let's say.
That's this curve. If I lower it a lot,
it will look like this. So, ay minus b y squared minus
h a lot, h twenty. This doesn't mean anything. Two.
Obviously, there's one interesting value to lower it
by. It's a value which would lower
it exactly by this amount. Let me put that in special.
If I lower it by just that amount, the curve always looks
the same. It's just been lowered.
I'm going to say this one is, so this one is the same thing,
except that I've subtracted h sub m. Where is h sub m on the picture?
Well, I lowered it by this amount.
So, this height is h sub m. In other words,
if I find the maximum height here, which is easy to do
because it's a parabola, and lower it by exactly that
amount, I will have lowered it to this point.
This will be a critical point. Now, the question is,
what's happened to the critical point as I did this?
I started with the critical points here and here.
As I lower h, the critical point changed to
this and that. And now, it changed to this one
when I got to the purple line. And, as I went still further
down, there were no critical points.
So, this curve has no critical points attached to it.
What are the corresponding pictures?
Well, the corresponding pictures, well,
we've already drawn, the picture for h equals zero
is drawn already. The pictures that I'm talking
about are how the solutions look.
How would the solution look like for this one for h one? For h1, the solutions look
like, here is a over b. Here is a over b,
but the critical points aren't at zero and a over b anymore.
They've moved in a little bit. So, they are here and here.
And, otherwise, the solutions look just like
they did before, and the analysis is the same.
And, similarly, if h two goes very far,
if h2 is very large, there are no critical points.
h, too large, no critical points.
Are the solutions decreasing all the time or increasing?
Well, they are always decreasing because the function
is always negative. Solutions always go down,
always. The interesting one is this
last one, where I decreased it just to (h)m.
And, what happens there is there is this certain,
magic critical point whose value we could calculate.
There's one constant solution. So, this is one that has the
value. Sorry, I'm calculating the
solutions out. So, y here and t here,
so here it is value, (h)m is the value by which it
has been lowered. So, this is the picture for
(h)m. And, how do the solutions look?
Well, to the right of that, they are decreasing.
And, to the left they are also decreasing because this function
is always negative. So, the solutions look like
this, if you start above, and if you start below,
they decrease. And, of course,
they can't get lower than zero because these are salmon.
What is the significance of (h)m?
(h)m is the maximum rate of harvesting.
It's an extremely important number for this industry.
If the maximum time rate at which you can pull the salmon
daily out of the water, and can them without what
happening? Without the salmon going to
zero. As long as you start above,
and don't harvest it more than this rate, it will be following
these curves. You will be following these
curves, and you will still have salmon.
If you harvest just a little bit more, you will be on this
curve that has no critical points, and the salmon in the
tank will decrease to zero.

Probability

The following
content is provided under a Creative
Commons license. Your support will help MIT
OpenCourseWare continue to offer high quality
educational resources for free. To make a donation or
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: Today's
our last lecture on dynamic programming,
the grand finale. And we have a bunch of fun
examples today on piano, guitar, Tetris and
Super Mario Brothers. What could be better? We are, again, going to
follow this five-step plan to dynamic programming, define
sub-problems, guess something in order to solve a sub-problem,
write a recurrence that uses that guessing to relate
different sub-problems, then build your dynamic
programming either by just implementing as
a recursive algorithm and memorizing or
building it bottom up. For the first, you need to check
with the recurrence is acyclic. For the second, you need an
actual topological order. These are, of course,
equivalent constraints. Personally, I like to write
down on topological order because that proves to
me that it is acyclic, if I think about it. But either way is fine. Then we get that the
total running time is number of sub-problems
times time per sub-problems, and then we need to solve
our original problem. Usually it's just one
of the sub-problems, but sometimes we have to
look at a few of them. So that's what
we're going to do. And we have one
new concept today that all these examples
will illustrate, which is a kind of
second kind of guessing. We've talked about
guessing in part two here in which we saw
the obvious thing. In the recurrence, we
usually take the min of a bunch of options or the
max of a bunch of options. And those options correspond
to a guessed feature. We don't know whether
the go left or go right, so we try them both. That's guessing. But there's another
way to guess. So two kinds of guessing. So you can do it in step two. Let's see what I have
to say about these. In step two and three,
you are guessing usually which sub-problems
to use in order to solve your
bigger sub-problem. So that's what we've seen
many, many times by now. Every DP that we've covered
except for Fibonacci numbers has used this kind of guessing. And it's sort of the most
common, I guess you might say. But there's a higher
level of guessing that you can use, which
we've sort of seen in the knapsack dynamic
programming, dynamic program, which is when you define your
sub-problems, you can add more. Add more sub-problems to
guess or you can think of it as remembering more
features of the solution. And we just leave it at that. Essentially what this does--
so remember with knapsack, we had a sequence of items. They had values and sizes. And we had some target
knapsack, some capacity. We wanted to pack those
items into that knapsack. And the obvious sub-problems
were suffixes of the items. Because we always know
suffixes, prefixes, substrings, those are the obvious
things to try. But suffixes wasn't
quite enough. Because if we
looked at a suffix, we didn't know of
the prefix that we've skipped over how
many of those items were in-- and in particular,
how much of the capacity we'd used up. And so we needed to
add more sub-problems to remember how much capacity
had we used up in the prefix. We did that by multiplying
every sub-problem by s different choices, which is
how many units of the knapsack still remain. So in some sense,
we're remembering more about the prefix. You can also think of it as--
in the more forward direction, we have the suffix problem. I'm going to solve
it s different times, or s plus 1 different times. I'm going to solve it. What if I had a big knapsack? What if I had a
smaller knapsack? What if I had a
zero-size knapsack? All of those different
versions of the problem. In some sense, you were
solving more sub-problems. You're, in some, sense
finding more solutions to that sub-problem. You're looking at a suffix. And I want to know all these
different solutions that use different amounts
of the knapsack. So in that sense, you're just
adding more sub-problems. But from a guessing
perspective, you're remembering more about the past. We're going to see a bunch of
examples of this type today. We'll always use
this type, but we'll see more of this where the
obvious sub-problems don't work and we need to add more. So the first example is
piano and guitar fingering. This is a practical problem for
any musicians in the audience. How many people here play
piano, or have played piano? OK, about a quarter. How many people
have played guitar? A few, all right. I brought my guitar. I've been learning
this semester. I'm not very good yet,
but we'll fool around with it a little bit. So the general idea is you're
given some musical piece that you want to play. And on a piano, there's
a bunch of keys. You have all these
keyboards, so you know what a piano looks
like, more or less. It's just like a
keyboard, but only row. It's crazy. Each key that you
press makes a note, and every key has
a different note. So it's very simple from
a computer scientist's perspective. You want to play a
note, you push the key. But you could push it with
any one of these fingers. Humans have 10 fingers. Most humans. I guess a few have more. But you want to know,
which finger should I use to play each note? It may not seem like a big deal. And if you're only playing
one note, it's not a big deal. But if you're going to play
a long sequence of notes, some transitions are
easier than others. So let's say we're given
a sequence of n notes we want to play. And we want to find a
fingering for each note. So fingering, so let's
say there are-- I'm going to label the fingers
on your hand, 1 up to f. For humans, f is
5 or 10, depending on if you're doing one
hand or two hand stuff. I think to keep it simple, let's
think about piano, right hand only, and just you're playing
one note at the time, OK? We're going to make it
more complicated later. But let's just think of a
note as being a single note. OK, or you can think of
guitar, single note, left hand is playing things. You want to assign one of
these fingers to each node. And then you have a
difficulty measure, d. And this you need to
think about for awhile musically, or anatomically,
how to define. If we have some note p
and we're on finger f and we want to transition
to note q using figure g, how hard is that? So this is-- the p and
q are notes to play. I guess p stands for pitch. And f and g are fingers. So this is how hard is it
to transition from f on p to g on q. There's a huge
literature on for piano. There are a lot of
rules like, well, if p is much smaller than
q, unless they're stretched, then that becomes hard. And if you want to
stretch, you probably need to use fingers that are
far away from each other. If your playing legato-- so you
have to smoothly go from one note the other-- you can't use
the same finger on both keys. So if f equals g and
you're playing legato, then p better be the
same as q sort of thing. There's a weak finger rule. You tend to avoid fingers
four and five, these two. Apparently going from-- I'm
not much of a pianist-- so going from between
three and four, which I can barely hold them
up, it's kind of difficult, is "annoying." That's what I wrote down. So between three and four
transitions you try and avoid. And so you can encode
into this function. It's a giant table. You can just put
in whatever values you want that you're
most comfortable with. And music theorists
work a lot on trying to define these function so. So you can do that. And for guitar, maybe I
should do a little example. Get this out. I can't play much,
so bear with me. Bet you didn't think
this would happen in 006. [LAUGHTER] So let's see. So let's say you're trying
to play your favorite song. [STRUMS "SUPER MARIO BROTHERS"
 THEME] [LAUGHTER] OK. So when I'm playing that, I have
to think about the fingering. Which finger is going to
go where to play each note? OK, so the first notes
are actually open, so it's really easy. And then I go up to holding the
first fret on the fifth string. OK, and I'm using my index
finger because everyone loves to use their index finger. And in particular because
the very next note I'm going to play-- well,
actually it's down here. Then the next note is
going to be this one. So I'm holding on the third
fret of the bottom string. And then I've got to
transition over here. And actually, usually I do
it with my middle finger. I don't know quite why I
find that easier, but I do. OK, and so I've actually
played that opening a zillion times with lots of
different things. This is the one I found to
be the most comfortable. And there's this issue, right? If your pinky is here, where
can I reach with this finger? Where can I reach
with this finger? It gets difficult. And in particular,
it's very hard for me to reach down here
when my pink is there. And so you can encode
that in this d function however you want. You get the idea. [APPLAUSE] Thanks. I'll skip to our lessons.
[? We're ?] [? worth ?] [? it. ?] So let's solve this
with dynamic programming, OK? That's the cool thing. So we can do it. And we follow our
five step procedure. So the first thing is
to define sub-problems. What are the sub-problems
for a set-up like this? What are the three
obvious candidates? Do you remember last lecture? How many people know the answer? Just checking. One person. Go for it. AUDIENCE: Prefixes,
suffixes, and substrings. PROFESSOR: Right. Prefixes, suffixes,
and substrings. We have a sequence of notes. We're not going to worry
about the sequence of fingers. I don't think that's
too big a deal. That's what we're finding. What we're given is
a sequence of notes, so we should try suffixes,
prefixes, or substrings. I'll just tell you,
suffixes are fine. Kind of. So a sub-problem
will be suffixes, so how to play notes
from i onwards. Intuitively, we
want to figure out, how should we play
the first note? And then we go on to the
second note and so on. So we're applying
them one by one from left to right
from the prefix side. And so we'll always
be left with a suffix. OK, then we need
to guess something. What's the obvious
thing to guess, given I need to
play notes i onward? Think little harder. This one you shouldn't
have to think. That's what I tell you. Try suffixes, try
prefixes, try substrings. Yeah? AUDIENCE: Maybe which
finger to just put around i? PROFESSOR: Yeah, which we're
going to use for note i. Our whole point is
to assign fingering. The first note here is i. So let's think about i,
what could you do for i? We'll try all the possibilities. Which finger to use for note i? OK, now the really hard part--
because it's impossible-- is to write a recurrence. This is wrong, by the way, but
it's the first thing to try. So this is what
I want to ask you to do because it's not possible. But intuitively, what
we might try to do is we're trying
to solve DP for i. And we want to find--
this is difficulty, so you want to
minimize difficulty. So we'll take a min
over all of our guesses of what it would take to
solve the rest of the notes, to play the rest of the
notes, plus somehow the cost of playing the first note. So what's the cost of
playing the first note? And then is going to be
a for loop over fingers. OK, that's going to be the min. We want to try all possible
fingers for note i. Then we have to play
all the remaining notes. And then there's
this transition cost where you're going from
note i to i plus 1. So it's going to
be something like d of if-- we know that we use
finger f to play i-- then we have to go to note i plus 1. But then the problem is we have
no idea what to write here, because we don't know
what finger we're going to guess
for note i plus 1. So this cannot be known. OK, but it's the
first thing you should try, because often this works. For simple DPs, that's
enough for sub-problems. But we need to know
more information about what we're
going to do next. And this seems very
worrisome, maybe now we have to guess two things. Do we have to guess
more than two things? Turns out two things is enough. But we cannot use
this type of guessing. We need to use-- we need
to add more sub-problems. More sub-problem, more power. So any guesses what we
could do for sub-problem? A couple of right answers here. Yeah? AUDIENCE: Maybe like all
the suffixes [INAUDIBLE] like the i, for all written i's,
like all the possible fingers for i? PROFESSOR: All the
possible fingers for i in the sub-problem. Yeah, good. How to play-- it's still
about the suffixes. We're still going to use that. But we're going to
suppose we already know what finger to use
for the first note, note i. OK, this is a little weird,
because we were guessing that before. Now we're just
supposing someone tells us, use finger f for that note. This will work. That's the one I had in mind. But the question becomes,
what should we guess? Anyone else? You clearly get
[? a pillow. ?] I don't know how many
you have by now. Have another one. That's tough. This is not easy to figure out. Now, given that that's
our sub-problem, what is the next thing to guess? Do you have an idea? AUDIENCE: I got an
idea to define it. Like either the next or
previous finger for the-- PROFESSOR: The next
or previous finger. Well, I'm looking at suffixes. So I only care
about the next one. Yeah. I see what you mean
by next or previous. But what we mean
is note i plus 1, that's the next thing
we don't know about. So we're going to
guess finger-- we'll call it g-- for note i plus 1. And now magically,
this recurrence becomes easy to write. So it's almost the same thing. I wish I could just copy and
paste this over, but I can't. It's not a digital blackboard. Are there digital blackboards? That would be cool. Someone should make that. I don't know why switched from
open parens to square brackets, but I did. Then we have-- I think it's just
the obvious thing, if i plus 1 g. Ahh, this is a
slightly wrong, though. It's a copy paste error. This should really
be DP of i comma f, because now sub-problem
consists of two things-- which suffix am I in, and what's
my finger for note I? And so when I call DP, I also
have to provide two arguments. It's going to be DP
of i plus 1 comma g. And then I'm looping over g. I'm trying all
possibilities for g. That's the recurrence. So if I want starting
with finger f on note i, how do I solve
the suffix from i? Well, I guess what
finger am I going to use for the very next note. Then I have to pay this
transition cost for f on i to g on i plus 1. Yeah, OK. So slightly, I'm cheating
the notation here. This probably should be
the note, what is note i, and this thing should be
what is note i plus 1. If you think of this d
function just being given notes, pitches that
you need to play, instead of indices
into the array. It doesn't really matter, but
that's how I defined it before. OK, so I have to pay
this transition cost. What does it take to make that
transition from i to i plus 1? And then what does it take to
do the rest of the notes, given that now my finger is-- or now
finger g is playing the note i plus 1? So we transition from
f to g, and that's now kept track of in
the sub-problem. This is the magic of
defining more sub-problem. We needed to know where
our finger used to be. And now we're telling it,
oh, your finger right now is finger f. Finger f is the one that's
currently playing the note. And then afterwards, g is
the finger that's currently playing the note, and we
can keep track of that. You could also define
this to say, oh, f was the finger that was
used for the previous note, note i minus 1. But it's just a shifting
of the indices here. You can do i minus 1 to i
instead of i to i plus 1. But this is, I think,
slightly cleaner. OK, and then we
have a DP, right? We've just memoized
that recurrence. We get a recursive DP, or
you could build it bottom up. If you were building
it bottom up, you'd want to know
a topological order. And this requires a
little bit of care because there's two parameters. And so it's going
to be a for loop over those two
parameters in some order. And I believe the
right order is for i has to go from right to left
because this is suffixes. So I would write reversed range
n python if there are n notes. And then within that loop, I
would do a for loop over f. If you reverse the order
of these for loops, it would not be in the right
order, I'm pretty sure. But this one will work. You can check it. And then to solve our
original problem, here we require a little
more work because none of these sub-problems
are what we want to solve because we don't
know what the first finger is. We know what the first note is. That's note 0. But what finger goes there? I don't know. And DP of 0 requires
us to give it a finger. Give it the finger, ha. Give it the finger for
whatever is the first note. So this is pretty easy though. We just take a min
over those choices. Which finger should we give it? That should do it. So we don't know what
finger to start with. Just try them all, take the min. This is just like the
guessing that we did here, just a slightly simpler version. There's no transition cost
because there's no transition. We weren't anywhere before. Just what finger
do you start with? I don't care what
finger I start with. It's how I transition from one
note to the next that's hard. OK, done. That's the DP. Now, if this is not
obvious or not clear, I think it's easier to think
about it in the DAG form. So let's draw all
the sub problems. We have here a two dimensional
matrix of sub-problems. We have the different
suffixes on the one hand. So this is it, it stats
a 9, goes to n minus 1. And then in the
other dimension, we have what finger
to use from 1 to f. And so in each of these
positions, there a note. There's a sub-problem. Race. I wanted to get five rows
because there are five fingers. And then our
transitions basically look-- if we're at
finger one on this note, we can go to finger
one on the next note. Or we can go, if
we're not legato, or we can go to finger
two on the next note, or finger three or finger
four or finger five. And then if we're
starting with finger two, we could go to any one of these. So you get a complete
bipartite graph, which you usually
draw like this. That is how graph theorists
draw complete bipartite graphs. OK, but I tried to draw a
little more explicitly here. It's just any
possible transition. And for each of
these, the point is you can compute the
D cost, because you know what figure you were at. You know what finger
you are going to and what note
you're starting from and what note you're going to. Those are the four
arguments you need for D. So you put those
weights on, and then you solve shortest
paths on this DAG. And that is exactly what
this DP is doing, OK? Except there's no
single source here, which is kind of annoying. And so you need to take
this min over what's the shortest path from
here, what's the shortest path from here, from here,
from here, from here. Of course, you
don't actually need to do that by running single
source shortest paths f times. If you're a clever
shortest paths person, you would add an extra
source note, connect that with 0 weight to
all of these sources. So put 0s on there. And then do single
shortest paths from here. And you will find the best way. You don't really care
where you started, so this is trying
all the options. That's exactly what
we're doing here. But here I'm doing it with
the shortest paths trick, here I'm doing it with guessing
and taking a min like DP style. OK, so that's how to do
piano figuring and guitar fingering for single
hand, one note at time. Questions? And this even worse
for aliens if you have arbitrarily many
fingers on your hand. I guess we should figure
out what's the running time. So we have sub-problems. We see how many
sub-problems there are here. There's n times f sub-problems. How much time, or how
many choices are there for our guess? Well there's f different
choices for what finger we use. And when we do this min,
we spend theta F time. Because there's a
for loop over F, we're doing constant
work assuming D lookups take constant time. This is theta F time. So we multiply those
two things together, and we get the total time, the
number of sub-problems which is n times F, and
we multiply them by theta F for each sub-problem. So this is nF squared. And given F is usually pretty
small, it's almost linear time. So that's a pretty
good algorithm. But in reality, you tend
to play multiple notes at the same time. In music, typically
you're playing a chord. With piano, you're
playing several notes with one hand, maybe several
notes with another hand. Two handed piano, it's crazy. You could do four handed piano,
make it a little more exciting. With the guitar, play-- I
don't know very many chords, but I know at least one. You play, I don't know. This looks like something. That's a G chord. Do I know any others? And that's an E chord. All right, you get the idea. I mean, for each
of these chords, different people use
different fingers, even for a single cord. So it's sort of a
personal taste how you're going to define
your difficulty measure. But I could play an E like
this, or I could-- I don't know, play it like this. Or I could play like this. And there's lots of crazy
ways to put your finger here and your finger here
and your finger here. And for each of them, you
could define some difficulty. And then, of course,
is a transition from one chord to another. And because there's different
ways to play different chords, that wasn't a very good
example because they all look pretty bad. Well, this one for example,
this is the G again. I could use my-- one, two,
three, four-- fourth finger here, or I could
use my fifth finger. My instructor says we should
use our pinky because people tend not to use their pinky. But it makes a
difference what I'm going to transition to next. Maybe my pinky really
needs to go over here next and I should free
it up for later, or maybe it's better if this
one's freed because then I can move it somewhere else. So that's what we'd
like to capture in a generalized form
or this dynamic program, and we can do it. So I'll try to do
it quickly so we can get on to the
other examples. All right, other fun stuff. Actually, there's
another fun thing with guitar, which
is that there's more than one way
to play each note. There are six strings here. And you could play like this
note for the Super Mario Brothers. I could also play that
doing the fifth thing here. It's slightly out of tune, but
those sound almost the same. Or I could play on the 10th
fret on the third string. That's the same as bottom one. So a lot of options, so you
also like to capture that. This is actually not too hard. You just need to generalize
the notion of finger to what finger you're using
and what string you're using. So there are f different choices
for what finger you're using. If you use a generalized
guitar, there's s choices for what
string you're playing. There's a lot of different
guitars with various numbers of strings, so we can
just generalize that. And now it's not only, which
finger am I going to use, but what sting
will I play it on? And then you can still
define a difficulty measure like this for this
set up, depending both on the finger
and the string. And then the running
time grows slightly. It's now n times F
squared S squared, because now I have to take
the product of F and S. OK, so that's first thing. But then if I wanted
to do multiple notes, well, you can imagine it's
a similar type of deal. It's going to get harder though. First thing we need to
generalize is the input. Before the input was
a sequence of notes. Now it's going to be a
sequence of multi-notes. So notes of i is now
going to be, let's say, a list of notes that all
need to be played at once. And conveniently, it's probably
going to be, at most, F notes, because you really can only
play one note with each finger pretty much. I guess you could try to
play two notes at once on a piano with
a finger, but eh. It sounds difficult. For a guitar, it's
at most s notes. You can only play one note
per string, more or less. So that's our input. And now we need to adjust
the dynamic program. And I think I'll tell
you how to do this. Basically, now you need to know
where all your fingers are. So you go from one
pose to another pose, from one chord to another. Different ways to finger that. Which fingers are
and which strings and which frets on the
guitar, which fingers are on which keys
on the keyboard. But you just need
to know all that. And all your fingers
might be doing something, and you've got to know for
each finger what note is it on, or is it not being used at all. So how many different ways to
do such a mapping are there? I mean, this is just a function. So it's the number of
targets of the function. So how many of these are there. Gosh, well, I guess we said
there are, at most, f notes. So f plus 1 is the maximum
number of possible things each finger can do. And we raise that to the power
of the number of fingers. That's the possible mappings
of what all of my fingers could be doing. It's exponential
in f, not so great. But if f is 5, it's all right. And then-- well, then you
just generalize the rest. I don't think I'll
write it down in detail. But our sub-problems
now are going to be-- let me
switch boards here. How do we play these
multi-notes from i onwards, given that we're going to
use that pose-- or I called it the state of all my fingers--
for the first notes of i is now a whole bunch of notes. So given I'm now going
to play those notes with this particular
finger assignment, how do I play the rest? And then what we'll guess is
the entire finger assignment for the next set of
notes, i plus 1-- the next chord, if you will. And that guessing involves
now F plus 1 to the F time. And then we just write the
recurrence in the same way. So we're basically generalizing
here we call the finger, now it's an entire
pose for your hand. Instead of F, you might write
H for hand or something. And so the running
time in this situation is going to go up to something
like n times F of plus 1 to the F. Did I miss anything? Probably have to square that. 2F. Before it was F
squared, now it's just F plus 1 to the F squared. So if F is small,
this is all right. Otherwise, not so great. This is the best algorithm
I know for chord fingering. Questions? Just trying to
make it practical, solve the real life problem. I would love, I
think-- I don't know if this has been
implemented, but someone should implement this
in some-- I don't know, score program,
musical score program. I would love as
learning guitar, it'd be great for someone to just
tell me how to finger things. Then I can
retroactively figure out why using the dynamic program. All right, let's
move on to Tetris. All these problems are going
to have the same flavor. You can solve them
with basically the same dynamic program. It's all about figuring out
what should the sub-problems be. So let me-- does anyone
here not know Tetris? OK, good. No one's willing to admit it. So you've got these
blocks falling. But I'm going to make several
artificial constraints. First of all, I tell
you the entire sequence of pieces that
are going to come. This is more like
a Tetris puzzle. OK, we're given sequence
of n pieces that will fall. For each of them, we must
drop the piece from the top. OK, and if you're a
fancy Tetris player, you can let a piece fall and
then rotate it at very end to do some clever, clever thing. I disallow that. You always have to
push the drop button. So the piece starts here, it
goes instantly to the ground. This will be necessary. I don't know how to
solve the problem without this constraint. OK, and then the
other weird thing-- this is very weird for Tetris--
full rows normally clear, but now they don't clear. This is like hardcore Tetris. You're guaranteed
to die eventually. The question is, can you
survive these n pieces? That's the question. Can you survive? Oh, I've got one
other constraint. This is actually
kind of natural. The width of the board is
small, relatively small, because we're going to
be exponential in w. In real life it's 12, I think? AUDIENCE: Ten. PROFESSOR: Ten, sorry. It's been a while since
I wrote my Tetris paper. So all right, these are all
kind of weird constraints. If you don't make all of
these constraints-- oh, also the board is
initially empty. That's like level one of Tetris. If all of these things
are not the case, which is regular Tetris, if you
just have the first thing then this problem is
called NP-complete. We'll be defining
that next class. So it's computationally
intractable. But if you make all
of these assumptions, the problem becomes
easy, and you can do it by
dynamic programming. So how do we do it? We define sub-problems
just like before. The obvious thing
to try is suffixes. How do we play a suffix
of pieces i onwards? How to play those guys. And just like fingering, this is
not enough information, right? Because if we're going to play
from pieces i onward, what we need to now is what the
board currently looks like. I said here the board
is initially empty. That's not going to
be the case after you place the very first piece. So in general, after we've
placed the first i pieces, we need to know what
the board looks like. And here's where I'm going to
use all of these assumptions. If you always drop things from
the top and rows don't clear, then all you really care about
is how high each column is. This is what you might call
the skyline of the board. OK, now in reality,
there might be holes here because you drop
things in silly ways. Maybe you drop a
piece like this. And then I claim, because
I'm dropping things from infinity from
the sky, I really don't care about that
there's a whole here. I can just fill that in and
say, OK, that's my new skyline. Because if you can't do
these last minute twists and if lines never clear,
that's going to be gone. That material is wasted. OK, so all I need to remember
is how high is each column. So I should say given
the board skyline. Now, how many choices
are there for that? It's quite similar to this
function, the fingering. Let's see. There's the height of the
board, different choices. It's going to be h. For each column it could be
anywhere between 0 and h, so I guess h plus 1 if
you want to get technical. And then we raise
it to the power w, because there's w different
columns and each of them is independent choice. So this is going to n times
that different sub-problems. And here's what I
need the is small because this is
exponential in w. So it's reasonable in
h, but exponential in w. OK, then what do I guess? Any suggestions what to guess? AUDIENCE: So where the new
piece falls, as in [INAUDIBLE]? PROFESSOR: Yeah. What should I do with piece i? There's not that many choices. I can rotate it zero,
one, two, or three times. I can choose
someplace to drop it, but those are my only choices. So it's just how
to play piece i. And given that guess,
you can figure out how the skyline updates,
like I did here. If I drop that piece like
that, then I fill in this part and recompute my new skyline. So it's going to
be something like 4 times w different choices,
roughly-- 4 for the rotation, w for the x-coordinate. And so the running
time is just going to be the product of these. n times w times h
plus 1 to the w. Open problem, if I drop any
one of these assumptions, can you get a dynamic
program that's reasonable? Could you do it if w is large? I don't know. Could you do if rows do clear? That's the least
natural constraint here. I don't know. Puzzle for you to think about. I'd love to know the answer. You can obviously do the
rest of the steps, right? You can write down
the recurrence. It's the same thing. You take the min
over all guesses. What are we minimizing? Hmm. I guess here the
question is survival. Can you survive? So this is one of the first
examples where the answer is a Boolean value, true or false. But if you think of true
or false as 0 and 1, then it's still a
maximization problem. You want to maximize. You want 1 if possible. Otherwise, you'll get
0 when you maximize. So you can write the
recurrence using max. And in the base case, you have
truth values, true or false. And you'll see, did I survive? Did I die? That sort of thing. I want to go on to
Super Mario Brothers, because everyone loves
Super Mario Brothers. has? Anyone not played NES
Super Mario Brothers 1? Aww, you got to play it, man. You're the only one. You can play it on an emulator. Maybe not legally, but you
can play it on an emulator and just see how it is. So what I'm going
to talk about next, in theory, works for many
different platform games, side-scrolling platform games. But Super Mario Brothers
1 has some nice features. In particular, a nice feature
is that whenever anything moves off of the screen, it
disappears from the world. So the monster moves
off, it's gone. You can think of there's
a static level there. When the level comes into
screen, when a monster comes on screen, then
it starts acting. But as soon as you move the
screen-- you can't actually move backwards in Super
Mario 1, but as soon as you move forwards and
that character is offscreen, it's gone. So in a sense, that
part of the level reset to its initial state. Now, as long as your screen is
not too big-- and thankfully, on NES screens
were not very big. It's 320p, or whatever. This will work. If you are given
the entire level-- so let's say there's n
bits of information there-- and you have a small screen,
w by h screen, w and h are not too big. Then I claim we can
solve Super Mario Brothers by dynamic programming. So let's say we want
to maximize our score. Want to run through the level
and maximize your score, or you want to minimize
the amount of time you use. You're doing level runs. Pick your favorite measure,
all of those can be solved. And the way to do it, this
sort of general approach to all these DPs is
we need to write down what do I need to know
about the game state. I'll call that a configuration. What can we care about
for Super Mario Brothers? Well, I guess
everything on screen. This is a bit tricky, but
there's stuff on screen. There are monsters and objects. For the monsters, I need to
know their current position. For the objects, I
need to know-- like, is there a question mark box? Did I hit it already? Did I already get the coin or
did I already get the mushroom? So for each of those
things, there's some amount of information
you need to store. How much information? I think something like constant
to the w times h should do. That's saying for every
pixel on the screen or for every square
on the screen, however you-- whatever you
define the resolution here to be. Let's say for every little
unit square in Mario land, is it a brick? Is it a hard brick, or has
it been a destroyed brick? Is a monster there right now? Is Mario there right now? All these kinds of information. OK, so there's a cost number
of choices for each pixel. You can write them all down. You might also want
Mario's velocity. I had to play it
again just to check that there is indeed velocity. Turning around is slower
than going forward. You do accelerate a little bit. So you've got to remember that. There's probably only
a constant number of choices for what
your velocity is. What else? Ah, I want to
remember the score. You want to maximize score. And let's say you also--
how much time is left. There's a time counter. If it hits zero, you die. Now, these are kind of annoying,
because they're integers. They could be kind of large. So I'm going to say the
score could be capital S big, and time could be capital T big. So this'll be a
pseudopolynomial algorithm. The number of
configurations in total here is the product
of these things. It's exponential in w and h. And then multiply by
S and T. So that's the number of configurations. And that's also going
to be our sub-problem. I guess we should
also write down where is the screen
relative to the level. OK, how far to the
right have you gone? That's another w. That's not a big deal. OK, given this information,
you know everything you need to know about
playing from here on. And the time counter's
always going to keep ticking. So you can draw a graph
of all configurations, just enumerate all
of these things. It's this many of them. And then draw, for
every configuration, what are the possible
things I can do? I could push this button. I can push the A button, I
can release the A button. I can push the B button, I
can release the B button. I can push the up arrow. Those are all the
things you could do. It's a constant
number of choices. So each vertex will have
constant out degree. If you did this, what
configuration would I reach? Just draw that whole graph. Do shortest paths. Or dynamic programming,
these are your sub-problems. There are no suffixes here. These are your sub-problem. And then you take
a max, if you're trying to maximize score or max
if you're trying to maximize time, minimize the time you use. This is time remaining. And you can relate
each sub-problem to a constant number
of other sub-problems. So your running
time will be this, because you only pay constant
time per sub-problem. And now you can solve Super
Mario Brothers optimally, as long as your screen is
not too big and as long as your scores and times
don't get too big either, because we're only
pseudopolynomial with respect to S and T. Questions? All right. That's-- yeah? AUDIENCE: So are we
going to be trying to memoize all of these
possible configurations? PROFESSOR: If you do
the recursive version, you will end up memoizing all
of these configuration values. Well, anyone that's reachable
from the initial state. Some configurations
might not be reachable, but the ones that
are reachable you're going to start doing them. When you finish doing them,
you will memoize the result.

Diff. Eq.

ANNOUNCER: Open content is
provided under a creative commons license. Your support will help MIT
OpenCourseWare continue to offer high quality educational
resources for free. To make a donation, or view
additional materials from hundreds of MIT courses, visit
MIT OpenCourseWare at ocw.mit.edu . PROFESSOR JOHN GUTTAG: OK. I finished up last time
talking about lists. And I pointed out that lists are
mutable, showed you some examples of mutation. We can look at it here; we
looked at append, which added things to lists, we looked at
delete, deleting things from a list. You can also assign to a
list, or to an element of a list. So ivy sub 1, for example,
could be assigned minus 15, and that will actually
mutate the list. So heretofore, when we wrote
assignment, what we always meant, was changing the binding
of a variable to a different object. Here, we are overloading the
notation to say, no, no, ivys is still bound to the same
object, but an element of ivys is bound to a different
object. If you think about it, that
makes sense, because when we have a list, what a list is,
is a sequence of objects. And what this says is, is the
object named by the expression ivys sub 1, is now bound to the
object, if you will, named by the constant minus 15. So we can watch this run here. Idle can-- that's exciting. I hadn't expected that answer. All right, your question. STUDENT: [INAUDIBLE] four
elements to ivys, and you tell it to change the fifth element
of ivys to negative 15, will it add it or [INAUDIBLE] PROFESSOR JOHN GUTTAG: Well,
I'll tell you how ol-- let's answer that the easy way. We'll start up a shell
and we'll try it. All right, we'll just get out
of what we were doing here. And so, we now have some things,
so we, for example, have ivys, I can print ivys
, and it's only got three elements but your question
probably is just as good for adding the fourth as adding
the fifth, so what would happen if we say ivys sub 3--
because that of course is the fourth element, right? Let's find out. OK. Because what that does is, it's
changing the binding of the name ivys, in this
case, sub 1. What it looked at here, with the
name ivys sub 3, and said that name doesn't-- isn't
bound, right? That isn't there. So it couldn't do it, so instead
that's what append is for, is to stick things on to
the end of the list. But a very good question. So we can see what we did here,
and, but of course I can now, if I choose, say something
like, ivys sub 1 is assigned minus 15, and now if
I print ivys, there it is. And again, this points out
something I wanted to me-- I mentioned last time, list can
be heterogeneous, in the sense that the elements can be
multiple different types. As you see here, some of the
elements are strings and some of the elements are integers. Let's look at another example. Let's suppose, we'll start
with the list, I'll call it l 1. This, by the way, is a really
bad thing I just did. What was-- what's really bad
about calling a list l 1? STUDENT: [INAUDIBLE] PROFESSOR JOHN GUTTAG: Is it l
1, or is it 11, or is it l l? It's a bad habit to get into
when you write programs, so I never use lowercase L except
when I'm spelling the word where it's obvious, because
otherwise I get all sorts of crazy things going on. All right, so let's make
it the list 123. All right? Now, I'll say L 2 equals L 1. Now I'll print L 2. Kind of what you'd guess, but
here's the interesting question: if I say L 1 is
assigned 0, L 1 sub 0 is assigned 4, I'll print L 1. That's what you expect, but
what's going to happen if I print L 2? 423 as well, and that's because
what happened is I had this model, which we looked at
last time, where I had the list L 1, which was bound to
an object, and then the assignment L 2 gets L 1, bound
the name L 2 to the same object, so when I mutated this
object, which I reached through the name L 1 to make
that 4, since this name was bound to the same object, when
I print it, I got 423. So that's the key thing to--
to realize; that what the assignment did was
have two separate paths to the same object. So I could get to that object
either through this path or through that path, it didn't
matter which path I use to modify it, I would see it when
I looked at the other. Yes. STUDENT: [INAUDIBLE] PROFESSOR JOHN GUTTAG: So the
question, if I said a is assigned 2, b is assigned a,
and then a is assigned 3. Is that your question? So the question is, a is
assigned 1, b is assigned a, a is assigned 2, and then if
I print b, I'll get 1. Because these are not mutable,
this is going to be assigned to an object in the store, so
we'll draw the picture over here, that we had initially a
is bound to an object with 1 in it, and then b got bound to
the same object, but then when I did the assignment, what that
did was it broke this connection, and now had a
assigned to a different object, with the number,
in this case, 2 in it. Whereas the list assignment you
see here did not rebind the object l 1, it
changed this. OK? Now formally I could have had
this pointing off to another object containing 4, but that
just seemed excessive, right? But you see the difference. Great question, and a very
important thing to understand, and that's why I'm belaboring
this point, since this is where people tend to get pretty
confused, and this is why mutation is very important
to understand. Yeah. STUDENT: [UNINTELLIGIBLE] PROFESSOR JOHN GUTTAG:
I'm just assuming it'll be a great question. STUDENT: [INAUDIBLE] PROFESSOR JOHN GUTTAG:
Exactly. So if-- very good question-- so,
for example, we can just do it here. The question was, suppose I now
type L 1 equals the empty list. I can print L 1, and I can
print L 2, because again, that's analogous to this
example, where I just swung the binding of the identifier. So this is important, it's a
little bit subtle, but if you don't really understand this
deeply, you'll find yourself getting confused a lot. All right? OK. Let me move on, and I want to
talk about one more type. By the way, if you look at the
handout from last time, you'll see that there's some other
examples of mutation, including a function
that does mutation. It's kind of interesting, but I
don't think we need-- think we've probably done
enough here that I hope it now make sense. That one type I want to talk
about still is dictionaries. Like lists, dictionaries are
mutable, like lists, they can be heterogeneous, but unlike
lists, they're not ordered. The elements in them don't
have an order, and furthermore, we have generalized
the indexing. So lists and strings, we can
only get at elements by numbers, by integers, really. Here what we use is, think
of every element of the dictionary as a key value pair,
where the keys are used as the indices. So we can have an example,
let's look at it. So, if you look at the function
show dics here, you'll see I've declared a
variable called e to f, ah, think of that as English to
French, and I've defined a dictionary to do translations. And so, we see that the string
one corresponds the-- the key one corresponds to the value un
the key soccer corresponds to the French word football,
et cetera. It's kind of bizarre, but the
French call soccer football. And then I can index in it. So if I print e to f of soccer,
it will print the string football. So you can imagine that this is
a very powerful mechanism. So let's look what happens
when I run-- start to run this. All right. So, it says not defined-- and
why did it say not defined, there's an interesting
question. Let's just make sure we get this
right, and we start the show up again-- All right, so, I run
it, and sure enough, it shows football. What happens if I
go e to f of 0? I get a key error. Because, remember, these
things are not ordered. There is no 0th element. 0 is not a key of this
particular object. Now I could have made 0 a key,
keys don't have to be strings, but as it happened, I didn't. So let's comment that out, so
we don't get stuck again. Where we were before, I've
printed it here, you might be a little surprised
of the order. Why is soccer first? Because the order of this
doesn't matter. That's why it's using
set braces, so don't worry about that. The next thing I'm doing is--
so that's that, and then-- I'm now going to create another
one, n to s, for numbers to strings, where my
keys are numbers, in this case the number 1 corresponds
to the word one, and interestingly enough, I'm also
going to have the word one corresponding to the number 1. I can use anything I want for
keys, I can use anything I want for values. And now if we look at this,
we see, I can get this. All right. So these are extremely
valuable. I can do lots of things with
these, and you'll see that as we get to future assignments,
we'll make heavy use of dictionaries. Yeah. Question. STUDENT: [INAUDIBLE] PROFESSOR JOHN GUTTAG: You can,
but you don't know what order you'll get them in. What you can do is you can
iterate keys, which gives you the keys in the dictionary, and
then you can choose them, but there's no guarantee
in the order in which you get keys. Now you might wonder, why
do we have dictionaries? It would be pretty easy to
implement them with lists, because you could have a list
where each element of the list was a key value pair, and if
I wanted to find the value corresponding to a key, I could
say for e in the list, if the first element of e is the
key, then I get the value, otherwise I look at the next
element in the list. So adding dictionaries, as
Professor Grimson said with so many other things, doesn't
give you any more computational power. It gives you a lot of expressive
convenience, you can write the programs much
more cleanly, but most importantly, it's fast. Because if you did what I
suggested with the list, the time to look up the key would be
linear in the length of the list. You'd have to look
at each element until you found the key. Dictionaries are implemented
using a magic technique called hashing, which we'll look at a
little bit later in the term, which allows us to retrieve
keys in constant time. So it doesn't matter how big
the dictionary is, you can instantaneously retrieve the
value associated with the key. Extremely powerful. Not in the next problems set but
in the problem set after that, we'll be exploiting that
facility of dictionaries. All right. Any questions about this? If not, I will turn the podium
over to Professor Grimson. PROFESSOR ERIC GRIMSON:
I've stolen it. This is like tag team
wrestling, right? Professor Guttag has you
on the ropes, I get to finish you off. Try this again. OK. We wanted to finish up that
section, we're now going to start on a new section, and I
want to try and do one and a half things in the
remaining time. I'm going to introduce one topic
that we're going to deal with fairly quickly, and then
we tackle the second topic, it's going to start today, and
we're going to carry on. So let me tell the two
things I want to do. I want to talk a little bit
about how you use the things we've been building in terms
of functions to help you structure and organize
your code. It's a valuable tool that you
want to have as a programmer. And then we're going to turn to
the question of efficiency. How do we measure efficiency
of algorithms? Which is going to be a really
important thing that we want to deal with, and we'll start
it today, it's undoubtedly going to take us a couple more
lectures to finish it off. Right, so how do you
use the idea of functions to organize code? We've been doing it implicitly,
ever since we introduced functions. I want to make it a little more
explicit, and I want to show you a tool for
doing that. And I think the easy way
to do is-- is to do it with an example. So let's take a really
simple example. I want to compute the
length of the hypotenuse of a right triangle. And yeah, I know you know how
to do it, but let's think about what might happen if
I wanted to do that. And in particular, if I think
about that problem-- actually I want to do this-- if I think
about that problem, I'm going to write a little piece
of pseudo code. Just to think about how I would
break that problem up. Pseudo code. Now, you're all linguistic
majors, pseudo means false, this sounds like code that ain't
going to run, and that's not the intent of the term. When I say pseudo code, what I
mean is, I'm going to write a description of the steps,
but not in a particular programming language. I'm going to simply write
a description of what do I want to do. So if I were to solve this
problem, here's the way I would do it. I would say, first thing I want
to do, is I want to input a value for the base
as a float. Need to get the base in. Second thing I want to do, I
need to get the height, so I'm going to input a value for the
height, also as a float, a floating point. OK. I get the two values in, what
do I need to do, well, you sort of know that, right? I want to then do, I need to
find the square root-- b squared plus h squared, right? The base plus the height, that's
the thing I want for the hypotenuse-- and I'm going
to save that as a float in hyp, for hypotenuse. And then finally I need to print
something out, using the value in hyp. OK. Whoop-dee-doo, right? Come on. We know how to do this. But notice what I did. First of all, I've used the
notion of modularity. I've listed a sequence of
modules, the things that I want to do. Second thing to notice, is that
little piece of pseudo code is telling me things
about values. I need to have a float. I need to have another float
here, it's giving me some information. Third thing to notice is,
there's a flow of control. The order which these things
are going to happen. And the fourth thing to notice
is, I've used abstraction. I've said nothing about how I'm
going to make square root. I'm using it as an abstraction,
saying I'm going to have square root from
somewhere, maybe I'll build it myself, maybe somebody gives it
to me as part of a library, so I'm burying the details
inside of it. I know this is a simple example,
but when you mature as a programmer, one of the
first things you should do when you sit down to tackle
some problem is write something like this
pseudo code. I know Professor Guttag
does it all the time. I know, for a lot of
you, it's like, OK, I got a heavy problem. Let's see, def Foobar open
paren, a bunch of parameters. Wrong way to start. Start by thinking about what
are the sequences. This also, by the way, in some
sense, gives me the beginnings of my comments for what
the structure of my code is going to be. OK. If we do that, if you look at
the handout then, I can now start implementing this. I wanted to show you that, so,
first thing I'm going to do is say, all right, I know I'm going
to need square root in here, so I'm going to,
in fact, import math. That's a little different from
other import statements. This says I'm going to get the
entire math library and bring it in so I can use it. And then, what's the first
thing I wanted to do? I need to get a value
for base as a float. Well OK, and that sounds like
I'm going to need to do input of something, you can see that
statement there, it's-- got the wrong glasses on
but right there-- I'm going to do an input with
a little message, and I'm going to store it in base. But here's where I'm going to
practice a little bit of defensive programming. I can't rely on Professor Guttag
if I give this-- if this code to him, I can't rely
on him to type in a float. Actually I can, because he's a
smart guy, but in general, I can't rely on the user-- PROFESSOR JOHN GUTTAG: I
wouldn't do it right to see if you did. PROFESSOR ERIC GRIMSON:
Actually, he's right, you know. He would not do it, just to
see if I'm doing it right. I can't rely on the user. I want to make sure I
get a float in it, so how do I do that? Well, here's one nice
little trick. First of all, having read in
that value, I can check to see, is it of the right type? Now, this is not the nicest way
to do it but it'll work. I can look at the type of the
value of base and compare it to the type of an actual float
and see, are they the same? Is this a real or a float? If it is, I'm done. How do I go back if it isn't? Well, I'm going to create
a little infinite loop. Not normally a good idea. I set up a variable here,
called input OK. Initially it's false, because
I have no input. And then I run a loop in which
I read something in, I check to see if it's the right type,
if it is, I change that variable to say it's now the
correct type, which means the next time through the loop, I'm
going to say I'm all set and I'm going to bounce out. But if it is not, it's going
to print out a message here saying, you screwed up, somewhat
politely, and it's going to go back around. So it'll just cycle until
I get something of the right type. Nice way of doing it. Right, what's the second
thing I do? Well, I get the same sort of
thing to read in the height, once I have that I'm going to
take base squared plus height squared, and there's a form
that we've just seen once before, and it's going to repeat
it, that is math.SQRT and it says the following: it
says, take from the math library the function
called sqrt. OK. We're going to come back to this
when we get to objects, it's basically picking up that
object and it's applying that, putting that value into hype,
and then just printing something out. And again, if I just run this,
just to show that it's going to do the right thing, it says
enter base, I'm obnoxious, it says oops, wasn't a float, so
we'll be nice about it, and I enter a height, and it prints
out what I expected. I just concatenated those
strings together, by the way, at the end. All right. Notice what I did. OK. I went from this description,
it gives me [UNINTELLIGIBLE] some information. I need to have a particular
type. I made sure I had the
particular type. I've used some abstraction to
suppress some details here. Now if you look at that list,
there is actually something I didn't seem to check, which is,
I said I wanted a float stored in hyp. How do I know I've got
a float in hyp? Well I'm relying on the
contract, if you like, that the manufacturer of square root
put together, which is, if I know I'm giving it two
floats, which I do because I make sure they're floats, the
contract, if you like, of square root says I'll give
you back a float. So I can guarantee I've got
something of the right type. OK. I know this is boring
as whatever. But there's an important
point here. Having now used this pseudo code
to line things up, I can start putting some additional
structure on this. And in particular, I'm sure
you're looking at this going-- will look at it if we look
at the right piece-- going, wait a minute. This chunk of code and this
chunk of code, they're really doing the same thing. And this is something
I want to use. If I look at those two pieces
of computation, I can see a pattern there. It's an obvious pattern
of what I'm doing. And in particular, I can then
ask the following question, which is, what's different
between those two pieces of code? And I suggest two
things, right? One is, what's the thing
I print out when I ask for the input? The second thing is, what do I
print out if I actually don't get the right input in? And so the only two differences
are, right there, and there versus
here and here. So this is a good place to
think about, OK, let me capture that. Let me write a function, in fact
the literal thing I would do is to say, identify the
things that change, give each of them a variable name because
I want to refer to them, and then write a function
that captures the rest of that computation
just with those variable names inside. And in fact, if you look down--
and I'm just going to highlight this portion, I'm not
going to run it-- but if you look down here, that's
exactly what that does. I happen to have it commented
out, right? What does it do? It has height, it says, I've got
two names of things: the request message and
the error message. The body of that function
looks exactly like the computation up above, except
I'm simply using those in place of the specific message
I had before. And then the only other
difference is obviously, it's a function I need to
return a value. So when I'm done, I'm going to
give the value back out. All right? And that then let's me get
to, basically, this code. Having done that, I simply call
base with get float, I call height with get float, and
do the rest of the work. All right. What's the point
of doing this? Well, notice again. What have I done? I've captured a module
inside of a function. And even though it's a simple
little thing here, there's some a couple of really nice
advantages to this. All right? First one is there's
less code to read. It's easier to debug. I don't have as much
to deal with. But the more important thing
is, I've now separated out implementation from
functionality, or implementation from use. What does that mean? It means anybody using that
little function get float doesn't have to worry about
what's inside of it. So for example, I decide I want
to change the message I print out, I don't have to
change the function, I just pass in a different parameter. Well if I-- you know, with
[UNINTELLIGIBLE PHRASE sorry, let me say it differently. I don't need to worry about
how checking is done, it's handled inside of
that function. If I decide there's a better
way to get input, and there is, then I can make it to change
what I don't have to change the code that
uses the input. So, if you like, I've built a
separation between the user and the implementer. And that's exactly one of the
reasons why I want to have the functions, because I've
separated those out. Another way of saying it is,
anything that uses get float doesn't care what the details
are inside or shouldn't, and if I change that definition, I
don't have to change anything elsewhere in my code, whereas if
I just have the raw code in there, I have to go
off and do it. Right, so the things we want
you to take away from this are, get into the habit of using
pseudo code when you sit down to start a problem, write
out what are the steps. I will tell you that a good
programmer, at least in my mind, may actually go back and
modify the pseudo code as they realize they're missing things,
but it's easier to do that when you're looking at a
simple set of steps, than when you're in the middle
of a pile of code. And get into the habit of using
it to help you define what is the flow of control. What are the basic modules, what
information needs to be passed between those
modules in order to make the code work. OK. That was the short topic. I will come back to this some
more and you're going to get lots of practice with this. What I want to do is
to start talking about a different topic. Which is efficiency. And this is going to sound
like a weird topic, we're going to see why it's of
value in a second. I want to talk about efficiency,
and we're going to, or at least I'm going to, at
times also refer to this as orders of growth, for reasons
that you'll see over the next few minutes. Now, efficiency is obviously
an important consideration when you're designing code,
although I have to admit, at least for me, I usually want
to at least start initially with code that works, and then
worry about how I might go back and come up with more
efficient implementation. I like to have something I
can rely on, but it is an important issue. And our goal over the next
couple of lectures, is basically to give you
a sense of this. So we're not going to turn
you into an expert on computational efficiency. That's, there are whole courses
on that, there's some great courses here on that,
it takes some mathematical sophistication, we're going to
push that off a little bit. But what we-- what we do want
to do, is to give you some intuition about how to
approach questions of efficiency. We want you to have a sense of
why some programs complete almost before you're
done typing it. Some programs run overnight. Some programs won't stop
until I'm old and gray. Some programs won't stop until
you're old and gray. And these are really different
efficiencies, and we want to give you a sense of how do you
reason about those different kinds of programs. And part of it is we want you
to learn how to have a catalog, if you like, of
different classes of algorithms, so that when you get
a problem, you try and map it into an appropriate class,
and use the leverage, if you like, of that class of algorithms. Now. It's a quick sidebar, I've got
to say, I'm sure talking about efficiency to folks like you
probably seems really strange. I mean, you grew up in an age
when computers were blazingly fast, and have tons of memory,
so why in the world do you care about efficiency? Some of us were not so lucky. So I'll admit, my first computer
I program was a PDP6, only Professor Guttag even knows
what PDP stands for, it was made by Digital Equipment
Company, which does not exist anymore, is now long gone. It had, I know, this is
old guy stories, but it had 160k of memory. Yeah. 160k. 160 kilobits of memory. I mean, your flash cards have
more than that, right? It had a processor speed
of one megahertz. It did a million operations
per second. So let's think about it. This sucker, what's
it got in there? That Air Mac, it's, see, it's
got, its go-- my Air Mac, I don't know about John's, his is
probably better, mine has 1.8 gigahertz speed. That's 1800 times faster. But the real one that blows me
away is, it has 2 gig of memory inside of it. That's 12 thousand times
more memory. Oh, and by the way? The PDP6, it was in a rack
about this tall. From the floor, not
from the table. All right, so you didn't grow
up in the late 1800s like I did, you don't have to
worry about this sort of stuff, right? But a point I'm trying to make
is, it sounds like anymore computers have gotten so
blazingly fast, why should you worry about it? Let me give you one other
anecdote that I can't resist. This is the kind of thing you
can use at cocktail parties to impress your friends
from Harvard. OK. Imagine I have a little lamp, a
little goose-- one of those little gooseneck lamps, I'd put
it on the table here, I'd put the height about a f-- about
a foot off the table. And if I was really good, I
could hit, or time it so that when I hurt-- yeah, try again. When I turn this on switch on
in the lamp, at exactly the same time, I'm going to hit a
key on my computer and start it running. OK. In the length of time it takes
for the light to get from that bulb to the table, this
machine processes two operations. Oh come on, that's amazing. Two operations. You know, you can do the
simple numbers, right? [UNINTELLIGIBLE PHRASE] Light travels basically a
foot in a nanosecond. Simple rule of thumb. Now, the nanosecond is what,
10 to the minus 9 seconds. This thing does 2 gig
worth of operations. A gig is 10 to the 9, so it
does two operations in the length of time it takes light
to get from one foot off the table down to the table. That's amazing. So why in the world do you
care about efficiency? Well the problem is that the
problems grow faster than the computers speed up. I'll give you two examples. I happen to work in
medical imaging. Actually, so does Professor
Guttag. In my in my area of research,
it's common for us to want to process about 100 images a
second in order to get real time display. Each image has about a million
elements in it. I've got to process about a half
a gig of data a second in order to get anything
out of it. Second example. Maybe one that'll hit a little
more home to you. I'm sure you all use Google, I'm
sure it's a verb in your vocabulary, right? Now, Google processes--
ten million? Ten billion pages? John? I think ten billion was the
last number I heard. Does that sound about right? PROFESSOR JOHN GUTTAG:
I think it might actually be more by now. PROFESSOR ERIC GRIMSON:
Maybe more by now. But let's, for the sake of
argument, ten billion pages. Imagine you want to search
through Google to find a particular page. You want to do it in a second. And you're going to just do it
the brute force way, assuming you could even reach all of
those pages in that time. Well, if you're going to do
that, you've got to be able to find what you're looking for
in a page in two steps. Where a step is a comparison
or an arithmetic operation. Ain't going to happen, right? You just can't do it. So again, part of the point here
is that things grow-- or to rephrase it, interesting
things grow at an incredible rate. And as a consequence, brute
force methods are typically not going to work. OK. So that then leads to the
question about what should we do about this? And probably the obvious thing
you'll think about is, we'll come up with a clever
algorithm. And I want to disabuse
you of that notion. It's a great idea if
you can do it, The guy who-- I think I'm going to say this
right, John, right? Sanjay? Ghemawat?-- with a guy who was
a graduate of our department, who is the heart and soul behind
Google's really fast search, is an incredibly smart
guy, and he did come up with a really clever algorithm about
how you structure that search, in order to make it happen. And he probably made a lot
of money along the way. So if you have a great idea,
you know, talk to a good patent attorney and get
it locked away. But in general, it's hard
to come up with the really clever algorithm. What you're much better at doing
is saying how do I take the problem I've got and map it
into a class of algorithms about which I know and use the
efficiencies of those to try and figure out how
to make it work. So what we want to do, is, I
guess another way of saying it is, efficiency is really about
choice of algorithm. And we want to help you learn
how to map a problem into a class of algorithms of
some efficiency. That's our goal. OK. So to do this, we need a little
more abstract way of talking about efficiency, and
so, the question is, how do we think about efficiency? Typically there's two things
we want to measure. Space and time. Sounds like an astrophysics
course, right? Now, space usually we--
ach, try it again. When we talk about space, what
we usually refer to is, how much computer memory does it
take to complete a computation of a particular size? So let me write that down, it's
how much memory do I need to complete a computation. And by that, I mean, not how
much memory do I need to store the size of the input, it's
really how much internal memory do I use up as I go
through the computation? I've got some internal variables
I have to store, what kinds of things do I
have to keep track of? You're going to see the
arguments about space if you take some of the courses that
follow on, and again, some nice courses about that. For this course, we're
not going to worry about space that much. What we're really going
to focus on is time. OK. So we're going to focus here. And the obvious question I
could start with is, and suppose I ask you, how long does
the algorithm implemented by this program take to run? How might I answer
that question? Any thoughts? Yeah. STUDENT: [INAUDIBLE] PROFESSOR ERIC GRIMSON:
Ah, you're jumping ahead of me, great. The answer was, find a
mathematical expression depending on the number
of inputs. It was exactly where
I want to go. Thank you. I was hoping for a simpler
answer, which is, just run it. Which is, yeah I know,
seems like a dumb thing to say, right? One of the things you could
imagine is just try it on and input, see how long it takes. You're all cleverer than that,
but I want to point out why that's not a great idea. First of all, that depends on
which input I've picked. All right? Obviously the algorithm is
likely to depend on the size of the input, so this
is not a great idea. Second one is, it depends on
which machine I'm running on. If I'm using a PDP6, it's going
to take a whole lot longer than if I'm
using an Air Mac. All right? Third one is, it may depend
on which version of Python I'm running. Depends on how clever the
implementer of Python was. Fourth one is, it may depend on
which programming language I'm doing it in. So I could do it empirically,
but I don't want to do that typically, it's just not a
great way to get at it. And so in fact, what we
want is exactly what the young lady said. I'm going to ask the following
question, which is-- let me write it down-- what is the
number of the basic steps needed as a function
of the input size? That's the question we're going
to try and address. If we can do this, this is good,
because first of all, it removes any questions about what
machine I'm running on, it's talking about
fundamentally, how hard is this problem, and the second
thing is, it is going to do it specifically in terms
of the input. Which is one of the things
that I was worried about. OK. So to do this, we're going to
have to do a couple of things. All right, the first one is,
what do we mean by input size? And unfortunately, this depends
on the problem. It could be what's the size of
the integer I pass in as an argument, if that's what
I'm passing in. It could be, how long is the
list, if I'm processing a list or a tuple It could
be, how many bits are there in something. So it-- that is something where
we have to simply be clear about specifying what
we're using as input size. And we want to characterize it
mathematically as some number, or some variable rather, the
length of the list, the size of the integer, would be the
thing we'd want to do. Second thing we've got
to worry about is, what's a basic step? All right, if I bury a whole lot
of computation inside of something, I can say, wow, this
program, you know, runs in one step. Unfortunately, that one step
calls the Oracle at Delphi and gets an answer back. Maybe not quite what you want. We're typically going to use
as basic steps the built-in primitives that a machine
comes with. Or another way of saying it is,
we're going to use as the basic steps, those operations
that run in constant time, so arithmetic operations. Comparisons. Memory access, and in fact one
of the things we're going to do here, is we're going to
assume a particular model, called a random access model,
which basically says, we're going to assume that the length
of time it takes me to get to any location in
memory is constant. It's not true, by the way, of
all programming languages. In fact, Professor Guttag
already talked about that, in some languages lists take
a time linear with the length to get to it. So we're to assume we can get
to any piece of data, any instruction in constant time,
and the second assumption we're going to make is that the
basic primitive steps take constant time, same amount
of time to compute. Again, not completely true,
but it's a good model, so arithmetic operations,
comparisons, things of that sort, we're all going to assume
are basically in that in that particular model. OK. Having done that, then, there
are three things that we're going to look at. As I said, what we want to do
is, we want to count the number of basic steps it takes
to compute a computation as a function of input size. And the question is, what
do we want to count? Now, one possibility
is to do best case. Over all possible inputs to
this function, what's the fastest it runs? The fewest, so the minimum,
if you like. It's nice, but not particularly
helpful. The other obvious one to
do would be worst case. Again, over all possible inputs
to this function, what's the most number of
steps it takes to do the computation? And the third possibility, is
to do the expected case. The average. I'm going to think
of it that way. In general, people focus
on worst case. For a couple of reasons. In some ways, this would be
nicer, do expected cases, it's going to tell you on average how
much you expect to take, but it tends to be hard to
compute, because to compute that, you have to know a
distribution on input. How likely are all the inputs,
are they all equally likely, or are they going to depend
on other things? And that may depend on the
user, so you can't kind of get at that. We're, as a consequence, going
to focus on worst case. This is handy for a
couple of reasons. One, it means there
are no surprises. All right? If you run it, you have a sense
of the upper bound, about how much time it's going
to take to do this computation, so you're not
going to get surprised by something showing up. The second one is, a lot of the
time, the worst case is the one that happens. Professor Guttag used an example
of looking in the dictionary for something. Now, imagine that dictionary
actually has something that's a linear search to go through
it, as opposed to the hashing he did, so it's a list,
for example. If it's in there, you'll find
it perhaps very quickly. If it's not there, you've got
to go through everything to say it's not there. And so the worst case often
is the one that shows up, especially in things
like search. So, as a consequence, we're
going to stick with the worst case analysis. Now, I've got two
minutes left. I was going to start showing
you some examples, but I think, rather than doing that,
I'm going to stop here, I'm going to give you two minutes
back of time, but I want to just point out to you that we
are going to have fun next week, because I'm going to show
you what in the world that has to do with
efficiency. So with that, we'll
see you next time.

Linear Algebra

The following content is
provided under a Creative Commons license. Your support will help MIT
OpenCourseWare continue to offer high quality educational
resources for free. To make a donation, or to view
additional materials from hundreds of MIT courses, visit
MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: OK, we're ready to
start the eleventh lecture. We're still in the middle
of sketching. And, indeed, one of the reasons
why we did not talk about hyperbolic functions is
this that we're running just a little bit behind. And we'll catch up
a tiny bit today. And I hope all the way on
Tuesday of next week. So let me pick up where we
left off, with sketching. So this is a continuation. I want to give you one
more example of how to sketch things. And then we'll go through
it systematically. So the second example that
we did as one example last time, is this. The function is x
+ + 1 / x + + 2. And I'm going to save you
the time right now. This is very typical of me,
especially if you're in a hurry on an exam, I'll
just tell you what the derivative is. So in this case, it's
1 / (x + 2)^2. Now, the reason why I'm bringing
this example up, even though it'll turn out to be
a relatively simple one to sketch, is that it's easy to
fall into a black hole with this problem. So let me just show you. This is not equal to 0. It's never equal to 0. So that means there are
no critical points. At this point, students, many
students who have been trained like monkeys to do exactly
what they've been told, suddenly freeze and give up. Because there's nothing to do. So this is the one thing that
I have to train out of you. You can't just give
up at this point. So what would you suggest? Can anybody get us
out of this jam? Yeah. STUDENT: [INAUDIBLE] PROFESSOR: Right. So the suggestion was
to find the x values where f (x) is undefined. In fact, so now that's a fairly
sophisticated way of putting the point that I want to
make, which is that what we want to do is go back to
our precalculus skills. And just plot points. So instead, you go back to
precalculus and you just plot some points. It's a perfectly reasonable
thing. Now, it turns out that the most
important point to plot is the one that's not there. Namely, the value of x = - 2. Which is just what
was suggested. Namely, we plot the
points where the function is not defined. So how do we do that? Well, you have to think about
it for a second and I'll introduce some new notation
when I do it. If I evaluate 2 at this place,
actually I can't do it. I have to do it from the
left and the right. So if I plug in - 2 on the
positive side, from the right, that's going to be equal to - 2
+ 1 / - 2, a little bit more than - 2, +2. Which is - 1 divided by - now,
this denominator is - 2, a little more than that, +2. So it's a little more than 0. And that is, well we'll fill
that in in a second. Everybody's puzzled. Yes. STUDENT: [INAUDIBLE] PROFESSOR: No, that's
the function. I'm plotting points, I'm
not differentiating. I've already differentiated
it. I've already got something
that's a little puzzling. Now I'm focusing on
the weird spot. Yes, another question. STUDENT: Wouldn't it be
a little less than 0? PROFESSOR: Wouldn't it be
a little less than 0? OK, that's a very good point
and this is a matter of notation here. And a matter of parentheses. So wouldn't this be a
little less than 2. Well, if the parentheses were
this way; that is, 2+ , with a - after I did the 2+ , then
it would be less. But it's this way. OK. So the notation is, you
have a number and you take the part of it. That's the part which is a
little bit bigger than it. And so this is what I mean. And if you like, here I can put
in those parentheses too. Yeah, another question. STUDENT: [INAUDIBLE] PROFESSOR: Why doesn't the
top one have a plus? The only reason why the top
one doesn't have a plus is that I don't need it
to evaluate this. And when I take the limit, I
can just plug in the value. Whereas here, I'm
still uncertain. Because it's going to be 0. And I want to know which
side of 0 it's on. Whether it's on the positive
side or the negative side. So this one, I could have
written here a parentheses 2+, but then it would have just
simplified to - 1. In the limit. So now, I've got a negative
number divided by a tiny positive number. And so, somebody want to
tell me what that is? Negative infinity. So, we just evaluated this
function from one side. And if you follow through the
other side, so this one here, you get something very similar,
except that this should be -- whoops, what
did I do wrong? I meant this. I want it -2 the same base
point, but I want to go from the left. So that's going to be -
2 + 1, same numerator. And then this - 2 on the left +
2, and that's going to come out to be - 1 / 0 -, which
is plus infinity. Or just plain infinity,
we don't have to put the plus sign. So this is the first part
of the problem. And the second piece, to get
ourselves started, you could evaluate this function
at any point. This is just the most
interesting point, alright? This is just the most
interesting place to evaluate it. Now, the next thing that I'd
like to do is to pay attention to the ends. And I haven't really said
what the ends are. So the ends are just all the
way to the left and all the way to the right. So that means x going to
plus or minus infinity. So that's the second thing I
want to pay attention to. Again, this is a little bit
like a video screen here. And we're about to discover
something that's really off the screen, in both cases. We're taking care of what's
happening way to the left, way to the right, here. And up above, we just took
care what happens way up and way down. So on these ends, I need to
do some more analysis. Which is related to a
precalculus skill which is evaluating limits. And here, the way to do it is
to divide by x the numerator and denominator. Write it as (1 + 1
/ x)/ (1 + 2 /x). And then you can see what
happens as x goes to plus or minus infinity. It just goes to 1. So, no matter whether x is
positive or negative. When it gets huge, these two
extra numbers here go to 0. And so, this tends to 1. So if you like, you could
abbreviate this as f (+ or - infinity) = 1. So now, I get to draw this. And we draw this using
asymptotes. So there's a level
which is y = 1. And then there's another
line to draw. Which is x = - 2. And now, what information
do I have so far? Well, the information that I
have so far is that when we're coming in from the right, that's
to - 2, it plunges down to minus infinity. So that's down like this. And I also know that it goes
up to infinity on the other side of the asymptote. And over here, I know it's
going out to the level 1. And here it's also going
to the level 1. Now, there's an issue. I can almost finish
this graph now. I almost have enough information
to finish it. But there's one thing
which is making me hesitate a little bit. And that is, I don't know, for
instance, over here, whether it's going to maybe dip below
and come back up. Or not. So what does it do here? Can anybody see? Yeah. STUDENT: [INAUDIBLE] PROFESSOR: It can't dip
below because there are no critical points. What a precisely
correct answer. So that's exactly right. The point here is that because
f' is not 0, it can't double back on itself. Because there can't be any of
these horizontal tangents. It can't double back, so
it can't backtrack. So sorry, if f' is not
0, f can't backtrack. And so that means that it
doesn't look like this. It just goes like this. So that's basically it. And it's practically the
end of the problem. Goes like this. Now you can decorate
your thing, right? You may notice that maybe it
crosses here, the axes, you can actually evaluate
these places. And so forth. We're looking right now for
qualitative behavior. In fact, you can see where
these places hit. And it's actually a little
higher up than I drew. Maybe I'll draw it accurately. As we'll see in a second. So that's what happens
to this function. Now, let's just take a look in
a little bit more detail, by double checking. So we're just going to double
check what happens to the sign of the derivative. And in the meantime, I'm going
to explain to you what the derivative is and also talk
about the second derivative. So first of all, the trick for
evaluating the derivative is an algebraic one. I mean, obviously you can do
this by the quotient rule. But I just point out that this
is the same thing as this. And now it has, whoops, that
should be a 2 in the denominator. And so, now this has the
form 1 - (1 / x + 2). So this makes it easy to see
what the derivative is. Because the derivative of
a constant is 0, right? So this is, derivative,
is just going to be, switch the sign. This is what I wrote before. And that explains it. But incidentally, it
also shows you that that this is a hyperbola. These are just two curves
of a hyperbola. So now, let's check the sign. It's already totally obvious
to us that this is just a double check. We didn't actually even have to
pay any attention to this. It had better be true. This is just going to check
our arithmetic. Namely, it's increasing here. It's increasing there. That's got to be true. And, sure enough, this is
positive, as you can see it's 1 over a square. So it is increasing. So we checked it. But now, there's one more thing
that I want to just have you watch out about. So this means that
f is increasing. On the interval minus infinity
&lt; x &lt; - 2. And also from - 2 all the
way out to infinity. So I just want to warn you, you
cannot say, don't say f is increasing on minus infinity
&lt; infinity, for all x. OK, this is just not true. I've written it on the board,
but it's wrong. I'd better get rid of it. There it is. Get rid of it. And the reason is, so first of
all it's totally obvious. It's going up here. But then it went zooming
back down there. And here this was true, but
only if x is not - 2. So there's a break. And you've got to pay attention
to the break. So basically, the moral here
is that if you ignore this place, it's like ignoring
Mount Everest, or the Grand Canyon. You're ignoring the most
important feature of this function here. If you're going to be figuring
out where things are going up and down, which is basically all
we're doing, you'd better pay attention to these
kinds of places. So don't ignore them. So that's the first remark. And now there's just a little
bit of decoration as well. Which is the role of the
second derivative. So we've written down the
first derivative here. The second derivative is now
- 2 / (x + 2)^3, right? So I get that from
differentiating this formula up here for the first
derivative. And now, of course, that's also,
only works for x not equal to - 2. And now, we can see that this is
going to be negative, let's see, where is it negative? When this is a positive
quantity, so when -2 &lt; x &lt; infinity. It's negative. And this is where this thing
is concave. Let's see. Did I say that right? Negative, right? This is concave down. Right. And similarly, if I look at this
expression, the numerator is always negative but the
denominator becomes negative as well when x &lt; -2. So this becomes positive. So this case, it was negative
over positive. In this case it was negative
divided by negative. So here, this is in the range
- infinity &lt; x &lt; -2. And here it's concave up. Now, again, this is just
consistent with what we're already guessing. Of course we already know it in
this case if we know that this is a hyperbola. That it's going to be concave
down to the right of the vertical line, dotted
vertical line. And concave up to the left. So what extra piece of
information is it that this is giving us? Did I say this backwards? No. That's OK. So what extra piece of
information is this giving us? It looks like it's giving
us hardly anything. And it really is giving
us hardly anything. But it is giving us something
that's a little aesthetic. It's ruling out the possibility
of a wiggle. There isn't anything like
that in the curve. It can't shift from curving this
way to curving that way to curving this way. That doesn't happen. So these properties say
there's no wiggle in the graph of that. Alright. So. Question. STUDENT: Do we define the
increasing and decreasing base purely on the derivative, or
the sort of more general definition of picking any
two points and seeing. Because sometimes there can be
an inconsistency between the two definitions. PROFESSOR: OK, so the question
is, in this course are we going to define positive
derivative as being the same thing as increasing. And the answer is no. We'll try to use these
terms separately. What's always true is that if
f' is positive, then f is increasing. But the reverse is not
necessarily true. It could be very flat, the
derivative can be 0 and still the function can
be increasing. OK, the derivative can
be 0 at a few places. For instance, like
some cubics. Other questions? So that's as much as I need
to say in general. I mean, in a specific case. But I want to get you a general
scheme and I want to go through a more complicated
example that gets all the features of this
kind of thing. So let's talk about a general
strategy for sketching. So the first part of
this strategy, if you like, let's see. I have it all plotted
out here. So I'm going to make sure I
get it exactly the way I wanted you to see. So I have, its plotting. The plot thickens. Here we go. So plot, what is it that
you should plot first? Before you even think about
derivatives, you should plot discontinuities. Especially the infinite ones. That's the first thing
you should do. And then, you should plot
end points, for ends. For x going to plus or minus
infinity if there don't happen to be any finite ends
to the problem. And the third thing you can do
is plot any easy points. This is optional. At your discretion. You might, for instance, on this
example, plot the places where the graph crosses
the axis. If you want to. So that's the first part. And again, this is
all precalculus. So now, in the second part
we're going to solve this equation and we're
going to plot the critical points and values. In the problem which we just
discussed, there weren't any. So this part was empty. So the third step is to decide
whether f', sorry, whether, f' is positive or negative
on each interval. Between critical points,
discontinuities. The direction of the sign, in
this case it doesn't change. It goes up here and it
also goes up here. But it could go up here and
then come back down. So the direction can change
at every critical point. It can change at every
discontinuity. And you don't know. However, this particular step
has to be consistent with 1 and 2, with steps 1 and 2. In fact, it will never, if you
can succeed in doing steps 1 and 2, you'll never
need step 3. All it's doing is
double-checking. So if you made an arithmetic
mistake somewhere, you'll be able to see it. So that's maybe the most
important thing. And it's actually the most
frustrating thing for me when I see people working on
problems, is they start step 3, they get it wrong, and then
they start trying to draw the graph and it doesn't work. Because it's inconsistent. And the reason is some
arithmetic error with the derivative or something like
that or some other misinterpretation. And then there's a total mess. If you start with these two
steps, then you're going to know when you get to this step
that you're making mistakes. People don't generally make
as many mistakes in the first two steps. Anyway, in fact you can skip
this step if you want. But that's at risk of not
double-checking your work. So what's the fourth step? Well, we take a look
at whether f'' is positive or negative. And so we're deciding on things
like whether it's concave up or down. And we have these points, f''
( x ) = 0, which are called inflection points. And the last step is just
to combine everything. So this is this the scheme,
the general scheme. And let's just carry it out
in a particular case. So here's the function
that I'm going to use as an example. I'll use f ( x ) = x / ln x. And because the logarithm
- yeah, question. Yeah. STUDENT: [INAUDIBLE] PROFESSOR: The question
is, is this optional. So that's a good question. Is this optional. STUDENT: [INAUDIBLE] PROFESSOR: OK, the question
is is this optional; this kind of question. And the answer is, it's more
than just -- so, in many instances, I'm not going
to ask you to. I strongly recommend that if I
don't ask you to do it, that you not try. Because it's usually awful to
find the second derivative. Any time you can get away
without computing a second derivative, you're better off. So in many, many instances. On the other hand, if I ask you
to do it it's because I want you to have the,
work to do it. But basically, if nobody forces
you to, I would say never do step 4. Other questions. Alright. So we're going to force
ourselves to do step 4, however, in this instance. But maybe this will be
one of the few times. So here we go, just for
illustrative purposes. OK, now. So here's the function that
I want to discuss. And the range has to be x
positive, because the logarithm is not defined
for negative values. So the first thing that I'm
going to do is, I'd like to follow the scheme here. Because if I don't follow the
scheme, I'm going to get a little mixed up. So the first part is to find
the singularities. That is, the places where
f is infinite. And that's when the logarithm,
the denominator, vanishes. So that's f ( 1 +),
if you like. So that's 1 / ln 1 +, which is
1 / 0, with a little bit of positiveness to it. Which is infinity. And second, we do it
the other way. And not surprisingly,
this comes out to be negative infinity. Now, the next thing I want
to do is the ends. So I call these the ends. And there are two of them. One of them is f ( 0 ) from
the right. f ( 0+). So that is 0 + / ln 0 +, which
is 0 plus divided by, well, ln 0 + is actually minus
infinity. That's what happens
to the logarithm, goes to minus infinity. So this is 0 over infinity,
which is definitely 0, there's no problem. about
what happens. The other side, so this is the
end, this is the first end. The range is this. And I just did the
left endpoint. And so now I have to do the
right endpoint, I have to let x go to infinity. So if I let x go to infinity,
I'm just going to have to think about it a little
bit by plugging in a very large number. I'll plug in 10 ^ 10, just
to see what happens. So if I plug in 10 ^ 10
into x ln x, I get 10 ^ 10 / ln 10^10. Which is 10 ^ 10
/ 10 ( log 10). So the denominator, this
number here, is about 2.something. 2.3 or something. So this is maybe 230 in the
denominator, and this is a number with ten 0's after it. So it's very, very large. I claim it's big. And that gives us the clue that
what's happening is that this thing is infinite. So, in other words, our
conclusion is that f of infinity is infinity. So what do we have so far
for our function? We're just trying to build the
scaffolding of the function. And we're doing it by taking
the most important points. And from a mathematician's
point of view, the most important points are the
ones which are sort of infinitely obvious. For the ends of the problem. So that's where we're heading. We have a vertical asymptote,
which is at x = 1. So this gives us x = 1. And we have a value which
is that it's 0 here. And we also know that when we
come in from the - sorry, so we come in from the left, that's
f, the one from the left, we get negative
infinity. So it's diving down. It's going down like this. And, furthermore, on the other
side we know it's climbing up. So it's going up like this. Just start a little higher. Right, so. So far, this is what we know. Oh, and there's one other
thing that we know. When we go to plus infinity,
it's going back up. So, so far we have this. Now, already it should be pretty
obvious what's going to happen to this function. So there shouldn't be
many surprises. It's going to come
down like this. Go like this, it's going to turn
around and go back up. That's what we expect. So we don't know that yet,
but we're pretty sure. So at this point, we can
start looking at the critical points. We can do our step 2 here -- we
need a little bit more room here -- and see what's happening
with this function. So I have to differentiate it. And it's, this is the
quotient rule. So remember the function
is up here, x / ln x. So I have a ln x^2 in
the denominator. And I get here the derivative of
x is 1, so we get 1 ( ln x) - x ( the derivative of
ln x, which is 1 /x). So all told, that's (ln
x - 1) / ln x^2. So here's our derivative. And now, if I set this equal
to 0, at least in the numerator, the numerator
is 0 when x = e. The ln e = 1. So here's our critical point. And we have a critical
value, which is f(e). And that's going
to be e / ln e. Which is e, again. Because ln e = 1. So now I can also plot
the critical point, which is down here. And there's only one of them,
and it's at (e e). That's kind of not to scale
here, because my blackboard isn't quite tall enough. It should be over here and
then, it's slope 1. But I dipped it down. So this is not to scale, and
indeed that's one of the things that we're not going to
attempt to do with these pictures, is to make
them to scale. So the scale's a little
squashed. So, so far I have this
critical point. And, in fact, I'm going
to label it with a c. Whenever I have a critical point
I'll just make sure that I remember that that's
what it is. And since there's only one, the
rest of this picture is now correct. That's the same mechanism that
we used for the hyperbola. Namely, we know there's only
one place where the derivative is 0. So that means there no more
horizontals, so there's no more backtracking. It has to come down to here. Get to there. This is the only place
it can turn around. Goes back up. It has to start here and it
has to go down to there. It can't go above 0. Do not pass go, do
not get positive. It has to head down here. So that's great. That means that this
picture is almost completely correct now. And the rest is more
or less decoration. We're pretty much done with
the way it looks, at least schematically. However, I am going to punish
you, because I warned you. We are going to go over here and
do this step 4 and fix up the concavity. And we're also going to do
a little bit of that double-checking. So now, let's again, just,
I want to emphasize. We're going to do
a double-check. This is part 3. But in advance, I already have,
based on this picture I already know what
has to be true. That f is decreasing on
0 to 1. f is also decreasing on 1 to e. And f is increasing
on e to infinity. So, already, because we've plot
a bunch of points and we know that there aren't any
places where the derivative vanishes, we already know
it goes down, down, up. That's what it's got to do. Now, we'll just make sure that
we didn't make any arithmetic mistakes, now. By actually computing
the derivative, or staring at it, anyway. And making sure that
it's correct. So first of all, we just take
a look at the numerator. So f,' remember, was (ln
x - 1) / ln x^2, the quantity squared. So the denominator
is positive. So let's just take a look
at the three ranges. So we have 0 &lt; x &lt; 1. And on that range, the logarithm
is negative, so this is negative divided by positive,
which is negative. That's decreasing,
that's good. And in fact, that also works
on the next range. 1 l&lt; x &lt; e, it's negative
divided by positive. And the only reason why we
skipped 1, again, is that it's undefined there. And there's something dramatic
happening there. And then, at the last range,
when x is bigger than e, that means the logarithm is already
bigger than 1. So the numerator is now
positive, and the denominator's still positive,
so it's increasing. So we've just double-checked
something that we already knew. Alright, so that's pretty
much all there is to say about step 3. So this is checking the
positivity and negativity. And now, step 4. There is one small point
which I want to make before we go on. Which is that sometimes, you
can't evaluate the function or its derivative particularly
well. So sometimes you can't plot
the points very well. And if you can't plot the points
very well, then you might have to do 3 first, to
figure out what's going on a little bit. You might have to skip. So now we're going to go on
the second derivative. But first, I want to use an
algebraic trick to rearrange the terms. And I want to notice
one more little point. Which I, as I say, this is
decoration for the graph. So I want to rewrite
the formula. Maybe I'll do it right
over here. Another way of writing this is
(1 / ln x) - (1 / (ln x)^2). So that's another way of
writing the derivative. And that allows me to
notice something that I missed, before. When I solved the equation ln
x - 1 - this is equal to 0 here, this equation here. I missed a possibility. I missed the possibility
that the denominator could be infinity. So actually, if the
denominator's infinity, as you can see from the other
expression there, it actually is true that the derivative
is 0. So also when x = 0 +, the
slope is going to be 0. Let me just emphasize
that again. If you evaluate using this other
formula over here, this is (1 / ln 0+) - (1
/ (ln 0+)^2). That's 1 / - infinity
- 1 / infinity, if you like, squared. Anyway, it's 0. So this is 0. The slope is 0 there. That is a little piece of
decoration on our graph. It's telling us, going back to
our graph here, it's telling us this is coming in with
slope horizontal. So we're starting
out this way. That's just a little start
here to the graph. It's a horizontal slope. So there really were two places
where the slope was horizontal. Now, with the help of this
second formula I can also differentiate a second time. So it's a little bit easier to
do that if I differentiate 1 / ln, that's -( ln x) ^ - 2 ( 1
/ x) + 2 (ln x) ^ -3 (1/x). And that, if I put it over a
common denominator, is x ln x^3 times, let's see here. I guess I'll have to
take the 2 - ln x. So I've now rewritten the
formula for the second derivative as a ratio. Now, to decide the sign, you see
there are two places where the sign flips. The numerator crosses when the
logarithm is 2, that's going to be when x = e ^2. And the denominator flips when
x = 1, that's when the log flips from positive
to negative. So we have a couple
of ranges here. So, first of all, we have
the range from 0 to 1. And then we have the range
from 1 to e^2. And then we have the
range from e ^2 all the way out to infinity. So between 0 and 1, the
numerator is, well this is a negative number in this, so
minus a negative number is positive, so the numerator
is positive. And the denominator is negative,
because the ln is negative it's taken to
the third power. So this is a negative numbers,
so it's positive divided by a negative number, which
is less than 0. That means it's concave down. So this is concave down plot. And that's a good thing, because
over here this is concave down. So there are no wiggles. It goes straight down,
like this. And then the other two pieces
are f'' is equal to, well it's going to switch here. The denominator becomes
positive. So it's positive
over positive. So this is concave up. And that's going over here. But notice that it's not the
bottom where it turns around, it's somewhere else. So there's another
transition here. This is e ^2. This is e. So what happens at the end is,
again, the sign flips again. Because the numerator,
now, when x &gt; e ^2, becomes negative. And this is negative divided by positive, which is negative. And part is concave down. And so we didn't quite
draw the graph right. There's an inflection point
right here, which I'll label with i. Makes a turn the other
way at that point. So there was a wiggle. There's the wiggle. Still going up, still
going to infinity. But kind of the slope of
the mountain, right? It's going the other way. This point happens to
be (e^2, e ^2 / 2). So that's as detailed
as we'll ever get. And indeed, the next game is
going to be avoid being, is to avoid being this detailed. So let me introduce
the next subject. Which is maxima and minima. OK, now, maxima and minima,
maximum and minimum problems can be described graphically
in the following ways. Suppose you have a function,
right, here it is. OK? Now, find the maximum. And find the minimum. OK. So this problem is done. The point being, that it is easy
to find max and the min with the sketch. It's very easy. The goal, the problem, is that
the sketch is a lot of work. We just spent 20 minutes
sketching something. We would not like to spend all
that time every single time we want to find a maximum
and minimum. So the goal is to do
it with, so our goal is to use shortcuts. And, indeed, as I said earlier,
we certainly never want to use the second
derivative if we can avoid it. And we don't want to decorate
the graph and do all of these elaborate, subtle, things which
make the graph look nicer and really, or
aesthetically appropriate. But are totally unnecessary
to see whether the graph is up or down. So essentially, this whole
business is out, which is a good thing. And, unfortunately, those early
parts are the parts that people tend to ignore. Which are typically, often,
very important. So let me first tell you
the main point here. So the key idea. Key to finding maximum. So the key point is,
we only need to look at critical points. Well, that's actually what
it seems like to, in many calculus classes. But that's not true. This is not the end
of the sentence. And, end points, and points
of discontinuity. So you must watch
out for those. If you look at the example that
I just drew here, which is the one that I carried out,
you can see that there are actually five extreme points
on this picture. So let's switch. So we'll take a look. There are five places where the
max or the min might be. There's this important point. This is, as I say, the
scaffolding of the function. There's this point, there
down at minus infinity. There's this, there's this,
and there's this. Only one out of five is
a critical point. So there's more that
you have to pay attention to on the function. And you always have to keep the
schema, the picture of the function, in the back
of your head. Even though this may be the most
interesting point, and the one that you're going
to be looking at. So we'll do a few examples
of that next time.

AI

The following content is
provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer
high quality educational resources for free.
To make a donation or to view additional materials from
hundreds of MIT courses, visit MIT OpenCourseWare at
OCW.mit.edu. Recall that yesterday we saw,
no, two days ago we learned about the curl of a vector field
in space. And we said the curl of F is
defined by taking a cross product between the symbol dell
and the vector F. Concretely, the way we would
compute this would be by putting the components of F into this
determinant and expanding and then getting a vector with
components Ry minus Qz, Pz minus Rx and Qx minus Py.
I think I also tried to explain very quickly what the
significance of a curl is. Just to tell you again very
quickly, basically curl measures, 
if you mention that your vector field measures the velocity in
some fluid then the curl measures how much rotation is
taking place in that fluid. Measures the rotation part of a
velocity field. More precisely the direction
corresponds to the axis of rotation and the magnitude
corresponds to twice the angular velocity. Just to give you a few quick
examples. If I take a constant vector
field, so everything translates at the
same speed, then obviously when you take
the partial derivatives you will just get a bunch of zeros so the
curl will be zero. If you take a vector field that
stretches things, let's say, for example,
we are going to stretch things along the x-axis,
that would be a vector field that goes parallel to the x
direction but maybe, say, x times i.
So that when you are in front of a plane of a blackboard you
are moving forward, when you are behind you are
moving backwards, things are getting expanded in
the x direction. If you compute the curl,
you can check each of these. Again, they are going to be
zero. There is no curl.
This is not what curl measures. I mean, actually,
what measures expansion, stretching is actually
divergence. If you take the divergence of
this field, you would get one plus zero
plus zero, it looks like it will be one, 
so in case you don't remember,  I mean divergence precisely
measures this stretching effect in your field.
And, on the other hand,  if you take something that
corresponds to, say,
rotation about the z-axis at unit angular velocity -- That
means they are going to moving in circles around the z-axis.
One way to write down this field, let's see,
the z component is zero because everything is moving
horizontally. And in the x and y directions, 
if you look at it from above,  well, it is just going to be
our good old friend the vector field that rotates everything
[at unit speed?]. And we have seen the formula
for this one many times. The first component is minus y,
the second one is x. Now, if you compute the curl of
this guy, you will get zero, zero, two, two k.
And so k is the axis of rotation, two is twice the
angular velocity. And now, of course, 
you can imagine much more complicated motions where you
will have -- For example, if you look at the Charles
River very carefully then you will see that water is flowing,
generally speaking,  towards the ocean.
But, at the same time, there are actually a few eddies
in there and with water swirling.
Those are the places where there is actually curl in the
flow. Yes.
I don't know how to turn out the lights a bit,
but I'm sure there is a way. Does this do it?
Is it working? OK.
You're welcome. Hopefully it is easier to see
now. That was about curl.
Now, why do we care about curl besides this motivation of
understanding motions? One place where it comes up is
when we try to understand whether a vector field is
conservative. Remember we have seen that a
vector field is conservative if and only if its curl is zero.
That is the situation in which we are allowed to try to look
for a potential function and then use the fundamental
theorem. But another place where this
comes up, if you remember what we did in
the plane, curl also came up when we tried
to convert nine integrals into double integrals.
That was Greene's theorem. Well, it turns out we can do
the same thing in space and that is called Stokes' theorem.
What does Stokes' theorem say? It says that the work done by a
vector field along a closed curve can be replaced by a
double integral of curl F. Let me write it using the dell
notation. That is curl F.
Dot ndS on a suitably chosen surface.
That is a very strange kind of statement.
But actually it is not much more strange than things we have
seen before. I should clarify what this
means. C has to be a closed curve in
space. And S can be any surface
bounded by C. For example, 
what Stokes' theorem tells me is that let us say that I have
to compute some line integral on maybe,
say, the unit circle in the x, y plane.
Of course I can set a line integral directly and compute it
by setting x equals cosine T, y equals sine T,
z equals zero. But maybe sometimes I don't
want to do that because my vector field is really
complicated. And instead I will want to
reduce things to a surface integral.
Now, I know that you guys are not necessarily fond of
computing flux of vector fields for surfaces so maybe you don't
really see the point. But sometimes it is useful.
Sometimes it is also useful backwards because,
actually, you have a surface integral that you would like to
turn into a line integral. What Stokes' theorem says is
that I can choose my favorite surface whose boundary is this
circle. I could choose,
for example, a half sphere if I want or I
can choose, let's call that s1, I don't know,
a pointy thing, s2.
Probably the most logical one, actually, would be just to
choose a disk in the x, y plane.
That would probably be the easiest one to set up for
calculating flux. Anyway, 
what Stokes' theorem tells me is I can choose any of these
surfaces, whichever one I want, 
and I can compute the flux of curl F through this surface.
Curl F is a new vector field when you have this formula that
gives you a vector field you compute its flux through your
favorite surface, and you should get the same
thing as if you had done the line integral for F.
That is the statement. Now, there is a catch here.
What is the catch? Well, the catch is we have to
figure out what conventions to use because remember when we
have a surface there are two possible orientations.
We have to decide which way we will counter flux positively,
which way we will counter flux negatively.
And, if we change our choice, then of course the flux will
become the opposite. Well, similarly to define the
work, I need to choose which way I am going to run my curve.
If I change which way I go around the curve then my work
will become the opposite. What happens is I have to
orient the curve C and the surface S in compatible ways.
We have to figure out what the rule is for how the orientation
of S and that of C relate to each other.
What about orientation? Well, we need the orientations
of C and S to be compatible and they have to explain to you what
the rule is. Let me show you a picture.
The rule is if I walk along C with S to my left then the
normal vector is pointing up for me.
Let me write that. If I walk along C,
I should say in the positive direction, in the direction that
I have chosen to orient C. With S to my left then n is
pointing up for me. Here is the example.
If I am walking on this curve, it looks like the surface is to
my left. And so the normal vector is
going towards what is up for me. Any questions about that?
I see some people using their right hands.
That is also right-handable which I am going to say in just
a few moments. That is another way to remember
this. Before I tell you about the
right-handable version, let me just try something.
Actually, I am not happy with this orientation of C and I want
to orient my curve C going clockwise on the picture.
So the other orientation. Then, if I walk on it this way,
the surface would be to my right.
You can remember, if it helps you,
that if a surface is to your right then the normal vector
will go down. The other way to think about
this rule is enough because if you are walking clockwise,
well, you can change that to counterclockwise just by walking
upside down. This guy is walking clockwise
on C. And while for him,
if you look carefully at the picture, the surface is actually
to his left when you flip upside down.
Yeah, it is kind of confusing. But, anyway,
maybe it's easier if you actually rotate in the picture.
And now it is getting actually really confusing because his
walking upside up with, actually, the surface is to his
left. I mean where he is at here is
actually at the front and this is the back, but that is kind of
hard to see. Anyway, whichever method will
work best for you. Perhaps it is easiest to first
do it with the other orientation,
this one,  and this side, 
if you want the opposite one,  then you will just flip
everything. Now, what is the other way of
remembering this with the right-hand rule?
First of all, take your right hand,
not your left. Even if your right hand is
actually using a pen or something like that in your
right hand do this. And let's take your fingers in
order. First your thumb.
Let's make your thumb go along the object that has only one
dimension in there. That is the curve.
Well, let's look at the top picture up there.
I want my thumb to go along the curve so that is kind of towards
the right. Then I want to make my index
finger point towards the surface.
Towards the surface I mean towards the interior of the
surface from the curve. And when I am on the curve I am
on the boundary of the surface, so there is a direction along
the surface that is the curve and the other one is pointing
into the surface. That one would be pointing kind
of to the back slightly up maybe, so like that.
And now your middle finger is going to point in the direction
of the normal vector. That is up, at least if you
have the same kind of right hand as I do.
The other way of doing it is using the right-hand rule along
C positively. The index finger towards the
interior of S. Sorry, I shouldn't say interior.
I should say tangent to S towards the interior of S.
What I mean by that is really the part of S that is not its
boundary, so the rest of the surface.
Then the middle finger points parallel to n.
Let's practice. Let's say that I gave you this
curve bounding this surface. Which way do you think the
normal vector will be going? Up. Yes. Everyone is voting up.
Imaging that I am walking around C.
That is to my left. Normal vector points up.
Imagine that you put your thumb along C, your index towards S
and then your middle finger points up.
Very good. N points up.
Another one. It is interesting to watch you
guys. I think mostly it is going up.
The correct answer is it goes up and into the cone.
How do we see that? Well, one way to think about it
is imagine that you are walking on C, on the rim of this cone.
You have two options. Imagine that you are walking
kind of inside or imagine that you are walking kind of outside.
If you are walking outside then S is to your right,
but it does not sound good. Let's say instead that you are
walking on the inside of a cone following the boundary.
Well, then the surface is to your left.
And so the normal vector will be up for you which means it
will be pointing slightly up and into the cone.
Another way to think about it,  through the right-hand rule, 
from this way index going kind of down because the surface goes
down and a bit to the back. And then the normal vector
points up and in. Yet another way,
if you deform continuously your surface then the conventions
will not change. See, this is kind of
topology in a way.
You can deform things and nothing will change.
So what if we somehow flatten our cone, push it a bit up so
that it becomes completely flat? Then, if you had a flat disk
with the curve going counterclockwise,
the normal vector would go up. Now take your disk with its
normal vector sticking up. If you want to paint the face a
different color so that you can remember that was beside with a
normal vector and then push it back down to the cone,
you will see that the painted face,
the one with the normal vector on that side is the one that is
inside and up. Does that make sense?
Anyway, I think you have just to play with these examples for
long enough and get it. OK. The last one.
Let's say that I have a cylinder.
So now this guy has actually two boundary curves,
C and C prime. And let's say I want to orient
my cylinder so that the normal vector sticks out.
How should I choose the orientation of my curves?
Let's start with, say, the bottom one.
Would the bottom one be going clockwise or counterclockwise.
Most people seem to say counterclockwise,
and I agree with that. Let me write that down and
claim C prime should go counterclockwise.
One way to think about it, actually, it's quite easy,
you mentioned that you're walking on the outside of the
cylinder along C prime. If you want to walk along C
prime so that the cylinder is to your left, that means you have
to actually go counterclockwise around it.
The other way is use your right hand.
Say when you're at the front of C prime,
your thumb points to the right,  your index points up because
that's where the surface is, and then your middle finger
will point out. What about C?
Well, C I claim we should be doing clockwise.
I mean think about just walking again on the surface of the
cylinder along C. If you walk clockwise,
you will see that the surface is to your left or use the
right-hand rule. Now, if a problem gives you
neither the orientation of a curve nor that of the surface
then it's up to you to make them up.
But you have to make them up in a consistent way.
You cannot choose them both at random.
All right. Now we're all set to try to use
Stokes' theorem. Well, let me do an example
first. The first example that I will
do is actually a comparison. Stokes' versus Green.
I want to show you how Green's theorem for work that we saw in
the plane, but also involved work and curl
and so on, is actually a special case of
this. Let's say that we will look at
the special case where our curve C is actually a curve in the x,
y plane. And let's make it go
counterclockwise in the x, y plane because that's what we
did for Green's theorem. Now let's choose a surface
bounded by this curve. Well, as I said,
I could make up any surface that comes to my mind.
But, if I want to relate to this stuff, I should probably
stay in the x, y plane.
So I am just going to take my surface to be the piece of the
x, y plane that is inside my curve.
So let's say S is going to be a portion of x,
y plane bounded by a curve C, and the curve C goes
counterclockwise. Well, then I should look at
[the table?]. For work along C of my favorite
vector field F dot dr. So that will be the line
integral of Pdx plus Qdy. Like I said,
if I call the components of my field P, Q and R,
it will be Pdx plus Qdy plus Rdz, but I don't have any Z
here. Dz is zero on C.
If I evaluate for line integral, I don't have any term
involving dz. Z is zero.
Now, let's see what Strokes says.
Stokes says instead I can compute for flux through S of
curve F. But now what's the normal
vector to my surface? Well, it's going to be either k
or negative k. I just have to figure out which
one it is. Well, if you followed what
we've done there, you know that the normal vector
compatible with this choice for the curve C is the one that
points up. My normal vector is just going
to be k hat, so I am going to replace my normal vector by k
hat. That means, actually,
I will be integrating curl dot k.
That means I am integrating the z component of curl.
Let's look at curl F dot k. That's the z component of curl
F. And what's the z component of
curl? Well, I conveniently still have
the values up there. It's Q sub x minus P sub y.
My double integral becomes double integral of Q sub x minus
P sub y. What about dS?
Well, I am in a piece of the x, y plane, so dS is just dxdy or
your favorite combination that does the same thing.
Now, see, if you look at this equality, integral of Pdx plus
Qdy along a closed curve equals double integral of Qx minus Py
dxdy. That is exactly the statement
of Green's theorem. I mean except at that time we
called things m and n, but really that shouldn't
matter. This tells you that,
in fact, Green's theorem is just a special case of Stokes'
in the x, y plane. Now, another small remark I
want to make right away before I forget,
you might think that these rules that we've made up about
compatibility of orientations are completely arbitrary.
Well, they are literally in the same way as our convention for
which we guy curl is arbitrary. We chose to make the curl be
this thing and not the opposite which would have been pretty
much just as sensible. And, ultimately,
that comes from our choice of making the cross-product be what
it is but of the opposite. Ultimately, it all comes from
our preference for right-handed coordinate systems.
If we had been on the planet with left-handed coordinate
systems then actually our conventions would be all the
other way around, but they are this way.
Any other questions? A surface that you use in
Stokes' theorem is usually not going to be closed because its
boundary needs to be the curve C.
So if you had a closed surface you wouldn't know where to put
your curve. I mean of course you could make
a tiny hole in it and get a tiny curve.
Actually, what that would say,  and we are going to see more
about that so not very important right now,
but what we would see is that for a close surface we would end
up getting zero for the flux. And that is actually because
divergence of curl is zero, but I am getting ahead of
myself. We are going to see that
probably tomorrow in more detail.
Stokes' theorem only works if you can make sense of this.
That means you need your vector field to be continuous and
differentiable everywhere on the surface S.
Now, why is that relevant? Well, say that your vector
field was not defined at the origin and say that you wanted
to do, you know, the example that I
had first with the unit circling the x, y plane.
Normally, the most sensible choice of surface to apply
Stokes' theorem to would be just the flat disk in the x,
y plane. But that assumes that your
vector field is well-defined there.
If your vector field is not defined at the origin but
defined everywhere else you cannot use this guy,
but maybe you can still use, say, the half-sphere,
for example. Or, you could use a piece of
cylinder plus a flat top or whatever you want but not
pressing for the origin. So you could still use Stokes
but you'd have to be careful about which surface you choose.
Now, if instead your vector field is not defined anywhere on
the z-axis then you're out of luck because there is no way to
find a surface bounded by this unit circle without crossing the
z-axis somewhere. Then you wouldn't be able to
Stokes' theorem at all or at least not directly.
Maybe I should write it F defines a differentiable
everywhere on this. But we don't care about what
happens outside of this. It's really only on the surface
that we need it to be OK. I mean, again,
99% of the vector fields that we see in this class are defined
everywhere so that's not an urgent concern,
but still. OK.
Should we move on? Yes. I have a yes.
Let me explain to you quickly why Stokes is true.
How do we prove a theorem like that?
Well,  the strategy,
I mean there are other ways, but the least painful strategy
at this point is to observe what we already know is a special
case of Stokes's theorem. Namely we know the case where
the curve is actually in the x, y plane and the surface is a
flat piece of the x, y plane because that's Green's
theorem which we proved a while ago.
We know it for C and S in the x, y plane.
Now, what if C and S were, say, in the y,
z plane instead of the x, y plane?
Well, then it will not quite give the same picture because
the normal vector would be i hat instead of k hat and they would
be having different notations and it would be integrating with
y and z. But you see that it would
become, again, exactly the same formula.
We'd know it for any of the coordinate planes.
In fact, I claim we know it for absolutely any plane.
And the reason for that is,  sure, when we write it in
coordinates, when we write that this line
integral is integral of Pdx plus Qdy plus Rdz or when we write
that the curl is given by this formula we use the x,
y, z coordinate system. But there is something I
haven't quite told you about. Which is if I switch to any
other right-handed coordinate system,
so I do some sort of rotation of my space coordinates,
then somehow the line integral,  the flux integral, 
the notion of curl makes sense in coordinates.
And the reason is that they all have geometric interpretations.
For example, when I think of this as the
work done by a force, well, the force doesn't care
whether it's being put in x, y coordinates this way or that
way. It still does the same work
because it's the same force. And when I say that the curl
measures the rotation in a motion, well,
that depends on which coordinates you use.
And the same for interpretation of flux.
In fact, if I rotated my coordinates to fit with any
other plane, I could still do the same things.
What I'm trying to say is, in fact, if C and S are in any
plane then we can still claim that it reduces to Green's
theorem. It will be Green's theorem not
in x, y, z coordinates but in some funny rotated coordinate
systems. What I'm saying is that work,
flux and curl makes sense independently of coordinates. Now, this has to stop somewhere.
I can start claiming that I can somehow bend my coordinates to a
plane, any surface is flat. That doesn't really work.
But what I can say is if I have any surface I can cut it into
tiny pieces. And these tiny pieces are
basically flat. So that's basically the idea of
a proof. I am going to decompose my
surface into very small flat pieces.
Given any S we are just going to decompose it into tiny almost
flat pieces. For example,
if I have my surface like this, what I will do is I will just
cut it into tiles. I mean a good example of that
is if you look at those geodesic things,
for example, it's made of all these hexagons
and pentagons. Well, actually,
they're not quite flat in the usual rule, but you could make
them flat and it would still look pretty much like a sphere.
Anyway, you're going to cut your surface into lots of tiny
pieces. And then you can use Stokes'
theorem on each small piece. What it says on each small flat
piece -- It says that the line integral along say,
for example, this curve is equal to the flux
of a curl through this tiny piece of surface.
And now I will add all of these terms together.
If I add all of the small contributions to flux I get the
total flux. What if I add all of the small
line integrals? Well, I get lots of extra junk
because I never asked to compute the line integral along this.
But this guy will come in twice when I do this little plate and
when I do that little plate with opposite orientations.
When I sum all of the little line integrals together,
all of the inner things cancel out,
and the only ones that I go through only once are those that
are at the outer most edges. So, when I sum all of my works
together, I will get the work done just along the outer
boundary C. Sum of work around each little
piece is just actually the work along C, the outer curve.
And the sum of the flux for each piece is going to be the
flux through S. From Stokes' theorem for flat
surfaces, I can get it for any surface.
I am cheating a little bit because you would actually have
to check carefully that this approximation where you flatten
the little pieces that are almost flat is varied.
But, trust me, it actually works. Let's do an actual example.
I mean I said example, but that was more like getting
us ready for the proof so probably that doesn't count as
an actual example. I should probably keep these
statements for now so I am not going to erase this side. Let's do an example.
Let's try to find the work of vector field zi plus xj plus yk
around the unit circle in the x, y plane counterclockwise.
The picture is conveniently already there.
Just as a quick review, let's see how we do that
directly. If we do that directly,
I have to find the integral along C.
So F dot dr becomes zdx plus xdy plus ydz.
But now we actually know that on this circle,
well, z is zero. And we can parameterize x and
y, the unit circle in the x, y plane, so we can take x
equals cosine t, y equals sine t.
That will just become the integral over C.
Well, z times dx, z is zero so we have nothing,
plus x is cosine t times dy is -- Well, if y is sine t then dy
is cosine tdt plus ydz but z is zero.
Now, the range of values for t, well, we are going
counterclockwise around the entire circle so that should go
from zero to 2pi. We will get integral from zero
to 2pi of cosine square tdt which, if you do the
calculation, turns out to be just pi.
Now, let's instead try to use Stokes' theorem to do the
calculation. Now, of course the smart choice
would be to just take the flat unit disk.
I am not going to do that. That would be too boring.
Plus we have already kind of checked it because we already
trust Green's theorem. Instead, just to convince you
that, yes, I can choose really any
surface I want, let's say that I'm going to
choose a piece of paraboloid z equals one minus x squared minus
y squared. Well, to get our conventions
straight, we should take the normal vector pointing up for
compatibility with our choice. Well, we will have to compute
the flux through S. We don't really have to because
we could have chosen the disk, it would be easier, 
but if we want to do it this way we will compute the flux of
curl F through our paraboloid. How do we do that?
Well, we need to find the curl and we need to find ndS.
Let's start with the curl. Curl F let's take the
cross-product between dell and F which is zxy.
If we compute this, the i component will be one
minus zero. It looks like it is one i.
Minus the j component is zero minus one.
Plus the k component is one minus zero.
In fact, the curl of the field is one, one, one.
Now, what about ndS? Well, this is a surface for
which we know z is a function of x and y.
ndS we can write as,  let's call this F of xy, 
then we can use the formula that says ndS equals negative F
sub x, negative F sub y, one dxdy,
which here gives us 2x, 2y, one dxdy.
Now, when we want to compute the flux, we will have to do
double integral over S of one, one, one dot product with 2x,
2y, one dxdy. It will become the double
integral of 2x plus 2y plus one dxdy.
And, of course, the region which we are
integrating, the range of values of x and y will be the shadow of
our surface. That is just going to be,
if you look at this paraboloid from above,
all you will see is the unit disk so it will be a double
integral of the unit disk. And the way we will do that,
one way is to switch to polar coordinates and do the
calculation and then you will end up with pi.
The other way is to try to do it by symmetry.
Observe, when you integrate x above this, x is as negative on
the left as it is positive on the right.
So the integral of x will be zero.
The integral of y will be zero also by symmetry.
Then the integral of one dxdy will just be the area of this
unit disk which is pi. That was our first example.
And, of course,  if you're actually free to
choose your favorite surface, there is absolutely no reason
why you would actually choose this paraboloid in this example.
I mean it would be much easier to choose a flat disk.
OK. Tomorrow I will tell you a few
more things about curl fits in with conservativeness and with
the divergence theorem, Stokes all together, 
and we will look at Practice Exam 4B so please bring the exam
with you.  
with you.

NLP

The following content is
provided under a Creative Commons license. Your support will help
MIT OpenCourseWare continue to offer high quality
educational resources for free. To make a donation or
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. ERIK DEMAINE: All right. Today is all about the
predecessor problem, which is a problem we've certainly
talked about implicitly with, say, binary search trees. You want to be able to
insert and delete into a set, and compute the predecessor
and successor of any given key. So maybe define that formally. And this is not
really our first, but it is an example of
an integer data structure. And for whatever
reason, I don't brand hashing as an integer
data structure, just because it's its own beast. But in particular, today,
I need to be a little more formal about the models of
computation we're allowing-- or I want to be. In particular, because, in the
predecessor problem, which is, insert, delete,
predecessor, successor, there are actually lower bounds
that say you cannot do better than such and such. With hashing, there aren't
really any lower bounds, because you can do
everything in constant time with high probability. So I mean, there are
maybe some lower bounds on deterministic hashing. That's harder. But if you allow randomization,
there's no real lower bounds, whereas predecessor, there is. And in general,
predecessor problem-- the key thing I
want to highlight is that we're maintaining
here a set of-- the set is called s
of n elements, which live in some universe, U-- just like last time. When you insert, you can
insert an arbitrary element of the universe. That probably shouldn't be an
s, or it will get thrown away. But the key thing is that
predecessor and successor operate not just on
the [INAUDIBLE] in s, but you can give it any key. It doesn't have to be in there. And it will find the
previous key that is in s, or the next key that is in s. So predecessor is
the largest key that is less than or equal
to x that's in your set. And successor is the
smallest that is larger-- of course, if there is one. So those are the kinds of
operations we want to do. Now, we know how to do all of
this n log n time, no problem, with binary search trees,
in the comparison model. But I want to introduce two
more, say, realistic models of computers, that ignore
the memory hierarchy, but think about
regular RAM machines-- random access machines--
and what they can really do. And it's a model we're going
to be working on for the next, I think, five lectures. So, important to
set the stage right. So these are models for
integer data structures. In general, we have
a unifying concept, which is a word of
information, a word of data, a word of memory. It's used all over the
place-- a word of input. A word is the machine
theoretic sense, not like the linguistic sense. It's going to be
a w-bit integer. And so this defines the
universe, which is-- I'm going to assume they're
all unsigned integers. So this is 2 to the w minus one. Those are all the
unsigned integers you can represent with w-bits. We'll also call this number,
2 to the w, little u. That is the size of the
universe, which is capital U. So this matches
notation from last time. But I'm really highlighting how
many bits we have, which is w. Now, here's where
things get interesting. I'm going to get to
a model called a word RAM, which is what you
might expect, more or less. But before I get there I
want to define something called transdichotomous RAM-- tough word to spell. It just means bridging
a dichotomy-- bridging two worlds, if you will. RAM is a random access machine. I've certainly mentioned
the word RAM before. But now we're going to get a
little more precise about it. So in general, in the
RAM, memory is an array. And you can do random
access into the array. But now, we're going to say,
the cells of the memory-- each slot in that array-- is a word. Everything is
going to be a word. Every input-- all these
x's are going to be words. Everything will be a word. And in particular, the things
in your memory are words. Let's say you have s of them. That's your space bound. In general, in
transdichotomous RAM, you can do any operation
that reads and writes a constant number
of words in memory. And in particular, you can do
random access to that memory. But in particular, we use
words to serve as pointers. Here's my memory of words. Each of them is w bits-- so s of them, from, I
guess, 0 to s minus one. And if you have, like,
the number 3 here, that can be used as a pointer
to the third slot of memory. One, two, three. You can use numbers as
indexes into memory. So that's what I mean by,
words serve as pointers. So particularly, you can
implement a pointer machine, which-- no surprise-- but for this to work, we
need a lower bound on w. This implies w has to be at
least log of the space bound. Otherwise, you just can't
index your whole memory. And if you've got s minus 1
things, this 2 to the w minus 1 better be at least s minus 1. So we get this lower bound. So in particular, presumably, s
is at least your problem size, n. If you're trying to
maintain n items, you've got to store them. So w is at least log n. Now, this relation is
essentially a statement bridging two worlds. Namely, you have,
on the one hand, your model of computation, which
has a particular word size. And in reality, we think of that
as being 32 or 64 or maybe 128. Some fancy operations
on Intel machines, you can do 128-bit or so. And then there's
your problem size, which we think of as an input. Now, this is relating the two. It's a little weird. I guess you could say it's just
a limitation for a given CPU. There's only certain
problems you can solve. But theoretically, it
makes a lot of sense to relate these two. Because if you're
in a RAM, and you've got to be able to
index your data, you need at least
that many bits just to be able to talk
about all those things. And so the claim is,
basically, machines will grow to
accommodate memory size. As memory size grows,
you'll need more bits. Now, in reality, there's
only about 2 to 256-- what do you call
them-- particles in the known universe. So word size probably
won't get that much bigger. Beyond 256 should be OK. But theoretically,
this is a nice way to formalize this claim
that word sizes don't need to get too big unless
memories get gigantic. So it may seem weird at
first, but it's very natural. And all real world machines
have big enough words to accommodate that. Word size could be bigger,
and that will give you, essentially, more parallelism. But it should be
at least that big. All right. Enough proselytizing. That's the transdichotomous RAM. The end. And the word RAM is
a specific version of the transdichotomous
RAM, where you restrict the operations
to c-like operations. These are sort of the standard-- they're instructions
on, basically, all computers, except a few
risk architectures don't have multiplication and division. But everything else
is on everything. So these are the operators,
unless I missed one, in c. They're all in Python,
and pick your-- most languages. You've got integer
arithmetic, including mod. You've got bitwise and,
bitwise or, bitwise x or, bitwise negation, and
shift left, and shift right. These we all view as
taking constant time. They take one or two integer
inputs-- words as inputs. They can compute an answer. They write out another word. Of course, there's also random
access-- array dereference, I guess. So that's the word RAM. You restrict to
these operations. Whereas transdichotomous
RAM, you can do weird things, as
long as they only involve a constant number of words. Word RAM-- it's
the regular thing. So this is basically
the standard model that all integer data
structures use, pretty much. If they don't use this
model, they have to say so. Otherwise this model has become
accepted as the normal one. It took several years
before people realized that's a good model--
good enough to capture pretty much everything we want. The cool thing
about word RAM is, it lets you do things
on w-bits in parallel. You can take the and of
w-bits, pairwise, all at once. So you get some speed up. But it's a natural
generalization of something like the comparison model. Comparison model-- I guess
I didn't write those. It's more operations-- less
than, greater than, and so on. You can compare two
numbers in constant time, get a Boolean output
via, say, subtraction, and computing the sine. And you think of comparisons
as taking constant time, so why not all of these things? Cool. One more model-- this
is kind of a weird one. It's called cell
probe model, which is, we just count
the number of memory reads and writes that we
need to do to solve a data structure or a query. Like, you you're
looking at predecessor, and you just want to know,
how much of the data structure do I have to read in order to be
able to answer the predecessor problem? How much do I have
to write out to do an insertion, or whatever? And so in this model,
computation is free. And this is kind of like
the external memory model, and the cache oblivious models. There, we were
measuring how many block reads and writes there are. Here, our blocks are
actually our words. So there is a bit of a relation,
except there's no real-- you can either think
of there being no cache here, because
you're just reading in a constant number of words,
doing something, spitting stuff out. Or in the cell probe
model, you could imagine there being an infinite
cache for this operation, but no cache from
operation to operation. It's just, how much do I have
to [INAUDIBLE] information, theoretically, to solve a
particular predecessor problem? We'll deal with this
a lot in a couple of lectures-- not quite yet. This model is just
used for lower bounds. It's not a realistic
model, because you have to pay for computation
in the real world. But if you can
prove that you need to read at least a certain
number of words, then, of course, you have to do at
least that many operations. So it's nice for lower bounds. In general, we have this
sort of hierarchy of models, where this is the most
powerful, strongest, and below cell probe, we
have transdichotomous RAM, then word RAM, then--
just to fit it in context, what we've been doing-- below that is pointer
machine, and below that would be binary search tree. I've mentioned before,
pointer machines are more powerful than
binary search tree. And of course, we can
implement a pointer machine on a word RAM. So we have these relations. There are, of
course, other models. But this is a quick picture
of models we've seen so far. So now, we have this
notion of a word. In the predecessor problem,
these elements are words. They're w-bit integers,
universe-defined. And we want to be able to
insert, delete, predecessor, and successor over words. So that's our challenge. In the binary search
tree model, we know the answer to this
problem is theta log n. In general, any
comparison-based data structure, you need theta log
n, in the worst case. It's an easy lower bound. But we're going to do better on
these other models in the word RAM. So here are some results. First data structure is
called Van Emde Boas. You might guess it
is by van Emde Boas-- Peter. It actually has a
couple other authors in some versions of
the papers, which makes a little bit confusing. But for whatever reason,
the data structure is just named Van Emde Boas. And it achieves log
w per operation. I think I'll rewrite this. This is log log u per operation. But it requires u space. So think of u space as
being, like, for every item in the universe I store,
yes or no, is it in the set? So that's a lot of space, unless
n and u are not too different. But we can do better. But the cool thing
is the running time. This is really fast-- log log u. If you think about,
for example-- I don't know-- the universe
being polynomial in n, or even if the universe
is something like-- polynomial in n is
the same as this-- 2 to the c log n. You can go crazy and
say log to the c power-- so, like, 2 to the log
to the fifth power. All those things,
you take log twice. Then log log u becomes
theta log log n. So as long as your word
size is not insanely large, you're getting log
log n performance. So in general, when, let's
say, w is is polylog n, then we're getting this
kind of performance. And I think on most computers,
w is polylogarithmic. We said it has to
be at least log. It's also, generally, not
so much bigger than log. So log squared is probably
fine most of the time, unless you have a
really small problem. OK, so cool. But the space is giant. So how do we do
better than that? Well, there's a
couple of answers. One is that you can achieve
log w with high probability, and order n space. With a slight tweak, basically,
you combine Van Emde Boas plus hashing, and you get that. I don't actually know what the
reference is for this result. It's been an exercise in
various courses, and so on. I can talk more
about that later. Then alternatively, there's
another data structure, which, in many ways, is simpler. It really embraces hashing. It's called y-fast trees. It achieves the same
bounds-- so log w with high probability
and linear space. It's basically just a hash
table with some cleverness. So we'll get there. Even though it's
simpler, we're going to start with this structure. Historically, this is
the way it happened-- Van Emde Boas, then y-fast
trees, which are by Willard. And it'll be kind
of a nice finale. There's another data structure
I want to talk about, which is designed for the
case when w is very large-- much bigger than polylog n. In that case, there's
something called fusion trees. And you can achieve
log base w of n-- and, I guess, with high
probability and linear space. The original fusion
trees are static. And you could just do log base
w of n deterministic queries. But there's a later
version that dynamic, achieves this using hashing
for updates, insertions, and deletions. Cool. So this is an
almost upside-down-- it's obviously
always an improvement over just log base 2 of n. But it's sometimes better and
sometimes worse than log w. In fact, it kind of makes
sense to take the min of them. When w is small, you
want to use log w. When w is big, you want
to use log base w of n. They're going to balance out
when w is 2 to the root log n-- something like that. The easy thing is,
when these balance out is when they're equal. And that will be when this
is log n divided by log w. So when log w equals
log n divided by log w-- let do that over here. log w is log n over log w. Then this is like saying
log squared w equals log n, or log w is root log n. So I was right. w is
2 to the root log in, which is a weird quantity. But the easy thing to
think about is this one-- log w is root log n. And in that case, the running
time you get is root log n. So it's always, at most, this. And the worst case is when
these things are balanced, or these two are the same, and
they both achieve root log n. But if w is smaller or
larger than this threshold, these structures will be
even better than root log n. But in particular,
it's a nice way to think about, oh, we're
doing sort of a square factor better than binary search trees. And we can do this high
probability in linear space. So that's cool. Turns out it's also
pretty much optimal. And that's not at all
obvious, and wasn't known for many years. So there's a cell
probe lower bound. So these are all in
the word RAM model-- all these results. The first one actually kind of
works in the pointer machine. I'll talk about that later. This lower bound's a
little bit messy to state. The bound is slightly
more complicated than what we've seen. But I'm going to restrict to
a special situation, which is, if you have n polylog n space. So this is a lower bound
on static predecessor. All you need to do is solve
predecessor and successor, or even just predecessor. There's no inserts and deletes. In that case, if you use lots of
space, like u space, of course, you can do constant
time for everything. You just store all the answers. But if you want space that's
not much bigger than n-- in particular, if you wanted
to be able to do updates in polylog, this
is the most space you could ever hope to achieve. So assuming that, which
is pretty reasonable, there's a bound of the
min of two things-- log base w of n,
which is fusion trees, and, roughly, log w,
which is Van Emde Boas. But it's slightly
smaller than that. Yeah, pretty weird. Let me tell you
the consequences-- a little easier to think about. Van Emde Boas is going to be
optimal for the kind of cases we care about, which
is when w is polylog n. And fusion trees are
optimal when w is big. Square root log n log log n. OK-- a little messy. So there's this divided by
log of log w over log n. If w is polylog n, then this
is just order log log n. And so this cancels. This becomes constant. So in these situations, which
are the ones I mentioned over here, w is polylog
n, which is when we get log log n performance. And that's kind of the
case we care about. Van Emde Boas is the
best thing to do. Turns out, this is
actually the right answer. You can do slightly better. It's almost an exercise. You can tweak Van Emde Boas and
get this slight improvement. But most word sizes, it
really doesn't matter. You're not saving much. Cool. So other than that
little factor, these are the right answers. You have to know
about Van Emde Boas. You have to know
about fusion trees. And so this lecture is
about Van Emde Boas. Next lecture is
about fusion trees. This result is from 2006
and 2007, so pretty recent. So let's start a Van Emde Boas. Yeah. Let's dive into it. I'll talk about
history a little later. The central idea,
I guess, if you wanted to sum up Van Emde
Boas in an equation, which is something we very rarely
get to do in algorithms, is to think about
this recurrence-- T of u is T of square
root of u plus order 1. What does this solve to? log log u. All right, just
think of taking logs. This is the same as
T of w equals T of w over 2 plus order 1. w is the word size. And so this is log w. It's the same thing. If we could achieve
this recurrence, then-- boom-- we get our
bound of log w. So how do we do it. We split the universe
into root u clusters, each of size root u. OK, so, if here is our
universe, then I just split every square
root of u items. So each of these is root u long. The number of them
is square root of u. And then somehow,
I want to recurse on each of these clusters. And I only get to
recurse on one of them-- so a pretty simple idea. Yeah. So I'll talk about
how to actually do that recursion in a moment. Before I get there,
I want to define a sort of hierarchical
coordinate system. This is a new way of
phrasing it for me. So I hope you like it. If we have a word x, I
want to write it as two coordinates-- c and i. I'm going to use
angle brackets, so it doesn't get too confusing. c
is which cluster you're in. So this is cluster 0, cluster
1, cluster 2, cluster three. i is your index
within the cluster. So this is 0, 1, 2, 3, 4,
5-- up to root u minus 1 within this cluster. Then 0, 1, 2, 3, 4, 5
up to root u minus 1 with in this
cluster-- so the i is your index within the
cluster, like this, and c is which
cluster you are in. OK. Pretty simple. And there's easy
arithmetic to do this. c is x integer divide root u. And i is x integer mod root u. I used Python notation here. So fine, I think
you all know this-- pretty simple. And if I gave you
c and i, you could reconstruct x by just
saying, oh, well, that's c times root u plus i. So in constant time, you
can decompose a number into its two coordinates. That's the point. In fact, it's much
easier than this. You don't even
have to do division if you think of
everything in binary, which computers tend to do. So the binary perspective
is that x is a word. So it's a bunch of bits. 0, 1, 1, 0, 1, 0,
0, 1-- whatever. Divide that bit sequence
in half, and then this part is c, this part is i. And if you assume that
w is a power of 2, these two are identical. If they're not a
power of 2, you've got to round a little bit here. It doesn't matter. But you can use this definition
instead of this one either way. So in this case, c is-- ooh, boy-- x shifted
right, w over 2, basically. So this w over 2-- w over 2. The whole thing is w bits. So if I shift right, I get
rid of the low order bits, if I want. i is slightly more annoying. But I can't do it
as an and with one shifted left w over 2 minus 1. That's probably
how you do it in c. I don't know if
you're used to this. But if I take it a 1 bit,
I shift it over to here, and I subtract 1. Then I get a whole
bunch of 1 bits. And then you mask
with that bit pattern. So I'm masking with 1, 1, 1, 1. Then I'll just get
the low order bits. Computers do the
super fast-- way faster than integer division. Because this is just
like routing bits around. So this is easy to
do on a typical CPU. And this will be much
faster than this code, even though looks like
more operations, typically. All right. So fine. The point is, I can
decompos x into c and i. Of course, I can
also do the reverse. This would be c shifted
left w over 2, ord with i. It's a slight diversion. Now, I can tell you
the actual recursion, and then talk about
how to maintain it. So we're going to define
a recursive Van Emde Boas structure of size
u and word size w. And what it's going
to look like is, we have a bunch of clusters,
each of size square root of u. So this represents the
first root u items. This represents the
next root u items. This represents the last
root u items, and so on. So that's the obvious
recursion from this. So this is going to
be a Van Emde Boas structure of size root u. And then we also
have a structure up top, which is called
the summary structure. And the idea is, it represents,
for each of these clusters, is the cluster empty or not? Does this cluster
have any items in it? Yes or no. If yes, then the
name of this cluster is in the summary structure. So notice, by this
hierarchical decomposition, the cluster number
and the index are valid names of items
within these substructures. And basically we're going to use
the i part to talk about things within the clusters. And we're going to use the
c part to talk about things within the summary structure. They're both numbers between
0 and root u minus 1. And so we get this perspective. All right. So formally, or some
notation, cluster i-- so we're going to have
an array of clusters. It is Van Emde Boas thing
of size square root u, and word size w over 2. This is slightly weird,
because the machine, of course, its word size remains w. It doesn't get smaller
as you recurse. We're not going to try
to spread the parallelism around or whatever. But this is just a
notational convenience. I want to say the
word size conceptually goes down to w over 2, so
that this definition still makes sense. Because as I look at a
smaller part of the word, in order to divide it in
half, I have to shift right by a smaller amount. So that's the w that I'm
passing into the structure. OK, and then v dot
summary is same thing. It's also Van Emde Boa's
thing of size root u. Then the one other clever idea,
which makes all of this work, is that we store the minimum
element in v dot min. And we do not store
it recursively. So there's also one item here,
size 1, which is the min. It's just stored
off to the side. It doesn't live in
these structures. Every other item
lives down here. And furthermore, if one
of these is not empty, there's also a
corresponding item up here. This turns out to be crucial
to make a Van Emde Boas work. And then v dot
max, we also need-- but it can be
stored recursively. So just think of it
as a copy of whatever the maximum element is. OK, so in constant time,
we can compute the min and compute the max. That's good. But then I claim also in log
w time-- log log u time-- we can do insert, delete,
predecessor, successor. So let's do that. This data structure--
the solution is both simple and
a little bit subtle. And so this will be
one of the few times I'm going to write
explicit pseudocode-- say exactly how to maintain
this data structure. It's short code, which is good. Each algorithm is
only a few lines. But every line matters. So I want to write them down
so I can talk about them. And with this new
hierarchical notation, I think it's even easier
to write these down. Let's see how I do. OK, so we'll start with
the successor code. Predecessor is,
of course, metric. And it basically has two cases. There's a special case
in the beginning, which is, if the thing you're
querying happens to be less than the minimum of the
whole thing, then of course, the minimum is the successor. This has to be done specially,
because the min is not stored recursively. And so you've got
to check for the min every single level
of the recursion. But that's just constant time. No big deal. Then the interesting
things is, we have recursions in both sides-- in both cases-- but only one. The key is, we want
this recurrence-- T of u is 1 times T of
root u plus order 1. That gives us log log u. If there was a 2 here, we would
get log u, which is no good. We want the one. So in one case, we call
successor on a cluster. In the other case,
we call successor on the summary structure. But we don't want to do both. So let's just think about,
intuitively, what's going on. We've got this-- I guess I can do it
in the same picture. We've got this summary
and a bunch of clusters. And let's say you want
to compute, what's the successor of this item? So via this
transformation, we compute which cluster it lives in and
where it is within the cluster. That's i. So it's some item here. Now, it could be the successor
is inside the same cluster. Maybe there's an
item right there. Then want to recurse in here. Or it could be, it's
in some future cluster. Let's do the first case. If, basically, we are less than
the max of our own cluster, that means that the
answer is in there. Figure out what the max
is in this structure-- the rightmost item in s
that's inside this cluster c. This is c. If our index is less than the
max's index, then if we recurse in here, we will find an answer. If we're bigger than
the max, then we won't find an answer down here. We have to recurse
somewhere else. So that's what we do. If we're less than
the max, then we just recursively find the successor
of our index within cluster c. And we have to add
on the c in front. Because successor
within this cluster will only give an index
within the cluster. And we have to prepend this
c part to give a global name. OK, so that's case 1. Very easy. The other case is where we're
slightly clever, in some sense. We say, OK, well, if there's no
successor within the cluster, maybe it's in the next cluster. Of course, that one might
be empty, in which case, it's in the next cluster. But that one might be empty,
so look at the next cluster. We need to find, what is
the next non-empty cluster? For that, we use the
summary structure. So we go up to position c here. We say, OK, what is the next
non-empty structure after c? Because we know that's
going to be where our answer lives for successor. So that's going to
give us, basically, a pointer to one of these
structures-- c prime, which-- all these guys are empty. And so there's no
successor in there. The successor is then the
min in this structure. So that's all we do. Compute the successor of c
in the summary structure. And then, in that
cluster, c prime, find the min, which
takes constant time, and then prepend c prime to
that to get a global name. And that's our successor. Yeah, question. AUDIENCE: Could you repeat
why min is not recursive? Because looking
at this, it looks like all these smaller
[INAUDIBLE] trees have [INAUDIBLE] ERIK DEMAINE: Ah, OK. Sorry. The question is, why is
the minimum not recursive? The answer to that
question is not yet clear. It will have to
do with insertion. But I think what
exactly this means, I maybe didn't state
carefully enough. Every Van Emde Boas
structure has a min-- stores a min. In that sense, this is done-- that's funny-- not
so recursively. But every one stores it. The point is that
this item doesn't get put into one
of these clusters recursively-- just the item. But each of these
has its own min, which is then not stored
at the next level down. And each of those has
its own min, which is not stored at the next level down. Think of this as kind
of like a little buffer. The first time I insert
it into the structure, I just stick it into the min. I don't touch anything else. You'll see when we get to
the insertion algorithm. But it sort of slows
things down from trickling. AUDIENCE: So putting that min,
is that what prevents from-- ERIK DEMAINE: That will
prevent the insertion from doing two recursions
instead of one. So we'll see that in a moment. At this point, just
successor is very clear. This would work whether the min
is stored recursively or not. But we need to know what the
min is of every structure, and we need to know the
max of every structure. At this point, you could just
say that min and max could be copies-- no big deal-- and we'd be happy. And of course, predecessor
does the same thing. So the slight cleverness here
is that we use the min here. This could have been a successor
operation with minus infinity as the query. But that would be
two recursions. We can only afford one. Fortunately, it's the
min item that we need. So we're done with successor. That was the easy case-- or the easy one. Insert is slightly harder. Delete is just slightly messier. It's basically the
same as insert. So insert-- let me
write the code again. Insertion also has
two main cases. There's this case,
and the other case. But there's no else here. This happens in both cases. And then there's some just
annoying little details at the beginning. Just like over here, we had to
check for the min specially, here, we've got to
update the min and max. And there's a special case,
which I haven't mentioned yet. v dot min-- special case is,
it will be this value, none, if the whole structure is empty. So this is the obvious way to
tell whether a structure is empty and has no min. Because if there's
any items in there, there's going to be
one in the min slot. So first thing we do is
check, is our structure empty? If it's empty, the min and the
max become the inserted item. We're done. So that's the easy case. We do not store it
recursively in here. That's what this means. This element does not get
stored in any of the clusters. If it's not the very first
item, or it's not the min item, then we're going to recursively
insert it into a cluster. So if we have x in
cluster c, we always insert index i into cluster
c, except if it's the min. Now, it could be where a
structure is non-empty. There is a min item there. But we are less than the min. In that case, we're the new
min, and we just swap those. And now, we have to
recursively insert the old min into the rest of the structure. So that's a simple case. Then we also have
to update v dot max, just in the obvious way. This is the easy way to
maintain v dot max in variant, that is the maximum item. OK, now we have the two cases. I mean, this is really
the obvious thing to get to do insertion. We have to update the
summary structure, meaning, if the cluster that we are
inserting into-- cluster c-- is empty, that means it was not
yet in the summary structure. We need to put it in there. So we just insert c
into v dot summary-- pretty obvious. And in all cases, we insert
our item into cluster c. This looks bad,
however, because there's two recursions in some cases. If this if doesn't hold,
it's one recursion. Everything's fine. So if the cluster was
already in use, great. This is one recursion. This is constant work. We're done. The worry is, if the
cluster was empty before, then this insertion
is a whole recursion. That's scary, because we can't
afford a second recursion. But it's all OK. Because if we do
this recursion, that means that this cluster
was empty, which means, in this recursion, we fall
into this very first case. That structure,
it's min is none. That's what we just checked for. If it's none, we do
constant work and stop. So everything's OK. If we recursed in the
summary structure, this recursion will be
a shallow recursion. It just does one thing. You could actually put this
code into this if case, and make this an else case. That's another way
to write the code. But this will be a
very short recursion. So either you just
do this recursion, which could be
expensive, or you just do this one, in which case,
we know this one was cheap. If this happens, we know
this will take constant time. So in both cases, we
get this recursion-- square root of u plus constant. And so we get log
log u insertion. Do you want to see delete? I mean, it's basically
the same thing. It's in the notes. I mean, you do the
obvious thing, which is, you delete in the cluster. And then if it became
empty, you also have to delete in the
summary structure. So there's, again, a chance
that you do two recursions. But-- OK, I'm talking about it. Maybe I'll write a
little bit of the code. I think I won't
write all the code, though-- just the main stuff. So if we want to
delete, then basically, we delete in cluster c, index i. And then if the cluster
has become empty as a result of
that, then we have to delete cluster c from
the summary structure, so that our predecessor and
successor queries actually still work. OK, so that's the
bulk of the code. I mean, that's where
the action happens. And the worry would be,
in this if case, we're doing two recursive deletes. The claim is, if we
do this second delete, which is potentially expensive--
this one was really cheap-- the claim is that emptying
a Van Emde Boas structure takes constant time--
like, if you're deleting the last element. Why? Because when you're
deleting the last element, it's in the min right here. Everything below it-- all
the recursive structures-- will be empty if
there's only one item, because it will be right here. And you can check that
from the insertion. If it was empty, all we did was
change v dot min and v dot max. So the inverse, which
I want right here, is just to clear out v
dot min and v dot max. So if this ends up happening,
this only took constant time. You don't have to recurse when
you're deleting the last item. So in either case, you're really
only doing one deep recursion. So you get the same recurrence,
and you get log log u. So for the details,
check out the notes. I want to go to other
perspectives of Van Emde Boas. This is one way
to think about it. And amusingly, and this is
probably the most taut way to do Van Emde Boas. It's, in CLRS,
described this way, because in 2001, when
I first came here, I presented Van Emde Boas like
this in an undergrad algorithms class with more details. You guys are grads, so I did
it like three times faster than I would in 6046. So now, it's in
textbooks and whatnot. But this is not
how Van Emde Boas presented this data
structure-- just out of historical interest. This is a way that I believe
was invented by Michael Bender and Martin Farach-Colton,
who are the co-authors on "Cache-oblivious B-trees." And around 2001,
they were looking at lots of old data structures
and simplifying them. And I think this is a
very clean, simple way to think about Van Emde Boas. But I want to tell you
the other way, which is the way it originally
appeared in their papers. There's actually three
papers by van Emde Boas about this structure. Many papers appear twice-- once in a conference,
once in a journal-- this one, there's
three relevant papers. There's conference
version, journal version. The only weird thing there is
that the conference version has one author-- van Emde Boas. The journal version
has three authors-- van Emde Boas,
Kaas, and Zijlstra. And they're acknowledged
in the conference version, so I guess they
helped even more. In particular, they, I think,
implemented this data structure for the first time. It's a really easy
data structure to implement, and very fast. Then there's a third
paper by van Emde Boas only in a journal which
improves the space a little bit. So we'll see a little
bit what that's about. But what I like about
both of these papers is they offer a simpler way
to get log log u, successor, predecessor. Let's not worry about insertions
and deletions for a little bit, and take what I'll call
the simple tree view. So I'm going to draw a picture-- 0, 1, 0, 0, 0, 0, 0-- OK. This is what we call a bit
vector, meaning, here's item zero, item one, item two. And here is u minus 1. And I'll put a 1 if that
element is in my set, and a 0 otherwise. OK, so one is in the set, nine-- I think-- is in the set,
10, and 15 are in the set. I kind of want to maintain this. This is, of course,
easy to maintain by insertions and deletions. I just flip a bit on or off. But I want to be able
to do successor queries. And if I want the
successor of, say, this 0, finding the next 1-- I don't want to
have to walk down. That would take order
u time-- very bad. So obvious thing to do is
build a tree on this thing. And I'm going to put in here
the or of the two children. Every node will store
the or of its children. And then keep building the tree. Now we have a binary tree,
with bits on the vertices. And I claim, if
I want to compute the successor of this
item, I can do it in a pretty natural way
in the log log u time. So keep in mind, this
height here is w-- log u. So I need to achieve log w. So of course, you could try
just walking down this tree, or walking up and
then back down. That would take order w time. That's the obvious BST approach. I want to do log w. So how do I do it? I'm going to binary
search on the height. How could I binary
search on the height? Well, what I'd really like
to do, in some sense-- if I look at the path of
this node to the route-- where is my red chalk? So here's the path to the root. These bits are saying, is
there anybody down here? That's what the or gives you. So it's like the
summary structure. If I want to search for this
guy-- well, if I walked up, eventually, I find a 1. And that's when I find
the first nearby element. Now, in this case it's
not the successor I find. It's really the
predecessor I found. When you get to the first one--
the transition from 0 to 1-- you look at your sibling-- the other child of that one. And down in this subtree, there
will be either the predecessor or the successor. In this case, we've
got the predecessor, because it was to the left. We take the max
element in there, and that's the
predecessor of this item. If instead, we had found
this was our first one, then we look over
here, take the min-- there's, of course,
nothing here. But in that situation,
the min over there would be our successor. So we can't guarantee
which one we find. But we will find either the
predecessor or the successor if we could find the first
transition from 0 to 1. And we can do that
via binary search, because this string is monotone. It's a whole bunch
of zeros for awhile, and then once you
get a 1, it's going to continue to be 1,
because those are or. That one will propagate up. So this is the new idea to
get log log u, predecessor, successor is to-- let's say-- any root-to-leaf
path is monotone. It's 0 for awhile, and
then it becomes 1 forever. So we should be able to
binary search for the 0 to 1 transition. And it either looks like
this, or it looks like this. So our query was somewhere
down here in the 0 part. I'm assuming that
our query is not a 1. Otherwise, it's an
immediate 0 to 1 transition. And that's a special case. It's easy to deal with. And then there's
the other tree-- the sibling of x-- the other child of the 1. And in this case, we
want to take the min. And that will give us
our successor of x. And in this case, we want
to take the max over here, and that will give us
the predecessor of x. So as long as we have
minimax of subtrees, this is constant time. We find either the
predecessor or the successor. Now, how do we
get the other one? Pretty easy. Just store a linked list
of all the items, in order. So I'm going to store a pointer
from this one to this one, and vice versa-- and this one or this one. This is actually really
easy to maintain. Because when you insert,
if you can compute the predecessor
and the successor, you can just stick it
in the linked list. That's really easy. We know how to do
that in constant time. So once you do this, it's
enough to find one of them, as long as you know
which one it is. Because then you just
follow a pointer-- either a forward or
a backward pointer-- and you get the other one. So whichever one you wanted-- you find both the
predecessor and successor at the cost of
finding either one. So that's a cute little trick. This is hard to maintain,
dynamically, at the moment. But this is, I think,
where the Van Emde Boas structure came from. It's nice to think about
it in the tree view. So we get log log you u,
predecessor, and successor. I should say what this relies on
is the ability to binary search on any route-to-node path. Now, there aren't enough
pointers to do that. So you have a choice. Either you realize,
oh, this is a bunch of bits in a
complete binary tree, so I can store them
sequentially in array. And given a particular node
position in that array, I can compute, what is
the second ancestor, or the fourth ancestor or
whatever, in constant time. I just do some arithmetic
and I can compute from here where to go to there. It's like the regular old
heaps, but a little bit embellished, because
you have to divide by a larger power of two,
not just one of them. So that's one way to do it. So in a RAM, that
all works fine. When van Emde Boas wrote this
paper, though, the RAM didn't-- it kind of existed. It just wasn't as
well-developed then. And the hot thing at the
time was the pointer machine, or I guess at that point, they
called it the Pascal machine, more or less. Pascal does have arrays. And the funny thing is, Van
Emde Boas does use arrays, but mostly it's pointers. And you can get rid of the
arrays from their structure. And essentially, in
the end, Van Emde Boas, as presented like this,
is in a pointer machine. Let me tell you a
little bit about that. So original Van Emde Boas, which
I'll call stratified trees-- that's what he called it-- is basically this tree structure
with a lot more pointers. So in particular, each leaf-- or every node,
actually, let's say-- stores a pointer to 2
to the ith ancestor, where i is 0, 1, up to log w. Because it was the
2 to the-- here. So once you get the
ancestor immediately above me, two steps above
me, four steps above me, eight steps above me,
that's what I really need to do this binary search. The first thing I
need is halfway up. And then if I have
to go down, I'm going to need a
quarter of the way up. And if I have to go down, I
want an eighth of the way up. Whenever I go up, from-- if
I decide, oh, this is a 0. I've got to go above here. Then I do the same
thing from here. I want to go halfway
up from here-- from this node. So as long as every node knows
how to go up by any power of 2, we're golden. We can do a binary search. The trouble with this
is, it increases space. This is u log w space, which
is a little bit bigger than u. And the original van Emde Boas
paper, conference and journal version, achieves this bound-- not u. Little historical fun fact-- not terribly well known. Cool. So that's stratified trees. Anything else? All right. Stratified tree. Right. At this point, we have fast
search, but slow update let. Me tell you about
updates in a second. Yeah, question. AUDIENCE: So once you do binary
search to find the first 1, how do you walk
back down the tree-- ERIK DEMAINE: Oh,
I didn't mention, but also, every node
stores min and max. So that lets me do the
teleportation back down. Every node knows the min
and the max of its subtree. Right. One more thing I was
forgetting here-- when I say, this a lot
of pointers to store. You can't store them
all in one node. And in the van Emde Boas
paper, it's stored in an array. But it doesn't really
need that its an array. It could just as well
be a linked list. And that's how you
get pointer machine. So this could be linked list. And then this whole thing
works in pointer machine, which is kind of neat. And it's a little weird,
because if you used a comparison pointer machine, where all
you can do is compare items, there's a lower bound of
log n, because you only have branching factor constant. But here, the formulation of
the problem is, when I say, give me the successor
of this, I actually give you a pointer to this item. And then from there, you can
do all this jumping around, and find your
predecessor or successor. So in this world, you
need at least u space, even to be able to
specify the input. So that's kind of a limitation
of the pointer machine. And you can actually show
in the pointer machine log, log u is optimal for any
predecessor data structure in the pointer machine. So there's a matching lower
bound log log u in this model. And you need to use space. So it's not very exciting. What we like is the word RAM. There, we can reduce space to n. And that's what I want
to do next, I believe-- almost next. One more mention--
actual stratified trees-- here, we got query
fast, update slow. Stratified trees actually
do update fast, as well. Essentially, it's this idea,
plus you don't recursively store the min, which,
of course, makes all these bits no
longer accurate, as it gets much messier. But in the end, it's doing
exactly the same thing as this recursion. In fact, you can
draw the picture. It is this part up here-- the top half of the tree-- this is summary. And each of these bottom
halves is a cluster. And there's root u
clusters down here. So those are smaller structures. And there's one root u sized
Van Emde Boas structure, which is a summary structure. These bits here is the
bit vector representation of the summary structures. It's, is there anyone
in this cluster? Is there anyone in this
cluster, and so on? This, of course, also looks
a lot like the Van Emde Boas layout. Take a binary tree, cut
it in half, do the top, recursively do the bottom. So that's why it was called
the Van Emde Boas layout, is this picture. But if you take
this tree structure, and then you don't
recursively store mins, and then the bits are not
quite accurate, it's messy. And so stratified
trees-- you should try to read the original paper. It's a mess. Whereas this code--
pretty clean. And so once you
say, oh, I'm just going to store all these
clusters as an array and not worry about
keeping track of the tree, it actually gets a lot easier. And that was the
Bender/Farach-Colton cleaning up, which never
appeared in print. But it's appeared in the lecture
notes all over the place-- and now CLRS. Cool. I want to tell you
about two more things. It's actually
going to get easier the more time we spend
with this data structure. All right. Let me draw a box. At this point, we've seen a
clean way to get Van Emde Boas. And we've seen a
cute way in a tree to get search fast,
but update slow. I want to talk a
little more about that. Let's suppose I have
this data structure. It's achieves log w
query, which is fast, but it only achieves w
update, which is slow. How do you update the structure? You update one
bit at the bottom, and then you've got to update
all the bits up the path. So you spend w time to
do an update over here. If updates are slow, I just
want to do less updates. We have a trick for
doing this, which is, you put little things
down here of size theta w. And then only one item
from here gets promoted into the top structure. We only end up having n over
w items up here, and about 1 over w as many updates. If I want to do an
insertion, I do a search here to figure out which
of these little-- I'll call these "chunks--" which
little chunk it belongs in. I do an insert there. If that structure
gets too big-- it's bigger than, say, 2
times w, or 4 times w, whatever-- then I'll split it. And if I delete from something,
and it gets too small, I'll merge with the
neighbor, or maybe re-split-- just like B-trees. We've done this
many times, by now. But only when it
splits, or I do a merge, do I have to do
an update up here. Only when the set
of chunks changes do I need to do a single
insertion or deletion up here-- or a constant number. So this update time goes
down by a factor of w. But I have to pay whatever
the update cost is here. So what I do with
this data structure? I don't want use Van
Emde Boas, because this could be a very big universe. Who knows what? I use the binary search tree. Here, I can afford a
binary search tree, because then it's only log w. log w is the bound
we're trying to get. So you can do these
binary search trees. It's trivial. Just do insert, delete, search. Everything will be log w. So if I want to do a search,
I search through here, which, conveniently, is
already fast-- log w-- and then I do a search through
here, which is also log w. So it's nice and balanced. Everything's log w. If I want to do an insertion,
I do an insertion here. If it splits, I do
an insertion here. But that order w update
cost, I charge to the order w updates I would have
had to do in this chunk before it got split. So this our good friend
indirection, a technique we will use over and
over in this class. It's very helpful when you're
almost at the right bound. And that's actually in the
follow-up van Emde Boas paper. A similar indirection
trick is in there. So we can charge the
order w update in top to-- that's the cost of the update-- to the order w updates
that have actually been performed in the bottom. Because when
somebody gets split, it's nice in its average
state-- or when it gets merged, it's going to be close
to its average state. You have to do a lot of
insertions or deletions to get it out of whack, and
cause a split or a merge. So-- boom. This means the
updates become log w. Searches are also log w. So we've got Van Emde
Boas again, in a new way. Bonus points-- if you
take this structure-- even this structure, if we
did it in the array form-- great. It was order u space. If we did it with
all these pointers, and we wanted a pointer
machine data structure, we needed u log w space. But this indirection trick, you
can also get rid of the log w in space factor. It's a little less obvious. But you take this-- here, we reduced n
by a factor of w. You can also reduce
u by a factor of w. I'll just wave my hands. That's possible. So u gets a little bit smaller. And so when we
pay u log w space, if you got smaller
by a factor of w, this basically disappears. So you get, at
most, order u space. But order u is not order n. I want order n space, darn it. So let's reduce space. As I said, this is going
to get easier and easier. By the end, we will have very
little of a data structure. But still, we'll have log log u. And you thought this was
easy, but wait, there's more. Right now, we have two
ways to get log log u-- query and order u space. There's the one I'm
erasing, and there's this-- take this tree structure
with the very simple pointers. Add indirection. So admittedly, it's more
complicated to implement. But conceptually,
it's super simple. It's like, do this
obvious tree binary search on the level thing. And then add indirection,
and it fixes all your bounds, magically. So conceptually, very simple-- practically, you definitely
want to do this-- much simpler. Now, what about saving space? Very simple idea--
which, I think, again, comes from Michael
Bender and Martin Farach-Colton. Don't store empty structures. So in this picture, we had
an array of all the clusters. But a cluster could be
entirely empty, like this one-- this entirely empty cluster. Don't store it. It's a waste. If you store them all, you're
going to spend order u space. If you don't store them all-- just don't store
the empty ones-- I claim your order n space. Done. So I'm going back to
the structure I erased. Ignore the tree
perspective for awhile. Don't store empty clusters. OK, now, this sounds easy. But in reality, it's a
little bit more annoying. Because we wanted to have
an array of clusters. So we could quickly
find the cluster. If you store an
array, you're going to spend at least
square rot of u space. Because at the very
beginning, you say, here are my root u clusters. Now, some of them
might be null pointers. But I can't afford to store
that entire array of clusters. So don't use an array. Use a perfect hash table. So v dot cluster, instead
of being an array, is now, let's say, a
dynamic perfect hashing. And I'm going to use the
version which I did not present. The version I presented,
which used universal hashing, was order 1 expected. But I said that it can be
constant with high probability per operation. It's a little bit stronger. So now, everything's fine. If I do an index
v dot cluster c, that's still constant time,
with high probability now. And I claim this structure
is now order n's space. Why is it order n's space? By simple amortization--
charge each table entry in that hash table to the
min of the cluster. We're only storing
non-empty ones. So if one of these guys
exists in the hash table, we had to store a
pointer to it, then that means the summary
structure is non-zero. It means this guy is not empty. So it has an item in its min. Charge the space up here to
store the pointer to that min guy. Then each item-- each min item-- only gets charged once. Because it only has one parent
that has a pointer to it. So you only charge once. And therefore-- charge
and table entry-- only charge each element once. And that's all your space. So it's order n space. Done. Kind of crazy. I guess, if you want, there's
also the pointer to the summary structure. You could charge
that to your own min. And then you're charging twice. But it's constant per item. So this is kind of funny. Again, it doesn't appear in
print anywhere, except maybe as an exercise in CLRS now. But you get linear
order n space, just by adding hashing
in the obvious way. Now, for whatever reason,
Willard didn't see this, or wanted to do his
own thing, and so he found another way to do
order n space log log u query with hashing. Well, I guess, also,
you had to think of it in this simple form. It's harder to do
this in the tree. It can be done, I think. But this is a simpler view
than the tree, I think. And then boom-- order n space. But it turns out there's
another way to do it. This is a completely
different way to do Van Emde Boas-- actually,
not that completely different. It's another way to
do this with hashing. And we're going to start with
what's called x-fast trees, and then we will modify
it to get y-fast trees. That's Willard's terminology. OK, so x-fast trees
is, store this tree, but don't store the zeros. So don't store zeros. Only store the ones in the-- we
call this the simple tree view. This is why I, in
particular, wanted to tell you about
the simple tree view, because it is really
what x fast trees do. So what do I mean by
only store the ones? Well, each of these
ones has sort of a name. What is the name of this item? Its name is one-- or in other words, 0, 0, 0, 1. Each of these nodes,
you can think of, what is the path to get here? Like, the path to get
to this one is 1, 0, 0. 1 means right. 0 means left. Those names give you
the binary indicator of where that node is in
the tree, in some sense. So store the ones as binary
strings in a hash table-- again, a dynamic
perfect hash table. Let's say I can get constant
with high probability. OK. And if you're a
little concerned-- so what this means--
the ones are exactly the prefixes of the paths
to each of the items. This was item one. And so I want to store this
one, which is empty string, this one, which is 0, this
one, which is 00, this one, which is 000, this
one, which is 0001. So I take 0001, which is
the item I want to store. And there's all
these prefixes, which are the items I want to store. And for this really
to make sense, you also need the
length of the string. Strings of different lengths
should be in different worlds. So the way, actually,
x-fast trees originally did it in the paper is,
have a different hash table for strings of
different lengths. So that's probably an easier
way to think about it. You store all the items
themselves in a hash table. You store all the
prefixes of all but the last bit in a
separate hash table, all but the last two bits in a
separate hash table, and so on. Now, what does this let you do? It lets you do this-- binary search for the
0 to 1 transition. What we did here was-- I look at the bit, is it 0 or 1? Instead of doing that, you do
a query into the hash table, and say, is it in
the hash table? It's in the hash table
if and only if it is one. So looking at a bit in
this conceptual tree is the same thing as
checking for containment in this hash table. But now, we don't have to
store the zeros, which is cool. We can now do search,
predecessor or successor, fast, in log w time, via
this old thing. Again, you have to have min
and max pointers, as well. So in this hash table,
you store the min and the max of your subtree. Or actually, from
a 1, you actually need the max of
the left subtree, and you need the min
of the right subtree. But it's a constant amount
of information per thing. This is not perfect, however,
in that it uses nw space. And also, updates are slow. It's order w updates. But we're almost there. Because we have fast
queries, slow updates, not optimal space. Take this. Add indirection-- done. And that's y-fast trees. y-fast trees-- you
take x-fast trees, you add this
indirection right here, and you get log w per
operation order and space. Of course, this is
a high probability because we're using hashing. Because we have a
factor w bad here, we have factor w bad here. You divide by w. You're done. Up here, you have n over w
space. n over w times w is n. Queries, just like
before, remain log w. But now-- boom-- updates, we pay log w
because of the binary search trees at the bottom,
but pretty cool. Isn't that neat? I've never seen this before. OK, I've seen x-fast
trees and y-fast trees. But it's really just the same-- we're taking Van Emde Boas,
looking at it in the tree view. You can see where
Willard got this stuff. It's like, oh, man I really
want to store all these bits, but hey, it's way too big. Just don't store the zeros. That means we should
use a hash table. Ah , hash table just gives you
whether the bit is in or out. Great. Now use indirection. And indirection was
already floating around as a concept at the time-- slightly different parameters. Van Emde Boas had
his own indirection to reduce the space
from u times log w to u. But Willard did it,
and-- boom-- it got down to n space in this way. But as you saw, you can also do
it directly to Van Emde Boas. All these ideas can
be interchanged. You can combine
any data structure you want with any
space saving trick you want, with
indirection, if you need to, to speed things up
and reduce space a little bit. So there's many,
many ways to do this. But in the end, you get
log w per operation, and order n space. And that sort of result one. And it's probably the most
useful predecessor data structure, in general. But next time, we'll
see fusion trees, which are good for when w is huge.

CS

We end this lecture sequence
by stepping back to discuss what probability theory really
is and what exactly is the meaning of the word
probability. In the most narrow view,
probability theory is just a branch of mathematics. We start with some axioms. We consider models that satisfy
these axioms, and we establish some consequences,
which are the theorems of this theory. You could do all that without
ever asking the question of what the word "probability"
really means. Yet, one of the theorems of
probability theory, that we will see later in this class,
is that probabilities can be interpreted as frequencies,
very loosely speaking. If I have a fair coin, and I
toss it infinitely many times, then the fraction of
heads that I will observe will be one half. In this sense, the probability
of an event, A, can be interpreted as the frequency
with which event A will occur in an infinite number of
repetitions of the experiment. But is this all there is? If we're dealing with coin
tosses, it makes sense to think of probabilities
as frequencies. But consider a statement such
as the "current president of my country will be reelected
in the next election with probability 0.7". It's hard to think of this
number, 0.7, as a frequency. It does not make sense to
think of infinitely many repetitions of the
next election. In cases like this, and in many
others, it is better to think of probabilities
as just some way of describing our beliefs. And if you're a betting person,
probabilities can be thought of as some numerical
guidance into what kinds of bets you might be
willing to make. But now if we think of
probabilities as beliefs, you can run into the argument
that, well, beliefs are subjective. Isn't probability theory
supposed to be an objective part of math and science? Is probability theory just an
exercise in subjectivity? Well, not quite. There's more to it. Probability, at the minimum,
gives us some rules for thinking systematically about
uncertain situations. And if it happens that our
probability model, our subjective beliefs, have some
relation with the real world, then probability theory can be
a very useful tool for making predictions and decisions that
apply to the real world. Now, whether your predictions
and decisions will be any good will depend on whether you
have chosen a good model. Have you chosen a model that's
provides a good enough representation of
the real world? How do you make sure that
this is the case? There's a whole field, the field
of statistics, whose purpose is to complement
probability theory by using data to come up with
good models. And so we have the following
diagram that summarizes the relation between the real
world, statistics, and probability. The real world generates data. The field of statistics and
inference uses these data to come up with probabilistic
models. Once we have a probabilistic
model, we use probability theory and the analysis tools
that it provides to us. And the results that we get
from this analysis lead to predictions and decisions
about the real world.

Probability

The following content is
provided under a Creative Commons license. Your support will help MIT
OpenCourseWare continue to offer high quality educational
resources for free. To make a donation or view
additional materials from hundreds of MIT courses, visit
MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: OK. Today we're all through with
Markov chains, or at least with finite state
Markov chains. And we're going on to
renewal processes. As part of that, we will spend
a good deal of time talking about the strong law of large
numbers, and convergence with probability one. The idea of convergence with
probability one, at least to me is by far the most difficult
part of the course. It's very abstract
mathematically. It looks like it's simple, and
it's one of those things you start to think you understand
it, and then at a certain point, you realize
that you don't. And this has been happening
to me for 20 years now. I keep thinking I really
understand this idea of convergence with probability
one, and then I see some strange example again. And I say there's something
very peculiar about this whole idea. And I'm going to illustrate
that you at the end of the lecture today. But for the most part, I will
be talking not so much about renewal processes, but this
set of mathematical issues that we have to understand in
order to be able to look at renewal processes in
the simplest way. One of the funny things about
the strong law of large numbers and how it gets applied
to renewal processes is that although the idea of
convergence with probability one is sticky and strange, once
you understand it, it is one of the most easy things
to use there is. And therefore, once you become
comfortable with it, you can use it to do things which would
be very hard to do in any other way. And because of that, most people
feel they understand it better than they actually do. And that's the reason why it
sometimes crops up when you're least expecting it, and
you find there's something very peculiar. OK, so let's start out by
talking a little bit about renewal processes. And then talking about this
convergence, and the strong law of large numbers, and what
it does to all of this. This is just review. We talked about arrival
processes when we started talking about Poisson
processes. Renewal processes are a special
kind of arrival processes, and Poisson processes
are a special kind of renewal process. So this is something you're
already sort of familiar with. All of arrival processes, we
will tend to treat in one of three equivalent ways, which is
the same thing we did with Poisson processes. A stochastic process,
we said, is a family of random variables. But in this case, we always view
it as three families of random variables, which
are all related. And all of which define
the other. And you jump back and forth from
looking at one to looking at the other, which is, as you
saw with Poisson processes, you really want to do this,
because if you stick to only one way of looking at it, you
really only pick up about a quarter, or a half
of the picture. OK. So this one picture gives us
a relationship between the arrival epochs of an arrival
process, the inter-arrival intervals, the x1, x2, x3, and
the counting process, n of t, and whichever one you use, you
use the one which is easiest, for whatever you plan to do. For defining a renewal process,
the easy thing to do is to look at the inter-arrival
intervals, because the definition of a
renewal process is it's an arrival process for which the
interrenewals are independent and identically distributed. So any process where the
arrivals, inter-arrivals have that property are IID. OK, renewal processes are
characterized, and the name comes from the idea that you
start over at each interval. This idea of starting over is
something that we talk about more later on. And it's a little bit strange,
and a little bit fishy. It's like with a Poisson
process, you look at different intervals, and they're
independent of each other. And we sort of know what
that means by now. OK, you look at the arrival
epochs for a Poisson process. Are they independent
of each other? Of course not. The arrival epochs are
the sums of the inter-arrival intervals. The inter-arrival intervals
are the things that are independent. And the arrival epochs
are the sums of inter-arrival intervals. If you know that the first
arrival epoch takes 10 times longer than its mean, then that
the second arrival epoch is going to be kind
of long, too. It's got to be at least 10 times
as long as the mean of the inter-arrival epochs,
because each arrival epoch is a sum of these inter-arrival
epochs. It's the inter-arrival epochs
that are independent. So when you say that the one
interval is independent of the other, yes, you know exactly
what you mean. And the idea is very simple. It's the same idea here. But then you start to think you
understand this, and you start to use it in
a funny way. And suddenly you're starting to
say that the arrival epochs are independent from one time
to the other, which they certainly aren't. What renewal theory does is it
lets you treat the gross characteristics of a process
in a very simple and straightforward way. So you're breaking up the
process into two sets of views about it. One is the long term behavior,
which you treat by renewal theory, and you use this one
exotic theory in a simple and straightforward way for every
different process, for every different renewal process
you look at. And then you have this usually
incredibly complicated kind of thing in the inside of
each arrival epoch. And the nice thing about renewal
theory is it lets you look at that complicated thing
without worrying about what's going on outside. So the local characteristics
can be studied without worrying about the long
term interactions. One example of this, and one
of the reasons we are now looking at Markov chains before
we look at renewal processes is that a Markov chain
is one of the nicest examples there is of a renewal
process, when you look at it in the right way. If you have a recurrent Markov
chain, then the interval from one time entering a particularly
recurrent state until the next time
you enter that recurrent state is a renewal. So we look at the sequence of
times at which we enter this one given state. Enter state one over here. We enter state one
again over here. We enter state one again,
and so forth. We're ignoring everything
that goes on between entries to state one. But every time you enter state
1, you're in the same situation as you were the last
time you entered state one. You're in the same situation,
in the sense that the inter-arrivals from state one
to state one again are independent of what
they were before. In other words, when you enter
state one, your successive state transitions from
there are the same as they were before. So it's the same situation as we
saw with Poisson processes, and it's the same kind of
renewal where when you talk about renewal, you have to be
very careful about what it is that's a renewable. Once you're careful about it,
it's clear what's going on. One of the things we're going to
find out now is one of the things that we failed to point
out before when we talked about finite state and
Markov chains. One of the most interesting
characteristics is the expected amount of time from one
entry to a recurrent state until the next time you enter
that recurrent state is 1 over pi sub i, where pi sub i is a
steady state probability of that steady state. Namely, we didn't do that. It's a little tricky to do that
in terms of Markov chains it's almost trivial to do it in
terms of renewal processes. And what's more, when we do it
in terms of renewal processes, you will see that it's
obvious, and you will never forget it. If we did it in terms of Markov
chains, it would be some long, tedious derivation,
and you'd get this nice answer, and you say, why did
that nice answer occur? And you wouldn't
have any idea. When you look at renewal
processes, it's obvious why it happens. And we'll see why that
is very soon. Also, after we finish renewal
processes, the next thing we're going to do is to
talk about accountable state Markov chains. Markov chains with accountable,
infinitely countable set of states. If you don't have a background
in renewal theory when you start to look at that, you
get very confused. So renewal theory will give us
the right tool to look at those more complicated
Markov chains. OK. So we carry from Markov chains
with accountably infinite state space comes largely
from renewal process. So yes, we'll be interested
in understanding that. OK. Another example is GTM queue. The text talked a little bit,
and we might have talked in class a little bit about this
strange notation a queueing theorist used. There are always at least three
letters separated by slashes to talk about
what kind of queue you're talking about. The first letter describes the
arrival process for the queue. G means it's a general rival
process, which doesn't really mean it's a general
arrival process. It means the arrival
process is renewal. Namely, it says the arrival
process is IID, inter-arrivals. But you don't know what
their distribution is. You would call that M if you
meant a Poisson process, which would mean memory lists,
inter-arrivals. The second G stands for the
service time distribution. Again, we assume that no matter
how many servers you have, no matter how the servers
work, the time to serve one user is independent
of the time to serve other users. But that the distribution of
that time has a general distribution. It would be M as you meant a
memory list distribution, which would mean exponential
distribution. Finally, the thing at the end
says we're talking about IQ with M servers. So the point here is we're
talking about a relatively complicated thing. Can you talk about this
in terms of renewals? Yes, you can, but it's not quite
obvious how to do it. You would think that the obvious
way of viewing a complicated queue like this is
to look at what happens from one busy period to the
next busy period. You would think the busy periods
would be independent of each other. But they're not quite. Suppose you finish one busy
period, and when you finish the one busy period, one
customer has just finished being served. But at that point, you're in the
middle of the waiting for the next customer to arrive. And as that's a general
distribution, the amount of time you have to wait for that
next customer to arrive depends on a whole
lot of things in the previous interval. So how can you talk about
renewals here? You talk about renewables by
waiting until that next arrival comes. When that next arrival comes to
terminate the idle period between busy periods, at that
time you're in the same state that you were in when the whole
thing started before. When you had the first
arrival come in. And at that point, you had one a
rival there being served you go through some long
complicated thing. Eventually the busy
period is over. Eventually, then, another
arrival comes in. And presto, at that point,
you're statistically back where you started. You're statistically back where
you started in terms of all inter-arrival times
at that point. And we will have to, even
though it's intuitively obvious that those things are
independent of each other, we're really going to have to
sort that out a little bit, because you come upon
many situations where this is not obvious. So if you don't know how to
sort it out when it is obvious, you're not going to
know how to sort it out when it's not obvious. But anyway, that's another
example of where we have renewals. OK. We want to talk about
convergence now. This idea of convergence
with probability one. It's based on the
idea of numbers converging to some limit. And I'm always puzzled about how
much to talk about this, because all of you, when you
first study calculus, talk about limits. Most of you, if you're
engineers, when you talk about calculus, it goes in one ear and
it goes out the other ear, because you don't have to
understand this very much. Because all the things you deal
with, the limits exist very nicely, and there's
no problem. So you can ignore it. And then you hear about these
epsilons and deltas, and I do the same thing. I can deal with an epsilon,
but as soon as you have an epsilon and a delta,
I go into orbit. I have no idea what's going on
anymore until I sit down and think about it very,
very carefully. Fortunately, when we have a
sequence of numbers, we only have an epsilon. We don't have a delta. So things are a little
bit simpler. I should warn you, though, that
you can't let this go in one ear and out the other ear,
because at this point, we are using the convergence of numbers
to be able to talk about convergence of random
variables, and convergence of random variables is indeed
not a simple topic. Convergence of numbers is a
simple topic made complicated by mathematicians. Any good mathematician,
when they hear me say this will be furious. Because in fact, when you think
about what they've done, they've taken something which
is simple but looks complicated, and they've turned
it into something which looks complicated in another
way, but is really the simplest way to deal with it. So let's do that and be done
with it, and then we can start using it for random variables. A sequence, b1, b2, b3, so
forth of real numbers. Real numbers are complex numbers
that doesn't make any difference, is said to converge
to a limit, b. If for each real epsilon greater
than zero, is there an integer M such that bn minus
b is less than or equal to epsilon for all n greater
than or equal to n? Now, how many people can look
at that and understand it? Be honest. Good. Some of you can. How many people look at that,
and their mind just, ah! How many people are
in that category? I am. But if I'm the only
one, that's good. OK. There's an equivalent way
to talk about this. A sequence of numbers, real or
complex, is said to converge to limit b. If for each integer k greater
than zero, there's an integer m of k, such that bn minus b
is less than or equal to 1 over k for all n greater
than or equal to m. OK. And the argument there is pick
any epsilon you want to, no matter how small. And then you pick a k, such that
1 over k is less than or equal to epsilon. According to this definition, bn
minus b less than or equal to 1 over k ensures that you
have this condition up here that we're talking about. When bn minus b is less than
or equal to 1 over k, then also bn minus b is less than
or equal to epsilon. In other words, when you look
at this, you're starting to see what this definition
really means. Here, you don't really care
about all epsilon. All you care about is that
this holds true for small enough epsilon. And the trouble is there's
no way to specify a small enough epsilon. So the only way we can do this
is to say for all epsilon. But what the argument is is if
you can assert this statement for a sequence of smaller and
smaller values of epsilon, that's all you need. Because as soon as this is true
for one value of epsilon, it's true for all smaller
values of epsilon. Now, let me show you a picture
which, unfortunately, there's a kind of a complicated
picture. It's the picture that says what
that argument was really talking about. So if you don't understand the
picture, you were kidding yourself when you said you
thought you understood what the definition said. So what the picture says,
it's in terms of this 1 over k business. It says if you have a sequence
of numbers, b1, b2, b3, excuse me for insulting you
by talking about something so trivial. But believe me, as soon as we
start talking about random variables, this trivial thing
mixed with so many other things will start to become less
trivial, and you really need to understand what
this is saying. So we're saying if we have a
sequence, b1, b2, b3, b4, b5 and so forth, what that second
idea of convergence says is that there's an M1 which says
that for all n greater than or equal to M1, b4, b5, b6, b7
minus b all lies within this limit here between b plus
1 and b minus 1. There's a number M2, which says
that as soon as you get bigger than M of 2, all
these numbers lie between these two limits. There's a number M3, which says
all of these numbers lie between these limits. So it's saying that, it's
essentially saying that you can a pipe, and as n increases,
you squeeze this pipe gradually down. You don't know how fast you
can squeeze it down when you're talking about
convergence. You might have something that
converges very slowly, and then M1 will be way out here. M2 will be way over there. M3 will be off on the
other side of Vassar Street, and so forth. But there always is such an
M1, M2, and M3, which says these numbers are getting
closer and closer to b. And they're staying closer
and closer to b. An example, which we'll come
back to, where you don't have convergence is the following
kind of thing. b1 is equal to 3/4,
in this case. b5 is equal to 3/4. b25 is equal to 3/4. b5 to the third is equal to 1. And so forth. These values at which b sub n is
equal to 3/4, b is equal to little b plus 3/4 get
more and more rare. So in some sense, this sequence
here where b2 up to b4 is zero. b6 up to b24 is zero
and so forth. This is some kind of
convergence, also. But it's not what anyone
would call convergence. I mean, as far as numbers are
concerned, there's only one kind of convergence that people
ever talk about, and it's this kind of convergence
here. This, although these numbers
are getting close to b in some sense. That's not viewed
as convergence. So here, even though almost all
the numbers are close to b, they don't stay close
to b, in a sense. They always pop up at some place
in the future, and that destroys the whole idea
of convergence. It destroys most theorems
about convergence. That's an example where you
don't have convergence. OK, random variables are really
a lot more complicated than numbers. I mean, a random variable is a
function from the sample space to real numbers. All of you know that's
not really what a random variable is. All of you know that a random
variable is a number that wiggles around a little bit,
rather than being fixed at what you ordinarily think of
a number as being, right? Since that's a very imprecise
notion, and the precise notion is very complicated, to build up
your intuition about this, you have to really think hard
about what convergence of random variables means. For convergence and
distribution, it's not the random variables, but the
distribution function of the random variables
that converge. In other words, in the
distribution function of z sub n, where you have a sequence of
random variables, z1, z2, z3 and so forth, the
distribution function evaluated at each real value z
converges for each z in the case where the distribution
function of this final convergent random variable
is continuous. We all studied that. We know what that means now. For convergence and probability,
we talked about convergence and probability
in two ways. One with an epsilon
and a delta. And then saying for every
epsilon and delta that isn't big enough, something happens. And then we saw that it was a
little easier to describe. It was a little easier to drive
describe by saying the convergence in probability. These distribution
functions have to converge to a unit step. And that's enough. They converge to a unit steps
at every z except where the step is. We talked about that. For convergence with probability
one, and this is the thing we want to talk about
today, this is the one that sounds so easy, and
which is really tricky. I don't want to scare
you about this. If you're not scared about
it to start with, I don't want to scare you. But I would like to convince
you that if you think you understand it and you haven't
spent a lot of time thinking about it, you're probably
due for a rude awakening at some point. So for convergence with
probability one, the set of sample paths that converge
has probability one. In other words, the sequence Y1,
Y2 converges to zero with probability one. And now I'm going to talk
about converging to zero rather than converging to
some random variable. Because if you're interested
in a sequence of random variables Z1, Z2 that converge
to some other random variable Z, you can get rid of a lot of
the complication by just saying, let's define a random
variable y sub n, which is equal to z sub n minus c. And then what we're interested
in is do these random variables y sub n
converged to 0. We can forget about what it's
converging to, and only worry about it converging to 0. OK, so when we do that, this
sequence of random variables converges to 0 with
probability 1. If the probability of the set of
sample points for which the sample path converges to 0. If that set of sample paths
has probability 1-- namely, for almost everything
in the space, for almost everything in its peculiar
sense of probability-- if that holds true, then you say
you have convergence with probability 1. Now, that looks straightforward, and I hope it is. You can memorize it
or do whatever you want to do with it. We're going to go on now and
prove an important theorem about convergence with
probability 1. I'm going to give a proof here
in class that's a little more detailed than the proof
I give in the notes. I don't like to give
proofs in class. I think it's a lousy idea
because when you're studying a proof, you have to go
at your own pace. But the problem is, I
know that students-- and I was once a
student myself, and I'm still a student. If I see a proof, I will only
look at enough of it to say, ah, I get the idea of it. And then I will stop. And for this one, you need a
little more than the idea of it because it's something
we're going to build on all the time. So I want to go through
this proof carefully. And I hope that most of you
will follow most of it. And the parts of it that you
don't follow, I hope you'll go back and think about
it, because this is really important. OK, so the theorem says, let
this sequence of random variables satisfy the expected
value of the magnitude of Y sub n, the sum from n equals 1
to infinity of this is less than infinity. As usual there's a
misprint there. The sum, the expected
value of Yn, the bracket should be there. It's supposed to be less
than infinity. Let me write that down. The sum from n equals 1 to
infinity of expected value of the magnitude of Y sub n
is less than infinity. So it's a finite sum. So we're talking about these
Yn's when we start talking about the strong law
of large numbers. Yn is going to be something like
the sum from n equals 1 to m divided by m. In other words, it's going to
be the sample average, or something like that. And these sample averages, if
you have a mean 0, are going to get small. The question is, when you sum
all of these things that are getting small, do you still get
something which is small? When you're dealing with the
weak law of large numbers, it's not necessary that
that sum gets small. It's only necessary that each
of the terms get small. Here we're saying, let's
assume also that this sum gets small. OK, so we want to prove that
under this condition, all of these sequences with probability
1 converge to 0, the individual sequences
converge. OK, so let's go through
the proof now. And as I say, I won't do
this to you very often. But I think for this one,
it's sort of necessary. OK, so first we'll use the
Markov inequality. And I'm dealing with a finite
value of m here. The probability that the sum of
a finite set of these Y sub n's is greater than alpha is
less than or equal to the expected value of that
random variable. Namely, this random
variable here. Sum from n equals 1 to m of
magnitude of Y sub n. That's just a random variable. And the probability that that
random variable is greater than alpha is less than or equal
to the expected value of that random variable
divided by alpha. OK, well now, this quantity here
is increasing in Y sub n. The magnitude of Y sub n is
a non-negative quantity. You take the expectation of a
non-negative quantity, if it has an expectation, which we're
assuming here for this to be less infinity, all of
these things have to have expectations. So as we increase m, this
gets bigger and bigger. So this quantity here is going
to be less than or equal to the sum from n equals 1 to
infinity of expected value of Y sub n divided by alpha. What I'm being careful about
here is all of the things that happen when you go from finite
m to infinite m. And I'm using what you know
about finite m, and then being very careful about going
to infinite m. And I'm going to try to explain
why as we do it. But here, it's straightforward. The expected value of a finite
sum is equal to the finite sum of an expected value. When you go to the limit, m goes
to infinity, you don't know whether these expected
values exist or not. You're sort of confused on both
sides of this equation. So we're sticking to
finite values here. Then, we're taking this
quantity, going to the limit as m goes to infinity. This quantity has to get bigger
and bigger as m goes to infinity, so this quantity
has to be less than or equal to this. This now, for a given alpha,
is just a number. It's nothing more than a number,
so we can deal with this pretty easily as we
make alpha big enough. But for most of the argument,
we're going to view alpha as being fixed. OK, so now the probability that
this sum, finite sum is greater than alpha, is less
than or equal to this. This was the thing we just
finished proving on the other page. This is less than or
equal to that. That's what I repeated, so I'm
not cheating you at all here. Now, it's a pain to write
that down all the time. So let's let the set, A sub m,
be the set of sample points such that its finite sum
of Y sub n of omega is greater than alpha. This is a random-- for each value of omega,
this is just a number. The sum of the magnitude of Y
sub n is a random variable. It takes on a numerical
value for every omega in the sample space. So A sub m is the set of points
in the sample space for which this quantity here
is bigger than alpha. So we can rewrite this
now as just the probability of A sub m. So this is equivalent to saying,
the probability of A sub m is less than or equal
to this number here. For a fixed alpha,
this is a number. This is something which
can vary with m. Since these numbers here now,
now we're dealing with a sample space, which is
a little strange. We're talking about sample
points and we're saying, this number here, this magnitude of Y
sub n at a particular sample point omega is greater
than or equal to 0. Therefore, a sub m is a subset
of A sub m plus 1. In other words, as m gets larger
and larger here, m here gets larger and larger. Therefore, this sum here
gets larger and larger. Therefore, the set of omega for
which this increasing sum is greater than alpha gets
bigger and bigger. And that's the thing that we're
saying here, A sub m is included in A sub m plus 1 for
m greater than or equal to 1. OK, so the left side of this
quantity here, as a function of m, is a non-decreasing
bounded sequence of real numbers. Yes, the probability
of something is just a real number. A probability is a number. So this quantity here
is a real number. It's a real number which is
non-decreasing, so it keeps moving upward. What I'm trying to do now
is now, I went to the limit over here. I want to go to the
limit here. And so I have a sequence
of numbers in m. This sequence of numbers
is non-decreasing. So it's moving up. Every one of those quantities
is bounded by this quantity here. So I have an increasing sequence
of real numbers, which is bounded on the top. What happens? When you have a sequence
of real numbers which is bounded-- I have a slide to prove this,
but I'm not going to prove it because it's tedious. Here we have this probability
which I'm calling A sub m probability of A sub m. Here I have the probability of
A sub m plus 1, and so forth. Here I have this
limit up here. All of this sequence of numbers,
there's an infinite sequence of them. They're all non-decreasing. They're all bounded by
this number here. And what happens? Well, either we go up to there
as a limit or else we stop sometime earlier as a limit. I should prove this, but it's
something we use all the time. It's a sequence of increasing
or non-decreasing numbers. If it's bounded by something, it
has to have a finite limit. The limit is less than or
equal to this quantity. It might be strictly less, but
the limit has to exist. And the limit has to be less
than or equal to b. OK, that's what we're
saying here. When we go to this limit, this
limit of the probability of A sub m is less than or equal
to this number here. OK, if I use this property of
nesting intervals, when you have A sub 1 nested inside of
A sub 2, nested inside of A sub 3, what we'd like to go
do is go to this limit. The limit, unfortunately,
doesn't make any sense in general. With this property of the
axioms, it's equation number 9 in chapter 1 says that
we can do something that's almost as good. What it says that as we go to
this limit here, what we get is that this limit is
the probability of this infinite union. That's equal to the limit
as m goes to infinity of probability of A sub m. OK, look up equation 9,
and you'll see that's exactly what it says. If you think this is
obvious, it's not. It ain't obvious at all because
it's not even clear that this-- well, nothing very much about
this union is clear. We know that this union must
be a measurable set. It must have a probability. We don't know much
more about it. But anyway, that property tells
us that this is true. OK, so where we are
at this point. I don't think I've skip
something, have I? Oh, no, that's the thing I
didn't want to talk about. OK, so A sub m is a set
of omega which satisfy this for finite m. The probability of this union
is then the union of all of these quantities over all m. And this is less than or equal
to this bound that we had. OK, so I even hate giving proofs
of this sort because it's a set of simple ideas. To track down every one
of them is difficult. The text doesn't track down
every one of them. And that's what I'm
trying to do here. We have two possibilities here,
and we're looking at this limit here. This limiting sum, which for
each omega is just a sequence, a non-decreasing sequence
of real numbers. So one possibility is that this
sequence of real numbers is bigger than alpha. The other possibility
is that it's less than or equal to alpha. If it's less than or equal to
alpha, then every one of these numbers is less than or equal
to alpha and omega has to be not in this union here. If the sum is bigger than
alpha, then one of the elements in this set is
bigger than alpha and omega is in this set. So what all of that says, and
you're just going to have to look at that because
it's not-- it's one of these tedious
arguments. So the probability of omega such
that this sum is greater than alpha is less than or equal
to this number here. At this point, we have
made a major change in what we're doing. Before we were talking about
numbers like probabilities, numbers like expected values. Here, suddenly, we are talking
about sample points. We're talking about the
probability of a set of sample points, such that the sum
is greater than alpha. Yes? AUDIENCE: I understand how if
the whole sum if less than or equal than alpha then
every element is. But did you say that if it's
greater than alpha, then at least one element is
greater than alpha? Why is that? PROFESSOR: Well, because either
the sum is less than or equal to alpha or it's
greater than alpha. And if it's less than or equal
to alpha, then omega is not in this set. So the alternative is that omega
has to be in this set. Except the other way of looking
at it is if you have a sequence of numbers, which is
approaching a limit, and that limit is bigger than alpha, then
one of the terms has to be bigger than alpha. Yes? AUDIENCE: I think the confusion
is between the partial sums and the
terms of the sum. That's what he's confusing. Does that make sense? He's saying instead of
each partial sum, not each term in the sum. PROFESSOR: Yes. Except I don't see how that
answers the question. Except the point here is, if
each partial sum is less than or equal to alpha, then the
limit has to be less than or equal to alpha. That's what I was saying
on the other page. If you have a sequence of
numbers, which has an upper bound on them, then you
have to have a limit. And that limit has to be less
than or equal to alpha. So that's this case here. We have a sum of numbers as
we're going to the limit as m gets larger and larger,
these partial sums have to go to a limit. The partial sums are all less
than or equal to alpha. Then the infinite sum is less
than or equal to alpha, and omega is not in this set here. And otherwise, it is. OK, if I talk more about it,
I'll get more confused. So I think the slides
are clear. Now, if we look at the case
where alpha is greater than or equal to this sum, and we take
the complement of the set, the probability of the set of omega
for which this sum is less than or equal
to alpha has-- oh, let's forget about
this for the moment. If I take the complement of this
set, the probability of the set of omega, such that the
sum is less than or equal to alpha, is greater
than 1 minus this expected value here. Now I'm saying, let's look at
the case where alpha is big enough that it's greater
than this number here. So this probability is greater
than 1 minus this number. So if the sum is less than or
equal to alpha for any given omega, then this quantity
here converges. Now I'm talking about
sample sequences. I'm saying I have an increasing
sequence of numbers corresponding to one particular
sample point. This increasing set of numbers
is less than or equal. Each element of it is less than
or equal to alpha, so the limit of it is less than
or equal to alpha. And what that says is the limit
of Y sub n of omega, this has to be equal to 0
for that sample point. This is all the sample
point argument. And what that says then is the
probability of omega, such that this limit here is equal
to 0, that's this quantity here, which is the same as this
quantity, which has to be greater than this quantity. This implies this. Therefore, the probability of
this has to be bigger than this probability here. Now, if we let alpha go to
infinity, what that says is this quantity goes to 0 and the
probability of the set of omega, such that this limit is
equal to 0, is equal to 1. I think if I try to spend 20
more minutes talking about that in more detail, it
won't get any clearer. It is one of these very tedious
arguments where you have to sit down and follow
it step by step. I wrote the steps that's
very carefully. And at this point, I have
to leave it as it is. But the theorem has been proven,
at least in what's written, if not in
what I've said. OK, let's look at an example
of this now. Let's look at the example where
these random variables Y sub n for n greater than or
equal to 1, have this following property. It's almost the same as the
sequence of numbers I talked about before. But what I'm going
to do now is-- these are not IID random
variables. If they're IID random variables,
you're never going to talk about the sum
being finite. Sum of the expected values
being finite. How they behave is that for one
less than or equal to 5, you pick one of these random
variables in here and make it equal to 1. And all the rest
are equal to 0. From 5 to 25, you pick one of
the random variables, make it equal to 1, and all the
others are equal to 0. You choose randomly in here. From 25 to 125, you pick
one random variable, set it equal to 1. All the other random variables,
set it equal to 0, and so forth forever after. OK, so what does that say
for the sample points? If I look at any particular
sample point, what I find is that there's one occurrence of a
sample value equal to 1 from here to here. There's exactly one that's equal
to 1 from here to here. There's exactly one that's equal
to 1 from here to way out here at 125, and so forth. This is not a sequence of sample
values which converges because it keeps popping up
to 1 at all these values. So for every omega, Yn
of omega is 1 for some n in this interval. For every j and it's
0 elsewhere. This Yn of omega doesn't
converge for omega. So the probability that that
sequence converges is not 1, it's 0. So this is a particularly
awful example. This is a sequence of random
variables, which does not converge with probability 1. At the same time, the expected
value of Y sub n is 1 over 5 to the j plus 1 minus
5 to the j. That's the probability that you
pick that particular n for a random variable to
be equal to 1. It's equal to this for 5 to the
j less than or equal to n, less than 5 to the j plus 1. When you add up all of these
things, when you add up expected value of Yn equal
to that over this interval, you get 1. When you add it up over the next
interval, which is much, much bigger, you get 1 again. When you add it up
over the next interval, you get 1 again. So the expected value
of the sum-- the sum of the expected
value of the Y sub n's is equal to infinity. And what you wind up with
then is that this sequence does not converge-- This says the theorem doesn't
apply at all. This says that the Y sub n of
omega does not converge for any sample function at all. This says that according to the
theorem, it doesn't have to converge. I mean, when you look at an
example after working very hard to prove a theorem, you
would like to find that if the conditions of the theorem are
satisfied what the theorem says is satisfied also. Here, the conditions
are not satisfied. And you also don't
have convergence with probability 1. You do have convergence in
probability, however. So this gives you a nice example
of where you have a sequence of random variables
that converges in probability. It converges in probability
because as n gets larger and larger, the probability that Y
sub n is going to be equal to anything other than 0 gets
very, very small. So the limit as n goes to
infinity of the probability that Y sub n is greater
than epsilon-- for any epsilon greater than 0,
this probability is equal to 0 for all epsilon. So this quantity does converge
in probability. It does not converge
with probability 1. It's the simplest example I know
of where you don't have convergence with probability 1
and you do have convergence in probability. How about if you're looking at a
sequence of sample averages. Suppose you're looking at S sub
n over n where S sub n is a sum of IID random variables. Can you find an example there
where when you have a-- can you find an example where
this sequence S sub n over n does converge in probability,
but does not converge with probability 1? Unfortunately, that's
very hard to do. And the reason is the main
theorem, which we will never get around to proving here is
that if you have a random variable x, and the expected
value of the magnitude of x is finite, then the strong law
of large numbers holds. Also, the weak law of large
numbers holds, which says that you're not going to find an
example where one holds and the other doesn't hold. So you have to go to strange
things like this in order to get these examples that
you're looking at. OK, let's now go from
convergence in probability 1 to applying this to the sequence
of random variables where Y sub n is now equal to
the sum of n IID random variable divided by n. Namely, it's the sample average,
and we're looking at the limit as n goes to infinity of this sample average. What's the probability of the
set of sample points for which this sample path converges
to X bar? And the theorem says that this
quantity is equal to 1 if the expected value of X is
less than infinity. We're not going to prove that,
but what we are going to prove is that if the expected value
of the fourth moment of X is finite, then we're going
to prove that this theorem is true. OK, when we write this from now
on, we will sometimes get more terse. And instead of writing the
probability of an omega in the set of sample points such that
this limit for a sample point is equal to X bar, this whole
thing is equal to 1. We can sometimes write it as
the probability that this limit, which is now a limit
of Sn of omega over n is equal to X bar. But that's equal to 1. Some people write it even more
tersely as the limit of S sub n over n is equal to X bar
with probability 1. This is a very strange statement
here because this-- I mean, what you're saying with
this statement is not that this limit is equal to
X bar with probability 1. It's saying, with probability 1,
this limit here exists for a sample point, and that limit
is equal to X bar. The thing which makes the strong
law of large numbers difficult is not proving
that the limit has a particular value. If there is a limit, it's
always easy to find what the value is. The thing which is difficult is
figuring out whether it has a limit or not. So this statement is fine for
people who understand what it says, but it's kind of
confusing otherwise. Still more tersely, people talk
about it as Sn over n goes to limit X bar with
probability 1. This is probably an even better
way to say it than this is because this is-- I mean, this says that there's
something strange in the limit here. But I would suggest that you
write it this way until you get used to what it's saying. Because then, when you write it
this way, you realize that what you're talking about is
the limit over individual sample points rather than some
kind of more general limit. And convergence with probability
1 is always that sort of convergence. OK, this strong law and the
idea of convergence with probability 1 is really pretty
different from the other forms of convergence. In the sense that it focuses
directly on sample paths. The other forms of convergence
focus on things like the sequence of expected values,
or where the sequence of probabilities, or sequences of
numbers, which are the things you're used to dealing with. Here you're dealing directly
with sample points, and it makes it more difficult to
talk about the rate of convergence as n approaches
infinity. You can't talk about the rate
of convergence here as n approaches infinity. If you have any n less than
infinity, if you're only looking at a finite sequence,
you have no way of saying whether any of the sample values
over that sequence are going to converge or whether
they're not going to converge, because you don't know what
the rest of them are. So talking about a rate of
convergence with respect to the strong law of large numbers doesn't make any sense. It's connected directly to
the standard notion of a convergence of a sequence of
numbers when you look at those numbers applied to
a sample path. This is what gives the strong
law of large numbers its power, the fact that it's
related to this standard idea of convergence. The standard idea of convergence
is what the whole theory of analysis
is built on. And there are some very powerful
things you can do with analysis. And it's because convergence is
defined the way that it is. When we talk about the strong
law of large numbers, we are locked into that particular
notion of convergence. And therefore, it's going
to have a lot of power. We will see this as soon
as we start talking about renewal theory. And in fact, we'll see it in
the proof of the strong law that we're going
to go through. Most of the heavy lifting with
the strong law of large numbers has been done by the
analysis of convergence with probability 1. The hard thing is this theorem
we've just proven. And that's tricky. And I apologize for getting a
little confused about it as we went through it, and not
explaining all the steps completely. But as I said, it's hard
to follow proofs in real time anyway. But all of that is done now. How do we go through the strong
law of large numbers now if we accept this
convergence with probability 1? Well, it turns out to
be pretty easy. We're going to assume that the
expected value of the fourth moment of this underlying
random variable is less than infinity. So let's look at the expected
value of the sum of n random variables taken to
the fourth power. OK, so what is that? It's the expected value of S
sub n times S sub n times S sub n times S sub n. Sub n is the sum of
Xi from 1 to n. It's also this. It's also this. It's also this. So the expected value of S to
the n fourth is the expected value of this entire
product here. I should have a big bracket
around all of that. If I multiply all of these terms
out, each of these terms goes from 1 to n, what I'm
going to get is the sum from 1 to n. Sum over j from 1 to n. Sum over k from 1 to n. And a sum over l from 1 to n. So I'm going to have the
expected value of X sub i times X sub j times X
sub k times X sub l. Let's review what this is. X sub i is the random variable
for the i-th of these X's. I have n X's-- X1, X2, X3, up to X sub n. What I'm trying to find is the
expected value of this sum to the fourth power. When you look at the sum of
something, if I look at the sum of numbers, [INAUDIBLE] of
a sub i, times the sum of b sub i, I write it as j. If i just do this, what's
it equal to? It's equal to the sum over
i and j of a sub i times a sub j. I'm doing exactly the same thing
here, but I'm taking the expected value of it. That's a finite sum. the
expected value of the sum is equal to the sum of the
expected values. So if I look at any particular
value of X-- of this first X here. Suppose I look at i equals 1 . I suppose I look at the expected
value of X1 times-- and I'll make this anything
other than 1. I'll make this anything other
than 1, and this anything other than 1. For example, suppose I'm trying
to find the expected value of X1 times X2
times X10 times X3. OK, what is that? Since X1, X2, X3 are all
independent of each other, the expected value of X1 times the
expected value of all these other things is the expected
value of X1 conditional on the values of these other
quantities. And then I average over all
the other quantities. Now, if these are independent
random variables, the expected value of this given the values
of these other quantities is just the expected value of X1. I'm dealing with a case where
the expected value of X is equal to 0. Assuming X bar equals 0. So when I pick i equal to 1
and all of these equal to something other than 1, this
expected value is equal to 0. That's a whole bunch of expected
values because that includes j equals 2 to n, k
equals 2 to n, and X sub l equals 2 to n. Now, I can do this for X sub i
equals 2, X sub i equals 3, and so forth. If i is different from j, and k,
and l, this expected value is equal to 0. And the same thing if
X sub j is different than all the others. The expected value
is equal to 0. So how can I get anything
that's nonzero? Well, if I look at X sub 1 times
X sub 1 times X sub 1 times X sub 1, that
gives me expected value of X to the fourth. That's not 0, presumably. And I have n terms like that. Well, I'm getting
down to here. What we have is two kinds
of nonzero terms. One of them is where i is equal
to j is equal to k is equal to l. And then we have X sub i
to the fourth power. And we're assuming that's
some finite quantity. That's the basic assumption
we're using here, expected value of X fourth is
less than infinity. What other kinds of things
can we have? Well, if i is equal to j, and
if k is equal to l, then I have the expected value of Xi
squared times expected value of Xi squared Xk squared. What is that? Xi squared is independent of
Xk squared because i is unequal to k. These are independent
random variables. So I have the expected value
of Xi squared is what? It's just a variance of X.
This quantity here is the variance of X also. So I have the variance of Xi
squared, which is squared. So I have sigma to
the fourth power. So those are the only terms that
I have for this second kind of nonzero term
where Xi-- excuse me. not Xi
is equal to Xj. That's not what we're
talking about. Where i is equal to j. Namely, we have a sum where i
runs from 1 to n, where j runs from 1 to n, k runs from 1 to
n, and l runs from 1 to n. What we're looking at is, for
what values of i, j, k, and l is this quantity
not equal to 0? We're saying that if i is equal
to j is equal to k is equal to l, then for all of
those terms, we have the expected value of X fourth. For all terms in which i is
equal to j and k is equal to l, for all of those terms, we
have the expected value of X sub i quantity squared. Now, how many of those
terms are there? Well, x sub i can be
any one of n terms. x sub j can be any one
of how many terms? It can't be equal. i is equal to j, how many
things can k be? It can't be equal to i because
then we would wind up with X sub i to the fourth power. So we're looking at n minus
1 possible values for k, n possible values for i. So there are n times n minus
1 of those terms. I can also have-- let me write in this way. Times Xk Xl equals k. I can have those terms. I can also have Xi
Xj unequal to i. Xk equal to k and
Xl equal to i. I can have terms like this. And that gives me a sigma
fourth term also. I can also have Xi
Xj unequal to i. k can be equal to i and
l can be equal to j. So I really have three
kinds of terms. I have three times n times n
minus 1 times the expected value of X squared, this
quantity squared. So that's the total value of
expected value of S sub n to the fourth. It's the n terms for which i is
equal to j is equal to k is equal to l plus the 3n times n
minus 1 terms in which we have two pairs of equal terms. So we have that quantity here. Now, expected value of X fourth
is the second moment of the random variable X squared. So the expected value of X
squared squared is the mean of X squared squared. And that's less than or equal to
the variance of X squared, which is this quantity. The expected value
of Sn fourth is-- well, actually it's less than
or equal to 3n squared times the expected value
of X fourth. And blah, blah, blah, until we
get to 3 times the expected value of X fourth times the
sum from n equals 1 to infinity of 1 over n squared. Now, is that quantity finite
or is it infinite? Well, let's talk of three
different ways of showing that this sum is going
to be finite. One of the ways is that this is
an approximation, a crude approximation, of the
integral from 1 to infinity of 1 over X squared. You know that that integral
is finite. Another way of doing it is you
already know that if you take 1 over n times 1 over n plus 1,
you know how to sum that. That sum is finite. You can bound this by that. And the other way of doing it is
simply to know that the sum of 1 over n squared is finite. So what this says is that the
expected value of S sub n fourth over n fourth is
less than infinity. That says that the probability
of the set of omega for which S to the fourth over n fourth
is equal to 0 is equal to 1. in other words, it's saying
that S to the fourth over omega over n fourth
converges to 0. That's not quite what
we want, is it? But the set of sample points
for which this quantity converges has probability 1. And here is where you see the
real power of the strong law of large numbers. Because if these numbers
converge to 0 with probability 1, what happens to the set of
numbers is Sn to the fourth of omega divided by n to the
fourth, this limit-- if this was equal to 0, then
what is the limit as n approaches infinity of Sn of
omega over n to the fourth? If I take the fourth root
of this, I get this. If this quantity is converging
to 0, the fourth root of this also has to be converging to 0
on a sample path basis of the fact that this converges means
that this converges also. Now, you see if you were dealing
with convergence in probability or something like
that, you couldn't play this funny game. And the ability to play this
game is really what makes convergence in probability
a powerful concept. You can do all sorts of strange
things with it. And we'll talk about
that next time. But that's why all
of this works. So that's what says that the
probability of the set of omega for which the limits
of Sn of omega over n equals 0 equals 1. Now, let's look at the
strange aspect of what we've just done. And this is where things
get very peculiar. Let's look at the Bernoulli
case, which by now we all understand. So we consider a Bernoulli
process, all moments of X exist. Moment-generating functions
of X exist. X is about as well-behaved as
you can expect because it only has the values 1 or 0. So it's very nice. The expected value of X
is going to be equal to p in this case. The set of sample paths for
which Sn of omega over n is equal to p has probability 1. In other words, with probability
1, when you look at a sample path and you look
at the whole thing from n equals 1 off to infinity, and
you take the limit of that sample path as n goes to
infinity, what you get is p. And the probability that you
get p is equal to 1. Well, now, the thing that's
disturbing is, if you look at another Bernoulli process where
the probability of the 1 is p prime instead of p. What happens then? With probability 1, you get
convergence of Sn of omega over n, but the convergence is
to p prime instead of to p. The events in these two spaces
are exactly the same. We've changed the probability
measure, but we've kept all the events the same. And by changing the probability
measure, we have changed one set of probability 1
into a set of probability 0. And we changed another set of
probability 0 into set of probability 1. So we have two different
events here. On one probability measure, this
event has probability 1. On the other one, it
has probability 0. They're both very nice, very
well-behaved probabilistic situations. So that's a little disturbing. But then you say, you can pick
p in an uncountably infinite number of ways. And for each way you
count p, you have uncountably many events. Excuse me, for each value of
p, you have one event of probability 1 for that p. So as you go through this
uncountable number of events, you go through this uncountable
number of p's, you have an uncountable number of
events, each of which has probability 1 for its own p. And now the set of sequences
that converge is, in fact, a rather peculiar sequence
to start with. So if you look at all the other
things that are going to happen, there are an awful
lot of those events also. So what is happening here is
that these events that we're talking about are indeed very,
very peculiar events. I mean, all the mathematics
works out. The mathematics is fine. There's no doubt about it. In fact, the mathematics
of probability theory was worked out. People like Kolmogorov went to
great efforts to make sure that all of this worked out. But then he wound up with
this peculiar kind of situation here. And that's what happens when you
go to an infinite number of random variables. And it's ugly, but that's
the way it is. So that what I'm arguing here
is that when you go from finite m to infinite n, and
you start interchanging limits, and you start taking
limits without much care and you start doing all the things
that you would like to do, thinking that infinite n is sort
of the same as finite n. In most places in probability,
you can do that and you can away with it. As soon as you start dealing
with the strong law of large numbers, you suddenly really
have to start being careful about this. So from now on, we have to be
just a little bit careful about interchanging limits,
interchanging summation and integration, interchanging all
sorts of things, as soon as we have an infinite number
of random variables. So that's a care that we have
to worry about from here on. OK, thank you.

Linear Algebra

Let us now examine
what conditional probabilities are good for. We have already discussed that
they are used to revise a model when we get new
information, but there is another way in which
they arise. We can use conditional
probabilities to build a multi-stage model of a
probabilistic experiment. We will illustrate this through
an example involving the detection of an object
up in the sky by a radar. We will keep our example
very simple. On the other hand, it turns
out to have all the basic elements of a real-world
model. So, we are looking up in the
sky, and either there's an airplane flying up
there or not. Let us call Event A the event
that an airplane is indeed flying up there, and we have
two possibilities. Either Event A occurs, or the
complement of A occurs, in which case nothing is
flying up there. At this point, we can also
assign some probabilities to these two possibilities. Let us say that through prior
experience, perhaps, or some other knowledge, we know that
the probability that something is indeed flying up there is
5% and with probability 95% nothing is flying. Now, we also have a radar that
looks up there, and there are two things that can happen. Either something registers
on the radar screen or nothing registers. Of course, if it's a good radar,
probably Event B will tend to go together with Event
A. But it's also possible that the radar will make
some mistakes. And so we have various
possibilities. If there's a plane up there,
it's possible that the radar will detect it, in which case
Event B will also happen. But it's also conceivable that
the radar will not detect it, in which case we have
a so-called miss. And so a plane is flying up
there, but the radar missed it, did not detect it. Another possibility is that
nothing is flying up there, but the radar does detect
something, and this is a situation that's called
a false alarm. Finally, there's the possibility
that nothing is flying up there, and the radar
did not see anything either. Now, let us focus on this
particular situation. Suppose that Event
A has occurred. So we are living inside this
particular universe. In this universe, there are two
possibilities, and we can assign probabilities to these
two possibilities. So let's say that if something
is flying up there, our radar will find it with probability
99%, but will also miss it with probability 1%. What's the meaning of
this number, 99%? Well, this is a probability that
applies to a situation where an airplane is up there. So it is really a conditional
probability. It's the conditional probability
that we will detect something, the radar will
detect the plane, given that the plane is already
flying up there. And similarly, this 1% can be
thought of as the conditional probability that the complement
of B occurs, so the radar doesn't see anything,
given that there is a plane up in the sky. We can assign similar
probabilities under the other scenario. If there is no plane, there is
a probability that there will be a false alarm, and there is
a probability that the radar will not see anything. Those four numbers here
are, in essence, the specs of our radar. They describe how the radar
behaves in a world in which an airplane has been placed in
the sky, and some other numbers that describe how the
radar behaves in a world where nothing is flying
up in the sky. So we have described various
probabilistic properties of our model, but is it
a complete model? Can we calculate anything that
we might wish to calculate? Let us look at this question. Can we calculate the
probability that both A and B occur? It's this particular
scenario here. How can we calculate it? Well, let us remember the
definition of conditional probabilities. The conditional probability of
an event given another event is the probability of their
intersection divided by the probability of the conditioning
event. But this doesn't quite help us
because if we try to calculate the numerator, we do not have
the value of the probability of A given B. We have the value
of the probability of B given A. What can we do? Well, we notice that we can
use this definition of conditional probabilities,
but use it in the reverse direction, interchanging the
roles of A and B. If we interchange the roles of A and
B, our definition leads to the following expression. The conditional probability of
B given A is the probability that both events occur divided
by the probability, again, of the conditioning event. Therefore, the probability that
A and B occur is equal to the probability that A occurs
times the conditional probability that B occurs
given that A occurred. And in our example, this is
0.05 times the conditional probability that B occurs,
which is 0.99. So we can calculate the
probability of this particular event by multiplying
probabilities and conditional probabilities along the path
in this tree diagram that leads us here. And we can do the same for any
other leaf in this diagram. So for example, the probability
that this happens is going to be the probability
of this event times the conditional probability of
B complement given that A complement has occurred. How about a different
question? What is the probability, the
total probability, that the radar sees something? Let us try to identify
this event. The radar can see something
under two scenarios. There's the scenario where there
is a plane up in the sky and the radar sees it. And there's another scenario
where nothing is up in the sky, but the radar thinks
that it sees something. So these two possibilities
together make up the event B. And so to calculate the
probability of B, we need to add the probabilities
of these two events. For the first event, we
already calculated it. It's 0.05 times 0.90. For the second possibility,
we need to do a similar calculation. The probability that this occurs
is equal to 0.95 times the conditional probability of B
occurring under the scenario where A complement has occurred,
and this is 0.1. If we add those two numbers
together, the answer turns out to be 0.1445. Finally, a last question, which
is perhaps the most interesting one. Suppose that the radar
registered something. What is the probability
that there is an airplane out there? How do we do this calculation? Well, we can start from the
definition of the conditional probability of A given B, and
note that we already have in our hands both the numerator
and the denominator. So the numerator is this number,
0.05 times 0.99, and the denominator is 0.1445, and
we can use our calculators to see that the answer is
approximately 0.34. So there is a 34% probability
that an airplane is there given that the radar
has seen or thinks that it sees something. So the numerical value of this
answer is somewhat interesting because it's pretty small. Even though we have a very good
radar that tells us the right thing 99% of the time
under one scenario and 90% under the other scenario. Despite that, given that the
radar has seen something, this is not really convincing or
compelling evidence that there is an airplane up there. The probability that there's an
airplane up there is only 34% in a situation where
the radar thinks that it has seen something. So in the next few segments, we
are going to revisit these three calculations and see
how they can generalize. In fact, a large part of what is
to happen in the remainder of this class will
be elaboration on these three ideas. They are three types of
calculations that will show up over and over, of course, in
more complicated forms, but the basic ideas are essentially
captured in this simple example.

Diff. Eq.

Everything I say today is going
to be for n-by-n systems, but for your calculations and
the exams two-by-two will be good enough.
Our system looks like that. Notice I am talking today about
the homogeneous system, not the inhomogenous system.
So, homogenous. And we have so far two basic
methods of solving it. The first one,
on which we spent the most time, is the method of where you
calculate the eigenvalues of the matrix, the eigenvectors,
and put them together to make the general solution.
So eigenvalues, e-vectors and so on.
The second method, which I gave you last time,
I called "royal road," simply calculates the matrix e to the
At and says that the solution is e to the At times x
zero, the initial condition.
That is very elegant. The only problem is that to
calculate the matrix e to the At, although sometimes you can
do it by its definition as an infinite series,
most of the time the only way to calculate the matrix e to the
At is by using the fundamental matrix.
In other words, the normal way of doing it is
you have to calculate it as the fundamental matrix time
normalized at zero. So, as I explained at the end
of last time and you practiced in the recitations,
you have to find the fundamental matrix,
which, of course, you have to do by eigenvalues
and eigenvectors. And then you multiply it by its
value at zero, inverse.
And that, by magic, turns out to be the same as the
exponential matrix. But, of course,
there has been no gain in simplicity or no gain in ease of
calculation. The only difference is that the
language has been changed. Now, today is going to be
devoted to yet another method which saves no work at all and
only amounts to a change of language.
The only reason I give it to you is because I have been
begged by various engineering departments to do so --
-- because that is the language they use.
In other words, each person who solves systems,
some like to use fundamental matrices, some just calculate,
some immediately convert the system by elimination into a
single higher order equation because they are more
comfortable with that. Some, especially if they are
writing papers, they talk exponential matrices.
But there are a certain number of engineers and scientists who
talk decoupling, express the problem and the
answer in terms of decoupling. And that is,
therefore, what I have to explain to you today.
So, the third method, today's method,
I stress is really no more than a change of language.
And I feel a little guilty about the whole business.
Instead of going more deeply into studying these equations,
what I am doing is like giving a language course and teaching
you how to say hello and good-bye in French,
German, Spanish, and Italian.
It is not going very deeply into any of those languages,
but you are going into the outside world,
where people will speak these things.
Here is an introduction to the language of decoupling in which
for some people is the exclusive language in which they talk
about systems. Now, I think the best way to,
well, in a general way, what you try to do is as
follows. You try to introduce new
variables. You make a change of variables.
I am going to do it two-by-two just to save a lot of writing
out. And it's going to be a linear
change of variables because we are interested in linear
systems. The problem is to find u and v
such that something wonderful happens, such that when you make
the change of variables to express this system in terms of
u and v it becomes decoupled. And that means the system turns
into a system which looks like u prime equals k1 times u
and v prime equals k2 times v.
Such a system is called decoupled.
Why? Well, a normal system is called
coupled. Let's write out what it would
be. Well, let's not write that.
You know what it looks like. This is decoupled because it is
not really a system at all. It is just two first-order
equations sitting side by side and having nothing whatever to
do with each other. This is two problems from the
first day of the term. It is not one problem from the
next to last day of the term, in other words.
To solve this all you say is u is equal to some constant times
e to the k1 t and v equals another constant times e
to the k2 t. Coupled means that the x and y
occur in both equations on the right-hand side.
And, therefore, you cannot solve separately for
x and y, you must solve together for both of them.
Here I can solve separately for u and v and, therefore,
the system has been decoupled. Now, obviously,
if you can do that it's an enormous advantage,
not just to the ease of solution, because you can write
down the solution immediately, but because something physical
must be going on there. There must be some insight.
There ought to be some physical reason for these new variables.
Now, that is where I plan to start with.
My plan for the lecture is first to work out,
in some detail, a specific example where
decoupling is done to show how that leads to the solution.
And then we will go back and see how to do it in general --
-- because you will see, as I do the decoupling in this
particular example, that that particular method,
though it is suggested, will not work in general.
I would need a more general method.
But let's first go to the example.
It is a slight modification of one you should have done in
recitation. I don't think I worked one of
these in the lecture, but to describe it I have to
draw two views of it to make sure you know exactly what I am
talking about. Sometimes it is called the two
compartment ice cube tray problem, a very old-fashion type
of ice cube tray. Not a modern one that is all
plastic where there is no leaking from one compartment to
another. The old kind of ice cube trays,
there were compartments and these were metal separated and
you leveled the liquid because it could leak through the bottom
that didn't go right to the bottom.
If you don't know what I am talking about it makes no
difference. This is the side view.
This is meant to be twice as long.
But, to make it quite clear, I will draw the top view of
this thing. You have to imagine this is a
rectangle, all the sides are parallel and everything.
This is one and this is two. All I am trying to say is that
the cross-sectional area of these two chambers,
this one has twice the cross-sectional area of this
one. So I will write a two here and
I will write a one there. Of course, it is this hole here
through which everything leaks. I am going to let x be the
height of this liquid, the water here,
and y the height of the water in that chamber.
Obviously, as time goes by, they both reach the same height
because of somebody's law. Now, what is the system of
differential equations that controls this?
Well, the essential thing is the flow rate through here.
That flow rate through the hole in units, let's say,
in liters per second. Just so you understand,
I am talking about the volume of liquid.
I am not talking about the velocity.
That is proportional to the area of the hole.
So the cross-sectional area of the hole.
And it is also times the velocity of the flow,
but the velocity of the flow depends upon the pressure
difference. And that pressure difference
depends upon the difference in height.
All those are various people's laws.
So times the height difference. Of course, you have to get the
sign right. I have just pointed out the
height difference is proportional to the pressure at
the hole. And it is that pressure at the
hole that determines the velocity with which the fluid
flows through. Where does this all produce our
equations? Well, x prime is equal to,
therefore, some constant, depending on the area of the
hole and this constant of proportionality with the
pressure and the units and everything else times the
pressure difference I am talking about.
Well, if fluid is going to flow in this direction that must mean
the y height is higher than the x height.
So, to make x prime positive, it should be y minus x here. Now, the y prime is different.
Because, again, the rate of fluid flow is
determined. This time, if y prime is
positive, if this is rising, as it will be in this case,
it's because the fluid is flowing in that direction.
It is because x is higher than y.
So this should be the same constant x minus y.
But notice that right-hand side is the rate at which fluid is
flowing into this tank. That is not the rate at which y
is changing. It is the rate at which 2y is
changing. Why isn't there a constant
here? There is.
It's one. That is the one,
this one cross-section. The area here is one and the
cross-sectional area here is two.
And that is the reason for the one here and the two here,
because we are interested in the rate at which fluid is being
added to this, which is only related to the
height, the rate at which the height is rising if you take
into account the cross-sectional area.
So there is the system. In order to use nothing but
integers here, I am going to take c equals to
two, so I don't have to put in halves.
The final system is x prime equals minus 2x,
you have to write them in the correct order,
and y prime equals, the twos cancel because c is
two, is x minus y. So there is our system.
Now the problem is I want to solve it by decoupling it.
I want, in other words, to find new variables,
u and v, which are more natural to the problem than the x and y
that are so natural to the problem that the new system will
just consistently be two side-by-side equations instead
of the single equation. The question is,
what should u and v be? Now, the difference between
what I am going to do now and what I am going to do later in
the period is later in the period I will give you a
systematic way of finding what u and v should be.
Now we are going to psyche out what they should be in the way
in which people who solve systems often do.
I am going to use the fact that this is not just an abstract
system of equations. It comes from some physical
problem. And I ask, is there some system
of variables, which somehow go more deeply
into the structure of what's going on here than the nave
variables, which simply tell me how high the two tank levels
are? That is the obvious thing I can
see, but there are some variables that go more deeply.
Now, one of them is sort of obvious and suggested both the
form of the equation and by this.
Simply, the difference in heights is, in some ways,
a more natural variable because that is directly related to the
pressure difference, which is directly related to
the velocity of flow. They will differ by just
constant. I am going to call that the
second variable, or the difference in height
let's call it. That's x minus y.
That is a very natural variable for the problem.
The question is, what should the other one be?
Now you sort of stare at that for a while until it occurs to
you that something is constant. What is constant in this
problem? Well, the tank is sitting
there, that is constant. But what thing,
which might be a variable, clearly must be a constant?
It will be the total amount of water in the two tanks.
These things vary, but the total amount of water
stays the same because it is a homogenous problem.
No water is coming in from the outside, and none is leaving the
tanks through a little hole. Okay.
What is the expression for the total amount of water in the
tanks? x plus 2y.
Therefore, that is a natural variable also.
It is independent of this one. It is not a simple multiply of
it. It is a really different
variable. This variable represents the
total amount of liquid in the two tanks.
This represents the pressure up to a constant factor.
It is proportional to the pressure at the hole.
Okay. Now what I am going to do is
say this is my change of variable.
Now let's plug in and see what happens to the system when I
plug in these two variables. And how do I do that?
Well, I want to substitute and get the new system.
The new system, or rather the old system,
but what makes it new is in terms of u and v.
What will that be? Well, u prime is x prime plus
2y prime. But I know what x prime plus 2y
prime is because I can calculate it for this.
What will it be? x prime plus 2y prime is
negative 2x plus twice y prime, so it's plus 2x,
which is zero. And how about these two?
2y minus twice this, because I want this plus twice
that, so it 2y minus 2y, again, zero.
The right-hand side becomes zero after I calculate x prime
plus 2y. So that is zero.
That would just, of course, clear.
Now, that makes sense, of course.
Since the total amount is constant, that says that u prime
is zero. Okay.
What is v prime? v prime is x prime minus y
prime. What is that?
Well, once again we have to calculate.
x prime minus y prime is minus 2x minus x, which is minus 3x,
and 2y minus negative y, which makes plus 3y.
All right. What is the system?
The system is u prime equals zero and v prime
equals minus three times x minus y. But x minus y is v. In other words, these new two variables
decouple the system. And we got them,
as scientists often do, by physical considerations.
These variables go more deeply into what is going on in that
system of two tanks than simply the two heights,
which are too obvious as variables.
All right. What is the solution?
Well, the solution is, u equals a constant and v is
equal to? Well, the solution to this
equation is a different arbitrary constant from that
one. These are side-by-side
equations that have nothing whatever to do with each other,
remember? Times e to the minus 3t. Now, there are two options.
Either one leaves the solution in terms of those new variables,
saying they are more natural to the problem, but sometimes,
of course, one wants the answer in terms of the old one.
But, if you do that, then you have to solve that.
In order to save a little time, since this is purely linear
algebra, I am going to write -- Instead of taking two minutes
to actually do the calculation in front of you,
I will just write down what the answer is -- -- in terms of u and x and y.
In other words, this is a perfectly good way to
leave the answer if you are allowed to do it.
But if somebody says they want the answer in terms of x and y,
well, you have to give them what they are paying for.
In terms of x and y, you have first to solve those
equations backwards for x and y in terms of u and v in which
case you will get x equals one-third of u plus 2v. Use the inverse matrix or just
do elimination, whatever you usually like to
do. And the other one will be
one-third of u minus v. And then, if you substitute in, you will see what you will get
is one-third of c1. Sorry.
u is c1. c1 plus 2 c2 e to the negative
3t. And this is one-third of c1
minus c2 e to the minus 3t. And so, the final solution is, in terms of the way we usually
write out the answer, x will be what?
Well, it will be one-third c1 times the eigenvector one,
one plus one-third times c2 times the eigenvector two,
negative one times e to the minus 3t.
That is the solution written out in terms of x and y either
as a vector in the usual way or separately in terms of x and y.
But, notice, in order to do that you have to
have these backwards equations. In other words,
I need the equations in that form.
I need the equations because they tell me what the new
variables are. But I also have to have the
equations the other way in order to get the solution in terms of
x and y, finally. Okay.
That was all an example. For the rest of the period,
I would like to show you the general method of doing the same
thing which does not depend upon being clever about the choice of
the new variables. And then, at the very end of
the period, I will apply the general method to this problem
to see whether we get the same answer or not.
What is the general method? Our problem is the decouple.
Now, the first thing is you cannot always decouple.
To decouple the eigenvalues must all be real and
non-defective. In other words,
if they are repeated they must be complete.
You must have enough independent eigenvectors.
So they must be real and complete.
If repeated, they must be complete.
They must not be defective. As I told you at the time when
we studied complete and incomplete, the most common case
in which this occurs is when the matrix is symmetric.
If the matrix is real and symmetric then you can always
decouple the system. That is a very important
theorem, particularly since many of the equilibrium problems
normally lead to symmetric matrices and are solved by
decoupling. Okay.
So what are we looking for? We are assuming this and we
need it. In general, otherwise,
you cannot decouple if you have complex eigenvalues and you
cannot decouple if you have defective eigenvalues. Well, what are we looking for?
We are looking for new variables.
u, v equals a1, b1, a2, b2 times the x,
y. And this matrix is called D,
the decoupling matrix and is what we are looking for.
How do I choose those new variables u and v when I don't
have any physical considerations to guide me as I did before?
Now, the key is to look instead at what you are going to need.
Remember, we are changing variables.
And, as I told you from the first days of the term,
when you change variables look at what you are going to need to
substitute in to make the change of variables.
Don't just start writing equations.
What we are going to need to plug into that system and change
it to the (u, v) coordinates is not u and v
in terms of x and y. What we need is x and y in
terms of u and v to do the substitution.
What we need is the inverse of this.
So, in order to do the substitution,
what we need is (x, y).
Oops. Let's call them prime.
Let's call these a1, b1, a2, b2 because these are
going to be much more important to the problem than the other
ones. Okay.
I am going to, I should call this matrix D
inverse, that would be a sensible thing to call it.
Since this is the important matrix, this is the one we are
going to need to do the substitution,
I am going to give it another letter instead.
And the letter that comes after D is E.
Now, E is an excellent choice because it is also the first
letter of the word eigenvector. And the point is the matrix E,
which is going to work, is the matrix whose columns are
the two eigenvectors. The columns are the two
eigenvectors. Now, even if you didn't know
anything that would be practically the only reasonable
choice anybody could make. What are we looking for?
To make a linear change of variables like this really means
to pick new i and j vectors. You know, from the first days
of 18.02, what you want is a new coordinate system in the plane.
And the coordinate system in the plane is determined as soon
as you tell what the new i is and what the new j is in the new
system. To establish a linear change of
coordinates amounts to picking two new vectors that are going
to play the role of i and j instead of the old i and the old
j. Okay, so pick two vectors which
somehow are important to this matrix.
Well, there are only two, the eigenvectors.
What else could they possibly be? Now, what is the relation? I say with this,
what happens is I say that alpha one corresponds,
and alpha two, these are vectors in the
xy-system. Well, if I change the
coordinates to u and v, in the uv-system they will
correspond to the vectors one, zero. In other words, the vector that we would
normally call i in the u, v system.
And this one will correspond to the vector zero, one. Now, if you don't believe that
I will calculate it for you. The calculation is trivial.
Look. What have we got?
(x, y) equals a1, b1, a2, b2. This is the column vector alpha
one. This is the column vector alpha
two. Now, here is u and v.
Suppose I make u and v equal to one, zero, what happens to x and
y? Your matrix multiply.
One, zero. So a1 plus zero,
b1 plus zero. It corresponds to the column
vector (a1, b1). And in the same way zero,
one corresponds to (a2, b2). Just by matrix multiplication.
And that shows that these correspond.
In the uv-system the two eigenvectors are now called i
and j. Well, that looks very
promising, but the program now is to do the substitution to
substitute into the system x prime equals Ax and
see if it is decoupled in the uv-coordinates.
Now, I don't dare let you do this by yourself because you
will run into trouble. Nothing is going to happen.
You will just get a mess and will say I must be missing
something. And that is because you are
missing something. What you are missing,
and this is a good occasion to tell you, is that,
in general, three-quarters of the civilized world does not
introduce eigenvalues and eigenvectors the way you learn
them in 18.03. They use a different definition
that is identical. I mean it is equivalent.
The concept is the same, but it looks a little
different. Our definition is what?
Well, what is an eigenvalue and eigenvector?
The basic thing is this equation. This is a two-by-two matrix,
right? This is a column vector with
two entries. The product has to be a column
vector with two entries, but both entries are supposed
to be zero so I will write it this way.
This way first defines what an eigenvalue is.
It is something that makes the determinant zero.
And then it defines what an eigenvector is.
It is, then, a solution to the system that
you can get because the determinant is zero.
Now, that is not what most people do.
What most people do is the following.
They write this equation differently by having something
on both sides. Using the distributive law,
what goes on the left side is A alpha one.
What is that? That is a column vector with
two entries. What goes on the right?
Well, lambda one times the identity times alpha one.
Now, the identity matrix times anything just reproduces what
was there. There is no difference between
writing the identity times alpha one and just alpha one all by
itself. So that is what I am going to
do. This is the definition of
eigenvalue and eigenvector that all the other people use.
Most linear algebra books use this definition,
or most books use a different approach and say,
here is an eigenvalue and an eigenvector.
And it requires them to define them in the opposite order.
First what alpha one is and then what lambda one is.
See, I don't have any determinant now.
So what is the definition? And they like it because it has
a certain geometric flavor that this one lacks entirely.
This is good for solving differential equations,
which is why we are using it in 18.03, but this has a certain
geometric content. This way thinks of A as a
linear transformation of the plane, a shearing of the plane.
You take the plane and do something to it.
Or, you squish it like that. Or, you rotate it.
That's okay, too.
And the matrix defines a linear transformation to the plane,
every vector goes to another vector.
The question it asks is, is there a vector which is
taken by this linear transformation and just left
alone or stretched, is kept in the same direction
but stretched? Or, maybe its direction is
reversed and it is stretched or it shrunk.
But, in general, if there are real eigenvalues
there will be such vectors that are just left in the same
direction but just stretched or shrunk.
And what is the lambda? The lambda then is the amount
by which they are stretched or shrunk, the factor.
This way, first we have to find the vector, which is left
essentially unchanged, and then the number here that
goes with it is the stretching factor or the shrinking factor.
But the end result is the pair alpha one and lambda one,
regardless of which order you find them, satisfied the same
equation. Now, a consequence of this
definition we are going to need in the calculation that I am
going to do in just a moment. Let me calculate that out.
What I want to do is calculate the matrix A times E.
I am going to need to calculate that.
Now, what is that? Remember, E is the matrix whose
columns are the eigenvectors. That is the matrix alpha one,
alpha two. Now, what is this?
Well, in both Friday's lecture and Monday's lecture,
I used the fact that if you do a multiplication like that it is
the same thing as doing the multiplication A alpha one and
putting it in the first column. And then A alpha two is the
column vector that goes in the second column.
But what is this? This is lambda one alpha one.
And this is lambda two alpha two by this other definition of
eigenvalue and eigenvector. And what is this?
Can I write this in terms of matrices?
Yes indeed I can. This is the matrix alpha one,
alpha two times this matrix lambda one, lambda two,
zero, zero. Check it out.
Lambda one plus zero, lambda one times this thing
plus zero, the first entry is exactly that.
And the same way the second column doing the same
calculation is exactly this. What is that?
That is e times this matrix lambda one, zero,
zero, lambda two. Okay.
We are almost finished now. Now we can carry out our work.
We are going to do the substitution.
I start with a system. Remember where we are.
I am starting with this system. I am going to make the
substitution x equal to this matrix E, whose columns are the
eigenvectors. I am in introducing,
in other words, new variables u and v according
to that thing. u is the column vector,
u and v. And x, as usual, is the column vector x and y.
So I am going to plug it in. Okay.
Let's plug it in. What do I get?
I take the derivative. E is a constant matrix so that
makes E times u prime, is equal to A times,
x is E times u again. Now, at this point, you would be stuck,
except I calculated for you A times E is E times that funny
diagonal matrix with the lambdas.
So this is E times that funny matrix of the lambdas,
the eigenvalues, and still the u at the end of
it. So where are we?
E times u prime equals E times this thing.
Well, multiply both sides by E inverse and you can cancel them
out. And so the end result is that
after you have made the substitution in terms of the new
variables u, what you get is u prime equals lambda one,
lambda two, zero, zero times u.
Let's write that out in terms of a system.
This is u prime is equal to, well, this is u,
v here. It is lambda one times u plus
zero times v. And the other one is v prime
equals zero times u plus lambda two times v.
We are decoupled. In just one sentence you would
say -- In other words,
if you were reading a book that sort of assumed you knew what
was going on, all it would say is as usual.
That is to make you feel bad. Or, as is well-known to make
you feel even worse. Or, the system is decoupled by
choosing as the new basis for the system the eigenvectors of
the matrix and in terms of the resulting new coordinates,
the decoupled system will be the following where the
constants are the eigenvalues. And so the solution will be u
equals c1 times e to the lambda1 t and v
is equal to c2 times e to the lambda2 t. Of course, if you want it back
in terms now of x and y, you will have to go back to
here, to these equations and then plug in for u and v what
they are. And then you will get the
answer in terms of x and y. Okay.
We have just enough time to actually carry out this little
program. It takes a lot longer to derive
than it does actually to do, so let's do it for this system
that we were talking about before.
Decouple the system, x, y prime equals the matrixes
negative two, two, one, negative one. Okay.
What do I do? Well, I first have to calculate
the eigenvalues in the eigenvectors,
so the Ev's and Ev's. The characteristic equation is
lambda squared. The trace is negative three,
but you have to change the sign.
The determinant is two minus two, so that is zero.
There is no constant term here. It is zero.
That is the characteristic equation.
The roots are obviously lambda equals zero, lambda equals
negative three. And what are the eigenvectors
that go with that? With lambda equals zero goes
the eigenvector, minus two.
Well, I subtract zero here, so the equation I have to solve
is minus 2 a1 plus -- I am not going to write 2 a2,
which is what you have been writing up until now.
The reason is because I ran into trouble with the notation
and I had to use, as the eigenvector,
not a1, a2 but a1, b1.
So it should be a b1 here, not the a2 that you are used
to. The solution,
therefore, is alpha one equals one, one.
And for lambda equals negative three, the corresponding
eigenvector this time will be -- Now I have to subtract negative
three from here, so negative two minus negative
three makes one. That is a1 plus 2 b1 equals
zero, a logical choice for the
eigenvector here. The second eigenvector would be
make b1 equal to one, let's say, and then a1 will be
negative two. Okay.
Now what do we have to do? Now, what we want is the matrix
E. The matrix E is the matrix of
eigenvectors, so it is the matrix one,
one, negative two, one. The next thing we want is what
the new variables u and v are. For that, we will need E
inverse. How do you calculate the
inverse of a two-by-two matrix? You switch the two diagonal
elements, there I have switched them, and you leave the other
two where they are but change their sign.
So it is two up here and negative one there.
Maybe I should make this one purple and then that one purple
to indicate that I have switched them.
I am not done yet. I have to divide by the
determinant. What is the determinant?
It is one minus negative two, which is three,
so I have to divide by three. I multiply everything here by
one-third. Okay.
And what is the decoupled system?
The new variables are u equals one-third. In other words,
the new variables are given by D.
It is u, v equals one, two, negative one, one
times one-third times x,y. That is the expression for u, v in terms of x and y.
It's this matrix D, the decoupling matrix which is
the one that is used. And that gives this system u
equals one-third of x plus 2y on top.
And what is the v entry? v is one-third of minus x plus
y. Now, are those the same
variables that I used before? Yes.
This is my new and better you, the one I got by just blindly
following the method instead of looking for physical things with
physical meaning. It differs from the old one
just by a constant factor. Now, that doesn't have any
effect on the resulting equation because if the old one is u
prime equals zero the new one is one-third u prime equals zero.
It is still the same equation, in other words.
And how about this one? This one differs from the other
one by the factor minus one-third.
If I multiply that v through by minus one-third,
I get this v. And, therefore,
that too does not affect the second equation.
I simply multiply both sides by minus one-third.
The new v still satisfies the equation minus three times v.

Probability

OK, here's the last lecture in
the chapter on orthogonality. So we met orthogonal
vectors, two vectors, we met orthogonal subspaces,
like the row space and null space. Now today we meet
an orthogonal basis, and an orthogonal matrix. So we really -- this chapter cleans
up orthogonality. And really I want -- I should use the
word orthonormal. Orthogonal is -- so my
vectors are q1,q2 up to qn -- I use the letter "q",
here, to remind me, I'm talking about orthogonal
things, not just any vectors, but orthogonal ones. So what does that mean? That means that every q is
orthogonal to every other q. It's a natural idea,
to have a basis that's headed off at
ninety-degree angles, the inner products are all zero. Of course if q is -- certainly
qi is not orthogonal to itself. But there we'll make
the best choice again, make it a unit vector. Then qi transpose qi is
one, for a unit vector. The length squared is one. And that's what I would
use the word normal. So for this part, normalized,
unit length for this part. OK. So first part of
the lecture is how does having an orthonormal
basis make things nice? It certainly does. It makes all the
calculations better, a whole lot of
numerical linear algebra is built around working
with orthonormal vectors, because they never
get out of hand, they never overflow
or underflow. And I'll put them
into a matrix Q, and then the second
part of the lecture will be suppose my
basis, my columns of A are not orthonormal. How do I make them so? And the two names associated
with that simple idea are Graham and Schmidt. So the first part is we've
got a basis like this. Let's put those into
the columns of a matrix. So a matrix Q that has -- I'll put these
orthonormal vectors, q1 will be the first column,
qn will be the n-th column. And I want to say, I want
to write this property, qi transpose qj
being zero, I want to put that in a matrix form. And just the right thing is
to look at Q transpose Q. So this chapter has been
looking at A transpose A. So it's natural to
look at Q transpose Q. And the beauty is it
comes out perfectly. Because Q transpose has
these vectors in its rows, the first row is q1 transpose,
the nth row is qn transpose. So that's Q transpose. And now I want to multiply by Q. That has q1 along to
qn in the columns. That's Q. And what do I get? You really -- this is the
first simplest most basic fact, that how do orthonormal
vectors, orthonormal columns in a matrix, what happens
if I compute Q transpose Q? Do you see it? If I take the first row
times the first column, what do I get? A one. If I take the first row
times the second column, what do I get? Zero. That's the orthogonality. The first row times the
last column is zero. And so I'm getting
ones on the diagonal and I'm getting zeroes
everywhere else. I'm getting the identity matrix. You see how that's -- it's
just like the right calculation to do. If you have orthonormal
columns, and the matrix doesn't have to be square here. We might have just two columns. And they might have
four, lots of components. So but they're orthonormal, and
when we do Q transpose times Q, that Q transpose times
Q or A transpose A just asks for all
those dot products. Rows times columns. And in this orthonormal case,
we get the best possible answer, the identity. OK, so this is -- so I mean now we have a new
bunch of important matrices. What have we seen previously? We've seen in the
distant past we had triangular matrices,
diagonal matrices, permutation matrices, that was
early chapters, then we had row echelon
forms, then in this chapter we've already seen
projection matrices, and now we're seeing this
new class of matrices with orthonormal columns. That's a very long expression. I sorry that I can't just
call them orthogonal matrices. But that word
orthogonal matrices -- or maybe I should be able to
call it orthonormal matrices, why don't we call
it orthonormal -- I mean that would be an
absolutely perfect name. For Q, call it an
orthonormal matrix because its columns
are orthonormal. OK, but the convention is
that we only use that name orthogonal matrix,
we only use this -- this word orthogonal,
we don't even say orthonormal for
some unknown reason, matrix when it's square. So in the case when this is a
square matrix, that's the case we call it an orthogonal matrix. And what's special about
the case when it's square? When it's a square matrix,
we've got its inverse, so -- so in the case if Q is square,
then Q transpose Q equals I tells us -- let me write that underneath -- tells us that Q
transpose is Q inverse. There we have the
easy to remember property for a square matrix
with orthonormal columns. That -- I need to write
some examples down. Let's see. Some examples like if I
take any -- so examples, let's do some examples. Any permutation matrix,
let me take just some random permutation matrix. Permutation Q equals let's say
oh, make it three by three, say zero, zero, one, one,
zero, zero, zero, one, zero. OK. That certainly has unit
vectors in its columns. Those vectors are certainly
perpendicular to each other. And if I -- and so that's it. That makes it a Q. And -- if I took its transpose,
if I multiplied by Q transpose, shall I do that -- and let
me stick in Q transpose here. Just to do that
multiplication once more, transpose it'll put the -- make that into a column,
make that into a column, make that into a column. And the transpose is also -- another Q. Another orthonormal matrix. And when I multiply that
product I get I. OK, so there's an example. And actually there's
a second example. But those are real
easy examples, right, I mean to get orthogonal
columns by just putting ones in different
places is like too easy. So let me keep
going with examples. So here's another
simple example. Cos theta sine theta,
there's a unit vector, oh, let me even
take it, well, yeah. Cos theta sine theta
and now the other way I want sine theta cos theta. But I want the inner
product to be zero. And if I put a minus
there, it'll do it. So that's -- unit vector,
that's a unit vector. And if I take the dot product,
I get minus plus zero. OK. For example Q equals say
one, one, one, minus one, is that an orthogonal matrix? I've got orthogonal
columns there, but it's not quite
an orthogonal matrix. How shall I fix it to
be an orthogonal matrix? Well, what's the length
of those column vectors, the dot product with themselves
is -- right now it's two, right, the -- the length squared. The length squared would be
one plus one would be two, the length would be
square root of two, so I better divide by
square root of two. OK. So there's a -- there now I
have got an orthogonal matrix, in fact, it's this one --
when theta is pi over four. The cosines and
well almost, I guess the minus sine is down
there, so maybe, I don't know, maybe minus
pi over four or something. OK. Let me do one
final example, just to show that you
can get bigger ones. Q equals let me take that
matrix up in the corner and I'll sort of
repeat that pattern, repeat it again, and
then minus it down here. That's one of the world's
favorite orthogonal matrices. I hope I got it right, is -- can you see whether -- if I take the inner product of
one column with another one, let's see, if I take
the inner product of that column with that I have
two minuses and two pluses, that's good. When I take the inner
product of that with that I have a plus and a
minus, a minus and a plus. Good. I think it all works out. And what do I have
to divide by now? To make those into unit vectors. Right now the vector one,
one, one, one has length two. Square root of four. So I have to divide by two
to make it unit vector, so there's another. That's my entire array
of simple examples. This construction is named after
a guy called Adhemar and we know how to do it for
two, four, sixteen, sixty-four and so on, but we --
nobody knows exactly which size matrices have -- which size -- which sizes allow
orthogonal matrices of ones and minus ones. So Adhemar matrix is an
orthogonal matrix that's got ones and minus ones, and a
lot of ones -- some we know, some other sizes, there couldn't
be a five by five I think. But there are some
sizes that nobody yet knows whether there could be
or can't be a matrix like that. OK. You see those
orthogonal matrices. Now let me ask what -- why
is it good to have orthogonal matrices? What calculation is made easy? If I have an orthogonal matrix. And -- let me remember that the
matrix could be rectangular. Shall I put down -- I better put a
rectangular example down. So the -- these were
all square examples. Can I put down just -- a rectangular one
just to be sure that we realize that
this is possible. let's help me out. Let's see, if I put like a
one, two, two and a minus two, minus one, two. That's -- a matrix -- oh its
columns aren't normalized yet. I always have to
remember to do that. I always do that last
because it's easy to do. What's the length
of those columns? So if I wanted them -- if I
wanted them to be length one, I should divide by their
length, which is -- so I'd look at one squared plus
two squared plus two squared, that's one and four
and four is nine, so I take the square root and
I need to divide by three. OK. So there is -- well, without that, I've
got one orthonormal vector. I mean just one unit vector. Now put that guy in. Now I have a basis
for the column space for a two-dimensional
space, an orthonormal basis, right? These two columns
are orthonormal, they would be an
orthonormal basis for this two-dimensional
space that they span. Orthonormal vectors by the way
have got to be independent. It's easy to show that
orthonormal vectors since they're headed off
all at ninety degrees there's no combination
that gives zero. Now if I wanted to
create now a third one, I could either just put in
some third vector that was independent and go to this
Graham-Schmidt calculation that I'm going to explain, or I
could be inspired and say look, that -- with that pattern,
why not put a one in there, and a two in there,
and a two in there, and try to fix up the
signs so that they worked. Hmm. I don't know if I've done
this too brilliantly. Let's see, what
signs, that's minus, maybe I'd make a minus sign
there, how would that be? Yeah, maybe that works. I think that those three columns
are orthonormal and they -- the beauty of this -- this is
the last example I'll probably find where there's no
square root, the -- the punishing thing
in Graham-Schmidt, maybe we better know
that in advance, is that because I want these
vectors to be unit vectors, I'm always running
into square roots. I'm always dividing by lengths. And those lengths
are square roots. So you'll see as soon as I
do a Graham-Schmidt example, square roots are
going to show up. But here are some
examples where we did it without any square root. OK. OK. So -- so great. Now next question is what's
the good of having a Q? What formulas become easier? Suppose I want to
project, so suppose Q -- suppose Q has
orthonormal columns. I'm using the letter
Q to mean this, I'll write it this
one more time, but I always mean
when I write a Q, I always mean that it
has orthonormal columns. So suppose I want to project
onto its column space. So what's the projection matrix? What's the projection matrix is
I project onto a column space? OK, that gives me a chance to
review the projection section, including that big formula,
which used to be -- those four As in a
row, but now it's got Qs, because I'm projecting
onto the column space of Q, so do you remember what it was? It's Q Q transpose Q
inverse Q transpose. That's my four Qs in a row. But what's good here? What -- what makes this formula
nice if I'm projecting onto a column space when I have
orthonormal basis for that space? What makes it nice is
this is the identity. I don't have to
do any inversion. I just get Q Q transpose. So Q Q transpose is
a projection matrix. Oh, I can't help -- I can't resist just
checking the properties, what are the properties
of a projection matrix? There are two properties to
know for any projection matrix. And I'm saying that this
is the right projection matrix when we've got
this orthonormal basis in the columns. OK. So there's the
projection matrix. Suppose the matrix is square. First just tell me
first this extreme case. If my matrix is square and it's
got these orthonormal columns, then what's the column space? If I have a square matrix and
I have independent columns, and even orthonormal columns,
then the column space is the whole space, right? And what's the projection
matrix onto the whole space? The identity matrix. If I'm projecting
in the whole space, every vector B is right
where it's supposed to be and I don't have to
move it by projection. So this would be -- I'll put in parentheses
this is I if Q is square. Well that we said that already. If Q is square, that's the case
where Q transpose is Q inverse, we can put it on the right,
we can put it on the left, we always get the identity
matrix, if it's square. But if it's not a square
matrix then it's not -- we don't get the
identity matrix. We have Q Q transpose,
and just again what are those two properties
of a projection matrix? First of all, it's symmetric. OK, no problem, that's
certainly a symmetric So what's that second property
of a projection? matrix. That if you project and project
again you don't move the second time. So the other property
of a projection matrix should be that Q
Q transpose twice should be the same as
Q Q transpose once. That's projection matrices. And that property
better fall out right away because
from the fact we know about orthonormal matrices,
Q transpose Q is I. OK, you see it. In the middle here is sitting
Q Q t- Q transpose Q, sorry, that's what I meant to
say, Q transpose Q is I. So that's sitting right in
the middle, that cancels out, to give the identity, we're
left with one Q Q transpose, and we're all set. OK. So this is the
projection matrix -- all the equation -- all the
messy equations of this chapter become trivial
when our matrix -- when we have this
orthonormal basis. I mean what do I mean
by all the equations? Well, the most
important equation was the normal equation, do
you remember old A transpose A x hat equals A transpose b? But now -- now A is Q. Now I'm thinking I have
Q transpose Q X hat equals Q transpose b. And what's good about that? What's good is that matrix on
the left side is the identity. The matrix on the left is
the identity, Q transpose Q, normally it isn't, normally it's
that matrix of inner products and you've to compute all those
dopey inner products and -- and -- and solve the system. Here the inner products
are all one or zero. This is the identity matrix. It's gone. And there's the answer. There's no inversion involved. Each component of
x is a Q times b. What that equation is saying
is that the i-th component is the i-th basis vector times b. That's -- probably the most
important formula in some major parts of mathematics, that
if we have orthonormal basis, then the component in the --
in the i-th, along the i-th -- the projection on the i-th basis
vector is just qi transpose b. That number x that we look
for is just a dot product. OK. OK, so I'm ready now for
the sort of like second half of the lecture. Where we don't start with
an orthogonal matrix, orthonormal vectors. We just start with
independent vectors and we want to make
them orthonormal. So I'm going to --
can I do that now? Now here comes Graham-Schmidt. So -- Graham-Schmidt. So this is a calculation,
I won't say -- I can't quite say it's like
elimination, because it's different, our goal
isn't triangular anymore. With elimination our goal was
make the matrix triangular. Now our goal is make
the matrix orthogonal. Make those columns orthonormal. So let me start
with two columns. So I start with vectors a and b. And they're just like --
here, let me draw them. Here's a. Here's b. For example. A isn't specially
horizontal, wasn't meant to be, just a is
one vector, b is another. I want to produce
those two vectors, they might be in
twelve-dimensional space, or they might be in
two-dimensional space. They're independent, anyway. So I better be sure I say that. I start with
independent vectors. And I want to produce
out of that q 1 and q2, I want to produce
orthonormal vectors. And Graham and
Schmidt tell me how. OK. Well, actually you could tell me
how, we don't need -- frankly, I don't know -- there's
only one idea here, if Graham had the idea, I
don't know what Schmidt did. But OK. So you'll see it. We don't need either
of them, actually. OK, so what I going to do. I'll take that --
this first guy. OK. Well, he's fine. That direction is fine except -- yeah, I'll say OK, I'll
settle for that direction. So I'm going to -- I'm going to get, so
what I going to -- my goal is I'm going to
get orthogonal vectors and I'll call those
capital A and B. So that's the key step is
to get from any two vectors to two orthogonal vectors. And then at the end, no problem,
I'll get orthonormal vectors, how will -- what will those
will be my qs, q1 and q2, and what will they be? Once I've got A and B
orthogonal, well, look, it's no big deal -- maybe
that's what Schmidt did, he, brilliant Schmidt, thought
OK, divide by the length, all right. That's Schmidt's contribution. OK. But Graham had a little
more thinking to do, right? We haven't done Graham's part. This part except OK,
I'm happy with A, A can be A. That first
direction is fine. Why should -- no
complaint about that. The trouble is the second
direction is not fine. Because it's not
orthogonal to the first. I'm looking for a vector
that's -- starts with B, but makes it orthogonal to A. What's the vector? How do I do that? How do I produce
from this vector a piece that's
orthogonal to this one? And the -- remember these
vectors might be in two dimensions or they might
be in twelve dimensions. I'm just looking for the idea. So what's the idea? Where did we have orthogonal -- a vector showing up that
was orthogonal to this guy? Well, that was the
first basic calculation of the whole chapter. We -- we did a projection and
the projection gave us this part, which was the
part in the A direction. Now, the part we want is
the other part, the e part. This part. This is going to be our -- that guy is that guy. This is our vector B. That gives
us that ninety-degree angle. So B is you could say -- B is really what we
previously called e. The error vector. And what is it? I mean what do I
-- what is B here? A is A, no problem. B is -- OK, what's this error piece? Do you remember? It's I start with the original
B and I take away what? Its projection, this P.
This -- the vector B, this error vector, is the
original vector removing the projection. So instead of wanting
the projection, now that's what I
want to throw away. I want to get the part
that's perpendicular. And there will be a
perpendicular part, it won't be zero. Because these vectors
were independent, so B -- if B was along the
direction of A, then if the original B and A
were in the same direction, then I'm -- I've only got one direction. But here they're in two
independent directions and all I'm doing
is getting that guy. So what's its formula? What's the formula
for that if -- I want to subtract
the projection, so do you remember
the projection? It's some multiple of A
and what's that multiple? It's -- it's that thing we
called x in the very very first lecture on this chapter. There's an A transpose
A in the bottom and there's an A transpose
B, isn't that it? I think that's Graham's formula. Or Graham-Schmidt. No, that's Graham. Schmidt has got to divide the
whole thing by the length, so he -- his formula makes a mess which
I'm not willing to write down. So let's just see that
what I saying here? I'm saying that this vector
is perpendicular to A. That these are orthogonal. A is perpendicular to B. Can you check that? How do you see that
yes, of course, we -- our picture is telling
us, yes, we did it right. How would I check that this
matrix is perpendicular to A? I would multiply by A transpose
and I better get zero, right? I should check that. A transpose B should
come out zero. So this is A transpose times
-- now what did we say B was? We start with the original
B, and we take away this projection, and that
should come out zero. Well, here we get an
A transpose B minus -- and here is another A
transpose B, and the -- and it's an A transpose A
over A transpose A, a one, those cancel, and
we do get zero. Right. Now I guess I can
do numbers in there. But I have to take
a third vector to be sure we've got
this system down. So now I have to say if I have
independent vectors A, B and C, I'm looking for orthogonal
vectors A, B and capital C, and then of course the
third guy will just be C over its length,
the unit vector. So this is now the problem. I got B here. I got A very easily. And now -- if you see the idea,
we could figure out a formula for C. So now that -- so this
is like a typical homework quiz problem. I give you two vectors, you do
this, I give you three vectors, and you have to make
them orthonormal. So you do this again,
the first vector's fine, the second vector is
perpendicular to the first, and now I need a
third vector that's perpendicular to the first
one and the second one. Right? Tthis is the end of a -- the
lecture is to find this guy. Find this vector -- this vector
C, that's perpendicular we n- at this point we know A and B. But C, the little c that
we're given, is off in some -- it's got to come out of the
blackboard to be independent, so -- so can I sort of draw
off -- off comes a c somewhere. I don't know, where I
going to put the darn thing? Maybe I'll put it
off, oh, I don't know, like that somehow, C, little c. And I already know that
perpendicular direction, that one and that one. So now what's the idea? Give me the Graham-Schmidt
formula for C. What is this C here? Equals what? What I going to do? I'll start with the given one. As before. Right? I start with the
vector I'm given. And what do I do with it? I want to remove out of
it, I want to subtract off, so I'll put a minus sign
in, I want to subtract off its components in the A, capital
A and capital B directions. I just want to get
those out of there. Well, I know how to do that. I did it with B. So I'll just -- so
let me take away -- what if I do this? What have I done? I've got little c and what
have I subtracted from it? Its component, its projection
if you like, in the A direction. And now I've got to subtract
off its component B transpose C over B transpose B,
that multiple of B, is its component
in the B direction. And that gives me the vector
capital C that if anything is -- if there's any justice, this
C should be perpendicular to A and it should be
perpendicular to B. And the only thing it's --
hasn't got is unit vector, so we divide by its
length to get that too. OK. Let me do an example. Can I -- I'll make my life easy,
I'll just take two vectors. So let me do a
numerical example. If I'll give you
two vectors, you give me back the Graham-Schmidt
orthonormal basis, and we'll see how to
express that in matrix form. OK. So let me give you
the two vectors. So I'll take the vector A
equals let's say one, one, one, why not? And B equals let's say
one, zero, two, OK? I didn't want to cheat
and make them orthogonal in the first place because
then Graham-Schmidt wouldn't be needed. OK. So those are not orthogonal. So what is capital A? Well that's the same as big A. That was fine. What's B? So B is this b --
is the original B, and then I subtract off
some multiple of the A. And what's the multiple? What goes in here? B -- here's the A -- this is
the -- this is the little b, this is the big A, also
the little a, and I want to multiply it by that
right -- that right ratio, which has A transpose
A, here's my ratio. I'm just doing this. So it's A transpose B,
what is A transpose B, it looks like three. And what is A -- oh, my -- what's A transpose A? Three. I'm sorry. I didn't know that
was going to happen. OK. But it happened. Why should we knock it? OK. So do you see it all right? That's A transpose B,
there's A transpose A, that's the fraction, so
I take this away, and I get one take away one
is a zero, zero minus this one is a minus one, and two
minus the one is a one. OK. And what's this vector
that we finally found? This is B. And how do I know it's right? How do I know I've
got a vector I want? I check that B is
perpendicular to -- that A and B are perpendicular. That A is perpendicular to B. Just look at that. That one -- the dot product
of that with that is zero. OK. So now what is my q1 and q2? Why don't I put
them in a matrix? Of course. Since I'm always putting
these -- so the Q, I'll put the q1 and
the q2 in a matrix. And what are they? Now when I'm writing
q-s I'm supposed to make things normalized. I'm supposed to make
things unit vectors. So I'm going to take that A
but I'm going to divide it by square root of three. And I'm going to
take this B but I'm going to divide it
by square root of two to make it a unit vector,
and there is my matrix. That's my matrix with
orthonormal columns coming from Graham-Schmidt and
it sort of it -- it came from the original
one, one, one, one, zero, two, right? That was my original guys. These were the two
I started with. These are the two that
I'm happy to end with. Because those are orthonormal. So that's what
Graham-Schmidt did. It -- well, tell me about
the column spaces of these matrices. How is the column space of Q
related to the column space of A? So I'm always asking
you things like this, and that makes you think,
OK, the column space is all combinations of the
columns, it's that plane, right? I've got two vectors in
three-dimensional space, their column space is a plane,
the column space of this matrix is a plane, what's the
relation between the planes? Between the two column spaces? They're one and the same, right? It's the same column space. All I'm taking is here this
B thing that I computed, this B thing that I computed
is a combination of B and A, and A was little A, so
I'm always working here with this in the same space. I'm just like getting
ninety-degree angles in there. Where my original column space
had a perfectly good basis, but it wasn't as
good as this basis, because it wasn't orthonormal. Now this one is orthonormal,
and I have a basis then that -- so now projections, all the
calculations I would ever want to do are -- are a cinch
with this orthonormal basis. One final point. One final point in this chapter. And it's -- just
like elimination. We learned how to
do elimination, we know all the
steps, we can do it. But then I came back to it and
said look at it as a matrix in matrix language and
elimination gave me -- what was elimination
in matrix language? I'll just put it up there. A was LU. That was matrix,
that was elimination. Now, I want to do the
same for Graham-Schmidt. Everybody who works
in linear algebra isn't going to write
out the columns are orthogonal, or orthonormal. And isn't going to write
out these formulas. They're going to write out the
connection between the matrix A and the matrix Q. And the two matrices have
the same column space, but there's some -- some
matrix is taking the -- and I'm going to call it R, so
A equals QR is the magic formula here. It's the expression
of Graham-Schmidt. And I'll -- let me
just capture that. So that's the -- my final
step then is A equal QR. Maybe I can squeeze it in here. So A has columns,
let's say a1 and a2. Let me suppose n is
two, just two vectors. OK. So that's some
combination of q1 and q2. And times some matrix R. They have the same column space. This is just -- this matrix just
includes in it whatever these numbers like three over three
and one over square root of three and one over
square root of two, probably that's what it's got. One over square root of three,
one over square root of two, something there, but actually
it's got a zero there. So the main point about
this A equal QR is this R turns out to be
upper triangular. It turns out that this
zero is upper triangular. We could see why. Let me see, I can put in
general formulas for what these This I think in here should
be the inner product of a1 with q1. are. And this one should be the -- the inner product of a1 with q2. And that's what I
believe is zero. This will be something here,
and this will be something here with inner -- a1 transpose q2,
sorry a2 transpose q1 and a2 transpose q2. But why is that guy zero? Why is a1 q2 zero? That's the key to this being
-- this R here being upper triangular. You know why a1q2 is
zero, because a1 -- that was my -- this was really a and b here. This was really a and b. So this is a transpose q2. And the whole point of
Graham-Schmidt was that we constructed these later q-s to
be perpendicular to the earlier vectors, to the earlier --
all the earlier vectors. So that's why we get
a triangular matrix. The -- result is
extremely satisfactory. That if I have a matrix
with independent columns, the Graham-Schmidt
produces a matrix with orthonormal columns, and
the connection between those is a triangular matrix. That last point, that the
connection is a triangular matrix, please look
in the book, you have to see that one more time. OK. Thanks, that's great.

Linear Algebra

The following content is
provided under a Creative Commons license. Your support will help
MIT OpenCourseWare continue to offer high-quality
educational resources for free. To make a donation or to
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at fsae@mit.edu. PATRICK WINSTON: It was in
2010, yes, that's right. It was in 2010. We were having our
annual discussion about what we would dump fro
6034 in order to make room for some other stuff. And we almost killed
off neural nets. That might seem strange
because our heads are stuffed with neurons. If you open up your skull
and pluck them all out, you don't think anymore. So it would seem
that neural nets would be a fundamental
and unassailable topic. But many of us felt that
the neural models of the day weren't much in the way
of faithful models of what actually goes on
inside our heads. And besides that,
nobody had ever made a neural net that was
worth a darn for doing anything. So we almost killed it off. But then we said,
well, everybody would feel cheated
if they take a course in artificial intelligence,
don't learn anything about neural nets,
and then they'll go off and invent
them themselves. And they'll waste
all sorts of time. So we kept the subject in. Then two years
later, Jeff Hinton from the University of
Toronto stunned the world with some neural network
he had done on recognizing and classifying pictures. And he published
a paper from which I am now going to show
you a couple of examples. Jeff's neural net, by the
way, had 60 million parameters in it. And its purpose was to determine
which of 1,000 categories best characterized a picture. So there it is. There's a sample of things
that the Toronto neural net was able to recognize
or make mistakes on. I'm going to blow
that up a little bit. I think I'm going
to look particularly at the example labeled
container ship. So what you see here is that
the program returned its best estimate of what it was
ranked, first five, according to the likelihood,
probability, or the certainty that it felt that a
particular class was characteristic of the picture. And so you can see this
one is extremely confident that it's a container ship. It also was fairly
moved by the idea that it might be a lifeboat. Now, I'm not sure about you,
but I don't think this looks much like a lifeboat. But it does look like
a container ship. So if I look at only the best
choice, it looks pretty good. Here are the other things
they did pretty well, got the right answer
is the first choice-- is this first choice. So over on the left,
you see that it's decided that the picture
is a picture of a mite. The mite is not anywhere near
the center of the picture, but somehow it managed to find
it-- the container ship again. There is a motor scooter, a
couple of people sitting on it. But it correctly characterized
the picture as a motor scooter. And then on the
right, a Leopard. And everything else
is a cat of some sort. So it seems to be
doing pretty well. In fact, it does do pretty well. But anyone who does
this kind of work has an obligation
to show you some of the stuff that
doesn't work so well on or doesn't get quite right. And so these pictures also
occurred in Hinton's paper. So the first one is
characterized as a grill. But the right answer was
supposed to be convertible. Oh, no, yes, yeah, right
answer was convertible. In the second case,
the characterization is of a mushroom. And the alleged right
answer is agaric. Is that pronounced right? It turns out that's a kind of
mushroom-- so no problem there. In the next case, it
said it was a cherry. But it was supposed
to be a dalmatian. Now, I think a dalmatian is
a perfectly legitimate answer for that particular picture--
so hard to fault it for that. And the last case,
the correct answer was not in any of the top five. I'm not sure if you've
ever seen a Madagascar cap. But that's a picture of one. And it's interesting
to compare that with the first choice of the
program, the squirrel monkey. This is the two side by side. So in a way, it's not
surprising that it thought that the
Madagascar cat was a picture of a squirrel
monkey-- so pretty impressive. It blew away the competition. It did so much better the
second place wasn't even close. And for the first time, it
demonstrated that a neural net could actually do something. And since that time, in the
three years since that time, there's been an enormous
amount of effort put into neural net technology,
which some say is the answer. So what we're going to
do today and tomorrow is have a look at this stuff
and ask ourselves why it works, when it might not work,
what needs to be done, what has been done, and all
those kinds of questions will emerge. So I guess the first thing to
do is think about what it is that we are being inspired by. We're being inspired
by those things that are inside our head-- all
10 to the 11th of them. And so if we take one of those
10 to the 11th and look at it, you know from 700 something
or other approximately what a neuron looks like. And by the way, I'm going
to teach you in this lecture how to answer questions
about neurobiology with an 80% probability that
you will give the same answer as a neurobiologist. So let's go. So here's a neuron. It's got a cell body. And there is a nucleus. And then out here is
a long thingamajigger which divides maybe a
little bit, but not much. And we call that the axon. So then over here, we've got
this much more branching type of structure that looks
maybe a little bit like so. Maybe like that-- and this
stuff branches a whole lot. And that part is called
the dendritic tree. Now, there are a
couple of things we can note about this is that
these guys are connected axon to dendrite. So over here, they'll be
a so-called pre-synaptic thickening. And over here will be some
other neuron's dendrite. And likewise, over here
some other neuron's axon is coming in here and hitting
the dendrite of our the one that occupies most
of our picture. So if there is enough
stimulation from this side in the axonal tree,
or the dendritic tree, then a spike will
go down that axon. It acts like a
transmission line. And then after that
happens, the neuron will go quiet for a while as
it's recovering its strength. That's called the
refractory period. Now, if we look at that
connection in a little more detail, this little piece right
here sort of looks like this. Here's the axon coming in. It's got a whole bunch
of little vesicles in it. And then there's a
dendrite over here. And when the axon is stimulated,
it dumps all these vesicles into this inner synaptic space. For a long time, it wasn't
known whether those things were actually separated. I think it was
Raamon and Cahal who demonstrated that one
neuron is actually not part of the next one. They're actually separated
by these synaptic gaps. So there it is. How can we model,
that sort of thing? Well, here's what's
usually done. Here's what is done in
the neural net literature. First of all, we've got
some kind of binary input, because these things either
fire or they don't fire. So it's an all-or-none
kind of situation. So over here, we have
some kind of input value. We'll call it x1. And is either a 0 or 1. So it comes in here. And then it gets multiplied
times some kind of weight. We'll call it w1. So this part here is modeling
this synaptic connection. It may be more or less strong. And if it's more strong,
this weight goes up. And if it's less strong,
this weight goes down. So that reflects the
influence of the synapse on whether or not the whole
axon decides it's stimulated. Then we got other inputs down
here-- x sub n, also 0 or 1. It's also multiplied
by a weight. We'll call that w sub n. And now, we have to
somehow represent the way in which these inputs
are collected together-- how they have collective force. And we're going to
model that very, very simply just by saying, OK,
we'll run it through a summer like so. But then we have to decide if
the collective influence of all those inputs is sufficient
to make the neuron fire. So we're going to
do that by running this guy through a
threshold box like so. Here is what the box looks like
in terms of the relationship between input and the output. And what you can see
here is that nothing happens until the input
exceeds some threshold t. If that happens, then
the output z is a 1. Otherwise, it's a 0. So binary, binary out-- we
model the synaptic weights by these multipliers. We model the cumulative effect
of all that input to the neuron by a summer. We decide if it's going to be
an all-or-none 1 by running it through this threshold
box and seeing if the sum of the products add
up to more than the threshold. If so, we get a 1. So what, in the end,
are we in fact modeling? Well, with this model,
we have number 1, all or none-- number 2, cumulative
influence-- number 3, oh, I, suppose synaptic weight. But that's not all
that there might be to model in a real neuron. We might want to deal with
the refractory period. In these biological models that
we build neural nets out of, we might want to model
axonal bifurcation. We do get some division
in the axon of the neuron. And it turns out that that
pulse will either go down one branch or the other. And which branch it
goes down depends on electrical activity in
the vicinity of the division. So these things might actually
be a fantastic coincidence detectors. But we're not modeling that. We don't know how it works. So axonal bifurcation
might be modeled. We might also have a
look at time patterns. See, what we don't
know is we don't know if the timing of the
arrival of these pulses in the dendritic
tree has anything to do with what that neuron
is going to recognize-- so a lot of unknowns here. And now, I'm going
to show you how to answer a question
about neurobiology with 80% probability
you'll get it right. Just say, we don't know. And that will be with
80% probability what the neurobiologist would say. So this is a model inspired
by what goes on in our heads. But it's far from clear
if what we're modeling is the essence of why those guys
make possible what we can do. Nevertheless, that's where
we're going to start. That's where we're going to go. So we've got this model
of what a neuron does. So what about what does a
collection of these neurons do? Well, we can think of your skull
as a big box full of neurons. Maybe a better way
to think of this is that your head
is full of neurons. And they in turn are full of
weights and thresholds like so. So into this box come a variety
of inputs x1 through xm. And these find their
way to the inside of this gaggle of neurons. And out here come a bunch
of outputs c1 through zn. And there a whole bunch
of these maybe like so. And there are a lot
of inputs like so. And somehow these inputs
through the influence of the weights of the thresholds
come out as a set of outputs. So we can write
that down a little fancier by just saying
that z is a vector, which is a function of, certainly
the input vector, but also the weight vector and
the threshold vector. So that's all a neural net is. And when we train
a neural net, all we're going to be able to
do is adjust those weights and thresholds so that what
we get out is what we want. So a neural net is a
function approximator. It's good to think about that. It's a function approximator. So maybe we've got some sample
data that gives us an output vector that's desired as
another function of the input, forgetting about what the
weights and the thresholds are. That's what we want to get out. And so how well we're
doing can be figured out by comparing the desired
value with the actual value. So we might think
then that we can get a handle on how well
we're doing by constructing some performance function, which
is determined by the desired vector and the input
vector-- sorry, the desired vector and
the actual output vector for some particular input
or for some set of inputs. And the question is what
should that function be? How should we
measure performance given that we have
what we want out here and what we actually
got out here? Well, one simple
thing to do is just to measure the magnitude
of the difference. That makes sense. But of course, that would give
us a performance function that is a function of the
distance between those vectors would look like this. But this turns out
to be mathematically inconvenient in the end. So how do you think we're going
to turn it up a little bit? AUDIENCE: Normalize it? PATRICK WINSTON: What's that? AUDIENCE: Normalize it? PATRICK WINSTON:
Well, I don't know. How about just we square it? And that way we're going to go
from this little sharp point down there to something
that looks more like that. So it's best when the
difference is 0, of course. And it gets worse as
you move away from 0. But what we're
trying to do here is we're trying to get
to a minimum value. And I hope you'll forgive me. I just don't like
the direction we're going here, because I like to
think in terms of improvement as going uphill
instead of down hill. So I'm going to dress this up
one more step-- put a minus sign out there. And then our performance
function looks like this. It's always negative. And the best value it
can possibly be is zero. So that's what we're going to
use just because I am who I am. And it doesn't matter, right? Still, you're trying to
either minimize or maximize some performance function. OK, so what do we got to do? I guess what we could do is we
could treat this thing-- well, we already know what to do. I'm not even sure why we're
devoting our lecture to this, because it's clear that
what we're trying to do is we're trying to take our
weights and our thresholds and adjust them so as
to maximize performance. So we can make a
little contour map here with a simple neural net
with just two weights in it. And maybe it looks like
this-- contour map. And at any given time
we've got a particular w1 and particular w2. And we're trying to
find a better w1 and w2. So here we are right now. And there's the contour map. And it's a 6034. So what do we do? AUDIENCE: Climb. PATRICK WINSTON: Simple matter
of hill climbing, right? So we'll take a step
in every direction. If we take a step in that
direction, not so hot. That actually goes pretty bad. These two are really ugly. Ah, but that one--
that one takes us up the hill a little bit. So we're done,
except that I just mentioned that
Hinton's neural net had 60 million parameters in it. So we're not going to hill
climb with 60 million parameters because it explodes
exponentially in the number of
weights you've got to deal with-- the number
of steps you can take. So this approach is
computationally intractable. Fortunately, you've all taken
1801 or the equivalent thereof. So you have a better idea. Instead of just taking a
step in every direction, what we're going to do is
we're going to take some partial derivatives. And we're going to
see what they suggest to us in terms of how we're
going to get around in space. So we might have a partial
of that performance function up there with respect to w1. And we might also take
a partial derivative of that guy with respect to w2. And these will tell us
how much improvement we're getting by making a little
movement in those directions, right? How much a change is
given that we're just going right along the axis. So maybe what we ought
to do is if this guy is much bigger than
this guy, it would suggest we mostly want to
move in this direction, or to put it in 1801
terms, what we're going to do is we're going
to follow the gradient. And so the change
in the w vector is going to equal to this
partial derivative times i plus this partial
derivative times j. So what we're going to end up
doing in this particular case by following that formula is
moving off in that direction right up to the steepest
part of the hill. And how much we
move is a question. So let's just have a rate
constant R that decides how big our step is going to be. And now you think we were done. Well, too bad for our side. We're not done. There's a reason
why we can't use-- create ascent, or in the case
that I've drawn our gradient, descent if we take the
performance function the other way. Why can't we use it? AUDIENCE: Local maxima. PATRICK WINSTON: The
remark is local maxima. And that is certainly true. But it's not our first obstacle. Why doesn't gradient
ascent work? AUDIENCE: So you're
using a step function. PATRICK WINSTON: Ah,
there's something wrong with our function. That's right. It's non-linear, but
rather, it's discontinuous. So gradient ascent requires
a continuous space, continuous surface. So too bad our side. It isn't. So what to do? Well, nobody knew what
to do for 25 years. People were screwing around
with training neural nets for 25 years before Paul
Werbos sadly at Harvard in 1974 gave us the answer. And now I want to tell
you what the answer is. The first part of the answer is
those thresholds are annoying. They're just extra
baggage to deal with. What we really like instead of
c being a function of xw and t was we'd like c prime
to be a function f prime of x and the weights. But we've got to account
for the threshold somehow. So here's how you do that. What you do is
you say let us add another input to this neuron. And it's going to
have a weight w0. And it's going to be
connected to an input that's always minus 1. You with me so far? Now what we're
going to do is we're going to say, let w0 equal t. What does that do to the
movement of the threshold? What it does is it
takes that threshold and moves it back to 0. So this little trick here
takes this pink threshold and redoes it so that the new
threshold box looks like this. Think about it. If this is t, and this is
minus 1, then this is minus t. And so this thing ought to
fire if everything's over-- if the sum is over 0. So it makes sense. And it gets rid of the
threshold thing for us. So now we can just
think about weights. But still, we've got
that step function there. And that's not good. So what we're going
to do is we're going to smooth that guy out. So this is trick number two. Instead of a step
function, we're going to have this
thing we lovingly call a sigmoid
function, because it's kind of from an s-type shape. And the function we're going
to use is this one-- one, well, better make it a little
bit different-- 1 over 1 plus e to the minus
whatever the input is. Let's call the input alpha. Does that makes sense? Is alpha is 0, then it's 1
over 1 plus 1 plus one half. If alpha is extremely big,
then even the minus alpha is extremely small. And it becomes one. It goes up to an asymptotic
value of one here. On the other hand, if alpha
is extremely negative, than the minus alpha
is extremely positive. And it goes to 0 asymptotically. So we got the right
look to that function. It's a very convenient function. Did God say that neurons
ought to be-- that threshold ought to work like that? No, God didn't say so. Who said so? The math says so. It has the right shape
and look and the math. And it turns out to
have the right math, as you'll see in a moment. So let's see. Where are we? We decided that
what we'd like to do is take these
partial derivatives. We know that it was awkward
to have those thresholds. So we got rid of them. And we noted that it was
impossible to have the step function. So we got rid of it. Now, we're a situation
where we can actually take those partial derivatives,
and see if it gives us a way of training
the neural net so as to bring the actual output into
alignment with what we desire. So to deal with
that, we're going to have to work with the
world's simplest neural net. Now, if we've got one
neuron, it's not a net. But if we've got two-word
neurons, we've got a net. And it turns out that's the
world's simplest neuron. So we're going to look at it--
not 60 million parameters, but just a few, actually,
just two parameters. So let's draw it out. We've got input x. That goes into a multiplier. And it gets multiplied times w1. And that goes into a
sigmoid box like so. We'll call this p1, by the
way, product number one. Out here comes y. Y gets multiplied
times another weight. We'll call that w2. The neck produces another
product which we'll call p2. And that goes into
a sigmoid box. And then that comes out as z. And z is the number
that we use to determine how well we're doing. And our performance
function p is going to be one
half minus one half, because I like
things are going in a direction, times the
difference between the desired output and the actual
output squared. So now let's decide what
those partial derivatives are going to be. Let me do it over here. So what are we
trying to compute? Partial of the performance
function p with respect to w2. OK. Well, let's see. We're trying to figure
out how much this wiggles when we wiggle that. But you know it goes
through this variable p2. And so maybe what we
could do is figure out how much this wiggles--
how much z wiggles when we wiggle p2
and then how much p2 wiggles when we wiggle w2. I just multiplied
those together. I forget. What's that called? N180-- something or other. AUDIENCE: The chain rule PATRICK WINSTON: The chain rule. So what we're going
to do is we're going to rewrite that partial
derivative using chain rule. And all it's doing is
saying that there's an intermediate variable. And we can compute how much
that end wiggles with respect how much that end
wiggles by multiplying how much the other guys wiggle. Let me write it down. It makes more sense
in mathematics. So that's going to be
able to the partial of p with respect to z times the
partial of z with respect to p2. Keep me on track here. Partial of z with respect to w2. Now, I'm going to do something
for which I will hate myself. I'm going to erase
something on the board. I don't like to do that. But you know what I'm
going to do, don't you? I'm going to say this is
true by the chain rule. But look, I can
take this guy here and screw around with it
with the chain rule too. And in fact, what
I'm going to do is I'm going to replace
that with partial of z with respect to p2 and partial
of p2 with respect to w2. So I didn't erase it after all. But you can see what
I'm going to do next. Now, I'm going to
do same thing with the other partial derivative. But this time, instead of
writing down and writing over, I'm just going to expand it
all out in one go, I think. So partial of p
with respect to w1 is equal to the partial
of p with respect to z, the partial of z with respect
to p2, the partial of p2 with respect to what? Y? Partial of y with respect
to p1-- partial of p1 with respect to w1. So that's going like a zipper
down that string of variables expanding each by
using the chain rule until we got to the end. So there are some
expressions that provide those partial derivatives. But now, if you'll
forgive me, it was convenient to write
them out that way. That matched the
intuition in my head. But I'm just going
to turn them around. It's just a product. I'm just going to
turn them around. So partial p2, partial
w2, times partial of z, partial p2, times the
partial of p with respect to z-- same thing. And now, this one. Keep me on track, because
if there's a mutation here, it will be fatal. Partial of p1-- partial
of w1, partial of y, partial p1, partial of p2,
partial of y, partial of z. There's a partial of p2,
partial of a performance function with respect to z. Now, all we have to do is figure
out what those partials are. And we have solved
this simple neural net. So it's going to be easy. Where is my board space? Let's see, partial of p2
with respect to-- what? That's the product. The partial of z-- the
performance function with respect to z. Oh, now I can see why I
wrote it down this way. Let's see. It's going to be d minus e. We can do that one in our head. What about the partial
of p2 with respect to w2. Well, p2 is equal to y
times w2, so that's easy. That's just y. Now, all we have to do
is figure out the partial of z with respect to p2. Oh, crap, it's going
through this threshold box. So I don't know exactly what
that partial derivative is. So we'll have to
figure that out, right? Because the function relating
them is this guy here. And so we have to figure out
the partial of that with respect to alpha. All right, so we got to do it. There's no way around it. So we have to destroy something. OK, we're going to
destroy our neuron. So the function
we're dealing with is, we'll call it
beta, equal to 1 over 1 plus e to the minus alpha. And what we want
is the derivative with respect to alpha of beta. And that's equal to d by
d alpha of-- you know, I can never remember
those quotient formulas. So I am going to rewrite
it a little different way. I am going to write it as 1
minus e to the minus alpha to the minus 1, because I
can't remember the formula for differentiating a quotient. OK, so let's differentiate it. So that's equal to 1 minus e to
the minus alpha to the minus 2. And we got that minus comes
out of that part of it. Then we got to differentiate
the inside of that expression. And when we differentiate the
inside of that expression, we get e to the minus alpha. AUDIENCE: Dr. Winston-- PATRICK WINSTON: Yeah? AUDIENCE: That should be 1 plus. PATRICK WINSTON: Oh,
sorry, thank you. That was one of those fatal
mistakes you just prevented. So that's 1 plus. That's 1 plus here too. OK, so we've
differentiated that. We've turned that
into a minus 2. We brought the
minus sign outside. Then we're differentiating
the inside. The derivative and the
exponential is an exponential. Then we got to
differentiate that guy. And that just helps us
get rid of the minus sign we introduced. So that's the derivative. I'm not sure how much
that helps except that I'm going to perform a parlor
trick here and rewrite that expression thusly. We want to say
that's going to be e to the minus alpha over
1 plus e to the minus alpha times 1 over 1 plus
e to the minus alpha. That OK? I've got a lot of
nodding heads here. So I think I'm on safe ground. But now, I'm going to
perform another parlor trick. I am going to add 1, which
means I also have to subtract 1. All right? That's legitimate isn't it? So now, I can rewrite
this as 1 plus e to the minus alpha over 1
plus e to the minus alpha minus 1 over 1 plus e to the
minus alpha times 1 over 1 plus e to the minus alpha. Any high school
kid could do that. I think I'm on safe ground. Oh, wait, this is beta. This is beta. AUDIENCE: That's the wrong side. PATRICK WINSTON: Oh,
sorry, wrong side. Better make this
beta and this 1. Any high school kid could do it. OK, so what we've
got then is that this is equal to 1 minus
beta times beta. That's the derivative. And that's weird
because the derivative of the output with
respect to the input is given exclusively
in terms of the output. It's strange. It doesn't really matter. But it's a curiosity. And what we get out of this is
that partial derivative there-- that's equal to well,
the output is p2. No, the output is z. So it's z time 1 minus e. So whenever we see
the derivative of one of these sigmoids with
respect to its input, we can just write the output
times one minus alpha, and we've got it. So that's why it's
mathematically convenient. It's mathematically
convenient because when we do this differentiation, we
get a very simple expression in terms of the output. We get a very simple expression. That's all we really need. So would you like to
see a demonstration? It's a demonstration of
the world's smallest neural net in action. Where is neural nets? Here we go. So there's our neural net. And what we're
going to do is we're going to train it to
do absolutely nothing. What we're going to do is
train it to make the output the same as the input. Not what I'd call a fantastic
leap of intelligence. But let's see what happens. Wow! Nothing's happening. Well, it finally
got to the point where the maximum error,
not the performance, but the maximum error
went below a threshold that I had previously
determined. So if you look at the
input here and compare that with the desired output
on the far right, you see it produces an output,
which compared with the desired output, is pretty close. So we can test the
other way like so. And we can see that
the desired output is pretty close to the actual
output in that case too. And it took 694 iterations
to get that done. Let's try it again. To 823-- of course, this is all
a consequence of just starting off with random weights. By the way, if you started with
all the weights being the same, what would happen? Nothing because it would
always stay the same. So you've got to put
some randomization in in the beginning. So it took a long time. Maybe the problem is our
rate constant is too small. So let's crank up the
rate counts a little bit and see what happens. That was pretty fast. Let's see if it was a
consequence of random chance. Run. No, it's pretty fast there--
57 iterations-- third try-- 67. So it looks like at my initial
rate constant was too small. So if 0.5 was not
as good as 5.0, why don't we crank it up
to 50 and see what happens. Oh, in this case, 124--
let's try it again. Ah, in this case 117-- so
it's actually gotten worse. And not only has
it gotten worse. You'll see there's a little a
bit of instability showing up as it courses along its
way toward a solution. So what it looks like is that
if you've got a rate constant that's too small,
it takes forever. If you've get a rate
constant that's too big, it can of jump too far, as in
my diagram which is somewhere underneath the board, you can
go all the way across the hill and get to the other side. So you have to be careful
about the rate constant. So what you really
want to do is you want your rate constant
to vary with what is happening as you progress
toward an optimal performance. So if your performance is going
down when you make the jump, you know you've got a rate
constant that's too big. If your performance is going
up when you make a jump, maybe you want to
increase-- bump it up a little bit until it
doesn't look so good. So is that all there is to it? Well, not quite, because
this is the world's simplest neural net. And maybe we ought to
look at the world's second simplest neural net. Now, let's call this--
well, let's call this x. What we're going to do is we're
going to have a second input. And I don't know. Maybe this is screwy. I'm just going to
use color coding here to differentiate between
the two inputs and the stuff they go through. Maybe I'll call this z2 and
this z1 and this x1 and x2. Now, if I do that-- if I've
got two inputs and two outputs, then my performance
function is going to have two numbers in it-- the
two desired values and the two actual values. And I'm going to
have two inputs. But it's the same stuff. I just repeat what I did in
white, only I make it orange. Oh, but what happens if--
what happens if I do this? Say put little cross
connections in there. So these two streams
are going to interact. And then there might
be some-- this y can go into another multiplier
here and go into a summer here. And likewise, this
y can go up here and into a multiplier like so. And there are weights all
over the place like so. This guy goes up in here. And now what happens? Now, we've got a
disaster on our hands, because there are all kinds
of paths through this network. And you can imagine that if this
was not just two neurons deep, but three neurons
deep, what I would find is expressions that
look like that. But you could go this way,
and then down through, and out here. Or you could go this way and
then back up through here. So it looks like there is an
exponentially growing number of paths through that network. And so we're back to
an exponential blowup. And it won't work. Yeah, it won't
work except that we need to let the math
sing to us a little bit. And we need to look
at the picture. And the reason I turned
this guy around was actually because from a point of view
of letting the math sing to us, this piece here is the
same as this piece here. So part of what we
needed to do to calculate the partial derivative
with respect to w1 has already been done
when we calculated the partial derivative
with respect to w2. And not only that,
if we calculated the partial wit respect
to these green w's at both levels, what
we would discover is that sort of repetition
occurs over and over again. And now, I'm going to try
to give you an intuitive idea of what's going on here
rather than just write down the math and salute it. And here's a way to think
about it from an intuitive point of view. Whatever happens to this
performance function that's back of these p's
here, the stuff over there can influence p only
by going through, and influence performance
only going through this column of p's. And there's a fixed
number of those. So it depends on the width,
not the depth of the network. So the influence of that
stuff back there on p is going to end up going
through these guys. And it's going to end
up being so that we're going to discover that a lot of
what we need to compute in one column has already been computed
in the column on the right. So it isn't going to
explode exponentially, because the influence-- let
me say it one more time. The influences of changes of
changes in p on the performance is all we care about when
we come back to this part of the network, because
this stuff cannot influence the performance except by going
through this column of p's. So it's not going to
blow up exponentially. We're going to be able to
reuse a lot of the computation. So it's the reuse principle. Have we ever seen the reuse
principle at work before. Not exactly. But you remember
that little business about the extended list? We know that we've
seen-- we know we've seen something before. So we can stop computing. It's like that. We're going to be able
to reuse the computation. We've already done it to
prevent an exponential blowup. By the way, for those of
you who know about fast Fourier transform-- same
kind of idea-- reuse of partial results. So in the end, what can
we say about this stuff? In the end, what we can say
is that it's linear in depth. That is to say if we
increase the number of layers to so-called depth,
then we're going to increase the
amount of computation necessary in a linear way,
because the computation we need in any column
is going to be fixed. What about how it goes
with respect to the width? Well, with respect to
the width, any neuron here can be connected to
any neuron in the next row. So the amount of work
we're going to have to do will be proportional to
the number of connections. So with respect to width,
it's going to be w-squared. But the fact is that in the end,
this stuff is readily computed. And this, phenomenally enough,
was overlooked for 25 years. So what is it in the end? In the end, it's an
extremely simple idea. All great ideas are simple. How come there
aren't more of them? Well, because frequently,
that simplicity involves finding
a couple of tricks and making a couple
of observations. So usually, we humans
are hardly ever go beyond one trick
or one observation. But if you cascade
a few together, sometimes something
miraculous falls out that looks in retrospect
extremely simple. So that's why we got the
reuse principle at work-- and our reuse computation. In this case, the
miracle was a consequence of two tricks plus
an observation. And the overall idea
is all great ideas are simple and easy to
overlook for a quarter century.

Diff. Eq.

[MUSIC] Stanford University. It's getting real today. So, let's talk about a little
bit of the overview today. So, we'll really get you into
the background for classification. And then, we'll do some interesting things
with updating these word vectors that we so far have learned in
an unsupervised way. We'll update them with some real
supervision signals such as sentiment and other things. Then, we'll look at the first real
model that is actually useful and you might wanna use in practice. Well, other than, of course, the word
vectors, but one sort of downstream task which is window classification and we'll
really also clear up some of the confusion around the cross entropy error and
how it connects with the softmax. And then, we'll introduce the famous
neural network, our most basic LEGO block that we may start to call deep to get
to the actual title of this class. Deep learning in NLP.
And then, we'll actually introduce another loss
function, the max margin loss and take our first steps into
the direction of backprop. So, this lecture will be,
I think very helpful for problem set one. We'll go into a lot of the math
that you'll need probably for number two in the problem set. So, I hope it'll be very useful and
I'm excited for you cuz at the end of this lecture, you'll feel hopefully a lot
better about the magic of deep learning. All right, are there any
organizational questions around problem sets or
programming sessions with the TAs? No, we're all good? Awesome, thanks to the TAs for
clearing up everything. Cool, so let's be very careful about
our notation today because that is one of the main things that
a lot of people trip up over as we go through very complex
chain-rules and so on. So, let's start at the beginning and
say, all right, we have usually a training dataset
of some input X and some output Y. X could be in the simplest case, words
in isolation, just a single word vector. It's not something you would
usually do in practice. But it'll be easy for
us to learn that way. So we'll start with that but then,
we'll move to context windows today. And then eventually, we'll use the same
basic building blocks that we introduce today for sentences and documents and
then complex interactions for everything. Now, the output in the simplest
case it's just a single label. It's just a positive or
a negative kind of sentence. It could be the named entities of
certain words in their context. It can also be other words, so
in machine translation, for instance, you might wanna output eventually
a sequence of other words as our yi and we'll get to that in a couple weeks. And, yeah, basically they have multiword
sequences as potential outputs. All right, so what is the intuition for
classification? In the standard machine learning case,
so not yet the deep learning world, we usually just, for
something as simple logistic regression, basically want to define and learn
a simple decision boundary where we say everything to the left of this or
in one direction is in one class and the other one,
all the other things in the other class. And so, in general machine learning,
we assume our inputs, the Xs are kinda fixed,
they're just set and we'll only train the W parameter,
which is our softmax weights. So, we'll compute the probability of Y,
given the input X with this kind of input. And so, one notational comment here is for the whole dataset,
we often subscript with i but then, when I drop the i we're just looking
at a single example of x and y. Eventually, we're going to overload
at the subscript a little bit and look at the indices of certain vector so,
if you get confused, just raise your hand and ask. I'll try to make it clear
which one is which. Now, let's dive into the softmax. We mentioned it before but we wanna really
carefully define and recall the notation here cuz we'll go and take derivatives
with respect to all of these parameters. So, we can tease apart two steps here for
computing this probability of y given x. The first thing is, we'll take the y'th
row of W and multiply that row with x. And so again this notation here,
when we have Wy. And that means we'll have,
we're taking the y'th row of this matrix. And then, multiplying it here with x. Now if we do that multiple times for
all c from one to our classes. So let's say, this is 1, 2, 3,
the 4th row and multiply each of these. So then we get four numbers here. And these are unnormalized scores. And then, we'll basically,
pipe this vector through the softmax to compute a probability
distribution that sums to one. All right, that's our step one. Any questions around that? Cuz it's just gonna keep
on going from here. All right, great. And, I get that sometimes
in general from previous sort of surveys, it seems to be that
15% of the class are usually bored when we go through all of these,
like all of these derivatives. 15% are super overwhelmed and then the
majority of people are like, okay, it's a good speed, I'm learning something, I'm
getting it, and you're making progress. So, sorry for the 30% for
whom this is too slow or too fast. You can probably just skim
through the lecture slides or speed it up if you're watching online. If you're super familiar with taking
super complex derivatives and if it's a little overwhelming, then
definitely come to all the office hours. We have an awesome set of
TAs who will help you. All right, now we,
let's look at a single example of an x and y that we wanna predict. In general, we want our model to
essentially maximize the probability of the correct class. We wanted to output the right class at the
end by taking the argmax of that output. And maximizing probability is the same
as maximizing log probability, it's the same as minimizing the negative
of that log probability and that is often our objective function. So, why do we call this
the cross-entropy error? Well, we can define the cross-entropy
in the abstract in general as follows. So let's assume we have
the ground truth or gold or target probability distribution,
we use those three terms interchangeably. Basically, what the ideal target
in our training dataset, the y and we'll assume that, that is one at
the right class and zero everywhere else. So if we have for instance, five
classes here and it's the center class. Its the third class and this would be one
and all the other, numbers would be zero. So, if we define this as p
in our computed probability, that our softmax outputs as q then we
would define here the cross-entropy is basically this sum
over all the classes. And in our case, p here is just
one-hot vector that's really only 1 in one location and 0 everywhere else. So, all these other terms
are basically gone. And we end up with just log of q and that's exactly the log of what
our softmax outputs, all right? And then, there are some nice connections
to Kullback-Leibler divergence and so on. I used to talk about it but
we don't have that much time today. So and you can also if you're
familiar of this in stats, you can see this as trying to minimize the
Kullback-Leibler divergence between these two distributions. But really, this is all you need to
know for the purpose of this class. So this is for
one element of your training data set. Now, of course, in general,
you have lots of training examples. So we have our overall objective
function we often denote with J, over all our parameters theta. And we basically sum these negative log
probabilities of the correct classes that we index here, a sub-index with yi. And basically we want to
minimize this whole sum. So that's our cross-entropy error
that we're trying to minimize, and we'll take lots of derivatives off in
a lot of the next couple of hours. All right, any questions so far? So this is the general ML case where
we assume our inputs here are fixed. Yes, it's a single number. So we are not multiplying a vector here,
so p(c) is the probability for that class, so that's one single number. Great question. So the cross entropy, a single number,
our main objective that we're trying to minimize, or
our error that we're trying to minimize. Now, whenever you write
this F subscript Y here, we don't want to forget that F is really
also a function of X, our inputs, right? It's sort of an intermediate step and
it's very important for us to play around with this notation. So we can also rewrite this as W y,
that row, times x, and
we can write out that whole sum. And that can often be helpful as you are
trying to take derivatives of one element at a time to eventually see the bigger
picture of the whole matrix notation. All right, so often we'll write f here
in terms of this matrix notation. So this is our f, this is our W,
and this is our x. So just standard matrix
multiplication with a vector. All right, now most of the time we'll
just talk about this first part of the objective function but
it's a bit of a simplification because in all your real applications you will
also have this regularization term here. As part of your overall
objective function. And in many cases,
this theta here for instance, if it's the W matrix of our
standard logistic regression, we'll essentially just try this
part of the objective function. We'll try to encourage the model to keep
all the weights as small as possible and as close as possible to zero. You can kind of assume if you want as
a Bayesian that you can have a prior, a Gaussian distributed prior that says
ideally all these are small numbers. Often times if you don't have
this regularization term your numbers will blow up and
it will start to overfit more and more. And in fact, this kind of plot is
something that you will very often see in your projects and
even in the problem sets. And when I took my very first statistical
learning class, the professor said, this is the number one plot to remember. So, I don't know if it's that important,
but it is very, very important for all our applications. And it's basically a pretty abstract plot. You can think of the x-axis as
a variety of different things. For instance, how powerful your model is. How many deep layers you'll have or
how many parameters you'll have. Or how many dimensions
each word vector has. Or how long you trained a model for. You'll see the same kind of pattern
with a lot of different, x-axis and then the y-axis here is
essentially your error. Or your objective function that you're
trying to optimize and minimize. And what you often observe is,
the more powerful your model gets, the better you are on
lowering your training error, the better you can fit these x-i,
y-i pairs. But at some point you'll actually start
to over-fit, and then your test error, or your validation or
development set error, will go up again. We'll go into a little bit more details
on how to avoid all of that throughout this course and
in the project advice and so on. But this is a pretty fundamental thing and
just keep in mind that for a lot of the implementations, and your projects you
will want this regularization parameter. But really it's the same one for
almost all the objective functions so we're going to chop it and mostly
focus on actually fitting our dataset. All right,
any questions around regularization? So basically, you can think of
this in terms of if you really care about one specific number,
then you can adjust all your parameters such that it will exactly
go to those different points. And if you force it to not do that,
it will kind of be a little smoother. And be less likely to fit
exactly those points and hence often generalize slightly better. And we'll go through a couple of examples
of what this will look like soon. All right, now as I mentioned
in general machine learning, we'll only optimize the W here,
the parameters of our Softmax classifier. And hence our updates and
gradients will only be pretty small, so in many cases we only have you
know a handful of classes and maybe our word vectors are hundred so if
we have three classes and 100 dimensional word vectors we're trying to classify,
we'd only have 300 parameters. Now, in deep learning,
we have these amazing word vectors. And we actually will want to
learn not just the Softmax but also the word vectors. We can back propagate into them and
we'll talk about how to do that today. Hint, it's going to be taking derivatives. But the problem is when we update
word vectors, conceptually as you are thinking through this, you
have to realize this is very, very large. And now all of the sudden have a very
large set of parameters, right? Let's say your word vectors
are 300 dimensional you have, you know 10,000 words in your vocabulary. All of the sudden you have an immensely
large set of parameters so on this kind of plot you're going
to be very likely to overfit. And so before we dive into all this
optimization, I want you to get a little bit of an intuition of what
it means to update word vectors. So let's go through a very simple example where we might want to
classify single words. Again, it's not something
we'll do very often, but let's say you want to classify single
words as positive or negative. And let's say in our training data set we
have the word TV and telly and say you know this is movie reviews and if you
say this movie is better suited for TV. It's not a very positive thing to say
about a movie that's just coming out into movie theaters. And so we would assume that
in the beginning telly, TV, and television are actually all
close by in the vector space. We learn something with word2vec or
glove vectors and we train these word vectors on a very, very large corpus and
it learned all these three words appear often in a similar context, so
they are close by in the vector space. And now we're going to train but,
our smaller sentiment data set only includes in the training set, the X-i
Y-i as TV and telly and not television. So now what happens as we
train these word vectors? Well, they will start to move around. We'll project sentiment into them and
so you now might see telly and TV, that's a British dataset, so like to
move somewhere else into the vector space. But television actually stays
where it was in the beginning. And now when we want to test it, we would actually now misclassify this
word because it's never been moved. And so what does that mean? The take home message here will be that if you have only a very
small training dataset. That will allow you especially with these
deep models to overfit very quickly, you do not want to train
your word vectors. You want to keep them fixed,
you pre-trained them with nice Glove or word2vec models on a very large corpus or you just downloaded them from the cloud
website and you want to keep them fixed, cuz otherwise you will
not generalize as well. However, if you have a very large dataset
it may be better to train them in a way we're going to describe in
the next couple of slides. So, an example for
where you do that is, for instance, machine translation where you might have
many hundreds of Megabytes or Gigabytes of training data and you don't really need to
do much with the word vectors other than initialize them randomly, and then train
them as part of your overall objective. All right, any questions around generalization
capabilities of word vectors? All right, it might still be
magical how we're training this, so that's what we're gonna describe now. So, we rarely ever really
classify single words. Really what we wanna do is
classify words in their context. And there are a lot of fun and
interesting. Issues that arise in context really
that's where language begins and grammar and
the connection to meaning and so on. So here, a couple of fun examples of
where context is really necessary. So for instance, we have some words
that actually auto-antonyms, so they mean their own opposite. So for instance to sanction can
mean to permit or to punish. And it really depends on the context for
you to understand which one is meant, or to seed can mean to place seeds or
to remove seeds. So without the context, we wouldn't really
understand the meaning of these words. And in one of the examples that you'll see
a lot, which is named entity recognition, let's say we wanna find locations or
people names, we wanna identify is this the location or
not. You may also have things like Paris, which
could be Paris in France or Paris Hilton. And you might have Paris
staying in Paris and you still wanna understand
which one is which. Or if you wanna use deep learning for
financial trading and you see Hathaway, you wanna make sure that if it's just a
positive movie review from Anne Hathaway. You're not all the sudden buying
stocks from Berkshire Hathaway, right? And so,
there are a lot of issues that are fun and interesting and
complex that arise in context. And so, let's now carefully walk
through this first useful model, which is Window classification. So, we'll use as our first motivating
example here 4-class named entity recognition, where we basically
wanna identify a person or location or organization or none of the above for
every single word in a large corpus. And there are lots of different
possibilities that exist. But we'll basically look
at the following model. Which is actually quite
a reasonable model. And also one that started in 2008. So the first beginning by Collobert and
Weston, a great paper, to do the first kind of useful state
of the art Text classification and word classification context. So, what we wanna do is basically train a
softmax classifier by assigning a label to the center word and then concatenating all
the words in a window around that word. So, let's take for example this
subphrase here from a longer sentence. We basically wanna classify
the center word here which is Paris, in the context of this window. And we'll define the window length as 2. 2 being 2 words to the left and 2 words to the right of the current center
word that we're trying to classify. All right, so what we will do
is we'll define our new x for this whole window as the concatenation
of these five word vectors. And just in general throughout all of this lecture all my
vectors are going to be column vectors. Sadly in number two of the problem set,
they're row vectors. Sorry for that. Eventually, all these programming
frameworks they're actually row-wise first and so it's faster in the low-level
optimization to use row vectors. For a lot of the math it's actually I find
it simpler to think of them as column vectors so. We're very clear in the problem set but
don't get tripped up on that. So basically, we'll define this here as
one five D dimensional column vector. So, we have T dimensional word vectors,
we have five of them and we stack them up in one column, all right. Now, the simplest window classifier that
we could think of is to now just put the softmax on top of this
concatenation of five word vectors and we'll define this, our x here. Our inputs is just the x of the entire
window for this concatenation. And we have the softmax on top of that. And so, this is the same
notation that we used before. We're introducing here y hat,
with sadly the subscript y for the correct current class. It's tough, I went through [LAUGH] several
iterations, it's tough to have like prefect notation that works
through the entire lecture always. But you'll see why soon. So, our overall objective here is,
again, this whole sum over all these probabilities that we have,
or negative log of those. So now, the question is, how do we
update these word vectors x here? One x is a window, and
x is now deep inside the softmax. All right, well, the short answer
is we'll take a lot of derivatives. But the long answer is, you're gonna have
to do that a lot in problem set one and maybe in the midterm. So, let's be a little more helpful, and
actually go through some of the steps and give you some hints. So some of this, you'll actually
have to do in your problem set, so I'm not gonna go through all the details. But I'll give you a couple of hints
along the way and then you can know if you're hitting those and then you'll
see if you're on the right track. So, step one, always very
carefully define your variables, their dimensionality and everything. So, y hat will define as the softmax
probability of the vector. So, the normalized scores or
the probabilities for all the different classes that we have. So, in our case we have four. Then we have the target distribution. Again, that will be a one hot
vector where it's all zeroes except at the ground truth index of the class y,
where it's one. And we'll define our f
here as f of x again, which is this matrix multiplication. Which is going to be a C dimensional
vector where capital C is the number of classes that we have, all right. So, that was step one. Carefully define all of your variables and
keep track of their dimensionality. It's very easy when you implement this and
you multiply two things, and they have wrong dimensionality, and
you can't actually legally multiply them, you know you have a bug. And you can do this also
in a lot of your equations. You'd be surprised. In the midterm, you're nervous. But maybe at the end you have some time. And you could totally grade it
by yourself in the first pass, by just making sure that all your
dimensionality of your matrix and vector multiplications are correct. All right, the second tip is the chain
rule, we went over this before, but I heard there's a little bit of
confusion still in the office hours. So, let's define this carefully for
a simple example and then we'll go and give you a couple more hints also for
more complex example. So again, if you have something
very simple, such as a function y, which you can defined here as f of u and
u can be defined as g of x as in the whole function, y of x,
can be described as f of g of x, then you would basically multiply dy,
u times the udx. And so very concretely here,
this is sort of high school level, but we'll define it properly in
order to show the chain rule. So here,
you can basically define u as g(x), which is just the inside in
the parentheses here, so x cubed + 7. It can have y as a function of f(u), where we use 5 times u,
just replacing the inside definition here. So it's very simple,
just replacing things. And now, we can take the derivative
with respect to u and we can take the derivative
with respect to x(u). And then we just multiply these two terms,
and we plug in u again. So in that sense, we all know,
in theory, the chain rule. But, now we're gonna have the softmax, and we're gonna have lots of matrices and
so on. So, we have to be very,
very careful about our notation. And we also have to be
careful about understanding, which parameters appear inside
what other higher level elements. So, f for instance is a function of x. So, if you're trying to take
a derivative with respect to x, of this overall soft max you're gonna have
to sum over all of the different classes inside which x appears. And you'll see here,
this first application, but not just of fy again this is just
a subscript the y element of the effector which is the function of x, but
also multiply it then here by this. So, when you write this out,
another tip that can be helpful is for this softmax part of he derivative
is to actually think of two cases. One where c = y, the correct class, and one where it's basically all
the other incorrect classes. And as you write this out,
you will observe and come up with something like this. So, don't just write that as your thing
you have to put in your problems, the steps on how to get there. Bur, basically at some point you
observe this kinda pattern when you now try to look at all the derivatives
with respect to all the elements of f. And now,
when you have this you realize ,okay at the correct class we're
actually subtracting one here, and all the incorrect classes,
you will not do anything. Now, the problem is when
you implement this, it kind of looks like
a bunch of if statements. If y equals the correct class for my training set, then, subtract 1,
that's not gonna be very efficient. Also, you're gonna go insane if you try
to actually write down equations for more complex neural network
architectures ever. And so, instead, what we wanna do is
always try to vectorize a lot of our notation, as well as our implementation. And so, what this means here,
in this case, is you can actually observe that,
well, this 1 is exactly 1, where t, our hot to target distribution,
also happens to be 1. And so, what you're gonna wanna do,
is basically describe this as y(hat)- t, so
it's the same thing as this. And don't worry if you don't
understand how we got there, cuz that's part of your problem set. You have to, at some point, see this equation while you're
taking those derivatives. And now, the very first baby step towards
back-propagation is actually to define this term, in terms of a simpler single
variable and we'll call this delta. We'll get good, we'll become good friends
with deltas because they are sort of our error signals. Now, the last couple of tips. Tip number six. When you start with this chain rule, you
might want to sometimes use explicit sums, before and
look at all the partial derivatives. And if you do that a couple of times
at some point you see a pattern, and then you try to think of how to
extrapolate from those patterns of single partial derivatives,
into vector and matrix notation. So, for example,
you'll see something like this here, in at some point in your derivation. S,o the overall derivative with respect to
x of our overall objective function for one element, for one element from our
training set x and y is this sum. And it turns out when you
think about this for a while, you take here this row vector but
then you transpose it, and becomes an inner product, well if you
do that multiple times for all the C's and you wanna get in the end a whole vector
out, it turns out you can actually just re-write the sum as W
transpose* the delta. So, this is one error signal here
that we got from our softmax, and we multiply the transpose of
our softmax weights with this. And again,
if some of these are not clear and you're confused,
write them out into full sum, and then you'll see that it's really
just re-write this in vector notation. All right, now what is the dimensionality
of the window vector gradient? So in the end, we have this derivative
of the overall cost here for one element of our training
set with respect to x. But x is a window. All right, so
each say we have a window of five words. And each word is d-dimensional. Now, what should be the dimensionality
of this derivative of this gradient? That's right,
it's five times the dimensionality. And that's another really good way, and
one of the reasons we make you implement this from scratch, if you have any kinda
parameter, and you have a gradient for that parameter, and they're not the same
dimensionality, you'll also know you screwed up and there's some mistake or
bug in either your code or your map. So, it's very simple debugging skill. And way to check your own equations. So, the final derivative with respect
to this window is now this five vector because we had five d-dimensional
vectors that we concatenated. Now, of course the tricky bit is, you actually wanna update your word
vectors and not the whole window, right? The window is just this
intermediate step also. So really, what you wanna do is update and take derivatives with respect to each
of the elements of your word vectors. And so it turns out, very simply,
that can be done by just splitting that error that you've got on the gradient
overall, at the whole window and that's just basically the concatenation of the
reduced of all the different word vectors. And those you can use to update your word
vectors, as you train the whole system. All right, any questions? Is there a mathematical what? Is there a mathematical notation for
the word vector t, other than it's just variable t? Or that seems like a fine notation. You can see this as a probability
distribution, that is very peaked. &gt;&gt; Yeah.
&gt;&gt; That's all, there's nothing else to it. Just a single vector with all zeroes,
except in one location. &gt;&gt; So I'll just write that down? &gt;&gt; You can write that up, yeah. You can always just write out and
it's also something very important. You always wanna define everything, so
that you make sure that the TAs know that you're thinking about the right thing,
as you're writing out your derivatives, you write out the dimensionality,
you define them properly, you can use dot, dot,
dot if it's a larger dimensional vector. You can just define t as your
target distribution [INAUDIBLE] &gt;&gt; The question is, do we still have two vectors for
each word? Great question, no. We essentially, when we did glove and
word2vec, and had these two u's and v's, for all subsequent lectures from now on,
we'll just assume we have the sum of u and v and that's our single vector x,
for each word. So, the question is does this gradient
appear in lots of other windows and it does. So, if you, the answer is yes. If you have the word "in," that vector
here and the gradients will appear in all the windows that have
the word "in" inside of them. And same with museums and so on. And so as you do stochastic gradient
descent you look at one window at a time, you update it, then you go to the next
window, you update it and so on. Great questions. All right. Now, let's look at how we update
these concatenated word vectors. So basically, as we're training this,
if we train it for instance with sentiment we'll push all
the positive words in one direction and the other words in other direction. If we train it, for
named entity recognition and eventually our model can learn that seeing
something like in as the word just before the center word, would be indicative for
that center word to be a location. So now what's missing for
training this full window model? Well mainly the gradient of J with
respect to the softmax weights W. And so
we basically will take similar steps. We'll write down all the partial
derivatives with respect to Wij first and so on. And then we have our full gradient for
this entire model. And again, this will be very sparse, and you're gonna wanna have some clever ways
of implementing these word vector updates. So you don't send a bunch of zeros
around at every single window, Cuz each window will
only have a few words. So in fact, it's so important for
your code in the problem set to think carefully through your
matrix implementations, that it's worth to spend two or
three slides on this. So there are essentially two very
expensive operations in the softmax. The matrix multiplication and
the exponent. Actually later in the lecture, we'll
find a way to deal with the exponent. But the matrix multiplication can also
be implemented much more efficiently. So you might be tempted in the beginning
to think this is probability for this class and
this is the probability for that class. And so implemented a for
loop of all my different classes and then I'll take derivatives or
matrix multiplications one row at a time. And that is going to be very,
very inefficient. So let's go through some very simple
Python code here to show you what I mean. So essentially,
always looping over these word vectors instead of concatenating
everything into one large matrix. And then multiplying these is
always going to be more efficient. So let's assume we have 500
windows that we want to classify, and let's assume each window
has a dimensionality of 300. These are reasonable numbers, and let's assume we have five
classes in our softmax. And so at some point during
the computation, we now have two options. So W here are weights for the softmax. It's gonna be C many rows and
d many columns. Now the word vectors here that
you concatenated for each window. We can either have the list of
a bunch of separate word vectors, or we can have one large matrix
that's going to be d times n. So d many rows and n many windows. So we have 500 windows, so
we have 500 columns here in this 1 matrix. And now essentially, we can multiply
the W here for each vector separately, or we can do this one matrix
multiplication entirely. And you literally have
a 12x speed difference. And sadly with these larger models,
one iteration or something might take a day, eventually for
more complex models large data sets. So the difference is between
literally 12 days or 1 day of you iterating and
making your deadlines and everything. So it's super important,
and now sometimes people are tripped up by what does it
mean to multiply and do this here. Essentially, it's the same
thing that we've done here for one softmax, but
what we did is we actually concatenated. A lot of different input vectors x, and so we'll get a lot of different
unnormalized scores out at the end. And then we can tease them apart again for
them. So you have here, c times t dimensional
matrix for the d dimensional input. So using the same notation, yeah, dimensional of each window times d times
n matrix to get a c times n matrix. So these are all
the probabilities here for your N many training samples. Any questions around that? So it's super important, all your code
will be way too slow if you don't do this. And so
this is very much an implementation trick. And so in most of the equations, we're not gonna actually go there cuz
that makes everything more complicated. And the equations look at only
a singular example at a time, but in the end you're gonna wanna
vectorize all your code. Yeah, matrices are your friend,
use them as much as you can. Also in many cases, especially for
this problem set where you really understand the nuts and bolts of how
to train and optimize your models. You will come across a lot
of different choices. It's like,
I could implement it this way or that way. And you can go to your TA and ask,
should I implement this way or that way? But you can also just use time it
as your magic Python and just let, make a very informed decision and
gain intuition yourself. And just basically wanna
speed test a lot of different options that you have in
your code a lot of the time. All right, so
this is was just a pure softmax, and now the softmax alone
is not play powerful. Because it really only gets with this
linear decision boundaries in your original space. If you have very, very little
training data that could be okay, and you kind of used a not so powerful model
almost as an abstract regularizer. But with more data,
it's actually quite limiting. So if we have here a bunch of words and
we don't wanna update our word vectors, softmax would only give us this linear
decision boundary which is kind of lame. And it would be way better if we could correctly classify these
points here as well. And so basically, this is one of the many
motivations for using neural networks. Cuz neural networks will give us much
more complex decision boundaries and allow us to fit much more complex
functions to our training data. And you could be snarky and actually rename neural networks
which sounds really cool. It's just general function approximators. Just wouldn't have quite the same ring to
it, but it's essentially what they are. So let's define how we get from
the symbol of logistic regression to a neural network and beyond,
and deep neural nets. So let's demystify the whole
thing by starting, defining again some of the terminology. And we can have more fun with the math,
and then one and a half lectures from now. We can just basically use
all of these Lego blocks. So bear with me,
this is going to be tough. And try to concentrate and
ask questions if you have any, cuz we'll keep building now a pretty
awesome large model that's really useful. So we'll have inputs, we'll have
a bias unit, we'll have an activation function and output for each single
neuron in our larger neuron network. So let's define a single neuron first. Basically, you can see it as
a binary logistic regression unit. We're going to have inside, again a set of weights that we
have in a product with our input. So we have the input x
here to this neuron. And in the end,
we're going to add a bias term. So we have an always on feature, and that kind of defines how likely
should this neuron fire. And by firing, I mean have a very
high probability that's close to one. For being on. And f here is always, from now on,
going to be this element wise function. In our case here the sigmoid that just
squashes whatever this sum gives us in our product plus the bias term and basically
just squashes it to be between 0 and 1. All right, so this is the definition
of the single neuron. Now if we feed a vector of inputs through
all this different little logistic regression functions and
neurons, we get this output. And now the main difference between
just predicting directly a softmax and standard machine learning and deep learning is that we'll actually not
force this to give directly the output. But they will themselves be inputs to yet
another neuron. And it's a loss function on top of that
neuron such as cross entropy that will now govern what these
intermediate hidden neurons. Or in the hidden layer what they
will actually try to achieve. And the model can decide itself
what it should represent, how it should transform this input
inside these hidden units here in order to give us a lower
error at the final output. And it's really just this
concatenation of these hidden neurons, these little binary
logistic regression units that will allow us to build very
deep neural network architectures. Now again, for sanity's sake, we're
going to have to use matrix notation cuz all of this can be very simply described
in terms of matrix multiplication. So a1 here is where going to be the final activation of the first neuron,
a2 in second neuron and so on. So instead of writing out the inner
product here, or writing even this as an inner product plus the bias term
we're going to use matrix notation. And it's very important now to pay
attention to this intermediate variables that we'll define because
we'll see these over and over again as we use a chain
rule to take derivatives. So we'll define z here as W
times x plus the bias vector. So we'll basically have here as
many bias terms and this vector has the same dimensionality as the number
of neurons that we have in this layer. And W will have number of rows for
the number of neurons that we have times number of columns for
the input dimensionality of x. And then, whenever we write a of f(z), what that means here is that we'll
actually apply f element wise. So f(z) when z is a vector is just f(z1),
f(z2) and f(z3). And now you might ask, well, why do we
have all this added complexity here with this sigmoid function. Later on we can actually have other
kinds of so called non linearities. This f function and
it turns out that if we don't have the non-linearities in between and
we will just stack a couple of this linear layers together it wouldn't
add a very different function. In fact it would be continuing to
just be a single linear function. And intuitively as you
have more hidden neurons, you can fit more and
more complex functions. So this is like a decision boundary
in a three dimensional space, you can think of it also in
terms of simple regression. If you had just a single hidden neuron, you kinda see here almost
an inverted sigmoid. If you have three hidden neurons,
you could fit this kind of more complex functions and with ten neurons,
each neuron can start to essentially, over fit and try to be very good
at fitting exactly one point. All right, now let's revisit our
single window classifier and instead of slapping a softmax directly
onto the word vectors we're now going to have an intermediate hidden layer
between the word vectors and the output. And that's when we really start to
gain an accuracy and expressive power. So let's define a single
layer neural network. We have our input x that will be again, our window, the concatenation
of multiple word vectors. We'll define z and we'll define a as
element wise on the areas a and z. And now, we can use this
neural activation vector a as input to our final classification layer. The default that we've had so
far was the softmax, but let's not rederive the softmax. We've done it multiple times now,
you'll do it again in a problem set and introduce an even simpler one and walk through all the glory details
of that simple classifier. And that will be a simple,
unnormalized score. And this case here, this will
essentially be the right mechanism for various simple binary
classification problems, where you don't even care that much
about this probability z is 0.8. You really just cares like, is it one,
is it in this class, or is it not? And so we'll define the objective function
for this new output layer in a second. Well, let's first understand
the feed-forward process. And well feed-forward process is what you
will end up using a test time and for each element also in training
before you can take derivative. Always be feed-forward and
then backward to take the derivatives. So what we wanna do here is for example, take basically each window and
then score it. And say if the score is high we want to
train the model such that it would assign high scores to windows where the center
word is a named entity location. Such as Paris, or London, or Germany,
or Stanford, or something like that. Now we will often use and you'll see a in a lot of papers this kind
of graph, so it's good to get used to it. There are various other kinds,
and we'll try to introduce them slowly throughout the lecture but
this is the most common one. So we'll define bottom up,
what each of these layers will do and then we'll take the derivatives and
learn how to optimize it. Now x window here is the concatenation
of all our word vectors. So let's hear, and
I'll ask you a question in a second, let's try to figure out the dimensionality
here of all our parameters so that you're, I know you're with me. So let's say each of our word
vectors here is four dimensional and we have five of these word vectors in
each window that are concatenated. So x is a 20 dimensional vector. And again,
we'll define it as column vectors. And then lets say we have
in our first hidden layer, lets say we have eight units here. So you want an eight unit hidden layer
as our intermediate representation. And then our final scores just
again a simple single number. Now what's the dimensionality
of our W given what I just said? 20 dimensional input, eight hidden units. 20 rows and eight columns. We have one more transfer,
[LAUGH] that's right. So it's going to be eight rows and
20 columns, right? And you can always
whenever you're unsure and you have something like this then
this will have some n times d. And then multiply this and then this
will have, this will always be d, and so these two always
have to be the same, right? So all right, now what's the main intuition behind this
extra layer, especially for NLP? Well, that will allow
us to learn non-linear interactions between these
different input words. Whereas before, we could only say
well if in appears in this location, always increase the probability
that the next word is a location. Now we can learn things and patterns like,
if in is in the second position, increase the probability of this being the location
only if museum is also the first vector. So we can learn interactions
between these different inputs. And now we'll eventually make
our model more accurate. Great question. So do I have a second W there. So the second layer here the scores
are unnormalized, so it'll just be U and because we just have a single U, this will
just be a single column vector and we'll transpose that to get our inner product
to get a single number out for the score. Sorry, yeah, so the question was
do we have a second W vector. So yeah, that's in some
sense our second matrix, but because we only have one hidden neuron in
that layer, we only need a single vector. Wonderful. All right, so,
now let's define the max-margin loss. It's actually a super powerful loss
function often is even more robust than the cross entropy error in softmax,
and is quite powerful and useful. So let's define here two examples. Basically, you want to give
a high score to windows, where the center word is a location. And we wanna give low scores to corrupt or incorrect windows where the center
word is not a named entity location. So museum is technically a location,
but it's not a named entity location. And so the idea for this training objective of max-margin is
to essentially try to make the score of the true windows larger than the ones of
the corrupt windows smaller or lower. Until they're good enough. And we define good enough as being
different by the value of one. And this one here is a margin. You can often see it as
a hyperparameter too and set it to m and try different ones but
in many cases one works fine. This is continuous and
we'll be able to use SGD. So now what's the intuition behind the
softmax, sorry the max-margin loss here? If you have for
instance a very simple data set and you have here a couple
of training samples. And here you have the other class c,
what a standard softmax may give you is a decision
boundary that looks like this. It's like perfectly separates the two. It's a very simple training example. Most standard softmax
classifiers will be able to perfectly separate these two classes. And again, this is just for
illustration in two dimensions. These are much higher
dimensional problems and so on. But a lot of the intuition
carries through. So now here we have our decision
boundary and this is the softmax. Now, the problem is maybe that
was your training data set. But your test set, actually,
might include some other ones that are quite similar to those stuff you saw
at training, but a little different. And now this kind of decision
boundary is not very robust. In contrast to this, what the max margin loss will attempt to do is to
try to increase the margin between the closest points
of your training data set. So if you have a couple of points here and
you have different points here. We'll try to maximize the distance between the closest points here, and
essentially be more robust. So then if at test time you have some
things that are kinda similar, but not quite there, you're more likely
to also correctly classify them. So it's a really great lost or
objective function. Now in our case here when we say a sc for
one corrupt window. In many cases in practice we're
actually going to have a sum over multiple of these. And you can think of this similar to the
skip-gram model where we sample randomly a couple of corrupt examples. So you really only need for
this kind of training a bunch of true examples of this
is a location in this context. And then all the other windows
where you don't have that as your training data are essentially
part of your negative class. All right, any questions around
the max-margin objective function? We're gonna take a lot of
derivatives of it now. That's right, is the corrupt
window just a negative class? Yes, that's exactly right. So you can think of any other window that
doesn't have as its center location just as the other class. All right, now how do we optimize this? We're going to take very similar steps to
what we've done with cross entropy, but now we actually have this hidden layer and
we'll take our second to last step towards the full back-propagation algorithm
which we'll cover in the next lecture. So let's assume our cost
J here is larger than 0. So what does that mean? In the very beginning you will initialize
all your parameters here again. Either randomly or maybe you'll initialize
your word vectors to be reasonable. But they're not gonna be quite perfect at
learning in this context in the window what is location and what isn't. And so in the beginning all your scores
are likely going to be low cuz all our parameters, U and W and b have been
initialized to small, random numbers. And so I'm unlikely going to be great
at distinguishing the window with a correct location at center
versus one that is corrupt. And so basically,
we will be in this regime. After a while of training, eventually
you're gonna get better and better. And then intuitively
if your score here for instance of the good window is five and
one of the corrupt is just two, then you'll see 1- 5 + 2 is less than 0 so you just basically have 0
loss on those elements. And that's another great property of
this objective function which is over time you can start ignoring more and more
of your training set cuz it's good enough. It will assign 0 cost as in 0 error to these examples and so
you can start to focus on your objective function only on the things that the model
still has trouble to distinguish. All right, so let's in the very
beginning assume most of our examples will J will be larger than 0 for them. And so what we're gonna have to do now
is take derivatives with respect to all the parameters of our model. And so what are those? Those are U, W, b and our word vectors x. So we always start from the top and then
we go down because we'll start to reuse different elements and just the simple
combination of taking derivatives and reusing variables is going to
lead us to back propagation. So derivative of s with respect to U. Well, what was s? s was just u transpose times a and so we all know that derivative
of that is just a. So that was easy, first element,
first derivative super straight forward. Now it's important when we
take the next derivative to also be aware of all our definitions. How we define these functions that
we're taking derivatives off. So s is basically U transpose a,
a was f(z) and z was just Wx + b. All right,
it's very important to just keep track. That's like almost 80% of the work. Now, let's take
the derivative like I said, first partial of only one
element of W to gain intuitions. And then we can put it back together and
have a more complex matrix notation. So we'll observe for
Wij that it will actually only appear in the ith activation of our hidden layer. So for example, let's say we have a very
simple input with a three dimensional x. And we have two hidden units,
and this one final score U. Then we'll observe that if we take
the derivative with respect to W23. So the second row and
the third column of W, well that actually only is needed in a2. You can compute a1 without using W23. So what does that mean? That means if we take
the derivative of weight Wij, we really only need to look at
the ith element of the vector a. And hence, we don't need to look
at this whole inner product. So what's the next step? Well as we're taking derivatives with W,
we need to be again aware of where does W appear and all the other parameters
are essentially constant. So U here is not something
we're taking a derivative off. So what we can do is just take it out,
just as like a single number, right. We'll just get it outside,
put the derivative inside here. And now, we just need to very
carefully define our ai. So a subscript i, so
that's where Wij appears. Now, ai was this function,
and we defined it as f of zi. So why don't we just
write this carefully out, and now this is first application
of the chain rule with derivative of ai with respect to zi,
and then zi with respect to Wij. So this is single application
of the chain rule. And then end of it it looks kind of
overwhelming, but each step is very clear. And each step is simple, we're really
writing out all the glory details. So application of the chain rule,
now we're going to define ai. Well ai is just f of zi, and f was just an
element y function on a single number zi. So we can just rewrite ai with
its definition of f of zi, and we keep this one intact, all right? And now derivative of f,
we can just for now assume is f prime. Just a single number, take derivative. We'll just define this as f prime for now. It's also just a single number,
so no harm done. Now we're still in this part here, where we basically wanna take
the derivative of zi with respect to Wij. Well let's define what zi was,
zi was just here. The W of the ith row times x
plus the ith element of b. So let's just replace zi
with it's definition. Any questions so far? All right, good or not? So we have our f prime and
we have now the derivative with respect to Wij of just
this inner product here. And we can again,
very carefully write out well, the inner product is just this
row times this column vector. That's just the sum, and now when we
take the derivative with respect to Wij, all the other Ws are constants. They fall out, and so
basically it's only the xk, the only one that actually appears
in the sum with Wij is xj and so basically this derivative is just Xj. All right, so now we have this
whole expressions of just taking carefully chain rule multiplications
definitions of all our terms and so on. And now basically, what we're gonna want
to do is simplify this a little bit, cuz we might want to
reuse different parts. And so we can define, this first term here
actually happens to only use subindices i. And it doesn't use any other subindex. So we'll just define Uif prime of zi for all the different is as delta i. At first notational simplicity and
xj is our local input signal. And one thing that's very helpful for you to do is actually look at also the
derivative of the logistic function here. Which can be very conveniently computed
in terms of the original values. And remember f of z here, or f of zi of each element is
always just a single number. And we've already computed it
during forward propagation. So we wanna ideally use hidden activation
functions that are very fast to compute. And here, we don't need to compute
another exponent or anything. We're not gonna recompute f of zi cuz
we already did that in the forward propagation step. All right, now we have the partial derivative
here with respect to one element of W. But of course, we wanna have the whole
gradient for the whole matrix. So now the question is,
with the definitions of this delta i for all the different elements of
i of this matrix and xj for all the different elements of the input. What would be a good way of trying to
combine all of these different elements to get a single gradient for the whole
matrix W, if we have two vectors. That's right. So essentially, we can use delta
times x transpose, namely the outer product to get all the combinations
of all elements i and all elements j. And so this again might seem
like a little bit like magic. But if you just think again of
the definition of the outer product here. And you write it out in terms of all
the indices, you'll see that turns out to be exactly what we would want in
one very nice, very simple equation. So we can kind of think of this delta
term actually as the responsibility of the error signal that's now arriving from
our overall loss into this layer of W. And that will eventually
lead us to flow graphs. And that will eventually lead us to you
not having to actually go through all this misery of taking all these derivatives. And being able to abstract it
away with software packages. But this is really the nuts and
bolts of how this works, yeah? Yeah, the question is, this outer product
will get all the elements of i and j? And that's right. So when we have delta times x transposed. Then now we have basically here,
x is usually this vector. So now let's take the right notation. So we wanna have derivative
with respect to W. W was a, 2x3 dimension matrix for
example, 2x3. We should be very careful of our notation. 2x3. So now,
the derivative of j with respect to our w has to, in the end, also be a 2x3 matrix. And if we have delta times x transposed,
then that means we'll have to have a two-dimensional delta, which is
exactly the dimensions that are coming in. [INAUDIBLE] Signal that I
mentions that we have for the number of hidden units that we have. Times this one dimensional,
basically row vector times xt which is a 1 x 3 dimensional
vector that we transpose. And so, what does that mean? Well, that's basically multiplying now,
standard matrix multiplication. You should write that. So now the last term that we haven't
taken derivatives of off the [INAUDIBLE], is our bi and
it'll eventually be very similar. We're going to go through it. We can pull Ui out, we're going to
take f prime, assume that's the same. So now, this is our delta i. We'll observe something very similar. These are very similar steps for bi. But in the end, we're going to
just end up with this term and that's just going to be one. And so,
the derivative of our bi element here, is just delta i and we can again
use all the elements of delta, to have the entire gradient for
the update of b. Any questions? Excellent, so this is essentially,
almost back-propagation. Weve so far only taken derivatives and
using the chain rule. And first thing, when I went through this, this is like a lot of the magic of deep
learning, is just becoming a lot clear. Weve just taken derivatives, we have
an objective function and then we update based on our derivatives, all
the parameters of these large functions. Now the main remaining trick, is to re-use
derivatives that we've computed for the higher layers in computing
derivatives for the lower layers. It's very much an efficiency trick. You could not use it and it would
just be very, very inefficient to do. But this is the main insight of why we re-named taking
derivatives as back propagation. So what is the last derivatives
that we need to take? For this model, well again,
it's in terms of our word vectors. So let's go through all of those. Basically, we'll have to take the
derivative of the score with respect to every single element of our word vectors. Where again, we concatenated all
of them into a single window. And now, the problem here is that each word vector actually
appears in both of these terms. And both hidden units use all of
the elements of the input here. So we can't just look at a single element. We'll really have to sum over, both of the
activation units in the simple case here, where we just have two hidden units and
three dimensional inputs. Keeps it a little simpler,
and there's less notation. So then, we basically start with this. I have to take derivatives with
respect to both of the activations. And now, we're just going to go
through similar kinds of steps. We have s. We defined s as u transpose
times our activation. That was just Ui then ai
was just f of w and so on. Now, what we'll observe as we're going
through all these similar steps again is that, we'll actually see the same
term here reused from before. It's Ui x F prime of Zi. This is exactly the same. That we've seen here. F prime of Zi. And what that means is,
we can reuse that same delta. And that's really one of the big insights. Fairly trivial but very exciting,
cuz it makes it a lot faster. But, what's still different now, is that of course we have to take
the the derivative with respect. To each of these, to this inner product
here in Xj, where we basically dumped the bias term, cuz that's just a constant,
when we were taking this derivative. And so, this one here again,
Xj is just inner product, it's the jth element of this matrix
W that's the relevant one for this inner product,
let me take the derivative. So now we have this sum here, and
now comes again this tricky bit of trying to simplify this sum into something
simpler in terms of matrix products. And again, the reason we're getting
towards back propagation is that we're reusing here these previous error signals,
and elements of the derivative. Now, the simplest, the first thing we'll
observe here as we're doing this sum, is that sum is actually also a simple inner
product, where we now take the jth column. So this again, this dot notation
when the dot is after the first, and next we take the row,
here we take the column. So it's a column vector. But then of course we transpose it, so it's a simple inner product for
getting us a single number. Just the derivative of this element of
the word vectors and the word window. Yes. Great question. So once we have the derivatives for all these different variables, what's
the sequence in which we update them, and there's really no sequence we
update them all in parallel. We just take one step in all the elements
that we now had a variable in or have seen that parameter in. And the complexity there,
is in standard machine learning you'll see in many models just like
standard logistic regression, you see all your parameters like
your W in all the examples. And ours, it's a little more complex,
because most words you won't see in a specific window and so, you only update
the words that you see in that window. And if you assumed all the other ones,
you'd just have very, very large, quite sparse updates, and that's not
very RAM efficient, great question. So now we have this simple
multiplication here and the sum is just is just inner product. So far so simple, and we have our D
dimension vector which I mentioned, is two dimensions. We have the sum over two elements. So, so far so good. Now, really, we would like to get the full
gradient here with respect to all XJs for J equals one to three and
its simple case, or five D if we have a five
word large window. So now the question is, how do we
combine this single element here. Into a vector that eventually gives us all
the different gradients for all the xij. And j equals 1 to however long our window
is Is anybody follow along this closely? That's right. W transposed delta. Well done. So basically our final derivative and
final gradient here for. Our score s with respect to the entire
window, is just W transpose times delta. Super simple very fast to implement,
I can easily think about how to vectorize this again by concatenating multiple
deltas from multiple Windows and so on. And it can be very efficiently,
like implemented and derived. All right, now the error message is
delta that arrives at this hidden layer, has of course the same dimensionality as
its hidden layer because we're updating all the windows. And now from the previous slides we
also know that when we update a window, it really means we now cut up that final gradient here into the different chunks
for each specific word in that window, and that's how we update our
first large neural network. So let's put all of this together again. So, our full objective function
here was this max and I started out with saying let's assume it's larger
than zero so you have this identity here. So this is simple indicator function if. The indication is true,
then it's one and if not, it's zero. And then you can essentially
ignore that pair of correct and corrupt windows x and
xc, respectively. So our final gradient when we have these kinds of max margin functions
is essentially implemented this way. And we can very efficiently
multiply all of this stuff. All right. So this is just that, this is not right. This is our [INAUDIBLE] But you still
have to take the derivative here, but basically this indicator function is
the main novelty that we haven't seen yet. All right. Yeah. &gt;&gt; [INAUDIBLE] &gt;&gt; Yeah, it's a long question. The gist of the question is how to we make
sure we don't get stuck in local optima. And you've kinda answered it a little
bit already which is indeed because of the stochasticity you keep making updates
anyway it's very hard to get stuck. In fact, the smaller your,
the more stochastic you are, as in the fewer windows you look at
each time you want to make an update, the less likely you're getting stuck. If you had tried to get through all the
windows and then make one gigantic update, so it's actually very inefficient and
much more likely to get you stuck. And then the other observation
that it's just slowly coming through some of the theory that
we couldn't get into this class. Is that it turns out a lot of the local
optima are actually pretty good. And in many cases, not even that far away from what you
might think the global optima would be. Also, you'll observe a lot of times,
and we'll go through this in some of the project advice in many cases,
you can actually perfectly fit. We have a powerful enough
neural network model. You can often perfectly fit your input and
your training dataset. And you'll actually, eventually spend
most of your time thinking about how to regularize your models better and often,
at least, even more stochasticity. We'll get through some of those. But yeah, good question. Yeah, in the end, we just have all
these updates and it's all very simple. All right, so let's summarize. This was a pretty epic lecture. Well done for sticking through it. Congrats again, this was our super
useful basic components lecture. And now this window model is actually
really the first one that you might observe and practice and
you might actually want to implement. In a real life setting. So to recap, we've learned word vector training,
we learned how to combine Windows. We have the softmax and
the cross entropy error and we went through some of the details there. Have the scores and the max margin loss,
and we have the neural network, and it's really these two steps here that you have
to combine differently for problem set. Number one and
especially number two in that. So, we just have one more
math heavy lecture and after that we can have fun and
combine all these things together. Thanks.

Linear Algebra

The following content is
provided under a Creative Commons license. Your support will help MIT
OpenCourseWare continue to offer high quality educational
resources for free. To make a donation or view
additional materials from hundreds of MIT courses, visit
MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: I want to take a few
minutes at the start of today's lecture to wrap
up a few more things about debugging. And then I'll move on
to the main event. Just a few small points. First of all, keep in mind that
the bug is probably not where you think it is. Because if a bug were there,
you'd have already found it. There are some simple things
that you should always look at when you're looking for bugs. Reversed order of arguments. You might have a function that
takes to floats and you just passed them in the
wrong order. You may have noticed that when
Professor Grimson and I used examples in class, we were
pretty careful how we name our parameters. And I often end up using the
same names for the actuals and the formals. And this just helps me not get
confused about what order to do things in. Spelling. These are just dumb little
things to look for. Did you spell all the
identifiers the way you think you did. The problem is, when you
read code you see what you expect to see. And if you've typed in l when
you should've typed a 1, or a 1 when you should've typed an
l, you're likely to miss it. If you've made a mistake with
upper and lower case. Things like that. So just be very careful
looking at that. Initialization. A very common bug is to
initialize a variable. Go through a loop, and then
forget to reinitialize it when it needs to be reinitialized
again. So it's initialized outside
the loop when it should be initialized inside the loop. Or conversely, inside the
loop when it should be outside the loop. So look carefully at when
variables are being initialized. Object versus value equality. Particularly when you use double
equals, are you asking whether you've got the same
object or the same value? They mean very different things,
as we have seen. Keep track of that. Related to that is aliasing. And you'll see that, by the way,
on the problem set that we're posting tomorrow. Where we're going to ask you
to just look at some code, that there's some issues there
with the fact that you alias things, maybe on purpose,
maybe on accident. And by that I mean two different
ways to get to the same value, the same object. Frequently that introduces
bugs into the program. A particular instance of that
is deep versus shallow copy. When you decide to make a copy
of something like a list, you have to think hard about are you
only copying the top level of the list, but if it's, say,
a list that contains a list, are you also getting
a new copy of everything it contains? It doesn't matter if it's a list
of integers or a list of strings or a list of tuples, but
it matters a lot if it's a list of lists. Or a list of anything that
could be mutable. So when you copy, what
are you getting? When you copy a dictionary,
for example. Think through all of that. And again, a related problem
that people run into is side-effects. You call a function and it
returns a value, but maybe it, on purpose or on accident,
modifies one of the actual parameters. So you could each make
a list of things. Every experienced programmer
over time develops a personal model of the mistakes
they make. I know the kinds of things
I get wrong. And so when I'm looking for a
bug, I always say, oh, have you done this dumb thing
you always do. So be a little bit
introspective. Keep track of the mistakes
you make. And if your program doesn't
work, that should be your first guess. Couple of other hints. Keep a record of what
you've tried. People will look at things and
they'll come in and that'll talk to a TA, and the TA will
say, did you try this? And the person will
say, I don't know. That leads you to end up
doing the same thing over and over again. This gets back to my main theme
of Tuesday, which is be systematic. Know what you've already tried,
don't waste your time trying it again. Think about reconsidering
your assumptions. A huge one is, are you actually
running the code you're looking at
in your editor. This is a mistake I make
all the time in Python. I sit there in idle, and
I edit some code. I then click on the shell,
rather than hitting F5, try and run it and say, it's
not doing what I thought I should do. And I'll sit there staring at
the output, staring at the code, not realizing that
that output was not produced by that code. It's a mistake I've learned
that I make. It's an easy one to
make in Python. Lots of assumptions like that. You thought you knew what the
built-in function sort did, method sort did. Well, does it do what
you think it does? Does append do what
you think it does? Go back and say, alright,
I've obviously, some assumption is not right. When you're debugging somebody
else's code, debug the code, not the comments. I've often made the mistake
of believing the comments somebody wrote in your code
about what some function does. Sometimes it's good
to believe it. But sometimes you
have to read it. Important thing. When it gets really
tough, get help. Swallow your pride. Any one of my graduate students
will tell you that from time to time I walk into
their office and say, do you have a minute? And if they foolishly say yes,
I drag them back to my office and say I'm stuck on this bug,
what am I doing wrong? And it's amazing what -- well,
a, they're smarter than I am which helps. But even if they weren't smarter
than I am, just a fresh set of eyes. I've read through the same thing
twenty times and I've missed something obvious. Someone who's never seen it
before looks at and says, did you really mean to do this? Are you sure you didn't want to
do for i in range of list, rather than for i in list? That sort of thing. Makes a big difference to just
get that set of eyes. And in particular, try and
explain to somebody else what you think the program
is doing. So I'll sit there with the
student and I'll say, I'm going to try and explain to
you why what I think this program is doing. And probably 80% of the time,
halfway through my explanation, I say,
oh, never mind. I'm embarrassed. And I send them away. Because just the act of
explaining it to him or her has helped me understand it. So when you're really stuck,
just get somebody. And try and explain to them why
you think your program is doing what it's doing. Another good thing to do
when you're really stuck is walk away. Take a break. Come back and look at it with
fresh eyes of your own. Effectively, what you're doing
here is perhaps trading latency for efficiency. It may, if you go back, come
back and two hours later, maybe you'll, it'll take you
at least two hours to have found the bugs, because you've
been away for two hours. And maybe if you'd stayed there
and worked really hard you'd have found it an hour
and fifty eight minutes. But is it really worth
the two minutes? This is another reason, by
the way, to start it long before it's due. That you actually have the
leisure to walk away. All right. What do you do when you've
found the bug and you need to fix it? Remember the old saw,
haste makes waste. I don't know this but I'll bet
Benjamin Franklin said this. Don't rush into anything. Think about the fix, don't make
the first change that comes to mind and
see if it works. Ask yourself, will it fix all
of the symptoms you've seen? Or if not, are the symptoms
independent, will at least fix some of them? What are the ramifications
of the proposed change. Will it break other things? That's a big issue. You can fix one thing, you
break something else. So think through what this
change might break. Does it allow you to tidy
up other things? This is important, I
think, that code should not always grow. We all have a tendency to
fix code by adding code. And the program just gets bigger
and bigger and bigger. The more code you have, the
harder it is to get it right. So, sometimes, what you need
to, so you just pull back. And say, well, let me
just tidy things up. That, by the way, is also
a good debugging tool. Sometimes when I'm really stuck,
I say, alright let me stop looking for the bug. Let me just clean up the
code a little bit. Make my program prettier. And in the process of tidying it
up and making it prettier, I'll often by accident
find the bug. So it's a good trick
to remember. Finally, and this is very
important, make sure that you can revert. There's nothing more frustrating
then spending four hours debugging, and realizing
at the end of four hours your program is worse than it
was when you started. And you can't get back to where
it was when you started. So if you look at one of my
directories, you'll find that I've been pretty anal about
saving old versions. I can always pretty much get
back to some place I've been. So if I've made a set of changes
and I realize I've broken something that used to
work, I can find a version of the code in which it used to
work, and figure out what was going on there. Disk space is cheap. Don't hesitate to save
your old versions. It's a good thing. Alright, that's the end of my,
maybe sermon is the right word on debugging. Polemic, I don't know. I hope it will be helpful. And I hope you'll remember some
of these things as you try and get your programs
to work. I now want to get back to
algorithms. And where we've sort of been for a while,
and where we will stay for a while. More fundamentally, I want to
get back to what I think of as the main theme of 6.00, which
is taking a problem and finding some way to formulate
the problem so that we can use computing to help us
get an answer. And in particular, we're going
to spend the next few lectures looking at a class of problems
known as optimization problems. In general,
every optimization problem is the same. It has two parts. Some function that you're either
attempting to maximize or minimize the value of. And these are duals. And some set of constraints
that must be honored. Possibly an empty set
of constraints. So what are some of the classic
optimization problems? Well one of the most well-known
is the so-called shortest path problem. Probably almost every one
of you has used a shortest path algorithm. For example, if you go to
Mapquest, or Google Maps and ask how do I get from
here to there. You give it the function,
probably in this case to minimize, and it gives you
a choice of functions. Minimize time, minimize
distance. And maybe you give it a
set of constraints. I don't want to drive
on highways. And it tries to find the
shortest way, subject to those constraints, to get from Point
A to Point B. And there are many, many other instances
of this kind of thing. And tomorrow it's recitation,
we'll spend quite a bit of time on shortest
path problems. Another classic optimization
problem is the traveling salesman. Actually, I should probably,
be modern, call it the traveling salesperson, the
traveling salesperson problem. So the problem here, roughly, is
given, a number of cities, and say the cost of traveling
from city to city by airplane, what's the least cost round
trip that you can find? So you start at one place, you
have to go to a number of other places. End up where you started. It's not quite the same as the
shortest path, and figure out the way to do that
that involves spending the least money. Or the least time, or
something else. What are some of the other
classic optimization problems? There's bin packing. Filling up some container with
objects of varying size and, perhaps, shape. So you've got the trunk of
your car and you've got a bunch of luggage. More luggage than can
actually fit in. And you're trying to figure
out what order to put things in. And which ones you can put in. How to fill up that bin. Very important in shipping. People use bin packing
algorithms to figure out, for example, how to load
up container ships. Things of that nature. Moving vans, all sorts of
things of that nature. In biology and in natural
language processing and many other things, we see a lot of
sequence alignment problems. For example, aligning DNA
sequences, or RNA sequences. And, one we'll spend a fair
amount of time on today and Tuesday is the knapsack
problem. In the old days people used to
call backpacks knapsacks. So we old folks sometimes even
still make that mistake. And the problem there is, you've
got a bunch of things. More than will fit into the
knapsack, and you're trying to figure out which things to
take and which things to leave. As you plan
your hiking trip. How much water should
you take. How many blankets? How much food? And you're trying to optimize
the value of the objects you can take subject to the
constraint that the backpack is of finite size. Now, why am I telling
you all of this at this lightning speed? It's because I want you to think
about it, going forward, about the issue of problem
reduction. We'll come back to this. What this basically means is,
you're given some problem to solve, that you've never
seen before. And the first thing you do is
ask is it an instance of some problem that other people
have already solved? So when the folks at Mapquest
sat down to do their program, I guarantee you somebody opened
an algorithms book and said, what have other
people done to solve shortest path problems? I'll rely on fifty years of
clever people rather than trying to invent my own. And so frequently what we try
and do is, we take a new problem and map it onto an old
problem so that we can use an old solution. In order to be able to do that,
it's nice to have in our heads an inventory of previously
solved problems. To which we can reduce the
current problem. So as we go through this
semester, we'll look at, briefly or not so briefly,
different previously solved problems in the hope that at
some time in your future, when you have a problem to deal
with, you'll say, I know that's really like
shortest path, or really like graph coloring. Let me just take my problem and
turn it into that problem, and use an existing solution. So we'll start looking in detail
at one problem, and that's the knapsack problem. Let's see. Where do I want to start? Oh yes, OK. So far, we've been looking
at problems that have pretty fast solutions. Most optimization problems do
not have fast solutions. That is to say, when you're
dealing with a large set of things, it takes a while to
get the right answer. Consequently, you have to
be clever about it. Typically up till now, we've
looked at things that can be done in sublinear time. Or, at worst, polynomial time. We'll now look at a problem that
does not fall into that. And we'll start with
what's called the continuous knapsack problem. So here's the classic
formulation. Assume that you are a burglar. And you have a backpack that
holds, say, eight pounds' worth of stuff. And you've now broken into a
house and you're trying to decide what to take. Well, let's assume in the
continuous world, what you is you walk into the house and you
see something like four pounds of gold dust. And you
see three pounds of silver dust, and maybe ten
pounds of raisins. And I don't actually
know the periodic table entry for raisins. So I'll have to write it out. Well, how would you solve
this problem? First, let's say, what
is the problem? How can we formulate it? Well, let's assume that what
we want to do is, we have something we want to optimize. So we're looking for
a function to maximize, in this case. What might that function be? Well, let's say it's some number
of, some amount of, the cost of the value of gold. Times however many
pounds of gold. Plus the cost of silver times
however many - no, gold, is a g, isn't it. Pounds of silver, plus the
cost of raisins times the number of pounds of raisins. So that's the function
I want to optimize. I want to maximize
that function. And the constraint is what? It's that the pounds of gold
plus the pounds of silver plus the pounds of raisins is
no greater than eight. So I've got a function to
maximize and a constraint that must be obeyed. Now, the strategy here
is pretty clear. As often is for the continuous
problem. What's the strategy? I pour in the gold till
I run out of gold. I pour in the silver until
I run out of silver. And then I take as many raisins
as will fit in and I leave. Right? I hope almost every one of you
could figure out that was the right strategy. If not, you're not well suited
to a life of crime. What I just described is an instance of a greedy algorithm. In a greedy algorithm, at
every step you do what maximizes your value
at that step. So there's no planning ahead. You just do what's ever best.
It's like when someone gets their food and they start
by eating dessert. Just to make sure they
get to the best part before they're full. In this case, a greedy algorithm
actually gives us the best possible solution. That's not always so. Now, you've actually all implemented a greedy algorithm. Or are in the process thereof. Where have we implemented a
greedy algorithm, or have been asked to do a greedy
algorithm? Well, there are not that many
things you guys have been working on this semester. Yeah? STUDENT: [INAUDIBLE] PROFESSOR: Exactly right. So what you were doing there,
it was a really good throw. But it was a really good answer
you gave. So I'll forgive you the bad hands. You were asked to choose
the word that gave you the maximum value. And then do it again with
whatever letters you had left. Was that guaranteed to win? To give you the best
possible scores? No. Suppose, for example, you had
the letters this, doglets. Well, the highest scoring word
might have been something like Doges, these guys used to rule
Venice, but if you did that you'd been left with the letters
l and t, which are kind of hard to use. So you've optimized
the first step. But now you're stuck
with something that's not very useful. Whereas in fact, maybe you would
have been better to go with dog, dogs, and let. So what we see here is an
example of something very important and quite general. Which was that locally optimal
decisions do not always lead to a global optimums. So you
can't just repeatedly do the apparently local thing
and expect to necessarily get to it. Now, as it happens with the
continuous knapsack problem as we've formulated it,
greedy is good. But let's look for a slight
variant of it, where greedy is not so good. And that's what's called the
zero-one knapsack problem. This is basically a discrete
version of the knapsack problem. The formulation is that we have
n items and at every step we have to either take
the whole item or none of the item. In the continuous problem, the
gold dust was assumed to be infinitely small. And so you could take as much
of it as you wanted. Here it's as if you
had gold bricks. You get to take the whole brick
or no brick at all. Each item has a weight and a
value, and we're trying to optimize it as before. So let's look at an example of
a zero-one knapsack problem. Again we'll go back
to our burglar. So the burglar breaks into the
house and finds the following items available. And you'll see in your handout
a list of items and their value and how much,
what they weight. Finds a watch, a nice Bose
radio, a beautiful Tiffany vase, and a large
velvet Elvis. And now this burglar finds, in
fact, two of each of those. Person is a real velvet Elvis
fan and needed two copies of this one. Alright, and now he's trying
to decide what to take. Well if the knapsack were large
enough the thief would take it all and run, but let's
assume that it can only hold eight pounds, as before. And therefore the thief
has choices to make. Well, there are three types of
thieves I want to consider: the greedy thief, the
slow thief, and you. We'll start with the
greedy thief. Well, the greedy thief follows
the greedy algorithm. What do you get if you follow
the greedy algorithm? What's the first thing
the thief does? Takes the most valuable item,
which is a watch. And then what does
he do after that? Takes another watch. And then? Pardon? STUDENT: [INAUDIBLE] PROFESSOR: And then? STUDENT: [INAUDIBLE] PROFESSOR: No. Not unless he wants to break
the vase into little pieces and stuff it in the corners. The backpack is now
full, right? There's no more room. So the greedy thief take
that and leaves. But it's not an optimal
solution. What should the thief
have done? What's the best thing
you can do? Instead of taking that
one vase, the thief could take two radios. And get more value. So the greedy thief, in some
sense, gets the wrong answer. But maybe isn't so dumb. While greedy algorithms are not
guaranteed to get you the right answer all the
time, they're often very good to use. And what they're good about is,
they're easy to implement. And they're fast to run. You can imagine coding
the solution up and it's pretty easy. And when it runs, it's pretty
fast. Just takes the most valuable, the next most
valuable, the next most valuable, I'm done. And the thief leaves,
and is gone. So that's a good thing. On the other hand, it's often
the case in the world that that's not good enough. And we're not looking for an OK
solution, but we're looking for the best solution. Optimal means best. And that's
what the slow thief does. So the slow thief thinks
the following. Well, what I'll do is I'll
put stuff in the backpack until it's full. I'll compute its value. Then I'll empty the backpack
out, put another combination of stuff compute its value,
try all possible ways of filling up the backpack, and
then when I'm done, I'll know which was the best. And that's
the one I'll do. So he's packing and unpacking,
packing and unpacking, trying all possible combinations of
objects that will obey the constraint. And then choosing the winner. Well, this is like an algorithm
we've seen before. It's not greedy. What is this? What category of algorithm
is that? Somebody? Louder? STUDENT: Brute force. PROFESSOR: Brute force,
exhaustive enumeration, exactly. We're exhausting all
possibilities. And then choosing the winner. Well, that's what the
slow thief tried. Unfortunately it took so long
that before he finished the owner returned home, called the
police and the thief ended up in jail. It happens. Fortunately, while sitting in
jail awaiting trial, the slow thief decided to figure
what was wrong. And, amazingly enough, he
had studied mathematics. And had a blackboard
in the cell. So he was able to work it out. So he first said, well, let me
try and figure out what I was really doing and why
it took so long. So first, let's think about what
was the function the slow thief was attempting
to maximize. The summation, from i equals 1
to n, where n is the number of items, so I might label watch
1-0, watch 2-2, I don't care that they're both watches. They're two separate items.
And then what I want to maximize is the sum of the price
of item i times whether or not I took x i. So think of x as a vector
of 0's and 1's. Hence the name of the problem. If I'm going to keep that item,
item i, if I'm going to take it, I give it a 1. If I'm not going to take
it I give it a 0. And so I just take the value of
that item times whether or not it's 0 or 1. So my goal here is to choose x
such that this is maximized. Choose x such that that function
is maximized, subject to a constraint. And the constraint, is it the
sum from 1 to n of the weight of the item, times x i, is less
than or equal to c, the maximum weight I'm allowed
to put in my backpack. In this case it was eight. So now I have a nice, tidy
mathematical formulation of the problem. And that's often the first step
in problem reduction. Is to go from a problem that has
a bunch of words, and try and write it down as a nice,
tight mathematical formulation. So now I know the formulation
is to find x, the vector x, such that this constraint
is obeyed and this function is maximized. That make sense to everybody? Any questions about that? Great. So as the thief had thought,
we can clearly solve this problem by generating all
possible values of x and seeing which one solves
this problem. But now the thief started
scratching his head and said, well, how many possible
values of x are there? Well, how can we think
about that? Well, a nice way to think
about that is to say, I've got a vector. So there's the vector with
eight 0's in it. Three, four, five, six,
seven, eight. Indicating I didn't take
any of the items. There's the vector, and again
you'll see this in your handout, says I only took
the first item. There's the one that says I
only took the second item. There's the one that says
I took the first and the second item. And at the last,
I have all 1's. This series look like anything
familiar to you? These are binary
numbers, right? Eight digit binary numbers. Zero, one, two, three. What's the biggest number
I can represent with eight of these? Somebody? Well, suppose we had
decimal numbers. And I said I'm giving you
three decimal digits. What's the biggest number you
can represent with three decimal digits? Pardon? STUDENT: [INAUDIBLE] PROFESSOR: Right. Which is roughly what? 10 to the? STUDENT: [INAUDIBLE] PROFESSOR: Right. Yes. STUDENT: [INAUDIBLE] PROFESSOR: Right. Exactly right. So, in this case it's
2 to the 8th. Because I have eight digits. But exactly right. More generally, it's
2 to the n. Where n is the number of
possible items I have to choose from. Well, now that we've figured
that out, what we're seeing is that the brute force algorithm
is exponential. In the number of items we
have to choose from. Not in the number that we take,
but the number we have to think about taking. Exponential growth
is a scary thing. So now we can look at
these two graphs, look at the top one. So there, we've compared n
squared, quadratic growth, which by the way Professor
Grimson told you was bad, to, really bad, which is
exponential growth. And in fact, if you look at
the top figure it looks as exponential or, quadratic isn't
even growing at all. You see how really fast
exponential growth is? You get to fifteen items and
we're up at seventy thousand already and counting. The bottom graph has exactly
the same data. But what I've done is, I've use
the logarithmic y-axis. Later in the term, we'll spend
quite a lot of time talking about how do we visualize
data. How do we make sense of data. I've done that because you can
see here that the quadratic one is actually growing. It's just growing a
lot more slowly. So the moral here
is simple one. Exponential algorithms are
typically not useful. n does not have to get very big for
exponential to fail. Now, imagine that you're trying
to pack a ship and you've got ten thousand
items to choose from. 2 to the 10,000 is a
really big number. So what we see immediately,
and the slow thief decided just before being incarcerated
for years and years, was that it wasn't possible to
do it that way. He threw up his hands and
said, it's an unsolvable problem, I should have been
greedy, there's no good way to do this. That gets us to the
smart thief. Why is this thief smart? Because she took 600. And she learned that in fact
there is a good way to solve this problem. And that's what we're going
to talk about next. And that's something called
dynamic programming. A lot of people think this is
a really hard and fancy concept, and they teach in
advanced algorithms classes. And they do, but in fact
as you'll see it's really pretty simple. A word of warning. Don't try and figure out
why it's called dynamic programming. It makes no sense at all. It was invented by a
mathematician called Bellman. And he was at the time being
paid by the Defense Department to work on something else. And he didn't want them to
know what he was doing. So he made up a name that he
was sure they would have no clue what it meant. Unfortunately, we now have
lived with it forever, so don't think of it as actually
being anything dynamic particularly. It's just a name. It's very useful, and why we
spend time on it for solving a broad range of problems that
on their surface are exponentially difficult. And, in fact, getting very
fast solutions to them. The key thing in dynamic
programming, and we'll return to both of these, is you're
looking for a situation where there are overlapping
sub-problems and what's called optimal substructure. Don't expect to know what
these mean yet. Hopefully by the end of the day,
Tuesday, they will both make great sense to you. Let's first look at
the overlapping sub-problems example. You have on your handout,
a recursive implementation of Fibonacci. Which we've seen before. What I've done is I've augmented
it with this global called num calls just so I can
keep track of how many times it gets called. And let's see what happens
if we call fib of 5. Well, quite a few steps. What's the thing that you notice
about this output? There's something here that
should tip us off that maybe we're not doing this the most
efficient way possible. I called fib with 5 and then
it calls it with 4. And then that call
calls fib with 3. So I'm doing 4, 3, 2, 2, 1, 0. And I'm doing 1, 2. Well, what we see is I'm calling
fib a lot of times with the same argument. And it makes sense. Because I start with fib of 5,
and then I have to do fib of 4 and fib of 3. Well, fib of 4 is going to also
have to do a fib of 3 and a fib of 2 and a fib of
1, and a fib of 0. And then the fib of 3 is going
to do a fib of 2 and a fib of 1 and a fib of 0. And so I'm doing the same thing
over and over again. That's because I have what
are called overlapping sub-problems. I have used divide
and conquer, as we seen before, to recursively break it
into smaller problems. But the smaller problem of fib of
4 and the smaller problem of fib of 3 overlap with
each other. And that leads to a lot of
redundant computation. And I've done fib of 5, which
is a small number. If we look at some other things,
for example, let's get rid of this. Let's try see fib of 10. Well, there's a reason I chose
10 rather than, say, 20. Here fib got called 177 times. Roughly speaking, the analysis
of fib is actually quite complex, of a recursive fib. And I won't go through it, but
what you can see is it's more or less, it is in fact,
exponential. But it's not 2 to
the something. It's a more complicated thing. But it grows quite quickly. And the reason it does is
because of this overlapping. On Tuesday we'll talk about a
different way to implement Fibonacci, where the growth will
be much less dramatic. Thank you.

Linear Algebra

As promised, we will now start
developing generalizations of the different calculations that
we carried out in the context of the radar example. The first kind of calculation
that we carried out goes under the name of the multiplication
rule. And it goes as follows. Our starting point is the
definition of conditional probabilities. The conditional probability of
A given another event, B, is the probability that both events
have occurred divided by the probability of the
conditioning event. We now take the denominator term
and send it to the other side of this equality to obtain
this relation, which we can interpret as follows. The probability that two events
occur is equal to the probability that a first event
occurs, event B in this case, times the conditional
probability that the second event, event A, occurs, given
that event B has occurred. Now, out of the two events, A
and B, we're of course free to choose which one we call the
first event and which one we call the second event. So the probability of the two
events happening is also equal to an expression of this form,
the probability that A occurs times the conditional
probability that B occurs, given that A has occurred. We used this formula in the
context of a tree diagram. And we used it to calculate the
probability of a leaf of this tree by multiplying the
probability of taking this branch, the probability that A
occurs, times the conditional probability of taking this
branch, the probability that event B also occurs given that
event A has occurred. How do we generalize
this calculation? Consider a situation in which
the experiment has an additional third stage that has
to do with another event, C, that may or may not occur. For example, if we have arrived
here, A and B have both occurred. And then C also occurs, then we
reach this particular leaf of the tree. Or there could be
other scenarios. For example, it could be the
case that A did not occur. Then event B occurred, and
finally, event C did not occur, in which case we end up
at this particular leaf. What is the probability of
this scenario happening? Let us try to do a calculation
similar to the one that we used for the case
of two events. However, we need to deal
here with three events. What should we do? Well, we look at the
intersection of these three events and think of it as the
intersection of a composite event, A complement intersection
B, then intersected with the
event C complement. Clearly, you can form the
intersection of three events by first taking the intersection
of two of them and then intersecting
with a third. After we group things this way,
we're dealing with the probability of two events
happening, this composite event and this ordinary event. And the probability of two
events happening is equal to the probability that the first
event happens, and then the probability that the second
event happens, given that the first one has happened. Can we simplify this
even further? Yes. The first term is
the probability of two events happening. So it can be simplified further
as the probability that A complement occurs times
the conditional probability that B occurs, given that A
complement has occurred. And then we carry over
the last term exactly the way it is. The conclusion is that we can
calculate the probability of this leaf by multiplying the
probability of the first branch times the conditional
probability of the second branch, given that the first
branch was taken, and then finally multiply with the
probability of the third branch, which is the probability
that C complement occurs, given that A
complement and B have already occurred. In other words, we can calculate
the probability of a leaf by just multiplying the
probabilities of the different branches involved and where we
use conditional probabilities for the intermediate branches. At this point, you can use your
imagination to see that such a formula should also be
valid for the case of more than three events. The probability that a bunch of
events all occur should be the probability of the first
event times a number of factors, each corresponding
to a branch in a tree of this kind. In particular, the probability
that events A1, A2, up to An all occur is going to be the
probability that the first event occurs times a product of
conditional probabilities that the i-th event occurs,
given that all of the previous events have already occurred. And we obtain a term of this
kind for every event, Ai, after the first one, so this
product ranges from 2 up to n. And this is the most general
version of the multiplication rule and allows you to calculate
the probability of several events happening by
multiplying probabilities and conditional probabilities.

Linear Algebra

-- one and -- the lecture on
symmetric matrixes. So that's the most
important class of matrixes, symmetric matrixes. A equals A transpose. So the first points, the
main points of the lecture I'll tell you right away. What's special about
the eigenvalues? What's special about
the eigenvectors? This is -- the way we
now look at a matrix. We want to know about its
eigenvalues and eigenvectors and if we have a
special type of matrix, that should tell us
something about eigenvalues and eigenvectors. Like Markov matrixes, they
have an eigenvalue equal one. Now symmetric matrixes, can I
just tell you right off what the main facts -- the
two main facts are? The eigenvalues of a
symmetric matrix, real -- this is a real
symmetric matrix, we -- talking mostly
about real matrixes. The eigenvalues are also real. So our examples of
rotation matrixes, where -- where we got E- eigenvalues
that were complex, that won't happen now. For symmetric matrixes,
the eigenvalues are real and the eigenvectors
are also very special. The eigenvectors are
perpendicular, orthogonal, so which do you prefer? I'll say perpendicular. Perp- well, they're
both long words. Okay, right. So -- I have a --
you should say "why?" and I'll at least answer why
for case one, maybe case two, the checking the Eigen --
that the eigenvectors are perpendicular, I'll leave
to, the -- to the book. But let's just realize what -- well, first I have to say, it -- it could happen, like for
the identity matrix -- there's a symmetric matrix. Its eigenvalues are
certainly all real, they're all one for
the identity matrix. What about the eigenvectors? Well, for the identity, every
vector is an eigenvector. So how can I say
they're perpendicular? What I really mean
is the -- they -- this word are should really
be written can be chosen perpendicular. That is, if we have --
it's the usual case. If the eigenvalues
are all different, then each eigenvalue has
one line of eigenvectors and those lines are
perpendicular here. But if an eigenvalue's
repeated, then there's a whole plane of eigenvectors
and all I'm saying is that in that plain, we can
choose perpendicular ones. So that's why it's a can
be chosen part, is -- this is in the case of a
repeated eigenvalue where there's some real,
substantial freedom. But the typical case is
different eigenvalues, all real, one dimensional
eigenvector space, Eigen spaces, and
all perpendicular. So, just -- let's just
see the conclusion. If we accept those as
correct, what happens -- and I also mean that
there's a full set of them. so forgive me for doing such
a thing, but, I'll look at the I -- so that's part of this
picture here, that there -- there's a complete
set of eigenvectors, perpendicular ones. So, having a complete set
of eigenvectors means -- so normal -- so the usual -- maybe I put
the -- usually -- usual -- usual case is that the matrix
A we can write in terms of its eigenvalue matrix and its
eigenvector matrix this way, right? We can do that in
the usual case, but now what's special when
the matrix is symmetric? So this is the
usual case, and now let me go to the symmetric case. So in the symmetric
case, A, this -- this should become
somehow a little special. Well, the lambdas on the
diagonal are still on the diagonal. They're -- they're real,
but that's where they are. What about the
eigenvector matrix? So what can I do now special
about the eigenvector matrix when -- when the A
itself is symmetric, that says something good
about the eigenvector matrix, so what is this --
what does this lead to? This -- these perpendicular
eigenvectors, I can not only -- I can not only guarantee
they're perpendicular, I could also make them
unit vectors, no problem, just s- scale their length to one. So what do I have? I have orthonormal eigenvectors. And what does that tell me
about the eigenvector matrix? What -- what letter should
I now use in place of S -- I've got -- those two equations
are identical,1 remember S has the eigenvectors in its columns,
but now those columns are orthonormal, so the
right letter to use is Q. So that's where -- so we've got
the letter all set up, book. so this should be
Q lambda Q inverse. Q standing in our minds always
for this matrix -- in this case it's square, it's -- so these are the Okay.
columns of Q, of course. And one more thing. What's Q inverse? For a matrix that has
these orthonormal columns, So I took the dot product
-- ye, somehow, it didn't -- I we know that the inverse
is the same as the transpose. So here is the beautiful -- there is the -- the great
haven't learned anything. description, the factorization
of a symmetric matrix. And this is, like, one
of the famous theorems of linear algebra, that if
I have a symmetric matrix, it can be factored in this form. An orthogonal matrix
times diagonal times the transpose of that
orthogonal matrix. And, of course, everybody
immediately says yes, and if this is
possible, then that's clearly symmetric, right? That -- take -- we've looked
at products of three guys like that and taken their transpose
and we got it back again. So do you -- do you see
the beauty of this -- of this factorization, then? It -- it completely displays
the eigenvalues and eigenvectors the symmetry of the -- of
the whole thing, because -- that product, Q times
lambda times Q transpose, if I transpose it, it -- this
comes in this position and we get that matrix back again. So that's -- in mathematics,
that's called the spectral Spectrum is the set of
eigenvalues of a matrix. theorem. Spec- it somehow comes from the
idea of the spectrum of light as a combination
of pure things -- where our matrix is broken
down into pure eigenvalues and eigenvectors -- in mechanics it's often called
the principle axis theorem. It's very useful. It means that if you have -- we'll see it geometrically. It means that if I
have some material -- if I look at the right axis, it
becomes diagonal, it becomes -- the -- the I- I've done something dumb,
because I've got the -- I should've taken the dot
product of this guy here with -- that's directions
don't couple together. Okay. So that's -- that -- that's
what to remember from -- from this lecture. Now, I would like to say why
are the eigenvalues real? Can I do that? So -- so -- because that --
something useful comes out. So I'll just come back --
come to that question why real eigenvalues? Okay. So I have to start
from the only thing we know, Ax equal lambda x. Okay. But as far as I
know at this moment, lambda could be complex. I'm going to prove it's not
-- and x could be complex. In fact, for the moment,
even A could be -- we could even think, well,
what happens if A is complex? Well, one thing we can
always do -- this is -- this is like always -- always okay -- I can -- if I have an equation,
I can take the complex conjugate of everything. That's -- no -- no -- so A
conjugate x conjugate equal lambda conjugate x conjugate, it
just means that everywhere over here that there was a -- an
equals x bar transpose lambda bar x bar. i, then here
I changed it to a-i. That's -- that -- you
know that that step -- that conjugate
business, that a+ib, if I conjugate it it's a-ib. That's the meaning of conjugate
-- and products behave right, I can conjugate every factor. So I haven't done anything yet
except to say what would be true if, x -- in any case, even
if x and lambda were complex. Of course, our -- we're
speaking about real matrixes A, so I can take that out. Actually, this already tells me
something about real matrixes. I haven't used any
assumption of A -- A transpose yet. Symmetry is waiting in
the wings to be used. This tells me that if a real
matrix has an eigenvalue lambda what I was going to do. and an
eigenvector x, it also has -- another of its
eigenvalues is lambda bar with eigenvector x bar. Real matrixes, the eigenvalues
come in lambda, lambda bar -- the complex eigenvalues come
in lambda and lambda bar pairs. But, of course,
I'm aiming to show that they're not complex at all,
here, by getting symmetry in. So how I going to use symmetry? I'm going to transpose
this equation to x bar transpose A transpose equals
x bar transpose lambda bar. That's just a number, so I don't
mind wear I put that number. This is -- this is -- this is a -- then again okay. Ax equals lambda x bar
transpose x, right? But now I'm ready
to use symmetry. I'm ready -- so this
was all just mechanics. Now -- now comes the
moment to say, okay, if the matrix is this
from the right with x bar, I get x bar transpose
Ax bar symmetric, then this A transpose
is the same as A. You see, at that moment
I used the assumption. Now let me finish
the discussion. Here -- here's the way I finish. I look at this original equation
and I take the inner product. I multiply both sides by -- oh, maybe I'll do
it with this one. I take -- I multiply both sides
by x bar transpose. x bar transpose Ax
bar equals lambda bar x bar transpose x bar. Okay, fine. All right, now
what's the other one? Oh, for the other one I'll
probably use this guy. A- I happy about this? No. For some reason I'm not. I'm -- I want to -- if I take the inner
product of Okay. So that -- that
was -- that's fine. That comes directly from that,
multiplying both sides by x bar transpose, but now
I'd like to get -- why do I have x bars over there? Oh, yes. Forget this. Okay. On this one -- right. On this one, I took it like
that, I multiply on the right by x. That's the idea. Okay. Now why I happier with
this situation now? A proof is coming here. Because I compare this
guy with this one. And they have the
same left hand side. So they have the
same right hand side. So comparing those two, can -- I'll raise the board to
do this comparison -- this thing, lambda
x bar transpose x is equal to lambda
bar x bar transpose x. Okay. And the conclusion
I'm going to reach -- I -- I on the right track here? The conclusion
I'm going to reach is lambda equal lambda bar. I would have to track down the
other possibility that this -- this thing is
zero, but let me -- oh -- oh, yes, that's important. It's not zero. So once I know that this
isn't zero, I just cancel it and I learn that lambda
equals lambda bar. And so what can you -- do you -- have you got the
reasoning altogether? What does this tell us? Lambda's an eigenvalue
of this symmetric matrix. We've just proved that
it equaled lambda bar, so we have just proved
that lambda is real, right? If, if a number is equal to
its own complex conjugate, then there's no
imaginary part at all. The number is real. So lambda is real. Good. Good. Now, what -- but it depended
on this little expression, on knowing that
that wasn't zero, so that I could cancel it out
-- so can we just take a second on that one? Because it's an
important quantity. x bar transpose x. Okay, now remember, as far
as we know, x is complex. So this is -- here -- x is complex, x
has these components, x1, x2 down to xn. And x bar transpose, well, it's
transposed and it's conjugated, so that's x1 conjugated x2
conjugated up to xn conjugated. I'm -- I'm -- I'm really reminding
you of crucial facts about complex numbers
that are going to come into the next
lecture as well as this one. So w- what can you tell
me about that product -- I -- I guess what
I'm trying to say is, if I had a complex vector, this
would be the quantity I would -- I would like. This is the quantity I like. I would take the vector times
its transpose -- now what -- what happens usually if I take a
vector -- a -- a -- x transpose x? I mean, that's a quantity we
see all the time, x transpose x. That's the length
of x squared, right? That's this positive length
squared, it's Pythagoras, it's x1 squared plus
x2 squared and so on. Now our vector's complex,
and you see the effect? I'm conjugating
one of these guys. So now when I do
this multiplication, I have x1 bar times x1 and
x2 bar times x2 and so on. So this is an --
this is sum a+ib. And this is sum a-ib. I mean, what's the point here? What's the point -- when
I multiply a number by its conjugate, a complex number by
its conjugate, what do I get? I get a n- the -- the
imaginary part is gone. When I multiply a+ib by
its conjugate, what's -- what's the result of that -- of
each of those separate little multiplications? There's an a squared and -- and
what -- how many -- what's -- b squared comes in
with a plus or a minus? A plus. i times minus i is
a plus b squared. And what about the
imaginary part? Gone, right? An iab and a minus iab. So this -- this is
the right thing to do. If you want a decent answer,
then multiply numbers by their conjugates. Multiply vectors by the
conjugates of x transpose. So this quantity is positive,
this quantity is positive -- the whole thing is positive
except for the zero vector and that allows me to know
that this is a positive number, which I safely cancel out
and I reach the conclusion. So actually, in this discussion
here, I've done two things. If I reached the
conclusion that lambda's real, which I wanted to do. But at the same time,
we sort of saw what to do if things were complex. If a vector is complex,
then it's x bar transpose x, this is its length squared. And as I said, the next
lecture Monday, we'll -- we'll repeat that this is
the right thing and then do the right thing for
matrixes and all other -- all other, complex
possibilities. Okay. But the main point, then,
is that the eigenvalues of a symmetric matrix, it
just -- do you -- do -- where did we use
symmetry, by the way? We used it here, right? Let -- can I just -- let -- suppose A was a complex. Suppose A had been
a complex number. Could -- could I have
made all this work? If A was a complex
number -- complex matrix, then here I should
have written A bar. I erased the bar because
I assumed A was real. But now let's suppose
for a moment it's not. Then when I took this
step, what should I have? What did I do on that step? I transposed. So I should have
A bar transpose. In the symmetric
case, that was A, and that's what made
everything work, right? This -- this led
immediately to that. This one led immediately to
this when the matrix was real, so that didn't matter,
and it was symmetric, so that didn't matter. Then I got A. But -- so now I
just get to ask you. Suppose the matrix
had been complex. What's the right equivalent
of sym- symmetry? So the good matrix --
so here, let me say -- good matrixes -- by good I mean
real lambdas and perpendicular x-s. And tell me now, which
matrixes are good? If they're -- If they're real
matrixes, the good ones are symmetric, because then
everything went through. The -- so the good -- I'm saying now what's good. This is -- this is -- these
are the good matrixes. They have real eigenvalues,
perpendicular eigenvectors -- good means A equal
A transpose if real. Then -- then that was
what -- our proof worked. But if A is complex, all -- our
proof will still work provided A bar transpose is A. Do you see what I'm saying? I'm saying if we have complex
matrixes and we want to say are they -- are they as good
as symmetric matrixes, then we should not only
transpose the thing, but conjugate it. Those are good matrixes. And of course,
the most important s- the most important
case is when they're real, this part doesn't
matter and I just have A equal A transpose symmetric. Do you -- I -- I'll just repeat that. The good matrixes, if
complex, are these. If real, that doesn't
make any difference so I'm just saying symmetric. And of course, 99% of
examples and applications to the matrixes are real
and we don't have that and then symmetric
is the key property. Okay. So that -- that's, these main
facts and now let me just -- let me just -- so that's
this x bar transpose x, okay. So I'll just, write it
once more in this form. So perpendicular orthonormal
eigenvectors, real eigenvalues, transposes of
orthonormal eigenvectors. That's the symmetric
case, A equal A transpose. Okay. Good. Actually, I'll even
take one more step here. Suppose -- I -- I can break this down
to show you really what that says about
a symmetric matrix. I can break that down. Let me here -- here
go these eigenvectors. I -- here go these eigenvalues,
lambda one, lambda two and so on. Here go these
eigenvectors transposed. And what happens if I actually
do out that multiplication? Do you see what will happen? There's lambda one
times q1 transpose. So the first row here is
just lambda one q1 transpose. If I multiply
column times row -- you remember I could do that? When I multiply matrixes, I can
multiply columns times rows? So when I do that, I
get lambda one and then the column and then
the row and then lambda two and then
the column and the row. Every symmetric matrix
breaks up into these pieces. So these pieces have real
lambdas and they have these Eigen -- these
orthonormal eigenvectors. And, maybe you even could
tell me what kind of a matrix have I got there? Suppose I take a unit
vector times its transpose? So column times row,
I'm getting a matrix. That's a matrix
with a special name. What's it's -- what
kind of a matrix is it? We've seen those matrixes,
now, in chapter four. It's -- is A A transpose
with a unit vector, so I don't have to
divide by A transpose A. That matrix is a
projection matrix. That's a projection matrix. It's symmetric and if I square
it there'll be another -- there'll be a q1 transpose
q1, which is one. So I'll get that
matrix back again. Every -- so every
symmetric matrix -- every symmetric matrix
is a combination of -- of mutually perpendicular --
so perpendicular projection matrixes. Projection matrixes. Okay. That's another way
that people like to think of the
spectral theorem, that every symmetric matrix
can be broken up that way. That -- I guess
at this moment -- first I haven't done an example. I could create a symmetric
matrix, check that it's -- find its eigenvalues,
they would come out real, find its eigenvectors, they
would come out perpendicular and you would see it in numbers,
but maybe I'll leave it here for the moment in letters. Oh, I -- maybe I will do it
with numbers, for this reason. Because there's one
more remarkable fact. Can I just put this
further great fact about symmetric
matrixes on the board? When I have
symmetric matrixes, I know their eigenvalues are
So then I can get interested in the question are they
positive real. or negative? And you remember why
that's important. For differential equations,
that decides between instability and stability. So I'm -- after I
know they're real, then the next question
is are they positive, are they negative? And I hate to have to compute
those eigenvalues to answer that question, right? Because computing the
eigenvalues of a symmetric matrix of order let's say 50 -- compute its 50 eigenvalues -- is a job. I mean, by pencil and paper
it's a lifetime's job. I mean, which -- and in fact,
a few years ago -- well, say, 20 years ago, or 30, nobody
really knew how to do it. I mean, so, like, science
was stuck on this problem. If you have a matrix
of order 50 or 100, how do you find its eigenvalues? Numerically, now,
I'm just saying, because pencil and paper is --
we're going to run out of time or paper or something
before we get it. Well -- and you
might think, okay, get Matlab to compute the
determinant of lambda minus A, A minus lambda I, this
polynomial of 50th degree, and then find the roots. Matlab will do it,
but it will complain, because it's a very bad way
to find the eigenvalues. I'm sorry to be saying
this, because it's the way I taught you to do it, right? I taught you to
find the eigenvalues by doing that
determinant and taking the roots of that polynomial. But now I'm saying, okay,
I really meant that for two by twos and three
by threes but I didn't mean you to
do it on a 50 by 50 and you're not too
unhappy, probably, because you didn't
want to do it. But -- good, because it would
be a very unstable way -- the 50 answers that would come
out would be highly unreliable. So, new ways are -- are
much better to find those 50 eigenvalues. That's a -- that's a part
of numerical linear algebra. But here's the
remarkable fact -- that Matlab would quite happily
find the 50 pivots, right? Now the pivots are not the
same as the eigenvalues. But here's the great thing. If I had a real matrix, I
could find those 50 pivots and I could see maybe
28 of them are positive and 22 are negative pivots. And I can compute those
safely and quickly. And the great fact is that 28
of the eigenvalues would be positive and 22
would be negative -- that the sines of the pivots
-- so this is, like -- I hope you think this --
this is kind of a nice thing, that the sines of the pivots -- for symmetric, I'm always
talking about symmetric matrixes -- so I'm really, like,
trying to convince you that symmetric matrixes
are better than the rest. So the sines of the pivots
are same as the sines of the eigenvalues. The same number. The number of pivots
greater than zero, the number of positive
pivots is equal to the number of positive eigenvalues. So that, actually, is a very
useful -- that gives you a g- a good start on a decent
way to compute eigenvalues, because you can
narrow them down, you can find out how
many are positive, how many are negative. Then you could shift the matrix
by seven times the identity. That would shift all the
eigenvalues by seven. Then you could take the
pivots of that matrix and you would know how many
eigenvalues of the original were above seven and below seven. So this -- this neat
little theorem, that, symmetric matrixes have this
connection between the -- nobody's mixing up and thinking
the pivots are the eigenvalues -- I mean, the only
thing I can think of is the product of
the pivots equals the product of the
eigenvalues, why is that? So if I asked you for
the reason on that, why is the product of the
pivots for a symmetric matrix the same as the product
of the eigenvalues? Because they both
equal the determinant. Right. The product of the pivots
gives the determinant if no row exchanges, the
product of the eigenvalues always gives the determinant. So -- so the products -- but
that doesn't tell you anything about the 50 individual
ones, which this does. Okay. So that's -- those are essential
facts about symmetric matrixes. Okay. Now I -- I said in the -- in
the lecture description that I would take the last minutes
to start on positive definite matrixes, because
we're right there, we're ready to say what's
a positive definite matrix? It's symmetric, first of all. On -- always I will
mean symmetric. So this is the -- this is
the next section of the book. It's about this -- if symmetric matrixes are
good, which was, like, the point of my lecture
so far, then positive, definite matrixes are -- a subclass that are
excellent, okay. Just the greatest. so what are they? They're matrixes --
they're symmetric matrixes, so all their
eigenvalues are real. You can guess what they are. These are symmetric
matrixes with all -- the eigenvalues are -- okay, tell me what to write. What -- well, it --
it's hinted, of course, by the name for these things. All the eigenvalues
are positive. Okay. Tell me about the pivots. We can check the eigenvalues
or we can check the pivots. All the pivots are what? And then I'll -- then I'll finally
give an example. I feel awful that I have got
to this point in the lecture and I haven't given you a single example. So let me give you one. Five three two two. That's symmetric, fine. It's eigenvalues
are real, for sure. But more than that, I know the
sines of those eigenvalues. And also I know the
sines of those pivots, so what's the deal
with the pivots? The Ei- if the eigenvalues are
all positive and if this little fact is true that the pivots
and eigenvalues have the same sines, then this must be true
-- all the pivots are positive. And that's the good way to test. This is the good
test, because I can -- what are the pivots
for that matrix? The pivots for that
matrix are five. So pivots are five and
what's the second pivot? Have we, like, noticed the
formula for the second pivot in a matrix? It doesn't necessarily
-- you know, it may come out a
fraction for sure, but what is that fraction? Can you tell me? Well, here, the product of
the pivots is the determinant. What's the determinant
of this matrix? Eleven? So the second pivot must
be eleven over five, so that the product is eleven. They're both positive. Then I know that the
eigenvalues of that matrix are both positive. What are the eigenvalues? Well, I've got to take
the roots of -- you know, do I put in a minus lambda? You mentally do this -- lambda
squared minus how many lambdas? Eight? Right. Five and three, the
trace comes in there, plus what number comes here? The determinant, the
eleven, so I set that to zero. So the eigenvalues are -- let's see, half of that is four,
look at that positive number, plus or minus the square
root of sixteen minus eleven, I think five. The eigenvalues -- well, two
by two they're not so terrible, but they're not so perfect. Pivots are really simple. And this is a -- this is the
family of matrixes that you really want in
differential equations, because you know the
sines of the eigenvalues, so you know the
stability or not. Okay. There's one other related
fact I can pop in here in -- in the time available for
positive definite matrixes. The related fact is to ask
you about determinants. So what's the determinant? What can you tell
me if I -- remember, positive definite means all
eigenvalues are positive, all pivots are positive, so
what can you tell me about the determinant? It's positive, too. But somehow that --
that's not quite enough. Here -- here's a matrix
minus one minus three, what's the determinant
of that guy? It's positive, right? Is this a positive,
definite matrix? Are the pivots --
what are the pivots? Well, negative. What are the eigenvalues? Well, they're also the same. So somehow I don't just want
the determinant of the whole matrix. Here is eleven, that's great. Here the determinant
of the whole matrix is three, that's positive. I also -- I've got to check,
like, little sub-determinants, say maybe coming
down from the left. So the one by one and the two
by two have to be positive. So there -- that's
where I get the all. All -- can I call them
sub-determinants -- are -- see, I have to -- I need to make the thing plural. I need to test n things, not
just the big determinant. All sub-determinants
are positive. Then I'm okay. Then I'm okay. This passes the test. Five is positive and
eleven is positive. This fails the test because that
minus one there is negative. And then the big determinant
is positive three. So t- this -- these -- this fact -- you see
that actually the course, like, coming together. And that's really my point now. In the next -- in this lecture
and particularly next Wednesday and Friday, the
course comes together. These pivots that we
met in the first week, these determinants that we met
in the middle of the course, these eigenvalues that
we met most recently -- all matrixes are square here,
so coming together for square matrixes means these three
pieces come together and they come together in that
beautiful fact, that if -- that all the -- that
if I have one of these, I have the others. That if I -- but for symmetric matrixes. So that -- this will be the
positive definite section and then the real climax of the
course is to make everything come together for
n by n matrixes, not necessarily symmetric -- bring everything
together there and that will be the final thing. Okay. So have a great
weekend and don't forget symmetric matrixes. Thanks.

Math for Eng.

Let's do some more examples
with exact differential equations. And I'm getting these problems
from page 80 of my old college differential equations books. This is the fifth edition of
Elementary Differential Equations by William Boyce
and Richard DiPrima. I want to make sure they get
credit, that I'm not making up these problems. I'm getting
it from their book. Anyway, so I'm just going to
give a bunch of equations. We have to figure out if they're
exact, and if they are exact, we'll use what we know
about exact differential equations to figure out
their solutions. So the first one they have is,
2x plus 3, plus 2y minus 2, times y prime is equal to 0. So this is our M of x and y--
although, this is only a function of x-- and then
this is our N, right? You could say that's
M, or that's N. You could also say that, if this
is exact-- well, first let's [? test ?] this exact, before we start
talking about psi. So what's the partial of this,
with respect to y? The partial of M, with
respect to y. Well, there's no y
here, so it's 0. The rate of change that
this changes with respect to y is 0. And what's the rate of
change this changes, with respect to x? The partial of N, with respect
to x is equal to-- well, there's no x here, right? So these are just constants from
an x point of view, so this is all going to be 0. But we do see that
they're both 0. So M sub y, or the partial with
respect to y, is equal to the partial with respect to x. So this is exact. And actually, we don't have to
use exact equations here. We'll do it, just so that
we get used to it. But if you look here, you
actually could have figured out that this is actually
a separable equation. But anyway, this is exact. So knowing that it's exact, it
tells us that there's some function psi, where psi is
a function of x and y. Where psi sub x is equal to this
function, is equal to 2x plus 3, and psi-- I shouldn't
say sub x. I say the partial of psi,
with respect to x. And the partial of psi, with
respect to y, is equal to this, 2y minus 2. And if we can find our psi, we
know that this is just the derivative of psi. Because we know that the
derivative, with respect to x of psi, is equal to the partial
of psi, with respect to x, plus the partial
of psi, with respect to y, times y prime. So this is this just the
same form as that. So if we can figure out y,
then we can rewrite this equation as dx, the derivative
of psi, with respect to x, is equal to 0. Let me switch colors, or it's
going to get monotonous. This right here, if we can find
a psi, where the partial with respect to x, is this, the
partial with respect to y, is this, then this can
be rewritten as this. And how do we know that? Because the derivative of psi,
with respect to x, using the partial derivative chain
rules, is this. This partial with respect
to x, that's this. This partial with respect to
y, is this, times y prime. So this is the whole point
of exact equations. But anyway, so let's figure
out what our psi is. Actually, before we figure out,
if the derivative of psi, with respect to x, is 0, then
if you integrate both sides, you just-- the solution
of this equation is psi is equal to c. So using this information, if we
can solve for psi, then we know that the solution of this
differential equation is psi is equal to c. And then if we have some initial
conditions, we could solve for c. So let's solve for psi. So let's integrate both sides
of this equation, with respect to x. And then we get psi is equal
to x squared plus 3x, plus some function of y. Let's call it h of y. And remember, normally when you
take an antiderivative, you have just a plus
c here, right? But you can kind of say we
took an anti-partial derivative. So when you took a partial
derivative, with respect to x, not only do you lose constants--
that's why we have a plus c, normally-- but you
also lose anything that's a function of just y, and not x. So for example, take the partial
derivative of this with respect to x, you're going
to get this, right? Because the partial derivative
of a function, purely of y, with respect to x,
is going to be 0. So it will disappear. So anyway, we take the
antiderivative of this, we get this. Now, we use this information. Well, we use this information. We take the partial of this
expression, and we say, well, the partial of this expression,
with respect to y, has to equal this, and then we
can solve for h of y, then we'll be done. So let's do that. So the partial of psi, with
respect to y, is equal to-- well, that's going
to be 0, 0, 0. This part is a function of x. If you take the partial with
respect to y, it's 0, because these are constants, from
a y point of view. So you're left with
h prime of y. So we know that h prime of y,
which is the partial of psi, with respect to y,
is equal to this. So h prime of y is equal
to 2y minus 2. And then if we wanted to figure
out what h of y is, we get h of y-- just integrate both
sides, with respect to y-- is equal to y squared
plus-- sorry-- y squared minus 2y. Now, you could have a plus c
there, but if you watched the previous example, you'll see
that that c kind of merges with the other c, so you
don't have to worry about it right now. So what is our psi function,
as we know it now, not worrying about the plus c? It is psi of x and y is equal
to x squared plus 3x, plus h of y-- which we figured
out is this-- plus y squared, minus 2y. And we know a solution of our
original differential equation is psi is equal to c. So the solution of our
differential equation is this is equal to c. x squared plus 3x, plus
y squared, minus 2y is equal to c. If you had some additional
conditions, you could test it. And I encourage you to test
this out on this original equation, or I encourage you to
take the derivative of psi, and prove to yourself that if
you took the derivative of psi, with respect to x, here,
implicitly, that you would get this differential equation. Anyway, let's do another one. Let's clear image. So the more examples you
see, the better. So let's see, this one says 2x
plus 4y, plus 2x minus 2y, y prime is equal to 0. So what's the partial of
this with respect to y? So M, the partial of M, with
respect to y-- this is 0-- so it's equal to 4. What's the partial of this, with
respect to x, just this part right here? The partial of N, with
respect to x, is 2. This is 0. So the partial of this, with
respect to y, is different than the partial of N,
with respect to x. So this is not exact. So we can't solve this using
our exact methodology. So that was a fairly
straightforward problem. Let's do another one. Let's see. I'm running out of time, so I
want to do one that's not too complicated. Let's see, 3x squared minus
2xy-- actually, let me do this in the next problem. I don't want to rush
these things. I will continue this
in the next video. See you soon.

Statistics

  Hi. In this problem, we're going
to get a bunch of practice working with multiple random
variables together. And so we'll look at joint
PDFs, marginal PDFs, conditional PDFs, and also get
some practice calculating expectations as well. So the problem gives us a pair
of random variables-- x and y. And we're told that the joint
distribution is uniformly distributed on this triangle
here, with the vertices being 0, 0 1, 0, and 0, 1. So it's uniform in
this triangle. And the first part of the
problem is just to figure out what exactly is disjoint PDF of
the two random variables. So in this case, it's pretty
easy to calculate, because we have a uniform distribution. And remember, when you have a
uniform distribution, you can just imagine it being
a sort of plateau coming out of the board. And it's flat. And so the height of the
plateau, in order to calculate it, you just need to figure
out what the area of this thing is, of this triangle is. So remember, when you had single
random variables, what we had to do was calculate, for
uniform distribution, we had to integrate to 1. So you took the length, and you
took 1 over the length was the correct scaling factor. Here, you take the area. And the height has to make it so
that the entire volume here integrates to 1. So the joint PDF is just
going to be 1 over whatever this area is. And the area is pretty
simple to calculate. It's 1/2 base times height. So it's 1/2. And so what we have is
that the area is 1/2. And so the joint PDF of x and
y is going to equal 2. But remember, you always have
to be careful when writing these things to remember
the ranges when these things are valid. So it's only 2 within
this triangle. And outside of the
triangle, it's 0. So what exactly does inside
the triangle mean? Well, we can write it
more mathematically. So this diagonal line, it's
given by x plus y equals 1. So everything in the triangle
is really x plus y is less than or equal to 1. It means everything under
this triangle. And so we need x plus y to be
less then or equal to 1 and also x to be non-negative and
y to be non-negative.   So with these inequalities,
that captures everything within this triangle. And otherwise, the joint
PDF is going to be 0.   The next part asks us to find,
using this joint PDF, the marginal of y. And remember, when you have
a joint PDF of two random variables, you essentially have
everything that you need, because from this joint PDF, you
can calculate marginals, you can calculate from the
margins, you can calculate conditionals. The joint PDF captures
everything that there is to know about this pair of
random variables. Now, to calculate a marginal PDF
of y, remember a marginal really just means collapsing
the other random variable down. And so you can just imagine
taking this thing and collapsing it down
onto the y-axis. And mathematically, that is just
saying that we integrate out the other random variable.   So the other random variable
in this case will be x. We take x and we get rid of
it by integrating out from negative infinity to infinity. Of course, this joint PDF
is 0 in a lot of places. And so a lot of these
will be 0. And only for a certain range
of x's will this integral actually be non-zero. And so again, the other time
when we have to be careful is when we have these limits of
integration, we need to make sure that we have the
right limits. And so we know that the
joint PDF is 2. It's nonzero only within
this triangle. And so it's only 2 within
this triangle, which means what for x? Well, depending on what
x and y are, this will be either 2 or 0. So let's just fix
some value of y. Pretend that we've picked some
value y, let's say here. We want this value of y. Well, what are the values of x
such that the joint PDF for that value y is actually
nonzero, it's actually 2? Well, it's everything from
x equals 0 to whatever x value this is. But this x value, actually, if
you think about it, is just 1 minus y, because this line
is x plus y equals 1. So whatever y is, x is going
to be 1 minus that. And so the correct limits
would actually be from 0 to 1 minus y.   And then the rest of that
is pretty simple. You integrate this. This is a pretty simple
integral. And you get that it's actually
two times 1 minus y. That's a y.   But of course, again, we need to
make sure that we have the right regions. So this is not always true
for y, of course. This is only true for
y between 0 and 1. And otherwise, it's actually 0,
because when you take a y down here, well, there's no
values of x that will give you a nonzero joint PDF. And if you take a value of y
higher than this, the same thing happens.   So we can actually draw
this out and see what it looks like. So let's actually draw
a small picture here. Here's y. Here's the marginal PDF of y. And here's 2. And it actually looks
like this. It's a triangle and a 0
outside this range. So does that make sense? Well, first of all, you see
that actually does in fact integrates to 1,
which is good. And the other thing we notice
is that there is a higher density for smaller
values of y. So why is that? Why are smaller values
of y more likely than larger values of y? Well, because when you have
smaller values of y, you're down here. And it's more likely because
there are more values of x that go along with it that
make that value of y more likely to appear. Say you have a large
value of y. Then you're up here
at the tip. Well, there aren't very many
combinations of x and y that give you that large
a value of y. And so that large value of
y becomes less likely. Another way to think about it
is, when you collapse this down, there's a lot more stuff
to collapse down its base. There's a lot of x's
to collapse down. But up here, there's only a
very little bit of x to collapse down. And the PDF of y becomes
more skewed towards smaller values of y.   So now, the next thing that we
want to do is calculate the conditional PDF of x, given y. Well, let's just recall
what that means. This is what we're looking for--
the conditional PDF of x, given y. And remember, this is calculated
by taking the joint and dividing by the
marginal of y.   So we actually have the
top and the bottom. We have to joint PDF from part
A. And from part B, we calculated the marginal
PDF of y. So we have both pieces. So let's actually
plug them in. Again, the thing that you have
to be careful here is about the ranges of x and y where
these things are valid, because this is only non-zero
when x and y fall within this triangle. And this is only non-zero when
y is between 0 and 1. So we need to be careful. So the top, when it's
non-zero, it's 2. And the bottom, when it's
non-zero, it's 2 times 1 minus y. So we can simplify that to
be 1 over 1 minus y. And when is this true? Well, it's true when x and y are
in the triangle and y is between 0 and 1. So put another way, that means
that this is valid when y is between 0 and 1 and x is between
0 and 1 minus y, because whatever x has to be,
it has to be such that they actually still fall within
this triangle. And outside of this, it's 0.   So let's see what this
actually looks like.   So this is x, and this is the
conditional PDF of x, given y.   Let's say this is
1 right here.   Then what it's saying is, let's
say we're given that y is some little y. Let's say it's somewhere here. Then it's saying that the
conditional PDF of x given y is this thing. But notice that this value,
1 over 1 minus y, does not depend on x. So in fact, it actually
is uniform. So it's uniform between
0 and 1 minus y. And the height is something
like 1 over 1 minus y. And this is so that the scaling
makes it so that actually is a valid PDF, because
the integral is to 1. So why is the case? Why is that when you condition
on y being some value, you get that the PDF of x is
actually uniform? Well, when you look over here,
let's again just pretend that you're taking this value of y. Well, when you're conditioning
on y being this value, you're basically taking a slice of this
joint PDF at this point. But remember, the original
joint PDF was uniform. So when you take a slice of a
uniform distribution, joint uniform distribution,
you still get something that is uniform. Just imagine that you have
a cake that is flat. Now, you take a slice
at this level. Then whatever slice you have
is also going to be imagine being a flat rectangle. So it's still going
to be uniform. And that's why the conditional
PDF of x given y is also uniform.   Part D now asks us to find a
conditional expectation of x. So we want to find the
expectation of x, given that y is some little y. And for this, we can
use the definition. Remember, expectations are
really just weighted sums. Or in the [? continuous ?]
case, it's an integral. So you take the value. And then you weight
it by the density. And in this case, because we're
taking conditional a expectation, what we weight it
by is the conditional density. So it's the conditional
density of x given that y is little y. We integrate with
respect to x.   And fortunately, we know what
this conditional PDF is, because we calculated it earlier
in part C. And we know that it's this-- 1 over 1 minus y. But again, we have to be
careful, because this formula, 1 over 1 minus y, is only
valid certain cases. So let's think about
this first. Let's think about some
extreme cases. What if y, little
y, is negative? If little y is negative,
we're conditioning on something over here. And so there is no density for
y being negative or for y, say, in other cases when
y is greater than 1. And so in those cases, this
expectation is just undefined, because conditioning on that
doesn't really make sense, because there's no density
for those values of y. Now, let's consider the case
that actually makes, sense where y is between 0 and 1. Now, we're in business, because
that is the range where this formula is valid. So this formula is valid,
and we can plug it in. So it's 1 over 1 minus y dx. And then the final thing that we
again need to check is what the limits of this
integration is. So we're integrating
with respect to x. So we need to write down what
values of x, what ranges of x is this conditional PDF valid. Well, luckily, we specified
that here. x has to be between
0 and 1 minus y.   So let's actually calculate
this integral. This 1 over 1 minus y is a
constant with respect to x. You can just pull that out. And then now, you're really
just integrating x from 0 to 1 minus y. So the integral of x is
[? 1 ?], 1/2x squared. So you get a 1/2x squared, and
you integrate that from 0 to 1 minus y. And so when you plug in
the limits, you'll get a 1 minus y squared. That will cancel out the
1 over 1 minus y. And what you're left with is
just 1 minus y over 2.   And again, we have to specify
that this is only true for y between 0 and 1. Now, we can again actually
verify that this makes sense. What we're really looking for is
the conditional expectation of x given some value of y. And we already said that
condition on y being some value of x is uniformly
distributed between 0 and 1 minus y. And so remember for our uniform
distribution, the expectation is simple. It's just the midpoint. So the midpoint of 0
and 1 minus y is exactly 1 minus y/2. So that's a nice way of
verifying that this answer is actually correct.   Now, the second part of
part D asks us to do a little bit more.   We have to use the total
expectation theorem in order to somehow write the expectation
of x in terms of the expectation of y.   So the first thing we'll
do is use the total expectation theorem. So the total expectation theorem
is just saying, well, we can take these conditional
expectations. And now, we can integrate this
by the marginal density of y, then we'll get the actual
expectation of x. You can think of it as just kind
of applying the law of iterated expectations as well.   So this integral is going
to look like this. You take the conditional
expectation. So this is the expectation of x
if y were equal to little y. And now, what is that
probability? Well, now we just multiply that
by the density of y at that actual value of little y. And we integrate with
respect to y.   Now, we've already calculated
what this conditional expectation is. It's 1 minus y/2. So let's plug that in. 1 minus y/2 times the
marginal of y.   There's a couple ways of
attacking this problem now. One way is, we can actually
just plug in that marginal of y. We've already calculated that
out in part B. And then we can do this integral and calculate
out the expectation. But maybe we don't really want
to do so much calculus. So let's do what the
problem says and try a different approach. So what the problem suggests is
to write this in terms of the expectation of y. And what is the expectation
of y? Well, the expectation of y is
going to look something like the integral of y times
the marginal of y. So let's see if we can identify
something like that and pull it out. Well, yeah, we actually
do have that. We have y times the marginal
of y, integrated. So let's isolate that. So besides that, we
also have this. We have the integral of the
first term, is 1/2 times the marginal of y. And then the second term is
minus 1/2 times the integral of y of dy. This is just me splitting
this integral up into two separate integrals. Now, we know what this is. The 1/2 we can pull out. And then the rest of it is
just the integral of a marginal of a density from minus
infinity to infinity. And by definition, that
has to be equal to 1. So this just gives us a 1/2. And now, what is this? We get a minus 1/2. And now this, we already said
that is the expectation of y. So what we have is the
expectation of y. So in the second part of this
part D, we've expressed the expectation of x in terms
of the expectation of y. Now, maybe that seems like
that's not too helpful, because we don't know what
either of those two are. But if we think about this
problem, and as part E suggests, we can see that
there's symmetry in this problem, because x and y are
essentially symmetric. So imagine this is x equals y. There's symmetry in this
problem, because if you were to swap the roles of x and y,
you would have exactly the same joint PDF. So what that suggests is that
by symmetry then, it must be that the expectation of x and
the expectation of y are exactly the same. And that is using the
symmetry argument. And that helps us now, because
we can plug that in and solve for expectation of x. So expectation of x is 1/2 minus
1/2 expectation of x. So we have 3/2 expectation
of x equals 1/2. So expectation of
x equals 1/3. And of course, expectation
of y is also 1/3. And so it turns out that the
expectation is around there.   So this problem had
several parts. And it allowed us to start
out from just a raw joint distribution, calculate
marginals, calculate conditionals, and then from
there, calculate all kinds of conditional expectations
and expectations. And a couple of important points
to remember are, when you do these joint
distributions, it's very important to consider where
values are valid. So you have to keep in mind
when you write out these conditional PDFs and joint PDFs
and marginal PDFs, what ranges the formulas you
calculated are valid for. And that also translates to
when you're calculating expectations and such. When you have integrals, you
need to be very careful about the limits of your integration,
to make sure that they line up with the range
where the values are actually valid. And the last thing, which is
kind of unrelated, but it is actually a common tool that's
used in a lot of problems is, when you see symmetry in these
problems, that can help a lot, because it will simplify things
and allow you to use facts like these to help
you calculate what the final answer is. Of course, this is also comes
along with practice. You may not immediately see that
there could be a symmetry argument that will help
with this problem. But with practice, when you do
more of these problems, you'll eventually build up
that kind of--  

Diff. Eq.

If the distance between the plane Ax-2y+z = d and the plane containing the lines, and they give us two lines here in three-dimensions, if that distance is square-root of 6, then the absolute value of d is... So let's think about it for a little bit. They're talking about the distance between this plane and some plane that contains these two line. So in order to talk realistically about the distance between the planes, those planes will have to be parallel, because if they're not parallel - if they intersect with each other, the distance is clearly zero, and they're telling us here that the distance is square-root of 6. So we have a situation so that the planes can't intersect they must be parallel. So you have this plane up here, you have this plane up here, you could call this the equation here is ax - 2y + z = d, and then you're going to have another plane, that's going to be parallel to it, maybe it looks something like this. You have the other plane that is parallel, and it's going to contain both of these lines. So maybe it has this line - so this line is in green, maybe this line looks something like this, it's on that blue plane. And then this line, maybe in magenta, is also going to be on the blue plane. So how can we figure out the distances? Well a good starting point would be to try to figure out the equation for this blue plane here. And since these planes are parallel, this equation should look very much like this orange equation - at least on the left-hand-side, it might just have a different d-value, and that's because it has the exact same inclination. And then once we figure out the equation for this plane over here, then we could actually probably figure out what 'a' is, then we could find some point on the blue plane and then use our knowledge of finding the distance points and planes to figure out the actual distance from any point to this orange plane. So let's figure out the equation of this blue plane first, and a good place to start is just to try to figure out two vectors on this blue plane, then we can take the cross-product of those two vectors to find out a normal to this blue plane, and then use that information to actually figure out the equation for the blue plane. So let's figure out some points that sit on the blue plane. So on this green line right over here you have, see if I want all of these to be equal to zero, you'd have the point x is equal to 1, y is equal to 2, z is equal to 3, so you have the point (1,2,3) - that definitely sits on the blue plane. Let's come up with another point. let's see if I want all of these to be equal to one, I could make this, if I want this to evaluate to two, I'd have the point three, I would have the point, if I want this to evaluate to one, I would want five minus two over three, so 3, 5, and then I would want this to be seven. Seven minus three over four is also 1, so that's another point, actually both of these points sit on this line right over here. 1,2,3 and then 3,5,7. And then let's do the same thing for this - let's find two points on this plane, and actually we just need to find one point on this plane, because if you have three points that's enough to figure out two different vectors -vectors that aren't scalar multiples of each other, which would be enough to figure out the normal to this plane. So let's just figure out one more point over here and one more point would be, if we want all of these three to be equal to zero, would be the point (2,3,4). Because this would be zero, zero, zero. So that's also sitting the plane and that sits on the magenta line right over there. So let's use these three points to figure out two vectors on the plane that aren't multiples of each other, then we can take their cross-product to actually figure out a normal vector to the blue plane. So let's say the first vector, 'a', that sits on this plane, let's say it's the difference in the position vectors that specify these two points, and then we know that will be on the plane. And so that will be three minus one is 2i plus five minus two is 3j, plus seven minus three is 4k. So vector 'a' is actually going to sit on this green line, because both of these points are on this line, so it's going to sit on that line. If we put it on the plane or if we were to start it at one of those points it will sit on that line. And then we can do another vector, and it's essentially going between a point on the green line and a point on the purple line, but that's definitely going to be a vector on our blue plane. Let's go between these two points - that looks pretty straight-forward, so let's call vector b, so let's call vector b, let's call that, let's see, two minus one is 'i', three minus two is 'j', and then four minus three, that's just 1k. So this vector here is also sitting on the plane. So if I take the cross-product of 'a' and 'b' I am going to get a vector that is perpendicular to the plane, or a normal vector to the plane. So let's do that. So let's find what 'a' cross 'b' is. 'a' cross 'b' is equal to - and this is how I find it easiest - I just write 'i', 'j', 'k', this is really the definition of the cross product, or I guess one of them. And we write our first vector, we have 2, 3, 4, and then we have our second vector, which is just 1,1,1, and then this is going to be equal to, first we'll look at the 'i' component, so cross that row, that column out. three times one minus one times four, so that's just three minus four, so it's negative 'i', and then minus, we're going to have the 'j', so let we write up minus here, minus , let me just swap signs, we have positive, negative, positive, so 'j' , get rid of that column, that row, two times one which is two, minus one times four, so that's minus four, is negative 2, so we could have written negative 2 here, but the negatives cancel out, so it becomes plus 2j, and finally for the 'k', get rid of that row that column, two times one is two, minus one times three, is two minus three, which is negative one, so it's negative k. So this right here is a normal vector to the plane. So if we want to find the equation for that plane, we've done it multiple times, we'd just have to take the dot product of that normal vector and any arbitrary vector on that that specified that we can specify with with an arbitrary x y and z, and we've done this multiple times in multiple videos. If this is any point, x, y, z, that sits on the plane, then the vector - let me draw the vector, let's say we go to this point right over here, so this vector right over here is going to be - let me draw it the other way actually, so this vector right over here, let's say we're going between this point and x,y,z, this vector right here is going to be x minus three i, plus y minus five j, plus z minus 7 k. That's what this vector is, it sits on the plane, assuming x,y and z sit on the plane. So if we take the dot product of this and the normal vector that has got to be equal to zero, because it sits on the plane, and then we'll have our equation. So let's take 'n' dot that over there, so n dot x minus three i, plus y minus five j, plus z minus seven k. If any of this is confusing to you, I've gone into a little bit more depth in previous videos, especially in the linear algebra playlist where I talk about constructing the equation of a plane given a point on the plane and a normal vector, and even how you find that normal vector, so you might want to watch those if you want some review there. But these are going to be equal to zero, so when you take the dot product, n, our normal vector is this, so we just take the x term, which is negative one, times this x term right over here, so negative one times this is just three minus x, and then plus this y component times this y component, so it's two times this, so it's plus two y minus ten, and then finally the z component - negative one times this. So this is plus seven minus z is equal to zero, and what do we get? So we have our negative x, plus two y, minus z, and then is equal to, let's subtract three from both sides So if we take it out there it will be minus three. If we add ten to both sides, so then you have a plus ten over here, and then we subtract seven from both sides, this becomes a minus seven So then on the right-hand-side, negative three plus ten minus seven, well that's just going to be zero! And just like that we have the equation for this blue plane over here - the plane that contains these two lines. Now remember what we said at the beginning of the video - these two planes are parallel, so the ratio of the coefficients on the 'x' terms, the 'y' term and the 'z' term has got to be the same. So this one has a positive 2, that has a negative 2, this is just to simplify it, so it looks very similar to each other. Let's multiple this equation right here, both sides, by negative one. And then we're going to get x minus two y plus z is equal to zero; so this is a completely valid, another alternate way of expressing the same plane. And what like about this about this is that it looks very similar to this - at least the ratio of the x, y's and z's, negative two y, negative two y; one z, one z, and remember the ratios have to be the same. So here we have a one-to-one ratio between the z-coefficient and the z-coefficient, the y-coefficient and the y-coefficient, so it's also going to be for the x coefficient. So here we know if this is going to be parallel to the blue plane we know that 'a' has got to be equal to one. So this is x minus two y plus z is equal to d. So now let's figure out the actual distance between these two planes. So what we can do is we can take a point on this blue plane - and we have several examples of points on the blue plane, and find the distance between that point and this plane over here. And actually I just finished doing some videos on how to find the distance between a point and a plane, so I'm just going to use that formula (if you want it to be proved go watch that video - it's actually pretty interesting proof I think) but the distance between let's say this point, (1,2,3), and this plane over here, so this distance right here is going to be in the direction of the normal, the distance is going to be, you literally just evaluate this, let me do it this way: you literally just put in this point for the x, y and z, and then you subtract the d in the numerator, and we saw that as the formula for finding the distance. So it's literally going to be one, one, (I'm actually using this point right over here) So it's going to be one, one, because we just have one x so it's just going to be one minus two times two, one minus four (that's two times two), plus three, minus d, over here the d is just d, so we're just going to write minus d, just like that, all of that over what is essentially the magnitude of the normal vector, and we saw in several videos that's just the square of the coefficients on each of these terms right here, and taking the sum of those, taking the square root, so it's going to be equal to: 1 squared, plus negative 2 squared which is four, plus one squared which is one. So this is going to simplify to the distance is equal to one minus four plus three is zero. So in the numerator we have negative d, all of that over the square root of one plus four plus one, so all over the square root of six. So they say the distance between this plane and this plane over here is square root of six. So they're saying the distance is equal to the square root of six, that's what this information right over here is, maybe I should do that in another colour. The distance between the two planes is going to be the square root of six, and so then if we solve for d, multiple both sides of this equation times the square root of six, you get six is equal to negative d, or d is equal to negative six. Now, what they care about is the absolute value of d, or the absolute distance, so this would be kinda the signed distance - it specifies whether we're above or below the plane. Since we're below the plane we got a negative number - I just happened to draw it right, if we above the plane we would get a positive number. So this distance is negative six, the absolute value of it, the absolute value of d which is the same as the absolute value of negative six, is equal to six. So take any point on this blue plane and you look for the closest point on the orange plane, and they will be exactly six apart. Anyway, hopefully you found that interesting.

Data Structures

Before we move on past the
method of undetermined coefficients, I want to make and
interesting and actually a useful point. Let's say that I had the
following nonhomogeneous differential equation: the
second derivative of y minus 3 times the first derivative minus
4y is equal to-- now this is where gets interesting--
3e to the 2x plus 2 sine of x plus-- let me
make sure that I'm doing the same problems that I've
already worked on-- plus 4x squared. So you might say,
wow, this is a tremendously complicated problem. I have the 3 types of functions
I've been exposed to, I would have so many
undetermined coefficients, it would get really unwieldy. And this is where you need
to make a simplifying realization. We know the three particular
solutions to the following differential equations. We know the solution to the
second derivative minus 3 times the first derivative
minus 4y. Well, this is this the
homogeneous, right? And we know that the solution to
the homogeneous equation-- we did this a bunch of times--
is C1e to the 4x plus C2e to the minus x. We know the solution to-- and
I'll switch colors, just for a variety-- y prime prime minus 3y
prime minus 4y is equal to just this alone: 3e to the 2x. And we saw that that particular
solution there, y particular, was minus
1/2 e to the 2x. And we did this using
undetermined coefficients. We did that a couple
of videos ago. And then let me just write this
out a couple of times. We know the solution to
this one, as well. This was another particular
solution we found. I think it was two videos ago. And we found that the particular
solution in this case-- and this was a fairly
hairy problem-- was minus 5/17 x plus 3/17. Sorry. The particular solution was
minus 5/17 sine of x plus 3/17 cosine of x. And then finally this last
polynomial, we could call it. We know the solution when that
was just the right-hand side. That was this equation. And there we figured out-- and
this was in the last video. We figured out that the
particular solution in this case was minus x squared
plus 3/2 x minus 13/8. So we know the particular
solution when 0's on the right-hand side. We know it when just 3e to the
2x is on the right-hand side. We know it just when 2 sine of
x is on the right-hand side. And we know it just when
4x squared is on the right-hand side. First of all, the particular
solution to this nonhomogeneous equation, we
could just take the sum of the three particular solutions. And that makes sense, right? Because one of the particular
solutions, like this one when you put it on the left-hand
side, it will just equal this term. This particular solution, when
you put it in the left-hand side, will equal this term. And finally, this particular
solution, when you put it on the left-hand side, will
equal the 4x squared. And then you could add the
homogeneous solution to that. You put it in this side
and you'll get 0. So it won't change the
right-hand side. And then you will have the most
general solution because you have these two constants
that you can solve for depending on your initial
condition. So the solution to this
seemingly hairy differential equation is really just the sum
of these four solutions. Let me clean up some space
because I want everything to be on the board at
the same time. So the solution is going
to be-- well, I want that to be deleted. I'll do it in baby blue. Is going to be the solution to
the homogeneous C1e to the 4x plus C2e to the minus x
minus 1/2 e the to 2x. And I'll continue this line. Minus 5/17 sine of x plus 3/17
cosine of x minus x squared plus 3/2 x minus 13/8. And it seems daunting. When you saw this, it probably
looked daunting. This solution, if I told you
this was a solution and you didn't know how to do
undetermined coefficients, you're like, oh, I would never
be able to figure out something like that. But the important realization is
that you just have to find the particular solutions for
each of these terms and then sum them up. And then add them to the general
solution for the homogeneous equation,
if this was a 0 on the right-hand side. And then you get the general
solution for this fairly intimidating-looking second
order linear nonhomogeneous differential equation with
constant coefficients. See you in the next video, where
we'll start learning another method for solving
nonhomogeneous equations.

Probability

OK, this is the lecture on
positive definite matrices. I made a start on those
briefly in a previous lecture. One point I wanted to make was
the way that this topic brings the whole course together,
pivots, determinants, eigenvalues, and something
new- four plot instability and then something new in this
expression, x transpose Ax, actually that's the guy
to watch in this lecture. So, so the topic is
positive definite matrix, and what's my goal? First, first goal is, how
can I tell if a matrix is positive definite? So I would like to
have tests to see if you give me a, a
five by five matrix, how do I tell if it's
positive definite? More important is,
what does it mean? Why are we so interested
in this property of positive definiteness? And then, at the end
comes some geometry. Ellipses are connected with
positive definite things. Hyperbolas are not connected
with positive definite things, so we- it's this, we,
there's a geometry too, but mostly it's
linear algebra and -- this application of how do you
recognize 'em when you have a minim is pretty neat. OK. I'm gonna begin with two by two. All matrices are
symmetric, right? That's understood; the
matrix is symmetric, now my question is, is
it positive definite? Now, here are some -- each one of these is a complete
test for positive definiteness. If I know the eigenvalues,
my test is are they positive? Are they all positive? If I know these -- so, A is really -- I look at that number A,
here, as the, as the one by one determinant, and here's
the two by two determinant. So this is the determinant test. This is the eigenvalue test,
this is the determinant test. Are the determinants growing in
s- of all, of all end, sort of, can I call them
leading submatrices, they're the first
ones the northwest, Seattle submatrices coming
down from from there, they all, all those determinants
have to be positive, and then another
test is the pivots. The pivots of a
two by two matrix are the number A for sure,
and, since the product is the determinant,
the second pivot must be the determinant
divided by A. And then in here is gonna come
my favorite and my new idea, the, the, the the one to
catch, about x transpose Ax being positive. But we'll have to
look at this guy. This gets, like a star, because
for most, presentations, the definition of
positive definiteness would be this number four and
these numbers one two three would be test four. OK. Maybe I'll tuck this,
where, you know, OK. So I'll have to look
at this x transpose Ax. Can you, can we
just be sure, how do we know that the eigenvalue
test and the determinant test, pick out the same
matrices, and let me, let's just do a few examples. Some examples. Let me pick the matrix
two, six, six, tell me, what number do I have to
put there for the matrix to be positive definite? Tell me a sufficiently
large number that would make it
positive definite? Let's just practice with
these conditions in the two by two case. Now, when I ask
you that, you don't wanna find the eigenvalues, you
would use the determinant test for that, so, the first
or the pivot test, that, that guy is certainly
positive, that had to happen, and it's OK. How large a number here -- the
number had better be more than what? More than eighteen, right,
because if it's eight -- no. More than what? Nineteen, is it? If I have a nineteen here,
is that positive definite? I get thirty eight minus
thirty six, that's OK. If I had an eighteen, let
me play it really close. If I have an eighteen there,
then I positive definite? Not quite. I would call this
guy positive, so it's useful just to see that
this the borderline. That matrix is on
the borderline, I would call that matrix
positive semi-definite. And what are the
eigenvalues of that matrix, just since we're given
eigenvalues of two by twos, when it's semi-definite, but
not definite, then the -- I'm squeezing this
eigenvalue test down, -- what's the eigenvalue that
I know this matrix has? What kind of a matrix
have I got here? It's a singular matrix, one
of its eigenvalues is zero. That has an eigenvalue zero,
and the other eigenvalue is -- from the trace, twenty. OK. So that, that matrix has
eigenvalues greater than or equal to zero, and it's that
"equal to" that brought this word "semi-definite" in. And, the what are the
pivots of that matrix? So the pivots, so
the eigenvalues are zero and twenty, the pivots
are, well, the pivot is two, and what's the next pivot? There isn't one. We got a singular matrix here,
it'll only have one pivot. You see that that's a rank
one matrix, two six is a -- six eighteen is a
multiple of two six, the matrix is singular
it only has one pivot, so the pivot test doesn't quite The -- let me do
the x transpose Ax. pass. Now this is -- the novelty now. OK. What I looking at when I
look at this new combination, x transpose Ax. x is any vector now, so
lemme compute, so any vector, lemme call its components
x1 and x2, so that's x. And I put in here A. Let's, let's use this example
two six, six eighteen, and here's x
transposed, so x1 x2. We're back to real matrices,
after that last lecture that- that said what to do
in the complex case, let's come back to real matrices. So here's x transpose Ax,
and what I'm interested in is, what do I get if I
multiply those together? I get some function of x1
and x2, and what is it? Let's see, if I do this
multiplication, so I do it, lemme, just, I'll just
do it slowly, x1, x2, if I multiply that matrix,
this is 2x1, and 6x2s, and the next row
is 6x1s and 18x2s, and now I do this final
step and what do I have? I've got 2x1 squareds,
got 2X1 squareds is coming from this two, I've
got this one gives me eighteen, well, shall I do the
ones in the middle? How many x1 x2s do I have? Let's see, x1 times that
6x2 would be six of 'em, and then x2 times this one
will be six more, I get twelve. So, in here is going, this
is the number a, this is the number 2b, and in here is -- x2 times eighteen x2 will be
eighteen x2 squareds and this is the number c. So it's ax1 -- it's
like ax squared. 2bxy and cy squared. For my first point that I
wanted to make by just doing out a multiplication is, that is as
soon as you give me the matrix, as soon as you give me
the matrix, I can -- those are the numbers
that appear in -- I'll call this
thing a quadratic, you see, it's not
linear anymore. Ax is linear, but now I've
got an x transpose coming in, I'm up to degree
two, up to degree two, maybe quadratic is the -- use the word. A quadratic form. It's purely degree two,
there's no linear part, there's no constant
part, there certainly no cubes or fourth powers,
it's all second degree. And my question is -- is that quantity
positive or not? That's -- for every x1 and x2,
that is my new definition -- that's my definition of a
positive definite matrix. If this quantity is positive,
if, if, if, it's positive for all x's and y's, all
x1 x2s, then I call them -- then that's the matrix
is positive definite. Now, is this guy
passing our test? Well we have, we anticipated
the answer here by, by asking first about
eigenvalues and pivots, and what happened? It failed. It barely failed. If I had made this
eighteen down to a seven, it would've totally failed. I do that with the eraser, and
then I'll put back eighteen, because, seven is such a
total disaster, but if -- I'll keep seven for a second. Is that thing in any
way positive definite? No, absolutely not. I don't know its eigenvalues,
but I know for sure one of them is negative. Its pivots are two and then
the next pivot would be the determinant over two, and
the determinant is -- what, what's the determinant
of this thing? Fourteen minus
thirty six, I've got a determinant minus twenty two. The next pivot will
be -- the pivots now, of this thing are two and
minus eleven or something. Their product being minus
twenty two the determinant. This thing is not
positive definite. What would be -- let me look
at the x transpose Ax for this guy. What's -- if I do out
this multiplication, this eighteen is temporarily
changing to a seven. This eighteen is temporarily
changing to a seven, and I know that there's
some numbers x1 and x2 for which that thing, that
function, is negative. And I'm desperately trying
to think what they are. Maybe you can see. Can you tell me a
value of x1 and x2 that makes this
quantity negative? Oh, maybe one and minus one? Yes, that's -- in this case,
that, will work, right, if I took x1 to be one,
and x2 to be minus one, then I always get something
positive, the two, and the seven minus one squared,
but this would be minus twelve and the whole thing
would be negative; I would have two minus twelve
plus seven, a negative. If I drew the graph, can I
get the little picture in here? If I draw the graph
of this thing? So, graphs, of the
function f(x,y), or f(x), so I say here f(x,y) equal
this -- x transpose Ax, this, this this ax squared,
2bxy, and cy squared. And, let's take the
example, with these numbers. OK, so here's the x axis,
here's the y axis, and here's -- up is the function. z, if you like, or f. I apologize, and let me,
just once in my life here, put an arrow over these,
these, axes so you see them. That's the vector and I just,
see, instead of x1 and x2, I made them x- the
components x and y. OK. So, so, what's a graph of 2x
squared, twelve xy, and seven y squared? I'd like to see -- I not the greatest
artist, but let's -- tell me something about
this graph of this function. Whoa, tell me one point
that it goes through. The origin. Right? Even this artist can get this
thing to go through the origin, when these are zero, I,
I certainly get zero. OK. Some more points. If x is one and y is zero,
then I'm going upwards, so I'm going up this
way, and I'm, I'm going up, like, two x
squared in that direction. So -- that's meant
to be a parabola. And, suppose x stays
zero and y increases. Well, y could be positive or
negative; it's seven y squared. Is this function going upward? In the x direction it's going
upward, and in the y direction it's going upwards,
and if x equals y then the forty-five degree
direction is certainly going upwards; because
then we'd have what, about, everything would
be positive, but what? This function -- what's
the graph of this function? Look like? Tell me the word that describes
the graph of this function. This is the non-positive
definite here, everybody's with me
here, for some reason got started in a negative
direction, your case that isn't positive definite. And what's the graph look like
that goes up, but does it -- do we have a minimum here, does
it go from, from the origin? Completely? No, because we just checked
that this thing failed. It failed along the direction
when x was minus y -- we have a saddle point,
let me put myself, let me, to the least, tell you the word. This thing, goes up
in some directions, but down in other directions,
and if we actually knew what a saddle looked like or
thinks saddles do that -- the way your legs go is,
like, down, up, the way, you, looking like, forward, and, the,
and drawing the thing is even worse than describing -- I'm just going to say in
some directions we go up and in other directions,
there is, a saddle -- Now I'm sorry I put
that on the front board, you have no way to cover
it, but it's a saddle. OK. And, and this is a
saddle point, it's the, it's the point that's at
the maximum in some directions and at the minimum
in other directions. And actually, the perfect
directions to look are the eigenvector directions. We'll see that. So this is, not a
positive definite matrix. OK. Now I'm coming back
to this example, getting rid of this seven,
let's move it up to twenty. Let's, let's let's make the
thing really positive definite. OK. So this is, this
number's now twenty. c is now twenty. OK. Now that passes the test, which
I haven't proved, of course, it passes the test
for positive pivots. It passes the test for
positive eigenvalues. How can you tell that the
eigenvalues of that matrix are positive without
actually finding them? Of course, two by two I could
find them, but can you see -- how do I know they're positive? I know that their product is -- I know that lambda one times
lambda two is positive, why? Because that's the
determinant, right, lambda one times lambda two is
the determinant, which is forty minus thirty-six is four. So the determinant is four. And the trace, the sum down
the diagonal, is twenty-two. So, they multiply to give four. So that leaves the
possibility they're either both positive or both negative. But if they're both negative,
the trace couldn't be So they're both
positive. twenty-two. So both of the eigenvalues
that are positive, both of the pivots
are positive -- the determinants are positive,
and I believe that this function is positive everywhere
except at zero, zero, of course. When I write down
this condition, So I believe that
x transposed, let me copy, x transpose Ax is
positive, except, of course, at the minimum point,
at zero, of course, I don't expect miracles. So what does its graph look
like, and how do I check, and how do I check that
this really is positive? So we take it's
graph for a minute. What would be the graph
of that function -- it does not have a saddle point. Let me -- I'll raise
the board, here, and stay with this
example for a while. So I want to do the graph
of -- here's my function, two x squared, twelve xy-s, that
could be positive or negative, and twenty y squared. But my point is, so you're
seeing the underlying point is, that, the things are
positive definite when in some way, these,
these pure squares, squares we know to be positive, and
when those kind of overwhelm this guy, who could be
m- positive or negative, because some like
or have same or have same or different signs,
when these are big enough they overwhelm this guy and
make the total thing positive, and what would the
graph now look like? Let me draw the x - well, let
me draw the x direction, the y direction, and the origin,
at zero, zero, I'm there, where do I go as I move
away from the origin? Where do I go as I move
away from the origin? I'm sure that I go up. The origin, the
center point here, is a minim because this thing I
believe, and we better see why, it's, the graph is like a bowl,
the graph is like a bowl shape, it's -- here's the minimum. And because we've
got a pure quadratic, we know it sits at the origin,
we know it's tangent plane, the first derivatives are zero,
so, we know, first derivatives, First derivatives are
all zero, but that's not enough for a minimum. It's first derivatives
were zero here. So, the partial derivatives,
the first derivatives, are zero. Again, because first derivatives
are gonna have an x or an a y, or a y in them, they'll
be zero at the origin. It's the second derivatives
that control everything. It's the second derivatives
that this matrix is telling us, and somehow -- here's my point. You remember in Calculus, how
did you decide on a minimum? First requirement was, that
the derivative had to be zero. But then you didn't know if
you had a minimum or a maximum. To know that you
had a minimum, you had to look at the
second derivative. The second derivative
had to be positive, the slope had to be
increasing as you went through the minimum point. The curvature had to
go upwards, and that's what we're doing now
in two dimensions, and in n dimensions. So we're doing what
we did in Calculus. Second derivative
positive, m- will now become that the matrix
of second derivatives is positive definite. Can I just -- like a translation of -- this is how minimum are coming
in, ithe beginning of Calculus -- we had a minimum was associated
with second derivative, being positive. And first derivative
zero, of course. Derivative, first
derivative, but it was the second derivative
that told us we had a minimum. And now, in 18.06,
in linear algebra, we'll have a minim
for our function now, our function will have, for
your function be a function not of just x but several variables,
the way functions really are in real life,
the minimum will be when the matrix of second
derivatives, the matrix here was one by one, there was
just one second derivative, now we've got lots. Is positive definite. So positive for a
number translates into positive
definite for a matrix. And it this brings
everything you check pivots, you check determinants,
you check all your values, or you check this minimum stuff. OK. Let me come back to this graph. That graph goes upwards. And I'll have to see why. How do I know that this,
that this function is always positive? Can you look at that and tell
that it's always positive? Maybe two by two, you
could feel pretty sure, but what's the good way to
show that this thing is always If we can express it, as, in
terms of squares, positive? because that's what we
know for any x and y, whatever, if we're
squaring something we certainly are not negative. So I believe that this
expression, this function, could be written as
a sum of squares. Can you tell me -- see, because all the
problems, the headaches are coming from this xy term. If we can get expressions
-- if we can get that inside a square, so actually, what
we're doing is something called, that you've seen
called completing the square. Let me start the square
and you complete it. OK, I think we
have two of x plus, now I don't remember how
many y-s we need, but you'll figure it out, squared. How many y-s should I
put in here, to make -- what do I want to do, the two
x squared-s will be correct, right? What I want to do is put
in the right number of y-s to get the twelve xy correct. And what is that number of y-s? Let's see, I've got
two times, and so I really want six xy-s
to come out of here, I think maybe if
I put three there, does that look right to you? I have two- this is, we
can mentally, multiply out, that's X squared,
that's right, that's six X Y, times the
two gives from, right, and how many Y squareds
have I now got? How many Y squareds have
I now got from this term? Eighteen. Eighteen was the key
number, remember? Now if I want to make it
twenty, then I've got two left. Two y squared-s. That's completing the
square, and it's, now I can see that that
function is positive, because it's all squares. I've got two squares,
added together, I couldn't go negative. What if I went
back to that seven? If instead of twenty that
number was a seven, then what would happen? This would still be correct,
I'd still have this square, to get the two x squared
and the twelve xy, and I'd have eighteen y squared
and then what would I do here? I'd have to remove eleven
y squared-s, right, if I only had a seven
here, then instead of -- when I had a twenty I had two
more to put in, when I had an eighteen, which
was the marginal case, I had no more to put in. When I had a seven, which
was the case below zero, the indefinite case,
I had minus eleven. Now, so, you can see now,
that this thing is a bowl. OK. It's going upwards, if I cut
it at a plane, z equal to one, say, I would get, I would
get a curve, what would be the equation for that curve? If I cut it at height
one, the equation would be this
thing equal to one. And that curve
would be an ellipse. So actually,
already, I've blocked into the lecture, the different
pieces that we're aiming for. We're aiming for the
tests, which this passed; we're aiming for the connection
to a minimum, which this -- which we see in the graph,
and if we chop it up, if we set this
thing equal to one, if I set that thing
equal to one, that -- what that gives me
is, the cross-section. It gives me this, this
curve, and its equation is this thing equals one,
and that's an ellipse. Whereas if I cut
through a saddle point, I get a hyperbola. But this minimum stuff is
really what I'm most interested OK. in. OK. By -- I just have to ask,
do you recognize, I mean, these numbers here, the
two that appeared outside, the three that appeared inside,
the two that appeared there -- actually, those numbers
come from elimination. Completing the square
is our good old method of Gaussian elimination,
in expressed in terms of these squares. The -- let me show
you what I mean. I just think those
numbers are no accident, If I take my matrix two,
six, six, and twenty, and I do elimination,
then the pivot is two and I take three,
what's the multiplier? How much of row one do I
take away from row two? Three. So what you're seeing in
this, completing the square, is the pivots outside and
the multiplier inside. Just do that again? The pivot is two, three -- three
of those away from that gives me two, six, zero, and
what's the second pivot? Three of this away from this,
three sixes'll be eighteen, and the second
pivot will also be a two. So that's the U, this is
the A, and of course the L was one, zero, one, and
the multiplier was three. So, completing the
square is elimination. Why I happy to see, happy
to see that coming together? Because I know about
elimination for m by m matrices. I just started talking about
completing the square, here, for two by twos. But now I see what's going on. Completing the square really
amounts to splitting this thing into a sum of squares, so
what's the critical thing -- I have a lot of squares,
and inside those squares are multipliers but
they're squares, and the question is, what's
outside these squares? When I complete the square,
what are the numbers that go outside? They're the pivots. They're the pivots, and that's
why positive pivots give me sum of squares. Positive pivots, those
pivots are the numbers that go outside the
squares, so positive pivots, sum of squares, everything
positive, graph goes up, a minimum at the origin,
it's all connected together; all connected together. And in the two by two case,
you can see those connections, but linear algebra now can go
up to three by three, m by m. Let's do that next. Can I just, before
I leave two by two, I've written this expression
"matrix of second derivatives." What's the matrix of
second derivatives? That's one second
derivative now, but if I'm in two dimensions,
I have a two by two matrix, it's the second x derivative,
the second x derivative goes there -- shall I write it --
fxx, if you like, fxx, that means the second derivative
of f in the x direction. fyy, second derivative
in the y direction. Those are the pure derivatives,
second derivatives. They have to be positive. For a minimum. This number has to be
positive for a minimum. That number has to be
positive for a minimum. But, that's not enough. Those numbers have to
somehow be big enough to overcome this
cross-derivative, Why is the matrix symmetric? Because the second derivative
f with respect to x and y is equal to -- I can, that's the beautiful fact
about second derivatives, is I can do those in either order
and I get the same thing. So this is the same as
that, and so, that's the matrix of
second derivatives. And the test is, it has
to be positive definite. You might remember,
from, tucked in somewhere near the end of eighteen o'
two or at least in the book, was the condition for a
minimum, For a function of two variables. Let's -- when do
you have a minimum? For a function of two
variables, believe me, that's what Calculus is for. The condition is first
derivatives have to be zero. And the matrix of
second derivatives has to be positive definite. So you maybe remember there
was an fxx times an fyy that had to be bigger than
an an fxy squared, that's just our
determinant, two by two. But now, we now know the
answer for three by three, m by m, because we can do
elimination by m by m matrices, we can connect eigenvalues
of m by m matrices, we can do sum of squares, sum
of m squares instead of only two squares; and so
let's take a, let me go over here to do a
three by three example. So, three by three example. OK. Oh, let me -- shall I use my favorite matrix? You've seen this matrix before. Yes, let's use the good matrix,
four by one, oops, open. Is that matrix
positive definite? What's -- so I'm going to ask
questions about this matrix, is it positive
definite, first of all? What's the function
associated with that matrix, what's the x transpose Ax? Is -- do we have a
minimum for that function, at zero? And then even
what's the geometry? OK. First of all, is the
matrix positive definite, now I've given you the
numbers there so you can take the determinants, maybe
that's the quickest, is that what you
would do mentally, if I give you all a
matrix on a quiz and say is it positive definite or not? I would take that determinant
and I'd give the answer two. I would take that
determinant and I would give the answer for
that two by two determinant, I'd give the answer
three, and anybody remember the answer for the
three by three determinant? It was four, you remember
for these special matrices, when we do determinants, they
went up two, three, four, five, six, they just went up linearly. So that matrix has -- the
determinants are two, three, and four. Pivots. What are the pivots
for that matrix? I'll tell you, they're --
the first pivot is two, the next pivot is
three over two, the next pivot is
four over three. Because, the product
of the pivots has to give me
those determinants. The product of these two pivots
gives me that determinant; the product of all the pivots
gives me that determinant. What are the eigenvalues? Oh, I don't know. The eigenvalues I've got, what
do I have a cubic equation -- a degree three equation? There are three
eigenvalues to find. If I believe what
I've said today, what do I know about
these eigenvalues, even though I don't
know the exact numbers. I -- I remember the numbers. Because these matrices
are so important that people figure them. But -- what do you believe
to be true about these three eigenvalues -- you believe
that they are all positive. They're all positive. I think that they are two
minus square root of two, two, and two plus the
square root of two. I think. Let me just -- I can't write those numbers
down without checking the simple checks, what
the first simple check is the trace, so if I add
those numbers I get six and if I add those
numbers I get six. The other simple test is
the determinant, if I -- can you do this, can you
multiply those numbers together? I guess we can multiply
by two out there. What's two minus square
root of two times two plus square root of two,
that'll be four minus two, that'll be two,
yeah, two times two, that's got the determinant,
right, so it's got, it's got a chance of being
correct and I think it is. Now, what's the x transpose Ax? I better give myself
enough room for that. x transpose Ax for this guy. It's two x1 squareds, and two x2
squareds, and two x3 squareds. Those come from the
diagonal, those were easy. Now off the diagonal
there's a minus and a minus, they come together there'll be
a minus two minus two whats? Are coming from this one two and
two one position, is the x1 x2. I'm doing mentally
a multiplication of this matrix
times a row vector on the left times a column
vector on the right, and I know that these numbers
show up in the answer. The diagonal is
the perfect square, this off diagonal is
a minus two x1 x2, and there are no x1 x3-s, and
there're minus two x2 x3-s. And I believe that that
expression is always positive. I believe that that
curve, that graph, really, of that function,
this is my function f, and I'm in more dimensions
now than I can draw, it -- but the graph of that
function goes upwards. It's a bowl. Or maybe the right word is -- just forgot, what's
a long word for bowl? Hm, maybe paraboloid, I
think, paraboloid comes in. I'll edit the tape
and get that word in. Bowl, let's say, is, that,
so that, and if I can -- I could complete the
squares, I could write that as the sum of three squares,
and those three squares would get multiplied
by the three pivots. And the pivots are all positive. So I would have positive
pivots times squares, the net result would
be a positive function and a bowl which goes upwards. And then, finally, if I cut --
if I slice through this bowl, if I -- now I'm asking you
to stretch your visualization here, because I'm
in four dimensions, I've got x1 x2 x3 in the base,
and this function is z, or f, or something. And its graph is going up. But I'm in four dimensions,
because I've got three in the base and then
the upward direction, but now if I cut through this
four-dimensional picture, at level one, so, suppose
I cut through this thing at height one. So I take all the points
that are at height one. That gives me -- it gave me an ellipse over
there, in that two by two case, in this case, this will be
the equation of an ellipsoid, a football in other words. Well, not quite a football. A lopsided football. What would be, can I try
to describe to you what the ellipsoid will look
like, this ellipsoid, I'm sorry that the, that I've
ended the matrix right -- at the point, let's -- let me be
sure you've seen the equation. Two x1 squared, two x2 squared,
two x3 squared, minus two of the cross parts, equal what? That is the equation
of a football, so what do I mean by a football
or an ellipsoid? I mean that, well,
I'll draw a few. It's like that,
it's got a center, and it's got it's got
three principal directions. This ellipsoid. So -- you see what I'm saying,
if we have a sphere then all directions would be the same. If we had a true football, or
it's closer to a rugby ball, really, because it's more
curved than a football, it would have one long
direction and the other two would be equal. That would be like
having a matrix that had one
eigenvalue repeated. And then one other different. So this sphere comes from,
like, the identity matrix, all eigenvalues the same. Our rugby ball comes
from a case where -- three, the three, two of the
three eigenvalues are the same. But we know how the case
where -- the typical case, where the three eigenvalues
were all different. So this will have -- How do I say it, if I look
at this ellipsoid correctly, it'll have a major axis,
it'll have a middle axis, and it'll have a minor axis. And those three axes
will be in the direction of the eigenvectors. And the lengths of
those axes will be determined by the eigenvalues. I can get -- turn this all into linear
algebra, because we have -- the right thing we know about
eigenvectors and eigenvalues, for that matrix is what? Of -- let me just tell you that,
repeat the main linear algebra point. How could we turn what
I said into algebra; we would write this A as
Q, the eigenvector matrix, times lambda, the eigenvalue
matrix times Q transposed. The principal axis
theorem, we'll call it, now. The eigenvectors tell us the
directions of the principal axes. The eigenvalues tell us the
lengths of those axes, actually the lengths, or
the half-lengths, or one over the
eigenvalues, it turns out. And that is the
matrix factorization which is the most
important matrix factorization in our
eigenvalue material so far. That's diagonalization
for a symmetric matrix, so instead of the inverse
I can write the transposed. OK. I've -- so what I've tried
today is to tell you the -- what's going on with
positive definite matrices. Ah, you see all how all
these pieces are there and linear algebra
connects them. OK. See you on Friday.

Diff. Eq.

The neat thing about linear
algebra in general is some very seemingly simple concepts
can be interpreted in a bunch of different ways, and can be
shown to represent different ideas or different problems. And
that's what I'm going to do in this video. I'm going to explore the
nullspace, or even better, I'm going to explore the
relationship-- if I have some matrix A times some vector
x, and that that is equal to the 0 vector. And we, of course, saw on the
last few videos that the nullspace of A, is equal to all
of the vectors x in Rn. So this will have
n components. This would have to be
an m by n matrix. If this was an m by a matrix,
I'd say all of the vectors in Ra. So this number right here has to
be the same as that number in order for the matrix vector
multiplication to be valid. But the nullspace of A is all
of the vectors in Rn that satisfy this equation. Where if I take A and I multiply
it times any one of the vectors in the nullspace,
I should get the 0 vector. And this is going to have m
components, and we've seen that in previous-- this is going
to have the 0 vectors. I'll put it this way, the
0 vector's going to be a member of Rm. So that's what our
nullspace is. Let's explore it a little bit. We know already that our vector,
our matrix, can be rewritten like this. We could just write it as
a set of column vectors. I could say this right here,
that's v1, then I'm going to have v2, and I have n columns. So this last column right here
is going to be v sub n. So if I define my vectors this
way, that's the first vector, that's the second vector, than
I can rewrite my matrix A. I could say A is equal to just
a bunch of column vectors. v1, v2, all the way to vn. And multiplying this matrix
times a vector x, so times x, so times x1, x2, all
the way to xn. We've seen in the past on the
matrix vector product definition video that this can
be interpreted as, this has actually just coming straight
out of the definition. This is the same thing as x1
times vector 1, times the first column, plus x2 times the
second column, times that column, all the way to, and you
just keep adding them up, all the way to xn. Times the nth column. This just comes straight out
of our definition of matrix vector products. Now, if we're saying that Ax is
equal to 0, we're looking for the solution set to that. If we're looking for the
solution set to Ax is equal to 0, then that means-- is equal
to the 0 vector, that that means that this sum, we're
trying to find a solution set of this sum is equaling 0. We want to figure out the x1's,
x2's, x3's, all the way to xn's, that make this
equal the 0 vector. What are we doing? We're taking linear
combinations of our column vectors. We're taking linear combinations
of our column vectors, and seeing if we can
take some linear combination and get it to the 0 vector. Now, this should start ringing
bells in your head. This little equation, or this
little expression right here, should start ringing bells. This was part of how we
defined what linear independence was. We said that if this was the
definition of linear independence, or we proved
this fell out of the definition of linear
independence, and if I have a bunch of vectors, v1, v2, all
the way to vn, we say that they are linearly independent. There's kind of the
non-mathematical way of describing it, I guess this is
mathematical as well, is that look, none of those vectors
can be represented as a combination of the other ones. And then we show that that means
that the only solution to this equation would be
that x1, x2, all of the coefficients on this, has
to be equal to 0. That this is the
only solution. Linear independence means that
this is the only solution to this equation right now. If the only way that you get
the 0 vector, by taking combination of all of these
common vectors, the only way to do that is to have all
of these guys equal 0. Then you are linearly
independent. Likewise, if v1, v2, all the
way to vn are linearly independent, then the only
solution to this is for these coefficients to be 0. And we saw that in our video
on linear independence. Now, if all of these
coefficients are 0, what does that mean? That means that our vector
x is the 0 vector, and only the 0 vector. That's the only solution. So we have something
interesting here. If our column vectors are
linearly independent, if v1, v2, all the way to vn, are
linearly independent, then that means that the only
solution to Ax equals 0, is that x has to be equal
to 0 vector. Or put another way, the solution
set of this equation, which is really just a
nullspace, the nullspace is all of the x's that satisfy
this equation. So that the nullspace of A has
to only contain the 0 vector. So that's an interesting
result. If we're linearly independent,
then the nullspace of A only contains the 0 vector. Which is another way of saying
that-- let me write this-- well, I already wrote it down,
that x1, x2, all of them, have to be equal to 0. Now if I were to multiply this
equation out and get it into reduced row echelon form,
what does that mean? We saw in a previous video that
the nullspace of A is equal to the nullspace of the
reduced row echelon form of A. And that's-- the nullspace of
A is 0, because its column vectors are each linearly
independent, and that means that the nullspace of the
reduced row echelon form of A must also equal the 0 vector. And that means that if I take
the reduced row echelon form of A, times-- maybe I'm being
a little redundant-- the reduced row echelon form of A,
and I multiply that times x, or I want to solve this
equation, the only solution right here is x is equal
to the 0 vector. And if you think about what
that means, if this is the only solution, that means that
this reduced row echelon form has no free variables. It literally would just have
to look like this. So this is x, x1, x2, all the
way to xn, the reduced row echelon form of A, in order
for this to have a unique solution, and that unique
solution being 0, the reduced echelon form is going to
have to look like this. 1 times x1 plus 0 times all
the other ones, so you're going to have just a bunch of
n0's, and you're going to have 1 times x2, plus 0's times
everything else. And those 1's are going to go
all the way down the diagonal, so it's going to look like that,
and then that is going to be equal to the 0 vector. And this is going to be a square
matrix, where this has to be n, and this has to be n. How do I know that? Because I said that x1, x2, and
all of these have to be equal to 0. So they have to be equal to 0. If I just write them as a system
of equations, if I write x1 is equal to 0, x2 is
equal to 0, x3 is equal to 0, all the way to xn
is equal to 0. This system of equations, if
I wrote it as an augmented matrix, remember this is
x1 plus 0, x2 plus 0. This as in augmented matrix, and
we've done this multiple times, it would look
like this. 1, you just have a bunch of 0's,
n0's, and then the 1's would just go down the diagonal,
and then you'd have n0's right there. So that's where I'm
getting it from. If we are linearly independent,
the nullspace of A is going to be just a 0
vector, and if the nullspace of A is just a 0 vector, then
the nullspace of the reduced row echelon form is
only the 0 vector. The only solution is all
of the x's equal to 0. Which means a reduced row
echelon form of A has to essentially just be 1's down
the diaganol, with 0's everywhere else. So anyway, I just want to make
this-- this is kind of a neat by-product of an interpretation of the nullspace. Let me write that. Let me summarize our results. The nullspace of A, if it just
equals 0, then that means, you can go both ways, that's true
if and only if the column vectors of A are linearly
independent. And all of that's only true--
this is true, I was going to do a triangle, it might turn
into a square-- if x1, x2, all of these have to be equal 0. This is the only solution. And then that implies that the
reduced row echelon, and I didn't do it as precisely as
I would have liked, but the reduced row echelon form of A
is essentially going to be a square n by n matrix. And, by the way, this can only
be true if we're dealing with an n by n matrix
to begin with. And maybe I'll do that a little
bit more precisely in a future video. But then the reduced row echelon
form of A is going to have to look like this, just
a bunch of 1's down the diagonal, with 0's
everywhere else. And these all imply
each other. Now, what if the nullspace of A
contains some other vectors? Well, then we would have to say
that the column vectors of A are linearly dependent. And if they're linearly
dependent, then we wouldn't have a reduced row echelon form
of A that looked like this, you would have something
that would have some free variables that allows you to
create more solutions there. But anyway, I just wanted to
give you this angle on how you can interpret the nullspace and
how it relates to linear Independence.

Data Structures

OK. So, coming nearer the
end of the course, this lecture will be a mixture
of the linear algebra that comes with a change of basis. And a change of basis from
one basis to another basis is something you really
do in applications. And, I would like to talk
about those applications. I got a little bit
involved with compression. Compressing a signal,
compressing an image. And that's exactly
change-of-basis. And then, the main
theme in this chapter is th- the connection between
a linear transformation, which doesn't have to
have coordinates, and the matrix that tells
us that transformation with respect to coordinates. So the matrix is the
coordinate-based description of the linear transformation. Let me start out with
the nice part, which is just to tell you something
about image compression. Those of you -- well,
everybody's going to meet compression, because you know
that the amount of data that we're getting -- well, these lectures
are compressed. So that, actually, probably
you see my motion as jerky? Shall I use that word? Have you looked on the web? I should like to
find a better word. Compressed, let's say. So the complete signal is, of
course, in those video cameras, and in the videotape,
but that goes to the bottom of building
nine, and out of that comes a jumpy motion because
it uses a standard system for compressing images. And, you'll notice that the
stuff that sits on the board comes very clearly, but
it's my motion that needs a whole lot of bits, right? So, and if I were to run up
and back up there and back, that would need too
many bits, and I'd be compressed even more. So, what does compression mean? Let me just think
of a still image. And of course, satellites,
and computations of the climate,
computations of combustion, the computers and
sensors of all kinds are just giving us
overwhelming amounts of data. The Web is, too. Now, some compression
can be done with no loss. Lossless compression is possible
just using, sort of, the fact that there are redundancies. But I'm talking here
about lossy compression. So I'm talking about
-- here's an image. And what does an
image consist of? It consists of a lot of
little pixels, right? Maybe five hundred and twelve
by five hundred and twelve. Two to the ninth by two
to the ninth pixels, and so this is pixel number
one, one, so that's a pixel. And if we're in black and
white, the typical pixel would tell us a gray-scale,
from zero to two fifty five. So a pixel is usually a
value of one of the xi, so this would be the
i-th pixel, is -- it's usually a real
number on a scale from zero to two fifty five. In other words, two to
the eighth possibilities. So usually, that's the standard,
so that's eight -- eight bits. But then we have
that for every pixel, so we have five hundred
and twelve squared pixels, we're really operating x is a
vector in R^n, but what is n? n is five hundred
and twelve squared. That's our problem, right there. A pixel is a vector
that gives us the information about the image. I'm sorry. The image that comes through is
a vector of that length that -- that's the information that
we have about the image, if it's a color image, we would
have three times that length, because we'd need three
coordinates to get color. So it would be three times five
hundred and twelve squared. It's an enormous
amount of information, and we couldn't send out
the image for these lectures without compressing it. It would overload the system. So it has to be compressed. The standard compression,
and still used with lectures is, called JPEG. I think that stands for Joint
Photographic Experts Group. They established a
system of compression. And I just want to tell
you what it's about. It's a change-of-basis. What basis do we have? The current basis
we have is, you could say, the standard basis
is, every pixel, give a value. So that's like we have a
vector x which is five hundred and twelve squared long
and, in the i-th position, we get a number like one
twenty one or something. The pixel next to it might
be one twenty four, maybe where my tie begins to enter,
so if it was mostly blue shirt, this would be a slight
difference in shading, but pretty close, then the tie
would be a different color, so we might have
quite a few pixels for the blue shirt,
and a whole lot more for the blackboard,
that are very close. And that's what are
very correlated. And that's what gives us the
possibility of compression. For example, before
the lecture starts, if we had a blank blackboard,
then there's an image, but it would make no
sense to take that image and tell you what it
is pixel by pixel. I mean, there's a case in
which all pixel values, all gray levels are the same -- or practically the same,
depending on the erasing of the board, but
extremely close -- and, so that's an image where
the standard basis is lousy. That's the basic fact, that
the standard basis which gives the value of every pixel
makes no use of the fact that we're getting a whole lot of
pixels whose gray levels -- the neighboring pixels tend
to have the same gray level as their neighbors. So how do we take
advantage of that fact? Well, one basis
vector that would be extremely nice to
include in the basis would be a vector of all ones. That's not in our
standard basis, so let me just write again,
the standard basis is our one, and all the rest zeroes, zero,
one, and all the rest, zeroes, everybody knows what
these standard basis is. Now, any other basis
for R -- so this is -- for this very
high-dimensional space -- now I'm going to speak
about a better basis. Better basis -- and
let me just emphasize, one vector that would be
extremely nice to have in that basis is the vector of all ones. Why is that? Let me just say again, because
that vector of all ones, by itself, one vector is
able to completely give the information
on a solid image. Of course, our image
won't be solid, it will have a mix
of solid and signal. So having that one
vector in the basis is going to save us a whole lot. Now, the question is, what other
vectors should be in the basis? The extreme vector
in the basis might be a vector of one minus one,
one minus one, one minus one. That would be a
vector that shows -- I mean, that's like a
checkerboard vector, right? That's a vector that
would, if the image was like a huge checkerboard
of plus, minus, plus, minus, plus,
minus, that vector would carry the whole signal. But much more common
would be maybe to have half the image, darker
and the other half lighter. So another vector that might
be quite useful in here would be half ones
and half minus ones. I'm just trying to get across
the idea of that a basis could be where, that
first of all, we've got the bases at our disposal. Like, we're free to choose that. And it's a billion-dollar
decision what we choose. So, and TV people
would rather pre- would prefer one basis based on
the way the signal is scanned, and movie people
would prefer another, I mean, there's giant
politics in this question that really reduces to a
linear algebra problem, what basis to choose. I'll just mention the best
known basis, which JPEG uses, -- let me put that here -- is the Fourier basis. So when you use the Fourier
basis, that includes -- this is the constant vector, the
D C vector if we're electrical engineers, the l- vector of all
ones, so it would include one, one, one, one. Often eight by eight
is a good choice. Eight by eight is a good choice. So, what do I mean by
this eight by eight? I mean that the big signal,
which is five twelve by five twelve, gets broken down, and
JPEG does this, into eight by eight blocks. And we -- sort of, this is
too much to deal with at once. So what JPEG does is take
this eight by eight block, which is sixty four
coefficients, sixty four, pixels, and changes
the basis on that piece. And then, now, let's see, I was
going to write down Fourier, so you remember Fourier as this
vector of all ones, and then, the vector -- oh,
well, actually, I gave a lecture earlier
about the Fourier matrix, this matrix whose columns are
powers of a complex number w. I won't repeat that,
because I don't want to go into the details
of the Fourier basis, just to tell you how
compression works. So what happens in JPEG? What happens to the video, to
each image, of these lectures? It gets broken into
eight by eight blocks. OK. Within each block, we have
sixty four coefficients, sixty four basis vectors,
sixty four pixels, and we change basis in
sixty four dimensional space using these Fourier vectors. Just note, that was
a lossless step. Let me emphasize. In comes the signal x. We change basis. This is the basis change. Change basis. Choose a better basis. So it produces,
the coefficients c. So sixty four pixels come
in, sixty four coefficients come out. Now comes the compression. Now come -- this was lossless. It's just -- we know that R -- R sixty four has plenty of
bases, and we've chosen one. Now, in that basis, we write
the signal in that basis, and that's what my lecture
-- that's the math part of my lecture. Now here's the application part. The next part is going to
be the compression step. And that's lossy. We're going to lose information. And what will actually
happen at that step? Well, one thing we could
do is just throw away the small coefficients. So that's called thresholding,
we set some threshold. Every coefficient, every
basis vector that's not in there more than
the threshold value, and we set them threshold
so that our eye can't see the difference, or can
hardly see the difference, whether we throw away that
little bit of that basis vector or keep it. So this compression step
produces a compressed set of I'll just keep going
here. coefficients. So it keeps going,
this compression step produces some coefficient c hat. And with many zeroes. So that's where the
compression came. Probably, there is enough of
this vector of all ones -- we very seldom throw that away. Usually, its coefficient
will be large. But the coefficient of
something like this, that quickly alternative
vector, there's probably very little of
that in any smooth signal. That's high-frequency -- this is
low-frequency, zero frequency. This stuff is the highest
frequency we could have, and if the noise, the jitter is
producing that sort of output, but a smooth lecture
like this one is, has very little of
that highest frequency, very little noise
in this lecture. OK, so we throw away
whatever there is, and we're left with
just a few coefficients, and then we reconstruct a
signal using those coefficients. We take those coefficients,
times their basis vectors, but this sum doesn't have
sixty four terms any more. Probably, it has about
two or three terms. So that would -- say it has three terms. From sixty four down
to three, that's compression of
twenty one to one. That's the kind of compression
you're looking for. And everybody is looking for
that sort of compression. Let's see, I guess
I met the problem with the FBI and fingerprints. So there's a whole
lot of still images. You know, with your thumb,
you make these inky marks which go somewhere. it used to go to Washington
and get stored in a big file. So Washington had a file of
thirty million murderers, cheaters on quizzes,
other stuff, and actually, there was no
way to retrieve them in time. So suppose you're at the
police station, they say, OK, this person may have done
this, check with Washington, have they got -- are his or
her fingerprints on file? Well, Washington won't know
the answer within a week if it's got filing cabinets
full of fingerprints. So of course, the natural
step is digitizing. So all fingerprints
are now digitized, so now it's at least electronic,
but still there's too much information in each one. I mean, you can't search
through that many, fingerprints if the digital image is five
twelve squared by five twelve squared, if it's
that many pixels. So you get compressed. So the FBI had to decide what
basis to choose for compression of fingerprints. And then they built a big new
facility in West Virginia, and that's where
fingerprints now are sent. So I think, if you get
your fingerprints done now at the police station, if it's
an up-to-date police station, it happens digitally, and
the signal is sent digitally, and then in West Virginia,
it's compressed and indexed. And then, if they
want to find you, they can do it within minutes
instead of within a week. OK. So this compression comes
up for signals, for images, for video -- which is,
like these lectures -- there's another aspect. You could treat the video as one
still image after another one, and compress each one, and
then run them and make a video. But that misses -- well, you can see why
that's not optimal. In a video thing, you
have a sequence of images, so video is really a sequence
of images but what about one image to the next image? They're extremely correlated. I mean that I'm getting an
image every split-second, and also, I'm moving slightly. That's what's producing the,
jumpy motion on the video. But I'm not, like, you know -- each image in the sequence is
pretty close to the one before. So you have to use, like,
prediction and correction. I mean, the image of me one
instant -- one time-step later, you would assume
would be the same, and then plus a
small correction. And you would only code and
digitize the correction, and compress the correction. So a sequence of images
that's highly correlated and the problem in
compression is always to use this
correlation, this fact that, in time, or
in space, things don't change instantly, they're
very often smooth changes, and, you can predict one
value from the previous value. OK. So those are applications
which are pure linear algebra. I could, well, maybe you'll
allow me to tell you, and the book describes,
the new basis that's the competition for Fourier. So the competition for
Fourier is called wavelets, and I can describe what
that basis is like, say, in the eight by eight case. So the eight by
eight wavelet basis is the vector of all
ones, eight ones, then the vector of four ones
and four minus ones, then the vector of two
ones, and two minus ones, and four zeroes. And also the vector
of four zeroes and two ones and two minus ones. So now I'm up to four, and
I need four more, right? For R^8? The next basis vector will be
one minus one and six zeroes, and then three more like
that, with the one minus one there, and there, and there. So those are eight vectors
in eight-dimensional space, those are called wavelets,
and it's a very simple wavelet choice, it's a
more sophisticated choice. This is a little jumpy, to
jump between one and minus one. And, actually, you
can see, now, suppose you compare the wavelet basis
with the Fourier basis above. How could I write this guy,
which is in the Fourier basis, it's an eight --
it's a vector in R^8. How would I write
that as a combination of the wavelet basis? Have I told you enough about the
wavelet basis that you can see, how does this very fast guy -- what combination of the wavelet
basis is that very fast guy? It would be this one -- it would be the sum
of these four, right? That very fast guy will
be that one minus one, and the next one, and the
next one, and the next one. So this is the sum of
those last four wavelets. This one, we've kept, and so on. So, each -- well, every --
well, that's what a basis does. Every vector in R^8 is
some combination of those, and for the linear algebra --
so the linear algebra is this step, find the coefficient. That's the step we want to take. What if I give you the basis,
like this wavelet basis, and I give you the pixel -- so
here are the pixel values, P1, P2, down to P8 -- what's the job? What's the linear algebra here? So these are the values, this
is in the standard basis, right? Those are just the values
at eight successive points. I guess I'm dropping down to
one dimension, instead of eight by eight, I'm just going
to take eight pixel values along that first top row. So what do I want to do? In standard basis, here
are the pixel values. I want to write that as a
combination of c1 times this guy, plus c2 times
this guy, plus c3, these are the coefficients,
plus c4 times this one -- do you see what I'm doing? I want to write this vector P
as a combination of c1 times the first wavelet plus c8
times the eighth wavelet. That's the transform step. That's the lossless step. That's the step from P --
oh, I'm calling it P here, and I called it x
there, so let me -- at the risk of moving, and
therefore making this jumpy -- suppose the signal I'm now
calling P, that a pixel values, and I'm looking for
the coefficients. OK, tell me how to do it. If I give you eight
basis vectors, and I give you the input signal,
and I ask for the coefficients, what do I do? What's the step? I'm trying to solve this, I want
to know the eight coefficients, so I'm changing from the
standard basis, which is just the eight gray-scale values
to the wavelet basis, where the same vector is
represented by eight numbers. It's got to take eight numbers
to tell you a vector in R^8, and those eight numbers are
the coefficients of the basis. Look, we've done
this thing before. There is the equation
in vector notation, we want to see it as a matrix. This is a combination of
columns of the wavelet matrix, right? This is P equals
c1, c2, down to c8, and these guys are the columns. I mean, this is the step
that we're constantly taking in this course,
the first basis vector goes in the first column,
the second basis vector goes in the second
column, and so on, the eight columns of
this wavelet matrix are the eight basis vectors. This is a wavelet matrix W. So, the step to change basis
-- so now I'm finally coming to this change-of-basis, so
the change of basis that, let me stay with this
board, but -- well, let me just go above it, here. So the standard basis, we know,
the wavelet basis we have here, and the transform is
simply, solve the equations, P=W C. So the coefficients
are W inverse P. Right. This shows a critical point. A good basis has a
nice, fast, inverse. So good basis means what? So this is like the
billion-dollar competition, Eh? and it's not over yet. People are going to come up
with better bases than these. So a good basis will be, first
good thing would be fast. I have to be able to multiply
by W fast, and multiply by W -- by its inverse fast. That's -- if a basis doesn't
allow you to do that fast, then it's going to take so much
time that you can't afford it. So these bases -- the Fourier basis,
everybody said, OK, I know how to deal quickly
with the Fourier basis, because we have something called
the Fast Fourier Transform. So there's a FFT that came
in my earlier lecture, and comes in the last
chapter of the book, so change-of-basis is done
-- if, for the Fourier basis, it's done fast by the FFT
and there's a fast wavelet transform. I can change, for
this wavelet example, this matrix is easy to invert. It's just somebody
had a smart idea in choosing that wavelet
basis and inverting it, it has a nice inverse. Actually, you can see why
it has a nice inverse. Do you see any property of
these eight basis vectors? Well, I've only
written five of them, but if you see that
property for those five, you'll see it for the three remaining. Well, if I give you
those eight vectors and ask, what's a nice property? Well, you would say, first,
they're all ones and minus ones and zeroes. So every multiplication is very
fast using -- just in binary. But what's the other great
property of those vectors? Anybody see it? So, of course, when I
think about a basis, one nice property -- I don't have to have it, but
I'm happy if it's there -- is that they're orthogonal. If the basis vectors
are orthogonal, then I'm in good shape. And these are... do you see? Take the dot product
of that with that, you get four plus ones and
four minus ones, you get zero. Take the dot product
of that with that. You get two plus ones
and two minus ones. Or the dot product
of that with that. Two plus ones and
two minus ones. You can easily check that
that's an orthogonal basis. It's not orthonormal. To fix it up, I should
divide by the length, to make them unit vectors. Let's suppose I do that. So somewhere in here, I've
got to account for the fact that this has length
square root of eight, that has length square
root of four, that has length square root of two. But that's just a constant
factor that's easy to -- so suppose we've done that. Then, tell me what's W inverse? That's what chapter four,
section four point four was about. If we have orthonormal
columns then the inverse is the
same as the transpose. So if we have a fast way to
multiply by W, which we do, the inverse is going
to look just the same, and we'll have a fast way to do W inverse. So that's the wavelet basis
passes this requirement for fast. We can use it fast. But there's a second
requirement, is it any good? Because the the
very fastest thing we could do is not to
change basis at all. Right? The fastest thing would be, OK,
stay with the standard basis, stay with eight pixel values. But that was poor from
compression point of view, right? Those eight pixel values, if I
just took those eight numbers, I can't throw some
of those away. If I throw away
ninety percent -- if I compress ten to
one, and throw away ninety percent of
my pixel values, well, my picture's
just gone dark. Whereas, the basis
that was good, the wavelet basis or
the Fourier basis, if I throw away c5, c6, c7,
and c8, all I'm throwing away is little blips that
are probably there in very small amounts. So the second property that
we need is good compression. So first, it has to be fast, and
secondly, a few basis vectors should come close to the signal. So a few is enough. Can I write it that way? A few basis vectors are enough
to reproduce the image just exactly as on a video
of these 18.06 lectures. Uh, I don't know what the
compression rate is, I'll ask, David, who does the
compression -- and, by the way, I'll try to get the lectures,
that are relevant for the quiz up onto the Web in time. So I'll send them
a message today. So, he's using the Fourier
basis because the JPEG -- so JPEG two thousand, which will
be the next standard for image compression, will
include wavelets. So, I mean, you're actually
getting a kind of up-to-date, picture of where this big world
of signal and image processing is. That Fourier is
what everybody knew, and what people
automatically used, and the new one is
wavelets, where this is the simplest set of wavelets. And this isn't the one that
the FBI uses, by the way, the FBI uses a smoother
wavelet, instead of jumping from one to minus
one, it's a smooth, Cutoff. and, that's what we'll be
in in JPEG two thousand. OK, so that's that application. Now, let me come to the
math, the linear algebra part of the lecture. Well, we've actually
seen a change-of-basis. So let -- let me just review
that eh-eh change-of-basis idea, and then the i- and then
the transformation to a matrix. OK. So this, I hope you see
that these applications are really big. Now, I have to talk a little
about change-of-basis, and a little about that. The matrix. OK. OK. OK. So change-of-basis. Basically, forgive
that put, OK, I have, I have my vector in
one basis, and I want to change to a different one. Actually, you saw it
for the wavelet case. So I need the -- let the matrix W,
and the columns of W be the new basis vectors. Then the change-of-basis
involves, just as it did there, W inverse. So we have the vector,
say, x, in the old basis, and that converts to a vector,
let's say, c, in the new basis, and the relation is exactly what
we had there, that x is W c. That's the step we have to take. There's a matrix W that
gives us a change-of-basis. OK. What I want to do is think about
transformations on matrices. So here's the question
to complete this lecture. Suppose I have a linear
transformation T. So we would think of it as an
eight -- as a n by n matrix. And it's computed with
respect to a certain basis. So T -- no, I'm sorry. I've got the
transformation T, period. That's taking
eight-dimensional space to eight-dimensional space. Now, let's get
matrices in there. OK. So, with respect to a first
basis, say v1 up to v8, it has a matrix A. I'm just setting
up letters here. With respect to a second basis,
say, I'll make it u1 up to -- or w1, since I've used (w)s,
w1 up to w8, it has a matrix B. And my question is, what's
the connection between A How is the matrix -- the
transformation T is settled. and B? We could say, it's a
rotation, for example. So that would be
one transformation of eight-dimensional space,
just spin it a little. Or project it. Or whatever linear
transformation we've got. Now, we have to remember -- my first step is to remind you
how you create that matrix A. Then my second step is, we would
use the same method to create B, but because it came from
the same transformation, there's got to be a
relation between A and B. What's the relation
between A and B? And let me jump to the
answer on that one. That if I have the
same transformation, and I'm compute on its matrix in
one basis, and then I computer it in another basis, those
two matrices are similar. So these two
matrices are similar. Now, do you remember what
similar matrices meant? Similar. A is similar to -- the
two matrices are similar. Similar. And what do I mean by that? I mean that I take the matrix
B, and I can compute it from the matrix A using
some similarity, some matrix M on one side, and M
inverse on the other. And this M will be the
change-of-basis matrix. This part of the lecture
is, admittedly, compressed. What I wanted you to -- it's really the conclusion
that I want you to spot. Now, I have to go back and
say, what does it mean for A to be the matrix of
this transformation T. So I have to remind
you what that meant, that was in the last lecture. Then this is the conclusion
that if I change to a different basis, we now know -- see, if
I change to a different basis, two things happen. Every vector has
new coordinates. There, the rule is this one,
between the old coordinates and the new ones. Every matrix changes, every
transformation has a new matrix. And the new matrix
is related this way, the M could be
the same as the W. The M there would be the W here. OK. So, can I, in the
remaining minutes, recapture my lecture -- the
end of my lecture that was just before Thanksgiving,
about the matrix? OK. What's the matrix? And I'll just take one basis. So now this part is going
to go onto this board here. What is the matrix? What is A? OK. Using a basis v1 up to v8. Mm. OK. What's the point? The point is, if I know
what the transformation does to those eight basis vectors,
I know it completely. I know T, I know
everything about T, I know T completely from knowing
T of V -- what T does to v1, what T does to v2,
what T does to v8. Why is that? It's because T is a
linear transformation. So that if I know what
these outputs are -- so these are the
inputs v1 up to v8, these are the outputs
from the transformation, like everyone rotated,
everyone projected, whatever transformation
I've done, then why is it that
I know everything? How does linearity work? Why? This is because every x is
some combination of these basis vectors, right? c1v1, c2v2, c8v8,
they were a basis. That's the whole
point of a basis, that every vector is a
combination of the basis vectors in exactly one way. And then, what is T of x? The point is, I claim that
we know T of x completely for every x, because every x
is a combination of those -- and now we use the linear
transformation part to say that the output from x has to be c1
times the output from v1 plus v2 times the output
from v2, and so on. Up through c8 times
the output from v8. So this is like just saying, OK. We know everything
when we know what T does to each basis vector. OK. So those are the
eight things we need. Now -- but we need these
answers in this basis. So this first output
is some combination of the eight basis vectors. So write T acting on the
first input -- in other words, write the first output as
a combination of the basis vectors, say a11 v1 +
a21 v2 and so on a81 v8. Write T of v2 as some
combination a12 of v1, a22 of v2 and so on. I'm creating the matrix
A, column by column. Those numbers go in
the first column, these numbers go in the second
column, the matrix A that thi- this -- this is our matrix that
represents T in this basis is these numbers. a11 down to a18, a21
down to a28, and so on. OK. That's the recipe. In other words, if I give you
a transformation, and a basis. So that's what I
have to give you. The inputs are the
basis and to tell you what the transformation is. And then, you tell me -- you compute T for
each basis, expand that result in the
basis, and that gives you the sixty four numbers
that go into the matrix A. Let me suppose -- let's close
with the best example of all. Suppose v1 to v8, this
basis, is the eigenvectors. Suppose we have an eigenvector
basis so that T(vi) is in the same direction of vi. Now, my question is, what is A? Can you carry through the steps? Let's do them together, because
we can do it in one minute. So, we've chosen
this perfect basis. And, actually, with
signal image processing, they might look for
the eigenvectors. But that would take
more calculation time that just saying, OK,
we'll use the wavelet basis. Or, OK, we'll use
the Fourier basis. But the very best basis
is the eigenvector basis. OK, what's the matrix? So, what's the first
column of the matrix? How do I get the first column? I take the first
basis vector v1. I opt -- I look to see, what
does the transformation do to it? The output is lambda one v1. I express that output as a
combination so the first input is v1. Its output is lambda one v1. Now write lambda one v1 as
a combination of the basis vectors, well,
it's already done. It's just lambda one times
the first basis vector and zero times the others. So this first column will
have lambda one and zeroes. OK. Second input is v2. Output is lambda two v2. OK, write that output as
a combination of the (v)s. It's already done. It's just lambda two
times the second v. So we need, in
the second column, we have lambda two
times the second v. Well, you see what's
coming, that in that basis, in the eigenvector basis,
the matrix is diagonal. So that's the
perfect basis, that's the basis we'd love to
have for image processing, but to find the eigenvectors
of our pixel matrix would be too expensive. So we do something
cheaper and close, which is to choose a
good basis like wavelets. OK, thanks. So I'll -- quiz review
on Wednesday, all day. Thanks.

CS

The following content it's
provided by MIT OpenCourseWare, under a Creative
Commons license. Additional information about our
license and MIT OpenCourseWare, in general, is available
at ocw.mit.edu. PROFESSOR: Several
pieces of good news. Since Brett is back now,
all of chapters five and six of my notes, that is to say,
all that we've covered in class, plus a little more, are
on the web page now. So, I was cautious at the
beginning of the semester and didn't put that up, and then
forgot that it wasn't there. So now you have
something to work with, particularly on any
experiments, for example, minimum degree ordering. Oh and also,
there's a movie now. Tim Davis from Florida and
[? Perils ?] [? Person ?], my friend here, created a little
movie to show the order that elimination occurs. And I had a lot of
email with Tim Davis. On that one-page handout,
for 2D Laplace's equation, it mentions N cubed for the
nested dissection, where you cut the region in half, and
half, and half, by separators. I believe, and I think
experiments will show, that minimum degree
is even better. But the analysis is
extremely weak, apparently, on minimum degree. So anyway, one possible
project either for now or for the final project
would be some experiments to see what the actual
convergence is like. And maybe a little
understanding of minimum degree. So we'll talk more about that. Anyway, there's a movie
and a lot more material there on the website. It'll get updated and
corrected as I do the edits. Second bit of good news. I made a mistake. That's not normally good
news, but this time, I reported a little
calculation for multigrid, and I computed the wrong thing. You remember that I computed
M. M was the Jacobi matrix. So M was the Jacobi matrix, the
iteration matrix with Jacobi, which is I minus D
inverse A This is for the 1D second differences. In fact, this was
the 5 by 5 matrix that we've been talking about. And now this is multiplied
by D inverse which is a half, but then with a weighting
factor, that becomes a third. So that's weighted Jacobi now. And this is what we would get
for ordinary, simple iteration. And it's not satisfactory. Remember it had an eigenvalue
lambda_max, the spectra radius, was about 0.9. Then I reported about
the eigenvalues of M*S, the multigrid matrix S, M, and
I was a little disappointed in its eigenvalues. The reason is, I should
have been doing I minus S M. When I do that,
I'm not disappointed anymore. What are the
eigenvalues of this? You remember, S is the
matrix that tells us how much error we're capturing. S is the matrix that came
from-- the error we captured at the end of
multigrid, was S times the error that we entered with. And so the error that's
left, the remaining error, the new error, is the rest. It's the part we don't get,
and that's when I forgot. That's I minus S e. So that's why I should
have been using I minus S. So then, the step one of
multigrid does a smoother. Steps two, three, four
leave me I minus S. And step five did
another smoother. And this is just with one Jacobi
step, which you would probably do three. Anyway, the eigenvalues
of this were, well -- this has two zero
eigenvalues, and three 1's. So this matrix has
eigenvalues 0, 0, 1, 1, 1. Because, why's that? Well, we get two 0's-- you
remember the eigenvalues had to be 0 or 1, because
S squared equaled S, and I minus S squared
equals I minus S. So. We remember that. Now, the question
is, what are these? And the answer is 1/9
is a triple eigenvalue. I was astonished to discover
1/9-- of course, 0, 0 survived. The rank is 3. And this is now
looking much better. What I reported last time
was some more like 7/8, or something, and I
thought oh, multigrid, it's not showing
its true colors. But actually it does. This is a kind eigenvalue
we expect, like 1/10, for a multigrid cycle. So much better. If we just did three
M's, for example, I would get down to 0.9
cubed, which is more than 0.7, where by doing multigrid
instead, we're down to 0.1. OK. So that's the good news. And, of course, if you
experiment a little, you'll find different
numbers for different sizes and really see what's happening. And, in fact, you could do 2D. I mentioned all
this partly for now, if you're wanting to do it
now, or partly for eventually, later. Oh, and thinking about projects,
just one word to repeat, that the Monday
after spring break, I think that's April
the third, no class. So it'll be Wednesday
that I'll see you again. But Mr. [? Cho ?]
will be available. He has office hours,
so that's April 3, if you wanted to discuss
issues with your project with Mr. [? Cho ?], that would
be at class time in his office 2-130, at one o'clock. Otherwise, take an
extra spring break, and I'll see you Wednesday. OK. So that's various
bits of good news. Oh. One other thing that I guess I
didn't finish in that lecture, was to identify the
two eigenvalues. Well, eigenvectors. Those will be in the notes too. I guess I found one
eigenvector, [1, 2, 2, 2, 1]. And glancing at the matrix,
I didn't spot the other one, but it has one oscillation. It's [1, 2, 0, -2, -1]. Anyway, this is the main thing. A much better result.
So that's multigrid, which we now can use
as our iteration. Or we can use it as
a preconditioner. Many of the
recommended methods use multigrid as a preconditioner
before something even more powerful. Well, I don't know about
even more powerful. Multigrid people would
say not possible. But some situations, we may
want to go to a different method but we want a preconditioner,
and multigrid excellent. By multigrid, I
include the smoother. So it'd be that. That would be a
possible preconditioner for something else. OK. So where are we now? We're into the new section
starting this moment. And what's the section about? It's about things associated
with this name, Krylov. So, I'm going to use the letter
K for the things that are associated-- and I'm always
solving A*u equals b. There's a Krylov matrix
that's created exactly as you see here. This is the j-th-- the
one with j columns. b, A*b, A squared b, up to
A to the the j minus 1 b. And the Krylov space is the
combinations of those vectors. That's what this
word, span, means. I could erase span, and
say, "all combinations of". Maybe that even more familiar. So "spanned by"
is the same thing is saying "all linear
combinations of". Of those j vectors. And, of course, that's
the same as saying it's the column
space of the matrix, because the column space
is all combinations. OK. So, why am I interested? Why was Krylov interested? Why is everybody interested
in these vectors? Because actually, that's what an
iteration like Jacobi produces. If I use Jacobi's method-- or
Gauss-Seidel, any of those-- after one step, I've got b. After two steps, there's a
multiplication by A in there, right? And some combination
is taken, depending on the particular method. So after two steps, I've got
a combination of b and A*b. After three steps, Jacobi
produces some combination of b, A*b, A squared b. In other words, all of
those iterative methods are picking their
j-th approximation in this space K_j, actually. I should put a little j on it,
to indicate that that's-- so we have these spaces are growing. They grow by one
dimension, by one new basis vector at each iteration. And the point is, Jacobi
makes a particular choice of x_j or u_j, the
approximation after j steps. But does it make
the best choice? Probably not. In fact, pretty definitely not. And so the idea is let's
make the best choice. Let's not just use
a simple iteration that builds in a choice that
might not be so terrific. Let's make the best choice. There are a whole lot of Krylov
methods, which all choose x_j-- since I think this
section will use x, I'm going to change that to x. They'll all be iterative. They'll all start
with x_0 as 0, say. And then the first
guess will be b, maybe. Or maybe not. Maybe some multiple of b. Actually, a good
Krylov method will take the best multiple
of b as the first guess, and not necessarily 1 times b. And onwards. So we have this word,
best, coming up. What's the best
vector in that space? And there are different
methods depending on what I mean by best. Oh, let me tell you
the name of the method that I'm going to concentrate
on first and most. Will be the conjugate
gradient method. CG. The conjugate gradient method. What determines x_j? It chooses x_j in K_j,
the space that we always look for our approximation in. Let me not forget to say: these
vectors, b, A*b, A squared b, and so on, they're
easy to compute, because each one is just a
matrix multiplication from the previous one. And the matrix is--
we're assuming we're working with sparse matrices. And mostly, and
especially, sometimes, especially symmetric ones. So just let me put in
here, before I even finish that sentence,
that CG, this is for A transpose
equal A symmetric. It's only for those. And positive definite so
symmetric, positive definite. So it's a limited class of
problems, but a highly, highly important class. And you may say what do
we do if the matrix is symmetric, but indefinite? Well, that comes next. Or what if the matrix is
not to symmetric at all. Well, if you're brave, you might
try conjugate gradients anyway. But if you're cautious,
then you would use one of the other
methods like MINRES. Maybe I'll just mention one
more on our eventual list. Actually that tells us
probably what choice it makes. MINRES chooses the x_j
to minimize the residual. Minimum r. Remember r is b minus A*x. The residual r
will always denote the error in the equation. And so it's minimum residual. OK. So that's a natural
choice, but you'll see that this is a
fantastic method. Superior, just quicker than
MINRES, for a nice reason. So it chooses x_j in K_j,
and I think the rule it uses is that I think x_j should be
orthogonal-- I'll go and check it in my notes-- to
the residual r_j. Let me just be sure-- if I'm
going to write that down, I better be sure I
said it correctly. I actually, I didn't
say it correctly. r_j is orthogonal to
the whole space K_j. It turns out that
we can choose x_j-- when I make a choice
of x_j, it's in K_j. Now when I compute r,
there's a multiplication by A to get the residual, so that
moves us up to the next space, and I'm going to
make a choice where this is orthogonal to
the whole space K_j. You have to do
conjugate gradient. And I can't start on
that for a good reason. I have to start with
something called Arnoldi. And and what's Arnoldi about? Well. Let me come back
to these vectors. Quick to compute. But what's the other property? Everything in applied
numerical mathematics, you're choosing basis
vectors, and you're looking for two properties. And I guess this is like
a general rule, whenever you meet a whole new
problem, in the end, you're going to construct
some basis vectors if you're going to compute. And what properties
are you after? You're after speed. So they have to be quick
to compute and work with. Well these are. Multiplying by A is sparse
matrix multiplication, you can't beat that. But the other
property that you need is some decent independence. And not just barely independent. You want the condition
number of the basis, somehow-- if I use
that word-- to be good. Not enormous. And best of all, you would
like the basis vectors to be orthonormal. That's a condition
number of one. That's the best basis
you can hope for. And the point is,
that this construction produces a lousy basis from
the point of view of condition. So Arnoldi's job is
orthogonalize the Krylov basis, which is b, A*b, and so on,
producing orthonormal basis q_1, q_2, up to q_j. It's just beautiful. So Arnoldi's taking
like a preliminary step that Krylov has to make to
get something that numerically reasonable to work with. Then if it's fast, and Arnoldi
doesn't-- we have to do a multiplication by A,
and you'll see one here in the middle of Arnoldi--
and then, of course, if you look at those
ten lines of MATLAB, and we could go through
them a little carefully, but let's just
take a first look. The first step is like
Gram-Schmidt, right? It's sort of a
Gram-Schmidt idea. You take that first vector
b, you accept that direction, and the only step remaining
is to normalize it. Divide by its length. Then you go on to the next. Then you have some
trial vector t, which would in this case be
A*b, in the direction of A*b, which won't be orthogonal
to the original b, right? So this won't be
orthogonal to that one. So, of course,
you've got to compute an inner product between them. And subtract off
the right multiple of the previous one
from the current t to get the improved t. And then you're going
to normalize it again. So you compute its length
and divide by the length. You see that overall
pattern in Arnoldi? It's the familiar idea
of Gram-Schmidt of, take a new vector,
find its projections onto the ones that
are already set, subtract those components
off, you're left with a piece, and you find its
length and normalize that to be a unit factor. It's very familiar. Its exactly the
type of algorithm that you would write
down immediately. And it just involved the
one multiplication by A so that it's not
going to cost us a lot to make the
vectors orthonormal. Well. Wait a minute. Is it going to be
expensive or not? That's like the key question. It's certainly going to
produce a good result. Producing orthonormal vectors is
going to be a good thing to do. But is it expensive or not? That depends. It's certainly not expensive
if the new trial vector t has only components
in one or two of the already
settled directions. In other words, if I
only have to subtract off a couple of earlier
components, then I'm golden. And that's the case
when A is symmetric. So that will be
the key point here. And I just make it here. If A is symmetric, A transpose
equals A, then I only need, in this subtracting off, where
I'm headed for the new q, I only need to subtract off
h_(j, j), multiplying the q_j, the q that was just
set, and the one before. h_(j-1, j). I'll call that a
short recurrence. It's short because it
only has two terms. And then there'll
be a new h_(j+1, j), which is just the length. So there'll be one of these
magic things in so many parts of mathematical analysis, a
three-term recurrence relation will hold-- in fact,
I guess the reason we see three-term recurrence
relations in all-- Legendre polynomials,
Chebyshev polynomials, all those classical
things-- the reason is actually the
same as it is here. That something in the
background is symmetric. I want to put a few
numbers on the board so you see what is typical
input and output to Arnoldi. And you'll see it's
symmetric and then you'll see it's short recurrence,
and then we want to see what? OK. So I worked out a
typical Arnoldi example. Not that one. Here. OK. I think this is a good
example to look at. So matrix A is not only
symmetric, it's diagonal. So of course, to find A
inverse b is not difficult. But we're going to do it
through conjugate gradients. So that means that we
don't figure out A inverse, which, of course,
we easily could. We just use A, very sparse. Here's our right-hand side. Here's our Krylov matrix. It has b in its first column. It has A times b in
the second column. Multiply by A again to
get the third column. And A one more time to
get the fourth column. So that's K_4, right? And for a 4 by 4
problem, that's the end. So actually here I'm carrying
Krylov to the end of it. There's no more to do. Because once I've got this
K_j, the combinations of those, are all of R^4, all of
four-dimensional space. Oh, yeah. That raises an important
point about how we're improving on iterations. If I was using Jacobi,
or Gauss-Seidel, or one of those others,
then-- well, I won't say for this particular A, but
usually-- after four steps, I am by no means finished. I've taken four steps,
I've gotten closer, but I haven't got
the exact answer. But now, in this the
world of Krylov methods, after four steps, I will
have the exact answer. Why? Because Krylov methods take
the best x_4 out of the space spanned by those four columns. Well, the whole space, R^4
is spanned by those columns and taking the best
one must be the answer. So x_4 will actually
be A inverse b, the answer we're looking for. So these methods are on the
one hand, direct methods. They finish after
four steps, finish. You've got the answer. Now. That's actually
how they were born. As direct methods
that gave the answer from an interesting
iteration and they nearly died for that reason. They were born and died,
or were on death's door, as direct methods. Because as direct methods,
there are better ones. Then some years later,
people went back to it, back to conjugate
gradients, and noticed that thought of as
just going partway, gave very successful answers. So we're going to think
of Krylov-- we're planning to stop before j equals n. In this picture, j
equals n equal 4 here. But, we plan to stop
for j much below n. We're thinking of applications
where n is 10 the fifth, and we're going to stop at
10 squared, 100 steps, maybe. Or ten steps. So that meant a total
reconsideration, reanalysis of
conjugate gradients, and it is now back
to a big success. Well it's a rather unusual
thing that a method that people have put aside gets picked up
again and becomes a favorite, as conjugate gradients have. First of all, I was saying that
the basis, those basis vectors are not very independent. That's not a good basis. Of course, it's a
detractive basis and maybe, do you know the
name for a matrix that has this particular form? The columns are constant, then,
that's the key column, there, and then it's first powers,
second powers, third powers. Do you remember whose name is
associated with that matrix? It comes up in interpolation,
and it starts with a V? Anybody know? Vandermonde, yeah. Vandermonde. So I could call it
V for Vandermonde. And Vandermonde matrices are
not very well conditioned. So now a little timeout to
say, because it's so important, how do you judge the good or
poor conditioning of a matrix? Those vectors are
linearly independent. The determinant
of V is not zero. The matrix is not singular,
but it's too close to singular, and how do you test
the-- suppose you have a basis, as we have here. And you want to know
is it good or not? So, of course, you always
put your bases vectors, let me call them-- well,I don't
want to call them v because -- well I guess I
could call them v, they come out of Vandermonde.
v_1, v_2, v_3, v_4. That's our matrix. Essentially I want
its condition number. And to find its
condition number, I-- it's not symmetric,
so I can't just take the eigenvalues of that. You might say, look at
lambda_max and lambda_min. The condition
number is associated with max divided by min. But when the matrix
isn't symmetric, just taking its eigenvalues
directly is not cool. It's not reliable. I could have a matrix
that's badly conditioned, but all its eigenvalues were 1. I mean a matrix with
1's on the diagonal and zillions up
above the diagonal would have eigenvalues of 1, but
it would be badly conditioned. So the right way to take
it is V transpose V. Look at v_1 transpose,
v_2 transpose... As always, if a matrix
isn't symmetric, if the matrix V
is not symmetric, good idea to form V transpose
V. That is symmetric. It does have
positive eigenvalues. And those eigenvalues, the
eigenvalues of V transpose V, are the singular values, or
rather the singular values squared, of V. So
I guess I'm saying, you can't trust the
eigenvalues of V. It's the singular
values you can trust. And the way to find
singular values is form V transpose
V, that gives you a symmetric matrix,
its eigenvalues-- so the i-th eigenvalue
would be the i-th singular value squared. So the condition number, is
sigma_max over sigma_min. And, well, it's not enormous
for this 4 by 4 matrix, but if I go up to 10
by 10 or 100 by 100. 100 by 100 would just
totally wipe me out. 10 by 10 would
already be disastrous. Completely disastrous actually. 10 by 10 Vandermonde matrix,
with 1, 2 3, 4, up to 10 as the points, would be--
well, it would have entries-- if that ended in a
10 and I had 10 rows, would that be something
like 10 to the ninth power. I mean the dynamics
scale would be terrible. So finally, just
to-- V transpose V is called the Gram matrix. So that guy Gram is coming back
in as giving the good measure. So the point was then, that
this measures the dependence or independence of the v's. That ratio. The bigger that ratio is,
the more dependency they are. What's the ratio if
they're orthonormal? What's the ratio of the q's? What's the condition
number of the q's? If we've run Arnoldi and
got a good basis, it's one. What's the Gram matrix? If this is Q transpose Q, with
the q's, the orthonormal basis in the columns, then Q
transpose Q is the identity. So it's a Gram matrix. So Q transpose Q
would be the identity, and its condition number
is the best possible one. Lambda_max is 1,
lambda_min is 1. The condition number is one. OK. So that's just some comments
about why Arnoldi gets brought in to fix the situation. OK. So I'll just leave those ten
Arnoldi steps on the board. The notes give ten comments. I'm quite proud of-- my
MATLAB is unreliable. Actually you'll
find a few errors like after end, I
accidentally put a semicolon, which was absurd. But the comments
I'm pleased with, and then the
numerical example that runs through one cycle of
this, with these numbers, to see what q_2 is,
is, I hope, useful. Why do we have a
short recurrence? This is a key point here. And in this example, if I work
out the h's, I'll discover sure enough, h_(1, 3) will be
0. h_(1, 4) will be 0. Here's the key equation. Here's Arnoldi in
matrix language. Let me see if I can remember
Arnoldi in matrix language. So, Arnoldi is taking
the matrix-- yeah. So Arnoldi in matrix
language is going to be this. It's going to A*Q equals Q*H.
I can't write out all of Q. So that's the big equation. Its a very important
equation that we now have all the pieces for. So A is our original
matrix that we were given. Symmetric let's say. Q is our basis out of Arnoldi
and H is the multipliers that gave that basis. So this Q*H is a little
bit like Gram-Schmidt. Do you remember, Gram-Schmidt
is described by Q times R. Q, again, is orthonormal. So it's an orthogonal matrix. In Gram-Schmidt R
is upper triangular. Here it's not. Here it's Hessenberg. So H stands for Hessenberg. I'll write down
what the actual H is for these for these numbers. I won't write down the
Q. I'll just write down what H turned out to
be for those numbers if I did it right. Five halves. Oh, interesting. The lengths all turned
out to be five halves. And this turned out
to be root 5 on 2. This turned out to be the
square root of 4 over 5, and this turned out to be
the square root of 9 over 20. And the point is from here, this
one is just below the diagonal. And it will show
up as symmetric. Root 5 over 2, root
4/5 and root 9/20. OK. So, what am I seeing from
that particular H which somehow can't be an accident? It must be that it's built in. It's the fact that H is
symmetric and tridiagonal. And what does that
tridiagonal tell us? It tells us that we
have short recurrences. It's the three-term
recurrence relation, is what I'm seeing here
in matrix language, because there were three
non-zeros in the columns of H. All right. I was going to write
out-- I was going to try to understand this one
by looking at the first column. What if I take the first
column of both sides? That'll be A times Q_1, right? Q_1 is the first column vector
in the-- first basis vector. And what do I have here
when I take q's times h's? Do you do matrix multiplication
a column at a time? You should. OK. So this says take 5/2
of the first column. And this says take that factor
times the second column. And I could track back
and see, yes that's what Arnoldi has produced. And then the second one, the
next one, would be an A*q_2. the next would an A*q_3. Well, look, here it is. I want to show that H
is symmetric when A is. Could you do that? We know what the
property of Q is here. We know that Q is, because
Arnoldi made it that way, has Q transpose Q equal
I. And I can, in one step, show that H is symmetric
if A is symmetric. How do you do that? I guess we need a
formula for H, so I just multiply by Q inverse. So that's good. And even better is to
recognize what Q inverse is. So what is Q inverse here? It's Q transpose. Anytime we see Q, that's
my letter, always, for an orthogonal matrix. So this is Q transpose
A Q. And now what? The argument's finished. We're here. If A is symmetric, what can
you say about that combination? If A is a symmetric matrix? It is symmetric. Right? That's how you get the
symmetric matrices. You start with one. You multiply on one side by a
matrix, and on the other side by its transpose. The thing has to be
symmetric, because if I transpose this whole
thing, what will happen? To transpose things
their transposes come in the opposite order. So Q, its transpose comes first. A, its transpose comes in
the middle, but what's that? The transpose of A is A. We're
assuming A to be symmetric. And then the transpose
of Q transpose is? Is Q. So we got this back
again when we transpose, so it's symmetric. So H is symmetric. So the conclusion is,
H equals H transposed. And then we know immediately
that it's tridiagonal, because every H from
Arnoldi is Hessenberg. We know that these
zeros are here. The Arnoldi cycle ended
produced h i j in column j but it's stopped one
below the diagonal. So we know these are
zero, but now, if we know the matrix is symmetric,
then we know these are zero. And, of course, it
works out that way. So the conclusion is that we
can orthogonalize the Krylov basis, quickly, easily,
and work with that basis, either explicitly
by computing it or by implicitly by
keeping things orthogonal and that's what conjugate
gradients will do. So next time, I'm
going to make an effort to describe the conjugate
gradients method. I'll pick the highlights of it. It has fantastic properties,
and to verify those properties in full detail is often
more confusing than not. If you see today's
lecture, you're seeing the important
points: the role of symmetry of A and the Arnoldi algorithm. OK. so that's our first
lecture on the Krylov ideas. And next time, we'll
probably complete that topic, and it will be on the web. So I'll see you Wednesday
for the end of Krylov. Thanks. Good.

Probability

The following content is
provided under a Creative Commons license. Your support will help
MIT OpenCourseWare continue to offer high-quality
educational resources for free. To make a donation or to
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. PHILIPPE RIGOLLET: We're talking
about goodness-of-fit tests. Goodness-of-fit tests
are, does my data come from a particular
distribution? And why would we
want to know this? Well, maybe we're
interested in, for example, knowing if the zodiac signs
of the Fortune 500 CEOs are uniformly distributed. Or maybe we actually
have slightly more-- slightly deeper endeavors,
such as understanding if you can actually apply the
t-test by testing normality of your sample. All right? So we saw that there's
the main result-- the main standard test for this. It's called the
Kolmogorov-Smirnov test that people use quite a bit. It's probably one of the
most used tests out there. And there's other versions of
it that I mentioned passing by. There's the Cramer-von
Mises, and there's the Anderson-Darling test. Now, how would you
pick one of such tests? Well, they're always are
going to-- they're always going to have their
advantages and disadvantages. And Kolmogorov-Smirnov is
definitely the most widely used because-- well, I guess because
it's a natural notion of distance between functions. You just look for each
point how far they can be, and you just look
at the farthest they can be everywhere. Now, Cramer-von Mises
involves L2 distance. So if you're not used to
Hilbert spaces or notions of Euclidean spaces, at least
it's a little more complicated. And then Anderson-Darling
is definitely even more complicated. Now, each of these
tests is going to be more powerful
against other alternatives. So unless you can really
guess which alternative you're expecting to
see, which you probably don't, because, again, you're
in a case where you want to typically declare H0
to be the correct one, then it's really a
matter of tossing a coin. Maybe you can run
all three of them and just sleep better at night,
because all three of them have failed to
reject, for example. All right? So as I mentioned, one of
the maybe primary goals to test goodness of fit
is to be able to check whether we can apply
Student's test, right, and if the Student
distribution is actually a valid distribution. And for that, we need to have
normally distributed data. Now, as I said several
times, normally distributed, it's not a specific
distribution. It's a family of
distributions that's indexed by means and variances. And the way I would want to test
if a distribution is normally distributed is,
well, I would just look at the most natural
normal distribution or Gaussian distribution
that my data could follow. That means that's the
Gaussian distribution that has the same mean as my data
and the same empirical variance as my data, right? And so I'm going to be
given some points x1, xn, and I'm going to be
asking, are those Gaussian? That means this is
equivalent to, say, are they N mu sigma square
for some mu sigma squared? And of course,
the natural choice is to take mu hat to be-- mu to be equal to mu
hat, which is xn bar. And sigma squared to be
sigma squared hat to be, well, Sn hat-- Sn-- what we wrote Sn, which
is 1/n sum from i equal 1 to n of xi minus xn bar squared. OK? So this is definitely
the natural one you would want to test. And maybe you could actually
just close your eyes and just stuff that in a
Kolmogorov-Smirnov test. OK? So here, there's a few
things that don't work. The first one is that
Donsker's theorem does not work anymore, right? Donsker's theorem
was the one that told us that,
properly normalized, this thing would
actually converge to the supremum of a Brownian
bridge, which is not true. So that's one problem. But there's actually
an even bigger problem is that this distribution,
we will check in a second, actually does not-- is pivotal itself, right,
the statistic is pivotal. It does not have a
distribution that depends on the known
parameters, which is sort of nice, at
least under the null. However, the distribution
is not the same as the one that had
fixed mu and sigma. The fact that they come
from some random variables is actually distorting
the distribution itself. And in particular, the quantiles
are going to be distorted, and we hinted at that last time. So one other thing I
need to tell you, though, is that this thing actually--
so I know there's some-- oh, yeah, that's where
there's a word missing. So we compute the quantiles
for this test statistic. And so what I need
to promise to you is that these
quantiles do not depend on any unknown parameter, right? I mean, it's not clear, right? So I want to test whether
my data has some Gaussian distribution. So under the null, all I
know is that my xi's are Gaussian with some mean mu
and some variance sigma, which I don't know. So it could be
the case that when I try to understand the
distribution of this quantity under the null, it depends on mu
and sigma, which I don't know. So we need to check
that this is the case. And what's actually
our redemption here is actually going
to be the supremum. The supremum is going
to basically allow us to, say, sup out
mu and sigma square. So let's check that, right? So what I'm interested in
is this quantity, supremum over t and R of the
difference between Fn of t and, what I write, phi mu
hat sigma squared of t. So phi mu hat
sigma hat squared-- sorry, sigma hat squared-- is the CDF of some Gaussian with
mean mu hat and variance sigma hat squared. And so in particular, this
thing here, phi hat of mu hat-- sorry, phi hat of mu hat
sigma hat squared of t is the probability that
some x is less than t, where x follows some N
mu hat sigma hat squared. So what it means is that
by just the translation and scaling trig
that we typically do for Gaussian to turn it into
some standard Gaussian, that implies that there
exists some z, which is standard Gaussian this
time, so mean 0 and variance 1, such that x is equal
to sigma hat x-- sorry, z plus mu hat. Agreed? That's basically saying that x
has some Gaussian with mean mu and variance sigma squared. And I'm not going to say the
hats every single time, OK? So OK, so that's what it means. So in particular, maybe
I shouldn't use x here, because x is going
to be my actual data. So let me write y. OK? So now what is this guy here? It's basically-- so phi hat. So this implies that phi mu
hat sigma hat squared of t is equal to the probability
that sigma hat z plus mu hat is
less than t, which is equal to the probability
that z is less than t minus mu hat divided
by sigma hat, right? But now when z is
the standard normal, this is really just the
cumulative distribution function of a standard
Gaussian but evaluated at a point which is
not t, but t minus mu hat divided by sigma hat. All right? So in particular, what I know--
so from this what I get-- well, maybe I'll remove that,
it's going to be annoying-- I know that phi mu hat
sigma hat squared-- sorry-- phi mu hat
sigma hat squared of t is simply phi of, say, 0, 1. And that's just the notation. Usually we don't put those,
but here it's more convenient. So it's phi 0, 1 of t minus
mu hat divided by sigma hat. OK? That's just something
you can quickly check. There's this nice way of writing
the cumulative distribution function for any
mean and any variance in terms of the cumulative
distribution function with mean 0 and variance 1. All right? Not too complicated. All right. So I know what I'm going to say
is that, OK, I have this sup here. So what I can write is
that this thing here is equal to the sup
routine R of 1/n. Let me write what Fn is-- sum from i equal 1
to n of the indicator that xi is less than
t minus phi 0, 1 of t minus mu hat
divided by sigma hat. OK? I actually want to make
a change of variable so that this thing
I'm going to call mu-- u, sorry. OK? And so I'm going to
make my life easier, and I'm going to
make it appear here. And so I'm just going to
replace this by indicator that xi minus mu hat divided
by sigma hat less than t minus mu hat divided
by sigma hat, which is sort of useless at this point. I'm just making my
formula more complicated. But now I see something
here that shows up, and I will call it u,
and this is another u. OK? So now what it means is that
suping over t, when t ranges from negative infinity
to plus infinity, the new range is from negative
infinity to plus infinity, right? So this sup, I can
actually write-- this suping t I can
write as the sup in u, as the indicator that xi minus
mu hat divided by sigma hat is less than u
minus phi 0, 1 of u. Now, let's pause for one second. Let's see where we're going. What we're trying to show
that this thing does not depend on the unknown
parameters, say, mu and sigma, which are the mean and the
variance of x under the null. To do that, we
basically need to make only quantities that are sort
of invariant under these values. So I tried to make this thing
invariant under anything, and it's just really something
that depends on nothing. It's the CDF. It doesn't depend on sigma
hat and mu hat anymore. But sigma hat and mu hat will
depend on mu and sigma, right? I mean, they're actually good
estimators of those guys, so they should be
pretty close to them. And so I need to make
sure that I'm not actually doing anything wrong here. So the key thing here is going
to be to observe that 1/n sum from i equal 1 to n of indicator
of xi minus u hat divided by sigma hat less than u, which
is the first term that I have in this absolute value,
well, this is what-- well, this is equal to 1/n sum from
i equal 1 to n of indicator that-- well, now under
the null, which is that x follows N mu sigma
squared, for some mu and sigma squared that are unknown. But they are here. They exist. I just don't know what they are. Then xi minus mu can be
written as sigma zi plus mu minus mu hat divided
by sigma hat, where z is equal to x minus mu
divided by sigma, right? That's just the same
trick that I wrote here. OK? Everybody agree? So I just standardize-- sorry, z-- yeah, so zi is xi
minus mu i minus mu divided by sigma. All right? Just a standardization. So now once I write
this, I can actually divide everybody by sigma. Right? So I just divided on top
here and in the bottom here. So now what I need to check
is that the distribution of this guy does not
depend on mu or sigma. That's what I claim. What is the distribution
of this indicator? It's a Bernoulli, right? And so if I want to
understand its distribution, all I need to do is to
compute its expectation, which is just the probability
that this thing happens. But the probability
that this thing happens is actually now depending
on mu and sigma. And the reason is
that mu is what? Well, it's x bar-- sorry, yeah,
so mu hat-- sorry, is xn bar. So mu hat minus mu,
which under the null follows N mu sigma
square over n, right? That's the property
of the average. So when I do mu hat minus
mu divided by sigma, this thing is what distribution? It's still a normal. It's a linear
transformation of a normal. What are the parameters? AUDIENCE: 0, 1/n. PHILIPPE RIGOLLET: Yeah, 0, 1/n. But this does not depend
on mu or sigma, right? Now, I need to check
that this guy does not depend on mu or sigma. What is the distribution
of sigma hat over sigma? AUDIENCE: It's a
chi-square, right? PHILIPPE RIGOLLET: Yeah,
it is a chi-square. So this is actually-- sorry, sigma hat squared
divided by sigma squared is a chi-square with n
minus 1 degrees of freedom. Does not depend on mu or sigma. AUDIENCE: [INAUDIBLE] AUDIENCE: [INAUDIBLE] AUDIENCE: Or sigma hat
squared over sigma squared? PHILIPPE RIGOLLET:
Yeah, thank you. So this is actually
divided by it. So maybe this guy. Let's write it like that. This is the proper
way of writing it. Thank you. Right? So now I have those two things. Neither of them
depends on mu or sigma. I these two things. There's just one
more thing to check. What is it? AUDIENCE: That
they're independent? PHILIPPE RIGOLLET: That
they're independent, right? Because the dependence
in mu and sigma could be hidden
in the covariance. It could be the case that the
marginal distribution of mu does not depend on mu or sigma,
that the marginal distribution of sigma-- of mu hat does not
depend on mu and sigma. The marginal
distribution of sigma hat does not depend on mu or
sigma, but their correlation could depend on mu and sigma. But we also have
that if I look at-- so if I look at-- so since mu hat is
independent of sigma hat, it means that the joint
distribution of mu hat divided by sigma and sigma
hat divided by sigma does not depend on blah,
blah, blah, on mu and sigma. OK? Agree? It's not in the individual
ones, and it's not in the way they interact
with each other. It's nowhere. AUDIENCE: [INAUDIBLE]
independence be [INAUDIBLE] theorem? PHILIPPE RIGOLLET: Yeah,
covariance theorem, right. So that's something we've
been using over and again. That's all under the null. If my data is not Gaussian,
nothing actually holds. I just use the fact
that under the null I'm Gaussian for some mean mu
and variance sigma squared. But that's all I care about. When I'm designing
a test, I only care about the distribution
under the null, at least to control the type I error. Then to control
the type II error, then I cross my
fingers pretty hard. OK? So now this basically implies
what's written on the board, that this distribution,
this test statistic, does not depend on any
unknown parameters. It's just something
that's pivotal. In particular, I could
go at the back of a book and check if there's a table for
the quantiles of these things, and indeed there are. This is the table that you see. So actually, this is
not even in a book. This is in Lilliefors
original paper, 1967, as you can tell from
the typewriting. And he actually probably
was rolling some dice from his office back in
the day and was checking that this was-- he
simulated it, and this is how he computed those numbers. And here you also have
some limiting distribution, which is not the sup of
a Brownian motion over 0, 1 of-- sorry, of a
Brownian bridge over 0, 1, which is the
one that you would see for the
Kolmogorov-Smirnov test, but it's something that's
slightly different. And as I said, these numbers are
actually typically much smaller than the numbers you
would get, right? Remember, we got something
that was about 0.5, I think, or maybe 0.41, for the
Kolmogorov-Smirnov test at the same
entrance, which means that using
Kolmogorov-Lilliefors test it's going to be
harder for you not to reject for the same data. It might be the case that
in one case you reject, and in the other one
you fail to reject. But the ordering is
always that if you fail to reject with
Kolmogorov-Lilliefors, you will fail to reject with
Kolmogorov-Smirnov, right? There's always one. So that's why people
tend to close their eyes and prefer Kolmogorov-Smirnov
because it just makes their life easier. OK? So this is called
Kolmogorov-Lilliefors. I think there's
actually an E here-- sorry, an I before the E.
Doesn't matter too much. OK? Are there any questions? Yes? AUDIENCE: Is there
like a place you can point to like [INAUDIBLE] PHILIPPE RIGOLLET: Yeah. AUDIENCE: [INAUDIBLE]. PHILIPPE RIGOLLET: So the
fact that it's actually a different distribution
is that here-- so if I actually knew
what mu and sigma were, I would do exactly
the same thing. But here, rather than having
this average with mu and sigma, I would just have the-- with mu hat and sigma
hat, I would just have the average
with mu and sigma. OK? So what it means is
that the key thing is that what I would compare is
the 1/n sum of some Bernoullis with parameter. And the parameter here would
be the probability that mu-- xi minus mu over
sigma is less than u, which is just the
probability that phi-- sorry, it's a Bernoulli
with probability F of t. Well, let me write
what it is, right? So that's minus phi 0, 1 of t. OK? So that's for the K-S test,
and then I sup over t, right? That's what I would have
had, because this is actually exactly the right thing. Here I would remove
the true mean. I would divide by the
true standard deviation. So that would actually end
up being a standard Gaussian, and that's why I'm allowed
to use phi 0, 1 here. Agreed? And these are Bernoullis
because they're just indicators. What happens in the
Kolmogorov-Lilliefors test? Well, here the
Bernoulli, the only thing that's going to change
is this guy, right? They still have a Bernoulli. It's just that the parameters
of the Bernoulli are weird. The parameters of the
Bernoulli looks like it's-- it becomes the probability that
some N(0, 1) plus some N(0, 1/n), right, divided by some
square root of chi-squared n minus 1 divided by
n is less than t. And those things
are independent, but those guys are not
necessarily independent, right? And so why is this
probability changing? Well, because this denominator
is actually fluctuating a lot. So that actually makes
this probability different. And so that's basically
where it comes from, right? So you could probably
convince yourself very quickly that this only
makes those guys closer. And why does it make
those guys closer? No, sorry. It makes those guys
farther, right? And it makes those guys farther
for a very clear reason, is that the expectation of this
Bernoulli is exactly that guy. Here I think it's
going to be true as well that the expectation
of this Bernoulli is going to be that guy,
but the fluctuations are going to be much bigger than
just the phi of the Bernoulli. Because the first
thing I do is I have a random parameter
from my Bernoulli, and then I flip the Bernoulli. So fluctuations are going to
be bigger than a Bernoulli. And so when I take
the sup, I'm going to have to [INAUDIBLE] them. So it makes things
farther apart, which makes it more
likely for you to reject. Yeah? AUDIENCE: You also said that if
you compare the same-- if you compare the table and you
set at the same level, the Lilliefors is like 0.2,
and for the Smirnov is at 0.4. PHILIPPE RIGOLLET: Yeah. AUDIENCE: OK. So it means that Lilliefors
is harder not to reject? PHILIPPE RIGOLLET: It means
that Lilliefors is harder not to reject, yes,
because we reject when we're larger than the number. So the number being smaller
with the same data, we might be, right? So basically, it
looks like this. What we run-- so here we have
the distribution for the-- so let's say this is
the density for K-S. And then we have the density for
Kolmogorov-Lilliefors, K-L. OK? And what the density
of K-L looks like, it looks like this, right? And so if I want to
squeeze in alpha here, I'm going to have to squeeze
in-- and I squeeze in alpha here, then this is the
quantile of order 1 minus alp-- well, let's say
alpha of the K-L. And this is the
quantile alpha of K-S. So now you give me data,
and what I do with it, I check whether they're
larger than this number. So if I apply K-S, I check
whether I'm larger or smaller than this thing. But if I apply
Kolmogorov-Lilliefors, I check whether I'm larger
or smaller than this thing. So over this entire range of
values for my test statistic-- because it is the
same test statistic, I just plugged in mu
hat and sigma hat-- for this entire range, the two
tests have different outcomes. And this is a big range
in practice, right? I mean, it's between-- I mean, it's pretty
much at scale here. OK? Any other-- yeah? AUDIENCE: [INAUDIBLE] when n
goes to infinity, the two tests become the same now, right? PHILIPPE RIGOLLET: Hmmm. AUDIENCE: Looking
at that formula-- PHILIPPE RIGOLLET: Yeah,
They should become the same very far. Let me see, though, because-- right. So here we have 8-- so here we have, say,
for 0.5, we get 0.886. And for-- oh, I don't have it. Yeah, actually, sorry. So you're right. You're totally right. This is the Brownian
bridge values. Because in the limit
by, say, Slutsky-- sorry, I'm lost. Yeah, these are
the values that you get for the Brownian bridge. Because in the limit
by Slutsky, this thing is going to have no
fluctuation, and this thing is going to have no fluctuation. So they're just going
to be pinned down, and it's going to look like as
if I did not replace anything. Because in the limit, I know
those guys much faster-- the mu hat and
sigma hat converge much faster to mu and sigma than
the distribution itself, right? So those are actually
going to be negligible. You're right. Actually even, I didn't have-- these are actually the
numbers I showed you for the bridge, the
Brownian bridge, last time, because I didn't have
it for the Kolmogorov-Smirnov one. OK? So there's actually-- so those
are numerical ways of checking things, right? I give you data. You just crank the
Kolmogorov-Smirnov test. Usually you press a 5 on MATLAB. But let's say you actually
compute this entire thing, and there's a number
that comes out, and you decide whether it's
large enough or small enough. Of course, statistical software
is going to make your life even simpler by spitting out a
p-value, because you can-- I mean, if you can compute
quantiles, you can also when compute p-values. And so your life is
just fairly easy. You just have red is bad, green
is good, and then you can go. The problem is that those are
numbers you want to rely on. But let's say you
actually reject. Let's say you reject. Your p-value is actually
just like slightly below 5%. So you can say, well, maybe
I'm just going to change my p-value-- my threshold to
1%, but you might want to see what's happening. And for that you need
a visual diagnostic. Like, how do I check
if something departs from being normal, for example? How do I check if
a distribution-- why is a distribution not
a uniform distribution? Why is a distribution not
an exponential distribution? There's many, many, right? If I have an
exponential distribution and half of my
values are negative, for example, well, there's
like pretty obvious reasons why it should not
be exponential. But it could be
the case that it's just the tails
are little heavier or there's more
concentration at some point. Maybe it has two modes. There's things like this. But the real thing,
we don't believe that the Gaussian is
so important because it looks like this close to 0. What we like about the
Gaussian is that the tails here decay at this rate--
exponential minus x squared over 2 that we described
in the maybe first lecture. And in particular, if there
were like kinks around here, it wouldn't matter too much. This is not what makes
issues for the Gaussian. And so what we want is to have a
visual diagnostic that tells us if the tails of my
distribution are comparable to the tails of
a Gaussian one, for example. And those are what's called
quantile-quantile plots, and in particular-- or QQ plots. And the basic QQ plots
we're going to be using are the ones that are
called normal QQ plots that are comparing your data to
a Gaussian distribution, or a normal distribution. But in general, you could
be comparing your data to any distribution you want. And the way you do
this is by comparing the quantiles of your data,
the empirical quantiles, to the quantiles of
the actual distribution you're trying to
compare yourself to. So this, in a way,
is a visual way of performing these
goodness-of-fit tests. And what's nice about visual is
that there's room for debate. You can see something that
somebody else cannot see, and you can always-- because
you want to say that things are Gaussian. And we'll see some examples
where you can actually say it if you are good at
debate, but it's actually going to be clearly not true. All right. So this is a quick
and easy check. That's something
I do all the time. You give me data, I'm
just going to run this. One of the first
things I do so I can check if I can start
entering the Gaussian world without compromising
myself too much. And the idea is to say, well,
if F is close to-- if F-- if my data comes
from an F, and if I know that Fn is close
to F, then rather than computing some norm,
some number that tells me how far they are,
summarizing how far they are, I could actually plot
the two functions and see if they're far apart. So let's think for one second
what this kind of a plot would look like. Well, I would go
between 0 and 1. That's where everything
would happen. Let's say my distribution is
the Gaussian distribution. So this is the CDF of N(0, 1). And now I have this guy
that shows up, and remember we had this piecewise constant. Well, OK, let's say we
get something like this. We get a piecewise constant
distribution for Fn, right? Just from this, and even despite
my bad skills at drawing, it's clear that it's
going to be hard for you to distinguish
those two things, even for a fairly
large amount of points. Because the problem is
going to happen here, and those guys look pretty
much the same everywhere you are here. You're going to see differences
maybe in the middle, but we don't care too much
about those differences. And so what's going to
happen is that you're going to want to compare
those two things. And this is basically you
have the information you want, but visually it just doesn't
render very well because you're not scaling things properly. And the way we actually do it
is by flipping things around. And rather than comparing the
plot of F to the plot of Fn, we compare the
plot of Fn inverse to the plot of F inverse. Now, if F goes from the real
line to the interval 0, 1, F inverse goes from 0, 1
to the whole real line. So what's going to
happen is that I'm going to compare things on
some intervals, which is the-- which are the entire real line. And then what values should I
be looking at those things at? Well, technically for
F, if F is continuous I could look at F inverse for
any value that I please, right? So I have F. And if I
want to look at F inverse, I pick a point here and I look
at the value that it gives me, and that's F inverse of,
say, u, right, if this is u. And I could pick
any value I want, I'm going to be able to find it. The problem is that
when I start to have this piecewise
constant thing, I need to decide what value I
assign for anything that's in between two jumps, right? And so I can choose
whatever I want, but in practice it's
just going to be things that I myself decide. Maybe I can decide
that this is the value. Maybe I can decide
that the value is here. But for all these guys, I'm
going to pretty much decide always the same value, right? If I'm in between-- for this value u, for this
jump the jump is here. So for this value,
I'm going to be able to decide whether I
want to go above or below, but it's always this value
that's going to come out. So rather than picking
values that are in between, I might as well just
pick only values for which this is the value
that it's going to get. And those values are
exactly 1/n, 2/n, 3/n, 4/n. It's all the way to n/n, right? That's exactly where
the flat parts are. We know we jump
from 1/n every time. And so that's
exactly the recipe. It says look at those
values, 1/n, 2/n, 3/n until, say, n minus 1 over n. And for those values,
compute the inverse of both the empiricial
CDF and the true CDF. Now, for the empirical
CDF, it's actually easy. I just told you this is
basically where the points-- where the jumps occur. And the jumps occur where? Well, exactly at
my observations. Now, remember I need to sort
those observations to talk about them. So the one that occurs
for the i-th jump is the i-th largest observation,
which we denoted by X sub (i). Remember? We had this formula that we
said, well, we have x1, xn. These are my data. And what I'm going
to sort them into is x sub (1), which is
less than or equal to x sub (2), which is
less than x sub (n). OK? So we just ordered them
from smallest to largest. And then now we've
done that, we just put this parenthesis notation. So in particular,
Fn inverse of i/n is the location where
the i-th jumps occur, which is the i-th
largest observation. OK? So for this guy, these
values, the y-axes are actually fairly easy. I know it's basically
my ordered observations. The x-values are-- well,
that depends on the function F I'm trying to test. If it's the Gaussian,
it's just the quantile of order 1 minus 1/n, right? It's this Q1 minus 1/n here
that I need to compute. It's the inverse of the
cumulative distribution function, which, given
the formula for F, you can actually compute or
maybe estimate fairly well. But it's something that
you can find in tables. Those are basically quantiles. Inverse of CDFs are
quantiles, right? And so that's basically the
things we're interested in. That's why it's called
quantile-quantile. Those are sometimes referred
to as theoretical quantiles, the one we're trying to test,
and empirical quantiles, the one that corresponds
to the empirical CDF. And so I'm plotting a plot
where the x-axis is quantile. The y-axis is quantile. And so I call this plot a
quantile-quantile plot, or QQ plot, because, well, just say
10 times quantile-quantile, and then you'll see why. Yeah? AUDIENCE: [INAUDIBLE] have
to have the [INAUDIBLE]?? PHILIPPE RIGOLLET:
Well, that's just-- we're back to the-- we're back to the
goodness-of-fit test, right? So if you look-- so you don't do it yourself. That's the simple answer. You don't-- I'm just telling you
how those plots are going to be seen spit out from a software
are going to look like. Now, depending on
the software, there's a different thing
that's happening. Some softwares are actually
plotting F with the right-- let's say you want to
do normal, as you asked. So some software are
just going to use F to be with mu hat and
sigma hat, and that's fine. Some software are actually
not going to do this. They're just going
to use a Gaussian. But then they're
going to actually have a different reference point. So what do we want to see here? What should happen
if all these points-- if all my points
actually come from F, from a distribution
that has CDF F? What should happen? What should I see? Well, since Fn
should be close to F, Fn inverse should be
close to F inverse, which means that this point should
be close to that point. This point should be
close to that point. So ideally, if I actually
pick the right F, I should see a plot that looks
like this, something where all my points are very
close to the line y is equal to x, right? And I'm going to have
some fluctuations, but something very
close to this. Now, that's if F is
exactly the right one. If F is not exactly the
right one, in particular, in the case of a Gaussian
one, if I actually plotted here the quantiles-- so if I plotted F
0, 1 of t, right? So let's say those are
the ones I actually plot, but I really don't know
what-- mu hat is not 0 and sigma hat is not 0. And so this is not the
one I should be getting. Since we actually know that
phi of mu hat sigma hat squared t is equal to phi 0,
1 of t minus mu hat divided by sigma hat, there's
just this change of axis, which is
actually very simple. This change of axis is just
a simple translation scaling, which means that
this line here is going to be transformed
into another line with a different slope
and a different intercept. And so some software
will actually decide to go with this curve
and just show you what the reference
curve should be, rather than actually
putting everything back onto the 45-degree curve. AUDIENCE: So if you
get any straight line? PHILIPPE RIGOLLET: Any
straight line, you're happy. I mean, depending
on the software. Because if the software actually
really rescaled this thing to have mu hat and sigma square
and you find a different line, a different straight
line, this is bad news, which is not
going to happen actually. It's impossible that happens,
because you actually-- well, it could. If it's crazy, it could. It shouldn't be very crazy. OK. So let's see what R does
for us, for example. So here in R, R actually
does this funny trick where-- so here I did not
actually plot the lines. I should actually add the lines. So the command is like
qqnorm of my sample, right? And that's really simple. I just stack all my data
into some vector, say, x. And I say qqnorm of x, and
it just spits this thing out. OK? Very simple. But I could actually
add another command, which I can't remember. I think it's like qqline,
and it's just going to add the line on top of it. But if you see, actually
what R does for us, it's actually doing the
translation and scaling on the axes themselves. So it actually changes
the x and y-axis in such a way that when you
look at your picture and you forget about what
the meaning of the axes are, the relevant straight
line is actually still the 45-degree line. It's Because it's actually done
the change of units for you. So you don't have to
even see the line. You know that, in your mind,
that this is basically-- the reference line is still
45 degree because that's the way the axes are made. But if I actually put my axes,
right-- so here, for example, it goes from-- let's look at some-- well, OK, those are all square. Yeah, and that's probably
because they actually have-- the samples are actually
from a standard normal. So I did not make
my life very easy to illustrate your
question, but of course, I didn't know you were
going to ask it. Next time, let's just prepare. Let's script more. We'll see another
one in the next plot. But so here what
you expect to see is that all the plots should be
on the 45-degree line, right? This should be the right one. And if you see, when I
start having 10,000 samples, this is exactly
what's happening. So this is as good as it gets. This is an N(0, 1) plotted
against the theoretical quantile of an N(0, 1). As good as it gets. And if you see, for the
second one, which is 50, sample size of size-- sample of size 50, there is
some fudge factor, right? I mean, those things-- doesn't look like there's
a straight line, right? It sort of appears that there
are some weird things happening here at the lower tail. And the reason why
this is happening is because we're trying to
compare the tails, right? When I look at this picture,
the only thing that goes wrong somehow is always at
the tip, because those are sort of rare
and extreme values, and they're sort of
all over the place. And so things are never really
super smooth and super clean. So this is what
your best shot is. This is what you will
ever hope to get. So size 10, right, so
you have 10 points. Remember, we actually--
well, I didn't really tell you how to deal
with the extreme cases. Because the problem is that
F inverse of 1 for the true F is plus infinity. So you have to make some sort
of weird boundary choices to decide what F inverse
of 1 is, and it's something that's like somewhere. But you still want to
put like 10 dots, right? 1, 2, 3, 4, 5, 6,
7, 8, 9, 10 dots. So I have 10 observations,
you will see 10 dots. I have 50 observations, you
will see 50 dots, right, because I have-- there are 1/n, 2/n,
3/n all the way to n/n. I didn't tell you the last one. OK. So this is when things
go well, and this is when things should not go well. OK? So here, actually,
the distribution is a Student's t with
15 degrees of freedom, which should depart somewhat
from a Gaussian distribution. The tails should be heavier. And what you can see is
basically the following, is that for 10 you actually see
something that's crazy, right, if I do 10 observations. But if I do 50
observations, honestly, it's kind of hard to say
that it's different from the standard normal. So you could still be
happy with this for 100. And then this is what's
happening for 10,000. And even here it's not the
beautiful straight line, but it feels like you
would be still tempted to conclude that it's a
beautiful straight line. So let's try to guess. So basically, there's-- for
each of those sides there's two phenomena. Either it goes like this
or it goes like this, and then it goes like
this or it goes like this. Each side corresponds to the
left tail, all the smallest values. So that's the left side. And that's the right
side-- corresponds to the large values. OK? And so basically
you can actually think of some sort of
a table that tells you what your QQ plot looks like. And so let's say it looks-- so we have our reference
45-degree line. So let's say this
is the QQ plot. That could be one thing. This could be the QQ plot
where I have another thing. Then I can do this guy,
and then I do this guy. So this is like this. OK? So those are the four cases. OK? And here what's changing
is the right tail, and here what's
changing is the-- and when I go from here to here,
what changes is the left tail. Is that true? No, sorry. What changes here is
the right tail, right? It's this part that
changes from top to bottom. So here it's something
about right tail, and here that's something
about left tail. Everybody understands what I
mean when I talk about tails? OK. And so here it's
just going to be a question of whether
the tails are heavier or lighter than the Gaussian. Everybody understand
what I mean when I say heavy tails and light tails? OK. So right, so heavy
tails just means that basically here
the tails of this guy are heavier than the
tails of this guy. So it means that if I draw
them, they're going to be above. Actually, I'm going to keep
this picture because it's going to be very useful for me. When I plug the quantiles
at the same-- so let's look at the right
tail, for example. Right here my picture
is for right tails. When I look at the quantiles of
my theoretical distribution-- so here you can see
the bottom curve we have the
theoretical quantiles, and those are the
empirical quantiles. If I look to the right here,
are the theoretical quantiles larger or smaller than
the empirical quantiles? Let me phrase it the other-- are the empirical
quantiles larger or smaller than the theoretical quantiles? AUDIENCE: This is a graph
of quantiles, right? So if it's [INAUDIBLE]
it should be smaller. PHILIPPE RIGOLLET: It
should be smaller, right? On this line, they are equal. So if I see the empirical
quantile showing up here, it means that here the
empirical quantile is less than the theoretical quantile. Agree? So that means that if
I look at this thing-- and that's for the
same values, right? So the quantiles are computed
for the same values i/n. So it means that the empirical
quantiles should be looking-- so that should be the
empirical quantile, and that should be the
theoretical quantile. Agreed? Those are the smaller
values for the same alpha. So that implies that the tails-- the right tail, is
it heavy or lighter-- heavier or lighter
than the Gaussian? AUDIENCE: Lighter. PHILIPPE RIGOLLET:
Lighter, right? Because those are the
tails of the Gaussian. Those are my
theoretical quantiles. That means that this is the tail
of my empirical distribution. So they are actually lighter. OK? So here, if I look
at this thing, this means that the right
tail is actually light. And by light, I mean
lighter than Gaussian. Heavy, I mean heavier
than Gaussian. OK? OK, now we can probably
do the entire thing. Well, if this is light, this
is going to be heavy, right? That's when I'm above the curve. Exercise-- is this light or is
this heavy, the first column? And it's OK. It should take you
at least 30 seconds. AUDIENCE: [INAUDIBLE]
different column? PHILIPPE RIGOLLET: Yeah,
this column, right? So this is something
that pertains-- this entire column is going
to tell me whether the fact that this guy is
above, does this mean that I have lighter
or heavier left tails? AUDIENCE: Well, on the
left, it's heavier. PHILIPPE RIGOLLET: On
the left, it's heavier. OK. I don't know. Actually, I need
to draw a picture. You guys are probably
faster than I am. AUDIENCE: [INTERPOSING VOICES]. PHILIPPE RIGOLLET:
Actually, let me check how much randomness is-- who says it's lighter? Who says it's heavier? AUDIENCE: Yeah,
but we're biased. AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: Yeah, OK. AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: All right. So let's see if it's heavier. So we're on the left tail, and
so we have one looks like this, one looks like that, right? So we know here that I'm
looking at this part here. So it means that here my
empirical quantile is larger than the theoretical quantile. OK? So are my tails
heavier or lighter? They're lighter. That was a bad bias. AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: Right? It's below, so it's lighter. Because the problem is that
larger for the negative ones means that it's smaller
[INAUDIBLE],, right? Yeah? AUDIENCE: Sorry but, what
exactly are these [INAUDIBLE]?? If this is the inverse-- if this is the inverse
CDF, shouldn't everything-- well, if this is
the inverse CDF, then you should
only be inputting values between 0 and 1 in it. And-- PHILIPPE RIGOLLET: Oh,
did I put the inverse CDF? AUDIENCE: Like on the
previous slide, I think. PHILIPPE RIGOLLET:
No, the inverse CDF, yeah, so I'm inputting-- AUDIENCE: Oh,
you're [INAUDIBLE].. PHILIPPE RIGOLLET: Yeah, so
it's a scatter plot, right? So each point is
attached-- each point is attached 1/n, 2/n, 3/n. Now, for each
point I'm plotting, that's my x-value, which
maps a number between 0 and 1 back onto the entire real line,
and my y-value is the same. OK? So what it means is that those
two numbers, this is in the-- this lives on the entire real
line, not on the interval. This lives on the entire real
line, not in the interval. And so my QQ plots take values
on the entire real line, entire real line, right? So you think of it as a
parameterized curve, where the time steps
are 1/n, 2/n, 3/n, and I'm just like putting a dot
every time I'm making one step. OK? OK, so what did we say? That was lighter, right? AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: OK? One of my favorite exercises
is, here's a bunch of densities. Here's a bunch of QQ plots. Map the correct QQ plot
to its own density. All right? And there won't be mingled
lines that allow you to do that, then you just have to follow,
like at the back of cereal boxes. All right. Are there any questions? So one thing--
there's two things I'm trying to
communicate here is if you see a QQ plot, now
you should understand, one, how it was built, and two,
whether it means that you have heavier tails or lighter tails. Now, let's look at this guy. What should we see? We should see heavy on the left
and heavy on the right, right? We know that this
should be the case. So this thing actually looks
like this, and it sort of does, right? If I take this line
going through here, I can see that this
guy's tipping here, and this guy's dipping here. But honestly-- actually, I can't
remember exactly, but t 15, if I plotted the density
on top of the Gaussian, you can see a difference. But if I just gave it to
you, it would be very hard for you to tell me if there's
an actual difference between t 15 and Gaussian, right? Those things are
actually very close. And so in particular,
here we're really trying to recognize what
the shape is the fact-- right? So t 15 compared to a standard
Gaussian was different, but t 15 compared to a Gaussian
with a slightly larger variance is not going to actually--
you're not going to see much of a difference. So in a way, such
distributions are actually not too far from the Gaussian,
and it's not too-- it's still pretty benign to
conclude that this was actually a Gaussian distribution because
you can just use the variance as a little bit of a buffer. I'm not going to
get really into how you would use a
t-distribution into a t-test, because it's kind of
like Inception, right? So but you could pretend
that your data actually is t-distributed and then
build a t-distribution from it, but let's not say that. Maybe that was a bad example. But there's like other
heavy-tailed distributions like Cauchy distribution, which
doesn't even have a-- it's not even integrable
because that's as heavy as the tails get. And this you can really tell
it's going to look like this. It's going to be like pfft. What does a uniform
distribution look like? Like this? It's going to be-- it's going
to look like a Gaussian one, right? So a uniform-- so
this is my Gaussian. A uniform is basically
going to look like this, one side take the right mean
and the right variance, right? So the tails are
definitely lighter. They're 0. That's as lighter as it gets. So the light-light is going
to look like this S shape. So an S-- light-tailed
distribution has this S shape. OK? What is the exponential
going to look like? So the exponential is
positively supported. It only has positive numbers. So there's no left tail. This is also as
light as it gets. But the right tail, is
it heavier or lighter than the Gaussian? AUDIENCE: Heavier. PHILIPPE RIGOLLET:
It's heavier, right? It's only the case like e of the
minus x rather e to the minus x squared. So it's heavier. So it means that on the
left it's going to be light, and on the right it's
going to be heavy. So it's going to be U-shaped. OK? That will be fine. All right. Any other question? Again, two messages,
like, more technical, and you can sort of fiddle
with it by looking at it. You can definitely
conclude that this is OK enough to be
Gaussian for your purposes. Yeah? AUDIENCE: So [INAUDIBLE] PHILIPPE RIGOLLET: I
did not hear the "if" at the beginning
of your sentence. AUDIENCE: I would want to
be lighter tail, right, because that'll be--
it's easier to reject? Is that correct? PHILIPPE RIGOLLET: So what
is your purpose as a-- AUDIENCE: I want to-- I have some [INAUDIBLE] right? I want to be able to say
I reject H0 [INAUDIBLE].. PHILIPPE RIGOLLET: Yes. AUDIENCE: So if you
wanted to make it easier to reject H0, then-- PHILIPPE RIGOLLET: Yeah, in
a way that's true, right? So once you've actually factored
in the mean and the variance, the only thing that actually-- right. So if you have Gaussian tails
or lighter-- even lighter tails, then it's harder for you
to explain deviations from randomness only, right? If you have a
uniform distribution and you see something which is-- if you're uniform on 0, 1 plus
some number and you see 25, you know this number is
not going to be 0, right? So that's basically
as good as it gets. And there's basically
some smooth interpolation if you have lighter tails. Now, if you start having
something that has heavy tails, then it's more likely
that pure noise will generate large observations
and therefore discovery. So yes, lighter
tails is definitely the better-behaved noise. Let's put it this way. The lighter it is, the
better behaved it is. Now, this is good-- this is good for some purposes,
but when you want to compute actual quantiles,
like exact quantiles, then it is true in general that
the quantiles of lighter-tail distributions are going to be
dominated by the-- are going to be dominated by the-- let's say on the
right tails, are going to be dominated by
those of a heavy distribution. That is true. But that's not always the case. And in particular,
there's going to be some like sort of weird points
where things are actually changing depending on what
level you're actually looking at those things,
maybe 5% or 10%, in which case things might
be changing a little bit. But if you started going
really towards the tail, if you start looking at levels
alpha which are 1% or 0.1%, it is true that it's always-- if you can actually--
so if you see something that looks light
tail, you definitely do not want to conclude
that it's Gaussian. You want to actually change
your modeling so that it makes your life even easier. And you actually
factor in the fact that you can see that the
noise is actually more benign than you would like it to be. OK? Stretching fingers, that's it? All right. OK. So I want to-- I mentioned at some point that
we had this chi-square test that was showing up. And I do not know what I did-- let's just-- oh, yeah. So we have this chi-square test
that we worked on last time, right? So the way I introduced the
chi-square test is by saying, I am fascinated
by this question. Let's check if it's correct, OK? Or something maybe
slightly deeper-- let's check if juries
in this country are representative of
racial distribution. But you could actually--
those numbers here come from a very specific thing. That was the uniform. That was our benchmark. Here's the uniform. And there was this guy,
which was a benchmark, which was the actual benchmark that we
need to have for this problem. And those things basically
came out of my hat, right? Those are numbers that exist. But in practice, you actually
make those numbers yourself. And the way you do it
is by saying, well, if I have a binomial
distribution and I want to test
if my data comes from a binomial
distribution, you could ask this question, right? You have a bunch of data. I did not promise
to you that this was the sum of independent
Bernoullis and [INAUDIBLE].. And then you can actually check
that it's a binomial indeed, and you have binomial. If you think about where
you've encountered binomials, it was mostly when
you were drawing balls from urns, which you probably
don't do that much in practice. OK? And so maybe one day you want
to model things as a binomial, or maybe you want to
model it as a Poisson, as a limiting binomial, right? People tell you photons arrive-- the rate of a photon
hitting some surface is actually a Poisson
distribution, right? That's where they
arise a lot in imaging. So if I have a colleague
who's taking pictures of the skies over night, and
he's like following stars and it's just like moving around
with the rotation of the Earth. And he has to do this
for like eight hours because he needs to get enough
photons over this picture to actually arise. And he knows they arrive
at like a Poisson process, and you know, chapter 7 of your
probability class, I guess. And And there's all
these distributions outside the classroom
you probably want to check that
they're actually correct. And so the first one you might
want to check, for example, is a binomial. So I give you a distribution,
a binomial distribution on, say, K trials, and
you have some number p. And here, I don't know
typically what p should be, but let's say I know it or
estimate it from my data. And here, since we're only
going to deal with asymptotics, just like it was the case for
the Kolmogorov-Smirnov one, in the asymptotic
we're going to be able to think of the estimated
p as being a true p, OK, under the null at least. So therefore, each outcome,
I can actually tell you what the probability of a binomial-- is this outcome. For a given K and a
given p, I can tell you exactly what a binomial
should give you as the probability
for the outcome. And that's what I actually use
to replace the numbers 1/12, 1/12, 1/12, 1/12 or the
numbers 0.72, 0.7, 0.12, 0.9. All these numbers I
can actually compute using the probabilities
of a binomial, right? So I know, for example, that the
probability that a binomial np is equal to, say, K is n
choose K p to the K 1 minus p to the n minus K. OK? I mean, so these are numbers. If you give me p
and you give me n, I can compute those numbers
for all K from 0 to n. And from this I can
actually build a table. All right? So for each K-- 0. So K is here, and
from 0, 1, et cetera, all the way to n, I can compute
the true probability, which is the probability that my
binomial np is equal to 0, the probability that my binomial
is equal to 1, et cetera, all the way to n. I can compute those numbers. Those are actually going
to be exact numbers, right? I just plug in the
formula that I had. And then I'm going to
have some observed. So that's going to be p
hat, 0, and that's basically the proportion of 0's, right? So here you have to remember
it's not a one-time experiment like you do in
probability where you say, I'm going to draw n
balls from an urn, and I'm counting how many-- how many I have. This is statistics. I need to be able to do
this experiment many times so I can actually, in the
end, get an idea of what the proportion of p's is. So you have not
just one binomial, but you have n binomials. Well, maybe I should
not use n twice. So that's why it's
the K here, right? So I have a binomial
[INAUDIBLE] at Kp and I just seize
n of those guys. And with this n of those
guys, I can actually estimate those probabilities. And what I'm going
to want to check is if those two
probabilities are actually close to each other. But I already know
how to do this. All right? So here I'm going
to test whether P is in some parametric
family, for example, binomial or not binomial. And testing-- if I know that
it's a binomial [INAUDIBLE],, and I basically just have to
test if P is the right thing. OK? Oh, sorry, I'm actually
lying to you here. OK. I don't want to test
if it's binomial. I want to test the parameter
of the binomial here. OK? So I know-- no, sorry,
[INAUDIBLE] sorry. OK. So I want to know if
I'm in some family, the family of binomials, or
not in the family of binomials. OK? Well, that's what I want to do. And so here H0 is basically
equivalent to testing if the pj's are the pj's
that come from the binomial. And the pj's here are the
probabilities that I get. This is the probability
that I get j successes. That's my pj. That's j's value here. OK? So this is the example,
and we know how to do this. We construct p hat,
which is the estimated proportion of successes
from the observations. So here now I have n trials. This is the actual maximum
likelihood estimator. This becomes a multinomial
experiment, right? So it's kind of confusing. We have a multinomial experiment
for a binomial distribution. The binomial here
is just a recipe to create some
test probabilities. That's all it is. The binomial here
doesn't really matter. It's really to create
the test probabilities. And then I'm going to define
this test statistic, which is known as the chi-square
statistic, right? This was the chi-square test. We just looked at sum of the
square root of the differences. Inverting the covariance matrix
or using the Fisher information with removing the part
that was not invertible led us to actually use
this particular value here, and then we had
to multiply by n. OK? And that, we know,
converges to what? A chi-square distribution. So I'm not going to
go through this again. I'm just telling you you
can use the chi-square that we've seen, where we just
came up with the numbers we were testing. Those numbers that were in this
row for the true probabilities, we came up with them
out of thin air. And now I'm telling
you you can actually come up with those guys
from a binomial distribution or a Poisson
distribution or whatever distribution you're happy with. Any question? So now I'm creating
this thing, and I can apply the entire theory
that I have for the chi-square and, in particular, that
this thing converges to a chi-square. But if you see, there's
something that's different. What is different? The degrees of freedom. And if you think about it,
again, the meaning of degrees of freedom. What does this word-- these words actually mean? It means, well, to
which extent can I play around with those values? What are the possible
values that I can get? If I'm not equal to this
particular value I'm testing, how many directions can I
be different from this guy? And when we had a
given set of values, we could be any other
set of values, right? So here, I had this-- I'm going to represent-- this
is the set of all probability distributions of vectors
of size K. So here, if I look at one
point in this set, this is something that looks
like p1 through pK such that their sum-- such that they're non-negative,
and the sum p1 through pK is equal to 1. OK? So I have all those points here. OK? So this is basically the
set that I had before. I was testing whether I
was equal to this one guy, or if I was anything else. And there's many ways
I can be anything else. What matters, of course,
is what's around this guy that I could actually
confuse myself with. But there's many ways I
can move around this guy. Agreed? Now I'm actually just testing
something very specific. I'm saying, well,
now the piece that I have had to come
from this-- have to be constructed from this
formula, this parametric family P of theta. And there's a fixed way for--
let's say this is theta, so I have a theta here. There's not that many ways
this can actually give me a set of probabilities, right? I have to move to another
theta to actually start being confused. And so here the number
of degrees of freedom is basically, how can I
move along this family? And so here, this
is all the points, but there might
be just the subset of the points that looks
like this, just this curve, not the half of this thing. And those guys on this
curve are the p thetas, and that's for all thetas
when theta runs across data. So in a way, this is just a
much smaller dimensional thing. It's a much smaller object. Those are only the
ones that I can create that are exactly of this
very specific parametric form. And of course, not
all are of this form. Not all probability
PMFs are of this form. And so that is going
to have an effect on what my PMF is going to be-- sorry, on what my-- sorry, what my degrees of
freedoms are going to be. Because when this thing is
very small, that means when-- that's happening when
theta is actually, say, a one-dimensional
space, then there's still many ways I can escape, right? I can be different
from this guy in pretty much every other direction,
except for those two directions, just
when I move from here or when I move in
this direction. But now if this
thing becomes bigger, your theta is, say,
two dimensional, then when I'm here
it's becoming harder for me to not be that guy. If I want to move
away from it, then I have to move away
from the board. And so that means that
the bigger the dimension of my theta, the smaller
the degrees of freedoms that I have, OK, because moving
out of this parametric family is actually very
difficult for me. So if you think, for
example, as an extreme case, the parametric family that I
have is basically all PMFs, all of them, right? So that's a stupid
parametric family. I'm indexed by the
distribution itself, but it's still
finite dimensional. Then here, I have basically
no degrees of freedom. There's no way I
can actually not be that guy, because this
is everything I have. And so you don't have
to really understand how the computation comes
into the numbers of dimension and what I mean by dimension
of this current space. But really, what's important is
that as the dimension of theta becomes bigger, I have
less degrees of freedom to be away from this family. This family becomes big,
and it's very hard for me to violate this. So it's actually shrinking
the number of degrees of freedom of my chi-square. And that's all you
need to understand. When d increases, the number of
degrees of freedom decreases. And I'd like to you to have an
idea of why this is somewhat true, and this is
basically the picture you should have in mind. OK. So now once I have done
this, I can just construct. So here I need to check. So what is d in the
case of the binomial? AUDIENCE: 1. PHILIPPE RIGOLLET: 1, right? It's just a
one-dimensional thing. And for most of
the examples we're going to have it's going
to be one dimensional. So we have this weird thing. We're going to have K
minus 2 degrees of freedom. So now I have this thing,
and I have this asymptotic. And then I can just basically
use a test that has-- that uses the fact that
the asymptotic distribution is this. So I compute my
quantiles out of this. Again, I made the same mistake. This should be q alpha,
and this should be q alpha. So that's just the
tail probability is equal to alpha when I'm
on the right of q alpha. And so those are
the tail probability of the appropriate chi-square
with the appropriate number of degrees of freedom. And so I can compute p-values,
and I can do whatever I want. OK? So then I just like [INAUDIBLE]
my testing machinery. OK? So now I know how to test if I'm
a binomial distribution or not. Again here, testing if I'm
a binomial distribution is not a simple goodness of fit. It's a composite one
where I can actually-- there's many ways I can
be a binomial distribution because there's as
many as there is theta. And so I'm actually plugging
in the theta hat, which is estimated from the data, right? And here, since everything's
happening in the asymptotics, I'm not claiming that Tn
has a pivotal distribution for finite n. That's actually not true. It's going to depend
like crazy on what the actual distribution is. But asymptotically,
I have a chi-square, which obviously does not
depend on anything [INAUDIBLE].. OK? Yeah? AUDIENCE: So in general, for
the binomial [INAUDIBLE] trials. But in the general
case, the number of-- the size of our PMF is
the number of [INAUDIBLE].. PHILIPPE RIGOLLET: Yeah. AUDIENCE: So let's
say that I was also uncertain about what
K was so that I don't know how big my [INAUDIBLE] is. [INAUDIBLE] PHILIPPE RIGOLLET:
That is correct. And thank you for this beautiful
segue into my next slide. So we can actually
deal with the case not only where it's
infinite, which would be the case of Poisson. I mean, nobody
believes I'm going to get an infinite
number of photons in a finite amount of time. But we just don't want to have
to say there's got to be a-- this is the largest
possible number. We don't want to
have to do that. Because if you start doing
this and the probabilities become close to 0, things become
degenerate and it's an issue. So what we do is we bin. We just bin stuff. OK? And so maybe if I have
a binomial distribution with, say, 200,000
possible values, then it's actually maybe
not the level of precision I want to look at this. Maybe I want to bin. Maybe I want to say,
let's just think of all things that
are between 0 and 100 to be the same thing, between
100 and 200 the same thing, et cetera. And so in fact, I'm
actually going to bin. I don't even have to think
about things that are discrete. I can even think about
continuous cases. And so if I want to test if I
have a Gaussian distribution, for example, I can just
approximate that by some, say, piecewise constant
function that just says that, well, if I have a Gaussian
distribution like this, I'm going to bin it like this. And I'm going to say, well,
the probability that I'm less than this value is this. The probability that I'm between
this and this value is this. The probability I'm
between this and this value is this, and then this
and then this, right? And now I've turned-- I've discretized, effectively,
my Gaussian into a PMF. The value-- this is p1. The value here is p1. This is p2. This is p3. This is p4. This is p5 and p6, right? I have discretized my Gaussian
into six possible values. That's just the probability that
they fall into a certain bin. And we can do this-- if you don't know what
K is, just stop at 10. You look at your data quickly
and you say, well, you know, I have so few of them that are--
like I see maybe one 8, one 11, and one 15. Well, everything
that's between 8 and 20 I'm just going to
put it in one bin. Because what else
are you going to do? I mean, you just don't
have enough observations. And so what we do is
we just bin everything. So here I'm going to actually
be slightly abstract. Our bins are going
to be intervals Aj. So here-- they don't even
have to be intervals. I could go crazy and just
like call the bin this guy and this guy, right? That would make no sense,
but I could do that. And then I'm-- and of course,
you can do whatever you want, but there's going to be some
consequences in the conclusions that you can take, right? All you're going
to be able to say is that my distribution
does not look like it could be binned in this way. That's all you're going
to be able to say. So if you decide to just
put all the negative numbers and the positive
numbers, then it's going to be very hard
for you to distinguish a Gaussian from a random
variable that takes values of minus 1 and plus 1 only. You need to just be reasonable. OK? So now I have my pj's
become the probability that my random variable
falls into bin j. So that's pj of theta under
the parametric distribution. For the true one, whether it's
parametric or not, I have a pj. And then I have
p hat j, which is the proportion of observations
that falls in this bin. All right? So I have a bunch
of observations. I count how many of
them fall in this bin. I divide by n, and that
tells me what my estimated probability for this bin is. And theta hat, well,
it's the same as before. If I'm in a
parametric family, I'm just estimating theta hat,
maybe the maximum likelihood estimator, plug it
in, and estimate those pj's of theta hat. From this, I form my
chi-square, and I have exactly the same thing as before. So the answer to your
question is, yes, you bin. And it's the answer to
even more questions. So that's why there
you can actually use the chi-square test
to test for normality. Now here it's going
to be slightly weaker, because there's only
an asymptotic theory, whereas Kolmogorov-Smirnov
and Kolmogorov-Lilliefors work actually even for
finite samples. For the chi-square test,
it's only asymptotic. So you just pretend you actually
know what the parameters are. You just stuff them
into a theta, a mu hat, and sigma square hat. And you just go to-- you
just cross your finger that n is large
enough for everything to have converged by the
time you make your decision. OK? And then this is a copy/paste,
with the same error actually as the previous slide, where
you just build your test based on whether you exceed
or not some quantile, and you can also
compute some p-value. OK? AUDIENCE: The error? PHILIPPE RIGOLLET: I'm sorry? AUDIENCE: What's the error? PHILIPPE RIGOLLET:
What is the error? AUDIENCE: You said [INAUDIBLE]
copy/paste [INAUDIBLE].. PHILIPPE RIGOLLET: Oh,
the error is that this should be q alpha, right? AUDIENCE: OK. PHILIPPE RIGOLLET: I've
been calling this q alpha. I mean, that's my
personal choice, because I don't want to-- I only use q alpha. So I only use quantiles where
alpha is to the right, so. That's what statisticians--
probabilists would use this notation. OK. And so some questions, right? So of course, in
practice you're going to have some issues
which translate. I say, well, how do you
pick this guy, this K? So I gave you some sort of a-- I mean, the way we
discussed, right? You have 8 and 10 and
20, then it's ad hoc. And so depending on whether
you want to stop K at 20 or if you want to bin those
guys is really up to you. And there's going to
be some considerations about the particular
problem at hand. I mean, is it
coarse-- too coarse for your problem to decide
that the observations between 8 and 20 are the same? It's really up to you. Maybe that's actually
making a huge difference in terms of what phenomenon
you're looking at. The choice of the bins, right? So here there's
actually some sort of rules, which are
don't use only one bin and make sure there's actually--
don't use them too small so that there's at least one
observation per bin, right? And it's basically
the same kind of rules that you would have
to build a histogram. If you were to build a
histogram for your data, you still want to
make sure that you bin in an appropriate fashion. OK? And there's a bunch
of rule of thumbs. Every time you ask
someone, they're going to have a
different rule of thumb, so just make your own. And then there's the
computation of pj of theta, which might
be a bit complicated because, in this
case, I would have to integrate the Gaussian
between this number and this number. So for this case, I
could just say, well, it's the difference of the CDF
in that value and that value and then be happy with it. But you can imagine that
you have some slightly more crazy distributions. You're going to have
to somewhat compute some integrals that might be
unpleasant for you to compute. OK? And in particular, I
said the difference of the PDF between that value
and that value of-- sorry, the CDF between that value
and that value, it is true. But it's not like
you actually have tables that compute the CDF
at any value you like, right? You have to sort of-- well, there might be
but at some degree, but you are going to have
to use a computer typically to do that. OK? And so for example, you
could do the Poisson. If I had time, if I had
more than one minute, I would actually do it for you. But it's basically the same. The Poisson, you are going
to have an infinite tail, and you just say,
at some point I'm going to cut everything
that's larger than some value. All right? So you can play around, right? I say, well, if you have extra
knowledge about what you expect to see, maybe you can
cut at a certain number and then just fold all the
largest values from K minus 1 to infinity so that
you actually have-- you have everything
into one large bin. OK? That's the entire tail. And that's the way people do
it in insurance companies, for example. They assume that the number of
accidents you're going to have is a Poisson distribution. They have to fit it to you. They have to know-- or at least to your pool of
insurance of injured people. So they just slice you
into what your character-- relevant characteristics
are, and then they want to estimate what the
Poisson distribution is. And basically, they can
do a chi-square test to check if it's indeed
a Poisson distribution. All right. So that will be it for today. And so I'll be-- I'll have your homework--

CS

We're going to start. We are going to start studying
today, and for quite a while, the linear second-order
differential equation with constant coefficients.
In standard form, it looks like,
there are various possible choices for the variable,
unfortunately, so I hope it won't disturb you
much if I use one rather than another.
I'm going to write it this way in standard form.
I'll use y as the dependent variable.
Your book uses little p and little q.
I'll probably switch to that by next time.
But, for today, I'd like to use the most
neutral letters I can find that won't interfere with anything
else. So, of course call the constant
coefficients, respectively,
capital A and capital B. I'm going to assume for today
that the right-hand side is zero.
So, that means it's what we call homogeneous.
The left-hand side must be in this form for it to be linear,
it's second order because it involves a second derivative.
These coefficients, A and B, are understood to be
constant because, as I said, it has constant
coefficients. Of course, that's not the most
general linear equation there could be.
In general, it would be more general by making this a
function of the dependent variable, x or t,
whatever it's called. Similarly, this could be a
function of the dependent variable.
Above all, the right-hand side can be a function of a variable
rather than simply zero. In that case the equation is
called inhomogeneous. But it has a different physical
meaning, and therefore it's customary to study that after
this. You start with this.
This is the case we start with, and then by the middle of next
week we will be studying more general cases.
But, it's a good idea to start here.
Your book starts with, in general, some theory of a
general linear equation of second-order,
and even higher order. I'm asking you to skip that for
the time being. We'll come back to it next
Wednesday, it two lectures, in other words.
I think it's much better and essential for your problems at
for you to get some experience with a simple type of equation.
And then, you'll understand the general theory,
how it applies, a lot better,
I think. So, let's get experience here.
The downside of that is that I'm going to have to assume a
couple of things about the solution to this equation,
how it looks; I don't think that will upset
you too much. So, what I'm going to assume,
and we will justify it in a couple lectures,
that the general solution, that is, the solution involving
arbitrary constants, looks like this.
y is equal-- The arbitrary constants occur in a certain
special way. There is c one y one plus c two
y two. So, these are two arbitrary
constants corresponding to the fact that we are solving a
second-order equation. In general, the number of
arbitrary constants in the solution is the same as the
order of the equation because if it's a second-order equation
because if it's a second-order equation, that means somehow or
other, it may be concealed. But you're going to have to
integrate something twice to get the answer.
And therefore, there should be two arbitrary
constants. That's very rough,
but it sort of gives you the idea.
Now, what are the y1 and y2? Well, as you can see,
if these are arbitrary constants, if I take c2 to be
zero and c1 to be one, that means that y1 must be a
solution to the equation, and similarly y2.
So, where y1 and y2 are solutions.
Now, what that shows you is that the task of solving this
equation is reduced, in some sense,
to finding just two solutions of it, somehow.
All we have to do is find two solutions, and then we will have
solved the equation because the general solution is made up in
this way by multiplying those two solutions by arbitrary
constants and adding them. So, the problem is,
where do we get that solutions from?
But, first of all, or rather, second or third of
all, the initial conditions enter into the,
I haven't given you any initial conditions here,
but if you have them, and I will illustrate them when
I work problems, the initial conditions,
well, the initial values are satisfied by choosing c1 and c2,
are satisfied by choosing c1 and c2 properly.
So, in other words, if you have an initial value
problem to solve, that will be taken care of by
the way those constants, c, enter into the solution.
Okay, without further ado, there is a standard example,
which I wish I had looked up in the physics syllabus for the
first semester. Did you study the
spring-mass-dashpot system in 8.01?
I'm embarrassed having to ask you.
You did? Raise your hands if you did.
Okay, that means you all did. Well, just let me draw an
instant picture to remind you. So, this is a two second
review. I don't know how they draw the
picture. Probably they don't draw
picture at all. They have some elaborate system
here of the thing running back and forth.
Well, in the math, we do that all virtually.
So, here's my system. That's a fixed thing.
Here's a little spring. And, there's a little car on
the track here, I guess.
So, there's the mass, some mass in the little car,
and motion is damped by what's called a dashpot.
A dashpot is the sort of thing, you see them in everyday life
as door closers. They're the thing up above that
you never notice that prevent the door slamming shut.
So, if you take one apart, it looks something like this.
So, that's the dash pot. It's a chamber with a piston.
This is a piston moving in and out, and compressing the air,
releasing it, is what damps the motion of the
thing. So, this is a dashpot,
it's usually called. And, here's our mass in that
little truck. And, here's the spring.
And then, the equation which governs it is,
let's call this x. I'm already changing,
going to change the dependent variable from y to x,
but that's just for the sake of example, and because the track
is horizontal, it seems more natural to call
it x. There's some equilibrium
position somewhere, let's say, here.
That's the position at which the mass wants to be,
if the spring is not pulling on it or pushing on it,
and the dashpot is happy. I guess we'd better have a
longer dashpot here. So, this is the equilibrium
position where nothing is happening.
When you depart from that position, then the spring,
if you go that way, the spring tries to pull the
mass back. If it goes on the site,
the spring tries to push the mass away.
The dashpot, meanwhile, is doing its thing.
And so, the force on the, m x double prime.
That's by Newton's law, the force, comes from where?
Well, there's the spring pushing and pulling on it.
That force is opposed. If x gets to be beyond zero,
then the spring tries to pull it back.
If it gets to the left of zero, if x gets to be negative,
that that spring force is pushing it this way,
wants to get rid of the mass. So, it should be minus kx,
and this is from the spring, the fact that is proportional
to the amount by which x varies. So, that's called Hooke's Law.
Never mind that. This is a law.
That's a law, Newton's law,
okay, Newton, Hooke with an E,
and the dashpot damping is proportional to the velocity.
It's not doing anything if the mass is not moving,
even if it's stretched way out of its equilibrium position.
So, it resists the velocity. If the thing is trying to go
that way, the dashpot resists it.
It's trying to go this way, the dashpot resists that,
too. It's always opposed to the
velocity. And so, this is a dash pot
damping. I don't know whose law this is.
So, it's the force coming from the dashpot.
And, when you write this out, the final result,
therefore, is it's m x double prime plus c x prime,
it's important to see where the various terms are,
plus kx equals zero. And now, that's still not in standard form.
To put it in standard form, you must divide through by the
mass. And, it will now read like
this, plus k divided by m times x equals zero.
And, that's the equation governing the motion of the
spring. I'm doing this because your
problem set, problems three and four, ask you to look at a
little computer visual which illustrates a lot of things.
And, I didn't see how it would make, you can do it without this
interpretation of spring-mass-dashpot,
-- -- but, I think thinking of it
of these constants as, this is the damping constant,
and this is the spring, the constant which represents
the force being exerted by the spring, the spring constant,
as it's called, makes it much more vivid.
So, you will note is that those problems are labeled Friday or
Monday. Make it Friday.
You can do them after today if you have the vaguest idea of
what I'm talking about. If not, go back and repeat
8.01. So, all this was just an
example, a typical model. But, by far,
the most important simple model.
Okay, now what I'd like to talk about is the solution.
What is it I have to do to solve the equation?
So, to solve the equation that I outlined in orange on the
board, the ODE, our task is to find two
solutions. Now, don't make it too trivial.
There is a condition. The solution should be
independent. All that means is that y2
should not be a constant multiple of y1.
I mean, if you got y1, then two times y1 is not an
acceptable value for this because, as you can see,
you really only got one there. You're not going to be able to
make up a two parameter family. So, the solutions have to be
independent, which means, to repeat, that neither should
be a constant multiple of the other.
They should look different. That's an adequate explanation.
Okay, now, what's the basic method to finding those
solutions? Well, that's what we're going
to use all term long, essentially,
studying equations of this type, even systems of this type,
with constant coefficients. The basic method is to try y
equals an exponential. Now, the only way you can
fiddle with an exponential is in the constant that you put up on
top. So, I'm going to try y equals e
to the rt. Notice you can't tell from that
what I'm using as the independent variable.
But, this tells you I'm using t.
And, I'm switching back to using t as the dependent
variable. So, T is the independent
variable. Why do I do that?
The answer is because somebody thought of doing it,
probably Euler, and it's been a tradition
that's handed down for the last 300 or 400 years.
Some things we just know. All right, so if I do that,
as you learned from the exam, it's very easy to differentiate
exponentials. That's why people love them.
It's also very easy to integrate exponentials.
And, half of you integrated instead of differentiating.
So, we will try this and see if we can pick r so that it's a
solution. Okay, well, I will plug in,
then. Substitute, in other words,
and what do we get? Well, for y double prime,
I get r squared e to the rt. That's y double prime because each time you
differentiate it, you put an extra power of r out
in front. But otherwise,
do nothing. The next term will be r times,
sorry, I forgot the constant. Capital A times r e to the rt, and then there's the last term,
B times y itself, which is B e to the rt. And, that's supposed to be
equal to zero. So, I have to choose r so that
this becomes equal to zero. Now, you see,
the e to the rt occurs as a factor in every
term, and the e to the rt is never zero.
And therefore, you can divide it out because
it's always a positive number, regardless of the value of t.
So, I can cancel out from each term.
And, what I'm left with is the equation r squared plus ar plus
b equals zero. We are trying to find values of r that satisfy that equation.
And that, dear hearts, is why you learn to solve
quadratic equations in high school, in order that in this
moment, you would be now ready to find how spring-mass systems
behave when they are damped. This is called the
characteristic equation. The characteristic equation of
the ODE, or of the system of the spring mass system,
which it's modeling, the characteristic equation of
the system, okay? Okay, now, we solve it,
but now, from high school you know there are several cases.
And, each of those cases corresponds to a different
behavior. And, the cases depend upon what
the roots look like. The possibilities are the roots
could be real, and distinct.
That's the easiest case to handle.
The roots might be a pair of complex conjugate numbers.
That's harder to handle, but we are ready to do it.
And, the third case, which is the one most in your
problem set is the most puzzling: when the roots are
real, and equal. And, I'm going to talk about
those three cases in that order. So, the first case is the roots
are real and unequal. If I tell you they are unequal,
and I will put down real to make that clear.
Well, that is by far the simplest case because
immediately, one sees we have two roots.
They are different, and therefore,
we get our two solutions immediately.
So, the solutions are, the general solution to the
equation, I write down without further ado as y equals c1 e to
the r1 t plus c2 e to the r2 t. There's our solution.
Now, because that was so easy, and we didn't have to do any
work, I'd like to extend this case a little bit by using it as
an example of how you put in the initial conditions,
how to put in the c. So, let me work a specific
numerical example, since we are not going to try
to do this theoretically until next Wednesday.
Let's just do a numerical example.
So, suppose I take the constants to be the damping
constant to be a four, and the spring constant,
I'll take the mass to be one, and the spring constant to be
three. So, there's more damping here,
damping force here. You can't really talk that way
since the units are different. But, this number is bigger than
that one. That seems clear,
at any rate. Okay, now, what was the
characteristic equation? Look, now watch.
Please do what I do. I've found in the past,
even by the middle of the term, there are still students who
feel that they must substitute y equals e to the rt,
and go through that whole little derivation to find that
you don't do that. It's a waste of time.
I did it that you might not ever have to do it again.
Immediately write down the characteristic equation.
That's not very hard. r squared plus 4r plus three
equals zero. And, if you can write down its roots immediately,
splendid. But, let's not assume that
level of competence. So, it's r plus three times r
plus one equals zero. Okay, you factor it. This being 18.03,
a lot of the times the roots will be integers when they are
not, God forbid, you will have to use the
quadratic formula. But here, the roots were
integers. It is, after all,
only the first example. So, the solution,
the general solution is y equals c1 e to the negative,
notice the root is minus three and minus one,
minus 3t plus c2 e to the negative t. Now, suppose it's an initial
value problem. So, I gave you an initial
condition. Suppose the initial conditions
were that y of zero were one. So, at the start, the mass has been moved over to
the position, one, here.
Well, we expected it, then, to start doing that.
But, this is fairly heavily damped.
This is heavily damped. I'm going to assume that the
mass starts at rest. So, the spring is distended.
The masses over here. But, there's no motion at times
zero this way or that way. In other words,
I'm not pushing it. I'm just releasing it and
letting it do its thing after that.
Okay, so y prime of zero, I'll assume, is zero. So, it starts at rest,
but in the extended position, one unit to the right of the
equilibrium position. Now, all you have to do is use
these two conditions. Notice I have to have two
conditions because there are two constants I have to find the
value of. All right, so,
let's substitute, well, we're going to have to
calculate the derivative. So, why don't we do that right
away? So, this is minus three c1 e to
the minus 3t minus c2 e to the negative t. And now, if I substitute in at
zero, when t equals zero, what do I get?
Well, the first equation, the left says that y of zero
should be one. And, the right says this is
one. So, it's c1 plus c2. That's the result of
substituting t equals zero. How about substituting? What should I substitute in the
second equation? Well, y prime of zero is zero. So, if the second equation,
when I put in t equals zero, the left side is zero according
to that initial value, and the right side is negative
three c1 minus c2. You see what you end up with,
therefore, is a pair of simultaneous linear equations.
And, this is why you learn to study linear set of pairs of
simultaneous linear equations in high school.
These are among the most important.
Solving problems of this type are among the most important
applications of that kind of algebra, and this kind of
algebra. All right, what's the answer
finally? Well, if I add the two of them,
I get minus 2c1 equals one. So, c1 is equal to minus one
half. And, if c1 is minus a half, then c2 is minus 3c1. So, c2 is three halves. The final question is,
what does that look like as a solution?
Well, in general, these combinations of two
exponentials aren't very easy to plot by yourself.
That's one of the reasons you are being given this little
visual which plots them for you. All you have to do is,
as you'll see, set the damping constant,
set the constants, set the initial conditions,
and by magic, the curve appears on the
screen. And, if you change either of
the constants, the curve will change nicely
right along with it. So, the solution is y equals
minus one half e to the minus 3t plus three halves e to the
negative t. How does it look? Well, I don't expect you to be
able to plot that by yourself, but you can at least get
started. It does have to satisfy the
initial conditions. That means it should start at
one, and its starting slope is zero.
So, it starts like that. These are both declining
exponentials. This declines very rapidly,
this somewhat more slowly. It does something like that.
If this term were a lot, lot more negative,
I mean, that's the way that particular solution looks.
How might other solutions look? I'll draw a few other
possibilities. If the initial term,
if, for example, the initial slope were quite
negative, well, that would have start like
this. Now, just your experience of
physics, or of the real world suggests that if I give,
if I start the thing at one, but give it a strongly negative
push, it's going to go beyond the equilibrium position,
and then come back again. But, because the damping is
big, it's not going to be able to get through that.
The equilibrium position, a second time,
is going to look something like that.
Or, if I push it in that direction, the positive
direction, that it starts off with a positive slope.
But it loses its energy because the spring is pulling it.
It comes and does something like that.
So, in other words, it might go down.
Cut across the equilibrium position, come back again,
it do that? No, that it cannot do.
I was considering giving you a problem to prove that,
but I got tired of making out the problems set,
and decided I tortured you enough already,
as you will see. So, anyway, these are different
possibilities for the way that can look.
This case, where it just returns in the long run is
called the over-damped case, over-damped.
Now, there is another case where the thing oscillates back
and forth. We would expect to get that
case if the damping is very little or nonexistent.
Then, there's very little preventing the mass from doing
that, although we do expect if there's any damping at all,
we expect it ultimately to get nearer and nearer to the
equilibrium position. Mathematically,
what does that correspond to? Well, that's going to
correspond to case two, where the roots are complex.
The roots are complex, and this is why,
let's call the roots, in that case we know that the
roots are of the form a plus or minus bi.
There are two roots, and they are a complex
conjugate. All right, let's take one of
them. What does a correspond to in
terms of the exponential? Well, remember,
the function of the r was, it's this r when we tried our
exponential solution. So, what we formally,
this means we get a complex solution.
The complex solution y equals e to this, let's use one of them,
let's say, (a plus bi) times t. The question is, what do we do that?
We are not really interested, I don't know what a complex
solution to that thing means. It doesn't have any meaning.
What I want to know is how y behaves or how x behaves in that
picture. And, that better be a real
function because otherwise I don't know what to do with it.
So, we are looking for two real functions, the y1 and the y2.
But, in fact, what we've got is one complex
function. All right, now,
a theorem to the rescue: this, I'm not going to save for
Wednesday because it's so simple.
So, the theorem is that if you have a complex solution,
u plus iv, so each of these is a function of time,
u plus iv is the complex solution to a real differential
equation with constant coefficients.
Well, it doesn't have to have constant coefficients.
It has to be linear. Let me just write it out to y
double prime plus A y prime plus B y equals zero. Suppose you got a complex
solution to that equation. These are understood to be real
numbers. They are the damping constant
and the spring constant. Then, the conclusion is that u
and v are real solutions. In other words,
having found a complex solution, all you have to do is
take its real and imaginary parts, and voila,
you've got your two solutions you were looking for for the
original equation. Now, that might seem like
magic, but it's easy. It's so easy it's the sort of
theorem I could spend one minute proving for you now.
What's the reason for it? Well, the main thing I want you
to get out of this argument is to see that it absolutely
depends upon these coefficients being real.
You have to have a real differential equation for this
to be true. Otherwise, it's certainly not.
So, the proof is, what does it mean to be a
solution? It means when you plug in A (u
plus iv) plus, prime,
plus B times u plus iv, what am I supposed to get? Zero.
Well, now, separate these into the real and imaginary parts.
What does it say? It says u double prime plus A u
prime plus B u, that's the real part of
this expression when I expand it out.
And, I've got an imaginary part, too, which all have the
coefficient i. So, from here,
I get v double prime plus i times A v prime plus 
i times B v. So, this is the imaginary part.
Now, here I have something with a real part plus the imaginary
part, i, times the imaginary part is zero.
Well, the only way that can happen is if the real part is
zero, and the imaginary part is separately zero.
So, the conclusion is that therefore this part must be
zero, and therefore this part must be zero because the two of
them together make the complex number zero plus zero i.
Now, what does it mean for the real part to be zero?
It means that u is a solution. This, the imaginary part zero
means v is a solution, and therefore,
just what I said. u and v are solutions to the
real equation. Where did I use the fact that A
and B were real numbers and not complex numbers?
In knowing that this is the real part, I had to know that A
was a real number. If A were something like one
plus i, I'd be screwed,
I mean, because then I couldn't say that this was the real part
anymore. So, saying that's the real
part, and this is the imaginary part, I was using the fact that
these two numbers, constants, were real constants:
very important. So, what is the case two
solution? Well, what are the real and
imaginary parts  of (a plus b i) t?
Well, y equals e to the at + ibt.
Okay, you've had experience. You know how to do this now.
That's e to the at times, well, the real part is,
well, let's write it this way. The real part is e to the at
times cosine b t. Notice how the a and b enter into the expression.
That's the real part. And, the imaginary part is e to
the at times the sine of bt. And therefore, the solution,
both of these must, therefore, be solutions to the
equation. And therefore,
the general solution to the ODE is y equals, now,
you've got to put in the arbitrary constants.
It's a nice thing to do to factor out the e to the at. It makes it look a little
better. And so, the constants are c1
cosine bt and c2 sine bt. Yeah, but what does that look
like? Well, you know that too.
This is an exponential, which controls the amplitude.
But this guy, which is a combination of two
sinusoidal oscillations with different amplitudes,
but with the same frequency, the b's are the same in both of
them, and therefore, this is, itself,
a purely sinusoidal oscillation.
So, in other words, I don't have room to write it,
but it's equal to, you know.
It's a good example of where you'd use that trigonometric
identity I spent time on before the exam.
Okay, let's work a quick example just to see how this
works out. Well, let's get rid of this.
Okay, let's now make the damping, since this is showing
oscillations, it must correspond to the case
where the damping is less strong compared with the spring
constant. So, the theorem is that if you
have a complex solution, u plus iv, so each of these is
a function of time, u plus iv is the
complex solution to a real differential equation with
constant coefficients. A stiff spring,
one that pulls with hard force is going to make that thing go
back and forth, particularly at the dipping is
weak. So, let's use almost the same
equation as I've just concealed. But, do you remember a used
four here? Okay, before we used three and
we got the solution to look like that.
Now, we will give it a little more energy by putting some
moxie in the springs. So now, the spring is pulling a
little harder, bigger force,
a stiffer spring. Okay, the characteristic
equation is now going to be r squared plus 4r plus five is
equal to zero. And therefore,
if I solve for r, I'm not going to bother trying
to factor this because I prepared for this lecture,
and I know, quadratic formula time, minus four plus or minus
the square root of b squared, 16, minus four times five,
16 minus 20 is negative four all over two.
And therefore, that makes negative two plus or
minus, this makes, simply, i.
2i divided by two, which is i.
So, the exponential solution is e to the negative two,
you don't have to write this in.
You can get the thing directly. t, let's use the one with the
plus sign, and that's going to give, as the real solutions,
e to the negative two t times cosine t,
and e to the negative 2t times the sine of t. And therefore,
the solution is going to be y equals e to the negative 2t
times (c1 cosine t plus c2 sine t). If you want to put initial
conditions, you can put them in the same way I did them before.
Suppose we use the same initial conditions: y of zero 
equals one, and y prime of zero equals one,
equals zero, let's say, wait,
blah, blah, blah, zero, yeah.
Okay, I'd like to take time to actually do the calculation,
but there's nothing new in it. I'd have to take,
calculate the derivative here, and then I would substitute in,
solve equations, and when you do all that,
just as before, the answer that you get is y
equals e to the negative 2t, so,
choose the constants c1 and c2 by solving linear equations,
and the answer is cosine t, so, c1 turns out to be one,
and c2 turns out to be two, I hope.
Okay, I want to know, but what does that look like?
Well, use that trigonometric identity.
The e to the negative 2t is just a real
factor which is going to reproduce itself.
The question is, what is cosine t plus 2 sine t
look like? What's its amplitude as a pure
oscillation? It's the square root of one
plus two squared. Remember, it depends on looking at that little triangle,
which is one, two, this is a different scale
than that. And here is the square root of
five, right? And, here is phi,
the phase lag. So, it's equal to the square
root of five. So, it's the square root of
five times e to  the negative 2t,
and the stuff inside is the cosine of, the frequency is one.
Circular frequency is one, so it's t minus phi,
where phi is this angle. How big is that,
one and two? Well, if this were the square
root of three, which is a little less than
two, it would be 60 infinity. So, this must be 70 infinity.
So, phi is 70 infinity plus or minus five, let's say.
So, it looks like a slightly delayed cosine curve,
but the amplitude is falling. So, it has to start.
So, if I draw it, here's one, here is,
let's say, the square root of five up about here.
Then, the square root of five times e to the negative 2t
looks maybe something like this.
So, that's square root of five e to the negative 2t. This is cosine t,
but shoved over by not quite pi over two.
It starts at one, and with the slope zero.
So, the solution starts like this.
It has to be guided in its amplitude by this function out
there, and in between it's the cosine curve.
But it's moved over. So, if this is pi over two,
the first time it crosses, it's 72 infinity to the right 
of that. So, if this is pi  over two, it's pi over two plus 
70 infinity where it crosses. So, it must be doing something
like this. And now, on the other side,
it's got to stay within the same amplitude.
So, it must be doing something like this.
Okay, that gets us to, if this is the under-damped
case, because if you're trying to do this with a swinging door,
it means the door's going to be swinging back and forth.
Or, our little mass now hidden, but you could see it behind
that board, is going to be doing this.
But, it never stops. It never stops.
It doesn't realize, but not in theoretical life.
So, this is the under-damped. All right, so it's like
Goldilocks and the Three Bears. That's too hot,
and this is too cold. What is the thing which is just
right? Well, that's the thing you're
going to study on the problem set.
So, just right is called critically damped.
It's what people aim for in trying to damp motion that they
don't want. Now, what's critically damped?
It must be the case just in between these two.
Neither complex, nor the roots different.
It's the case of two equal roots.
So, r squared plus Ar plus B equals zero
has two equal roots. Now, that's a very special
equation. Suppose we call the root,
since all of these, notice these roots in this
physical case. The roots always turn out to be
negative numbers, or have a negative real part.
I'm going to call the root a. So, r equals negative a,
the root. a is understood to be a
positive number. I want that root to be really
negative. Then, the equation looks like,
the characteristic equation is going to be r plus a,
right, if the root is negative a, squared because it's a double
root. And, that means the equation is
of the form r squared plus two times a r plus a squared equals
zero. In other words,
the ODE looked like this. The ODE looked like y double
prime plus 2a y prime plus, in other words, the damping and the spring
constant were related in this special wake,
that for a given value of the spring constant,
there was exactly one value of the damping which produced this
in between case. Now, what's the problem
connected with it? Well, the problem,
unfortunately, is staring us in the face when
we want to solve it. The problem is that we have a
solution, but it is y equals e to the minus at.
I don't have another root to get another solution with.
And, the question is, where do I get that other
solution from? Now, there are three ways to
get it. Well, there are four ways to
get it. You look it up in Euler.
That's the fourth way. That's the real way to do it.
But, I've given you one way as problem number one on the
problem set. I've given you another way as
problem number two on the problem set.
And, the third way you will have to wait for about a week
and a half. And, I will give you a third
way, too. By that time,
you won't want to see any more ways.
But, I'd like to introduce you to the way on the problem set.
And, it is this, that if you know one solution
to an equation, which looks like a linear
equation, in fact, the piece can be functions of
t. They don't have to be constant,
so I'll use the books notation with p's and q's.
y prime plus q y equals zero. If you know one solution,
there's an absolute, ironclad guarantee,
if you'll know that it's true because I'm asking you to prove
it for yourself. There's another of the form,
having this as a factor, one solution y one,
let's call it, y equals y1 u is
another solution. And, you will be able to find
u, I swear. Now, let's, in the remaining
couple of minutes carry that out just for this case because I
want you to see how to arrange the work nicely.
And, I want you to arrange your work when you do the problem
sets in the same way. So, the way to do it is,
the solution we know is e to the minus at.
So, we are going to look for a solution to this differential
equation. That's the differential
equation. And, the solution we are going
to look for is of the form e to the negative at times u. Now, you're going to have to
make calculation like this several times in the course of
the term. Do it this way.
y prime equals, differentiate,
minus a e to the minus a t u plus e to the minus a
t u prime. And then, differentiate again. The answer will be a squared. You differentiate this:
a squared e to the negative at u.
I'll have to do this a little fast, but the next term will be,
okay, minus, so this times u prime,
and from this are you going to get another minus.
So, combining what you get from here, and here,
you're going to get minus 2a e to the minus a t u prime. And then, there is a final
term, which comes from this, e to the minus a t u double
prime. Two of these,
because of a piece here and a piece here combine to make that.
And now, to plug into the equation, you multiply this by
one. In other words,
you don't do anything to it. You multiply this line by 2a,
and you multiply that line by a squared, and you add them. On the left-hand side,
I get zero. What do I get on the right?
Notice how I've arrange the work so it adds nicely.
This has a squared times this, plus 2a times that,
plus one times that makes zero. 2a times this plus one times
this makes zero. All that survives is e to the a
t u double prime, and therefore,
e to the minus a t u double prime is equal to zero. So, please tell me,
what's u double prime? It's zero.
So, please tell me, what's u?
It's c1 t plus c2. Now, that gives me a whole family of solutions.
Just t would be enough because all I am doing is looking for
one solution that's different from e to the minus a t. And, that solution,
therefore, is y equals e to the minus a t times t. And, there's my second
solution. So, this is a solution of the
critically damped case. And, you are going to use it in
three or four of the different problems on the problem set.
But, I think you can deal with virtually the whole problem set,
except for the last problem, now.

Algorithms

In our previous lessons, 
we described linked list, we saw the cost of various operations in linked
list and we also compared Linked List with arrays. Now, let us implement Linked List,
 the implementation will be pretty similar in c and c++. there will be slight differences that we will
discuss. the prerequisite for this lesson is that you
should have a working knowledge of pointers in C/C++
and you should also know the concept of dynamic memory allocation. If you want to refresh any of these concepts
check the description of this video for additional resources. Ok so let's get started. As we know in a 
linked list, data is stored in multiple non-contiguous blocks of
memory and we call each block of memory a node in
the linked list. So Let me first draw a linked list here, so
we have a linked list of integers here with three nodes as
we know each node has two fields or two parts one
to store the data and another to store the address of the next node
what we can also call link to the next node. So Let's say the address of the first node
is 200 and address of the second node is 100 and the address of
the third node is 300 for this linked list.
This is only a logical view of the linked list.So the address
part of the first node will be 100 and the address of the
second node will be 200 and we will have 300 here. The address part of the last node will be
null which is only a synonym or macro for address 0. 0 is an invalid address a pointer variable
equal to 0 or null with address 0 or null means
that the pointer variable does not point to a valid memory
location. The memory block the address of the memory
block allocated to each of the node is totally random, there
is no relation. Its not a guarantee that the addresses will
be in increasing order or decreasing order or adjacent to each
other. So that's why we need to keep these links.Now
the identity of the linked list that we always keep with us
is the address of the first node what we also
call the head node. So we keep another variable that will be of
type pointer to node and this guy will store the address of
the first node. And we can name this pointer variable whatever
lets say this pointer variable is named A.
The name of this particular pointer variable that points to the
head node or the first node can also be interpreted as the name
for the linked list also because this is the only identity of the linked
list that we keep with us all the time.
So let us now see how this logical view can be mapped to a real
program in C or C++. In our program node will be a data type that
will have two fields.one to store the data and another to
store the address. In C we can define such a datatype as structure,
so lets say we define a structure named node with two fields.
First field to store the data, the type of the data here is
integer so this will be node for a linked list of integers. If we wanted a Linked List of say Double,
this data type would be double.
The second field will be pointer to node struct node*. We can
name this Link or we can name this next or whatever This is C style of declaring node* or pointer
to node.If this was C++ we could simply write Node*.
I would write it this way the C++ way it looks better to me. In our Logical view here this variable A is
of type Node* or pointer to node.
Each of these three rectangles with two fields are of type node
and this field in the node, the first field is of type integer
and second field is of type pointer to node or Node*. It is important to note which one is what
in the logical view. We should have this visualization before we
go on to implement Linked List. OK so Let us now create this particular Linked
List of Integer that we are showing here through our code.To
be able to do so we will have to implement two
operations one to insert a node into the Linked List. One operation to insert a node in to Linked
List and another operation to traverse the Linked List.
But before that the first thing that we want to do is that we
want to declare a pointer to the head node, a variable that will store the address of
the head node. For the sake of clarity i will write head
node here.So I have declared a pointer to node named A.
Initially When the list is empty, when there is no element in
the list. This pointer should point no where. So we write a statement something like A is
equal to Null to say this same.With these two statements what
we have done is we have created a pointer to node named A
and this pointer points No-where so the list is empty. Now Lets say we want to insert a node in this
list so what we do is we first create a node.Creating a node
is nothing but creating a memory block to store a node. In C we use the function malloc to create
a memory block as argument we pass the number of bytes that
we want in the block. So we say that give me a memory block that
will be equal to the size of a node. So this call to malloc will create a memory
block.This is dynamically allocated memory,memory allocated
during runtime and
the only way to work with this kind of memory is through
reference to this memory location through pointers. Let us assume that this memory block assigned
here is at address 200. Now malloc returns a void pointer that gives
us the address of assigned memory block so we need to collect
it in some variable.
Lets say we create a variable named temp which is pointer to
node so we can collect the return of malloc the address in this
particular variable. We will need a type casting here because malloc
returns void pointer and we are having temp as pointer
to node So now we have created one node in the memory. Now what we need to do is fill in the data
in this particular node and adjust the links which will mean
writing the correct address in the variable A and the
link field of this newly created node. To do so we will have to de-reference this
particular pointer variable that we just created. As we know if we put a * sign in front of
the pointer variable.We mean de-referencing it to modify
the value at that particular address.
Now in this case we have a node which has two fields,so once we
de-reference if we want to access each of the fields
we need to put something like a dot data here to access the
data and a dot link to write to the link field. So we will write a statement like this to
fill in value 2 here and we have this temp variable pointing to
this right now and link part of this newly created
node should be null because this is the first and the last
node. And the final thing that we need to do is
write the address of this newly created node in A. So we will write something like A is equal
to temp. Ok temp was to temporarily store the address of the node
till the time we have
not fixed all the links properly. We can now use temp for some
other purpose.Our linked list is intact now it has one node. These two lines that we have written here
for de-referencing and writing the value into the new node.There
is alternate syntax for this
Instead of writing something like *temp bracketed dot data we
could also write temp followed by this arrow and data.we will have two characters to make
this arrow one hyphen and one this right angular brace. So we can write something like this.And same
thing below we can write something like this.To create a memory
block in C++ we can use malloc as well
as we can use the new operator. So in C++ it gets very simple.We could simply
write Node* temp = new Node like this and we would mean
the same thing. this is a lot cleaner and new operator is
always preferred over malloc. So if you are using C++ new is
recommended. So far through our program we created an empty
list by creating this pointer to the head node and assigning
the value null to it initially then
we created a node and we added first node in this linked
list.When the list is empty and we want to insert a node the
logic is pretty straight forward. When the list is not empty we may want to
insert a node at the beginning of the list or we may want to insert
a node at the end of the list or we may even want to
insert a node somewhere in the middle of the list at a particular
position. we will write separate functions and routines
for these different kind of insertions and we will see
running code in a compiler in our coming lessons.
Lets just talk about the logic here in this whatever
unstructured code i have right now here. So i want to write a code to insert two more
nodes each time at the end of the list.we actually want to create
the linked list with three nodes having values
2,4 and 6 that was our initial example in the beginning. Ok So let us add two more nodes with values
4 and 6 in to this linked list.At this stage in our code we already
have a variable temp which is
pointing to this particular node We will create a new node and
use the same variable name to collect the address of this new
node. So we will write a statement like this.So
a new node is created and temp now stores the address of this new
node which is located at
address 100 here.Once again you can set the data.And then
because this is going to be the last node we need to set the
link as null. Now all we need to do is to build the link
of this particular node,Write the address of this newly created
node in to the address field of
this last node.To do so we will have to traverse the list and
we will have to go to the end of the list, to do so we will
write something like this. We can create a new variable temp1 which will
be pointer to node and initially we can point this variable
to the head node by a statement like this.
we can write a loop like this. Now this is generic logic to
reach the end of the list. It will be not so clear if we see this logic
with only one node as we have in this example. Lets draw a list
with multiple nodes.
So we are pointing temp1 to the first node here and if the link
part of this node is null then we are at last node else we can
move to the next node. So temp1 equal to temp1-&gt; link will get us
to the next node.And we will go on till we reach the last node.
For the last node this particular condition temp1-&gt;link not
equal to null will be false because the link part will be null
and we will exit this while loop. So this is our code logic for traversal of
the list all the way till end.If we want to print the element in
the list we will write something like this, we will write
print temp-&gt;data inside the while loop.But right now we want to
insert at the end of the list and we are only traversing the
list to reach the last node. There is one more thing that i want to point
out, we are using this variable temp1 and initially storing
the address in A we are not doing something like A equal A.link
and using the variable A itself to traverse the list because
if we modify A we will loose the address of the head node. So A is never modified, the address of the
head node whatever variable stores the address of the head node
is never modified.Only this temporary variables are
modified to traverse the list.So finally after all this we will
write a statement like this temp1 dot link
is equal to temp. temp1 is now pointing here so this address
part is updated and this link is built. So we have two nodes in the list Once again
if we want to insert node with number 6 in this list we
will have to create a new node by this
logic and then we will have to traverse the list by this
logic.So We will point temp1 here first.Then the loop will move
the temp1 to the end. Lets say this new block is at address 300.So
this Last line finally will adjust the link of the node at
address 100.To insert the node at the end
there is one logic in these four line if this list is empty and
another logic in these remaining lines if list is not empty. Ideally, we will be writing all this logic
in a function. we will do that in our coming lessons. We will implement
a separate methods to print all
the element in the list or insert a node at the end or we will
implement a separate method to insert a node at beginning of
the list and at a particular position in the list.
This is all for this lesson. Thanks for watching.

Algorithms

We will now define the notion
of a random variable. Very loosely speaking, a random
variable is a numerical quantity that takes
random values. But what does this mean? We want to be a little more
precise and I'm going to introduce the idea through
an example. Suppose that our sample space
is a set of students labeled according to their names. Or for simplicity, let's just
label them as a, b, c, and d. Our probabilistic experiment is
to pick a student at random according to some probability
law and then record their weight in kilograms. So for example, suppose that the
outcome of the experiment was this particular student,
and the weight of that student is 62. Or it could be that the outcome
of the experiment is this particular student, and
that particular student has a weight of 75 kilograms. The weight of a particular
student is a number, little w. But let us think of the abstract
concept of weight, something that we will denote
by capital W. Weight is an object whose value is determined
once you tell me the outcome of the experiment,
once you tell me which student was picked. In this sense, weight is really
a function of the outcome of the experiment. So think of weight as an
abstract box that takes as input a student and produces a
number, little w, which is the weight of that particular
student. Or more concretely, think of
weight with a capital W as a procedure that takes a student,
puts him or her on a scale, and reports the result. In this sense, weight is an
object of the same kind as the square root function that's
sitting inside your computer. The square root function
is a function. It's a subroutine, perhaps it is
a piece of code, that takes as input a number, let's say
the number 9, and produces another number. In this case, it would be the
number 3, which is the square root of 9. Notice here the distinction that
we will keep emphasizing over and over. Square root of 9 is a number. It is the number 3. The box square root
is a function. Now, let us go back to our
probabilistic experiment. Note that a probabilistic
experiment such as the one in our example can have several
associated random variables. For example, we could have
another random variable denoted by capital H, which
is the height of a student recorded in meters. So if the outcome of the
experiment, for example, was student a, then this random
variable would take a value which is the height of that
student, let's say it was 1.7. Or if the outcome of the
experiment was student c, then we would record the height
of that student. And let's say it turns
out to be 1.8. Once again, height with a
capital H is an abstract object, a function whose value
is determined once you tell me the outcome of the experiment. Now, given some random
variables, we can create new random variables as
functions of the original random variables. For example, consider the
quantity defined as weight divided by height squared. This quantity is the so-called
body mass index, and it is also a function on
the sample space. Why is it a function on
the sample space? Well, because once an outcome
of the experiment is determined, suppose that the
outcome of the experiment was the blue student, then these two
numbers, 62 and 1.7, are also determined. And using those numbers, we can
carry out this calculation and find the body mass index
of that particular student, which in this case
would be 21.5. Or if it happened that this
student was selected, then the body mass index would turn out
to be some other number. In this case, it would be 23. So again, we see that the body
mass index can be viewed as an abstract concept defined
by this formula. But once an outcome is
determined, then the body mass index is also determined. And so the body mass index is
really a function of which particular outcome
was selected. Let us now abstract from the
previous discussion. We have seen that random
variables are abstract objects that associate a specific value,
a particular number, to any particular outcome of a
probabilistic experiment. So in that sense, random
variables are functions from the sample space to
the real numbers. They are numerical functions,
but as numerical functions they can either take discrete
values, for example the integers, or they can take
continuous values, let's say on the real line. For example, if your random
variable is the number of heads in 10 consecutive coin
tosses, this is a discrete random variable that
takes values in the set from 0 to 10. If your random variable is a
measurement of the time at which something happened, and
if your timer has infinite accuracy, then the timer reports
a real number and we would have a continuous
random variable. In this lecture sequence and in
the next few ones, we will concentrate on discrete random
variables because they are easier to handle. And then later on, we will
move to a discussion of continuous random variables. Throughout, we want to keep
noting this very important distinction that I already
brought in the discussion for a particular example, but it
needs to be emphasized and re-emphasized. That we make a distinction
between random variables, which are abstract objects. They are functions on the sample
space and they are denoted by uppercase letters. In contrast, we will use lower
case letters to indicate numerical values of the
random variables. So little x is always a real
number, as opposed to the random variable, which
is a function. One point that we made earlier
is that for the same probabilistic experiment we
can have several random variables associated with
that experiment. And we can also combine
random variables to form new random variables. In general, a function of random
variables has numerical values that are determined by
the numerical values of the original random variables. And so, ultimately, they are
determined by the outcome of the experiment. So a function of random
variables has a numerical value which is completely
determined by the outcome of the experiment. And so a function of
random variables is also a random variable. As an example, we could think of
two random variables, X and Y, associated with the same
probabilistic experiment. And then define a random
variable, let's say X plus Y. What does that mean? X plus Y is a random variable
that takes the value little x plus little y when the random
variable capital X takes the value little x and capital Y
takes the value little y. So X and Y are random
variables. X plus Y is another
random variable. X and Y will take numerical
values once the outcome of the experiment has been obtained. And if the numerical values that
they take are little x and little y, then the random
variable X plus Y will take the numerical value little
x plus little y. So we can now move on and start
doing some interesting things about random variables. Characterize them, describe
them, give some examples, and introduce some new concepts
associated with them.

Calculus

In the last video, I introduced
you to what is probably the most bizarro
function that you've encountered so far. And that was the Dirac
delta function. And I defined it to be--
and I'll do the shifted version of it. You're already hopefully
reasonably familiar with it. So Dirac delta of t minus c. We can say that it equals 0,
when t does not equal c, so it equals 0 everywhere, but it essentially pops up to infinity. And we have to be careful
with this infinity. I'm going to write
it in quotes. It pops up to infinity. And we even saw in the previous
video, it's kind of different degrees of infinity,
because you can still multiply this by other numbers to get
larger Dirac delta functions when t is equal to c. But more important than this,
and this is kind of a pseudodefinition here, is the
idea that when we take the integral, when we take the area
under the curve over the entire x- or the entire t-axis,
I guess we could say, when we take the area under this
curve, and obviously, it equals zero everywhere except
at t is equal to c, when we take this area, this is the
important point, that the area is equal to 1. And so this is what I meant by
pseudoinfinity, because if I have 2 times the Dirac delta
function, and if I'm taking the area under the curve of
that, of 2 times the Dirac delta function t minus c dt,
this should be equal to 2 times-- the area of just under
the Dirac delta function 2 times from minus infinity to
infinity of the delta function shifted by c dt, which is just
2 times-- we already showed you, I just said, by definition,
this is 1, so this will be equal to 2. So if I put a 2 out here, this
infinity will have to be twice as high, so that the
area is now 2. That's why I put that infinity
in parentheses. But it's an interesting
function. I talked about it at the end of
the last video that it can help model things that kind of
jar things all of a sudden, but they impart a fixed amount
of impulse on something and a fixed amount of change
in momentum. And we'll understand that a
little bit more in the future. But let's kind of get the
mathematical tools completely understood. And let's try to figure out what
the Dirac delta function does when we multiply it, what
it does to the Laplace transform when we multiply
it times some function. So let's say I have my Dirac
delta function and I'm going to shift it. That's a little bit
more interesting. And if you want to unshift
it, you just say, OK, well, c equals 0. What happens when c equals 0? And I'm going to shift it and
multiply it times some arbitrary function f of t. If I wanted to figure out the
Laplace transform of just the delta function by itself,
I could say f of t is equal to 1. So let's take our Laplace
transform of this. And we can just use the
definition of the Laplace transform, so this is equal to
the area from 0 to infinity, or we could call it the integral
from 0 to infinity of e to the minus -- that's just
part of the Laplace transform definition-- times this thing--
and I'll just write it in this order-- times
f of t times our Dirac delta function. Delta t minus c and times dt. Now, here I'm going to
make a little bit of an intuitive argument. A lot of the math we do is kind
of-- especially if you want to be very rigorous and
formal, the Dirac delta function starts to break down a
lot of tools that you might have not realized it would
break down, but I think intuitively, we can still
work with it. So I'm going to solve this
integral for you intuitively, and I think it'll
make some sense. So let's draw this. Let me draw this, what
we're trying to do. So let me draw what we're
trying to take the integral of. And we only care from zero to
infinity, so I'll only do it from zero to infinity. And I'll assume that c is
greater than zero, that the delta function pops
up someplace in the positive t-axis. So what is this first part
going to look like? What is that going to look
like? e to the minus st times f of t? I don't know. It's going to be some function.
e to the minus st starts at 1 and drops down, but
we're multiplying it times some arbitrary function, so I'll
just draw it like this. Maybe it looks something
like this. This right here is e to the
minus st times f of t. And the f of t is what kind of
gives it its arbitrary shape. Fair enough. Now, let's graph our Dirac
delta function. With zero everywhere except
right at c, right at c right there, it pops up infinitely
high, but we only draw an arrow that is of height 1 to
show that its area is 1. I mean, normally when you graph
things you don't draw arrows, but this arrow shows
that the area under this infinitely high thing is 1. So we do a 1 there. So if we multiply this, we care
about the area under this whole thing. When we multiply these two
functions, when we multiply this times this times the delta
function, this is-- let me write this. This is the delta function
shifted to c. If I multiply that times
that, what do I get? This is kind of the key
intuition here. Let me redraw my axes. Let me see if I can do it
a little bit straighter. Don't judge me by the
straightness of my axes. So that's t. So what happens when I
multiply these two? Everywhere, when t equals
anything other than c, the Dirac delta function is zero. So it's zero times anything. I don't care what this function
is going to do, it's going to be zero. So it's going to be zero
everywhere, except something interesting happens at
t is equal to c. At t equals c, what's the
value of the function? Well, it's going to
be the value of the Dirac delta function. It's going to be the Dirac delta
function times whatever height this is. This is going to be this point
right here or this right there, that point. This is going to be this
function evaluated at c. I'll mark it right here on the
y-axis, or on the f of t, whatever you want to call it. This is going to be e to the
minus sc times f of c. All I'm doing is I'm just
evaluating this function at c, so that's the point
right there. So if you take this point, which
is just some number, it could be 5, 5 times this, you're
just getting 5 times the Dirac delta function. Or in this case, it's not 5. It's this little more
abstract thing. I could just draw
it like this. When I multiply this thing times
my little delta function there, I get this. The height, it's a delta
function, but it's scaled now. It's scaled, so now my
new thing is going to look like this. If I just multiply that times
that, I essentially get e to the minus sc times f of c. This might look like some fancy
function, but it's just a number when we consider
it in terms of t. s, it becomes something when we
go into the Laplace world, but from t's point of view,
it's just a constant. All of these are just constants,
so this might as well just be 5. So it's this constant times my
Dirac delta function, times delta of t minus c. When I multiply that thing times
that thing, all I'm left with is this thing. And this height is still going
to be infinitely high, but it's infinitely high scaled in
such a way that its area is going to be not 1. And I'll show it to you. So what's the integral
of this thing? Taking the integral of this
thing from minus infinity to infinity, since this thing is
this thing, it should be the same thing as taking the
integral of this thing from minus infinity to infinity. So let's do that. Actually, we don't have to do
it from minus infinity. I said from zero to infinity. So if we take from zero to
infinity, what I'm saying is taking this integral
is equivalent to taking this integral. So e to the minus sc f of c
times my delta function t minus c dt. Now, this thing right here, let
me make this very clear, I'm claiming that this is
equivalent to this. Because everywhere else, the
delta function zeroes out this function, so we only care about
this function, or e to the minus st f of t when
t is equal to c. And so that's why we were able
to turn it into a constant. But since this is a constant,
we can bring it out of the integral, and so this is equal
to-- I'm going to go backwards here just to kind of save space
and still give you these things to look at. If we take out the constants
from inside of the integral, we get e to the minus sc times f
of c times the integral from 0 to infinity of f
of t minus c dt. Oh sorry, not f of t minus c. This is not an f. I have to be very careful. This is a delta. Let me do that in a
different color. I took out the constant terms
there, and it's going to be a delta of t minus c dt. Let me get the right color. Now, what is this thing
by definition? This thing is 1. I mean, we could put it from
minus infinity to infinity, it doesn't matter. The only time where it has any
area is right under c. So this thing is equal to 1. So this whole integral right
there has been reduced to this right there, because this
is just equal to 1. So the Laplace transform of
our shifted delta function times some other function is
equal to e to the minus sc times f of c. Let me write that
again down here. Let me write it all at once. So the Laplace transform of our
shifted delta function t minus c times some function
f of t, it equals e to the minus c. Essentially, we're just
evaluating e to the minus st evaluated at c. So e to the minus
cs times f of c. We're essentially just
evaluating these things at c. This is what it equals. So from this we can get a lot
of interesting things. What is the Laplace transform--
actually, what is the Laplace transform
of just the plain vanilla delta function? Well, in this case, we have c
is equal to 0, and f of t is equal to 1. It's just a constant term. So if we do that, then the
Laplace transform of this thing is just going to be e to
the minus 0 times s times 1, which is just equal to 1. So the Laplace transform of our
delta function is 1, which is a nice clean thing
to find out. And then if we wanted to just
figure out the Laplace transform of our shifted
function, the Laplace transform of our shifted delta
function, this is just a special case where f
of t is equal to 1. We could write it times 1, where
f of t is equal to 1. So this is going to be equal to
e to the minus cs times f of c, but f is just a constant,
f is just 1 here. So it's times 1, or it's
just e to the mine cs. So just like that, using a kind
of visual evaluation of the integral, we were able
to figure out the Laplace transforms for a bunch of
different situations involving the Dirac delta function. Anyway, hopefully, you found
that reasonably useful.

Probability

In our previous lesson, we saw how we can
evaluate prefix and postfix expressions, now in this lesson we will see an efficient
algorithm to convert infix to postfix. We already know of one way of doing this. We
have seen how we can do this manually. To convert an infix expression to postfix, we
apply operator precedence, and associativity rules. Let's do the conversion for this expression
we have written here. The precedence of multiplication operator is higher. So, we will first convert
this part B*C. B*C will become BC*. The operator will come in front of the operands. Now, we
can do the conversion for this addition. For addition, the operands are A and this postfix
expression. In the final step we can get rid of all the parentheses. So, finally this is
my post fix expression. We can use this logic in a program also. But,
it will not be very efficient. And the implementation will also be somewhat complex. I am going
to talk about one algorithm which is really simple and efficient and in this algorithm
we need to parse the infix expression only once from left to right. And, we can create
the postfix expression. If you can see in infix to postfix conversion, the positions
of operands and operators, may change but the order in which operands occur from left
to right will not change. The order of operators may change. This is an important observation.
In both infix and postfix forms here, the order of operands as we go from left to right
is first we have A, then we have B and then we have C. But, the order of operators is
different. In infix, first we have + and then we have multiplication. In postfix, first
we have multiplication and then we have addition. In postfix form we will always have the operators
in the same order, in which they should be executed. I am going to perform this conversion
once again but this time I am going to use a different logic. What I will do is, I will
parse the infix expression from left to right. So, I will go from left to right, looking
at each token that will either be an operand or an operator. In this expression we will
start at A, A is an operand. If it's an operand we can simply append it in the postfix string
or expression that we are trying to create. At least for A it should be very clear that
this is nothing that can come before A. Okay, so the first rule is that if it's in operand,
we can simply put it in the postfix expression. Moving on, next we have an operator. We cannot
put the operator in the postfix expression, because we have not seen it's right operand
yet. While parsing we have seen only it's left operand. We can place it only after it's
right operand is also placed. So, what I am going to do is I am going to keep this operator
in a separate list or collection and place it later in the postfix expression when it
can be placed and the structure that I am going to use for storage is stack. A stack
is only a special kind of list in which whatever comes in last goes out first. Insertion and
deletion happen from the same end. I have pushed + operator onto the stack here. Moving
on, next we have B which is an operand. As we had said operand can simply be appended.
There is nothing that can come before this operand. The operator on the stack is anyway
waiting for the operand to come. Now at this stage, can we place the addition operator,
to the postfix string. Actually, what's after B also matters. In this case, we have this
multiplication operator after B, which has higher precedence and so the actual operand
for addition is this whole expression B*C. We cannot perform the addition until multiplication
is finished. So while parsing, when I am at B and I have not seen what's ahead of B, I
cannot decide the fate of the operator in the stack. So, let's just move on. Now, we
have this multiplication operator. I want to make this expression further complex to
explain things better. So, I am adding something at tail here in this expression. Now, I want
to convert this expression to postfix form. I am not having any parentheses here. We will
see how we can deal with parentheses later, let's look at an expression where parentheses
does not override operator precedence. Okay, so right now in this expression while parsing
from left to right, we are at this multiplication operator. The multiplication operator itself
cannot go into the postfix expression because we have not seen it's right operand yet. And,
until it's right operand is placed in the postfix expression, we cannot place it. The
operator that we would be looking at while parsing. That operator itself cannot be placed
right away. But, looking at that operator, we can decide whether something from the collection,
something from the stack can be placed into the postfix expression that we are constructing
or not. Any operator in the stack having higher precedence than the operator that we are looking
at, can be popped and placed into the postfix expression. Let's just follow this as rule
for now and I will explain it later. There is only one operator in the stack and It is
not having higher precedence than multiplication so we will not pop it and place it in the
postfix expression. Multiplication itself, will be pushed. If an element in the stack
has something on top of it, that something will always be of higher precedence. So, let's
move on in this expression now. Now, we are at C, which is an operand, so, it can simply
go. Next, we have an operator subtraction. Subtraction itself cannot go but as we had
said if there is anything on the stack having higher precedence than the operator that we
are looking at, it should be popped out and it should go and the question is why? We are
putting these operators on the stack, we are not placing them on the postfix expression
because we are not sure whether we are done with their right operand or not. But after
that operator, as soon as I am getting an operator of lower precedence, that marks the
boundary of the right operand. For this multiplication operator, C is my right operand. It's this
simple variable. For addition, B*C is my right operand because subtraction has lower precedence.
Anything on or after that cannot be part of my right operand. Subtraction, I should say
has lower priority because of the associativity rule. If you remember the order of operation,
addition and subtraction have same precedence but the one that would occur in left would
be given preference. So, the idea is anytime for an operator, if I am getting an operator
of lower priority, we can pop it from the stack and place it in the expression. Here,
we will first pop multiplication and place it and then we can pop addition and now we
will push subtraction onto the stack. Let's move on now. D is an operand. So, it will
simply go. Next we have, multiplication. There is nothing in the stack having higher precedence
than multiplication. So, we will pop nothing. Multiplication will go on to the stack. Next,
we have an operand. It will simply go. Now, there are two ways in which we can find the
end of the right operand for an operator. a is if we get an operator of lesser precedence,
b if we reach the end of the expression. Now, that we have reached end of the expression,
we can simply pop and place these operators. So, first multiplication will go and then
subtraction will go. Let's quickly write pseudo code for whatever I have said so far and then
you can sit with some examples and analyse the logic. I am going to write a function
named infix to postfix that will take a string exp for expression as argument. For the sake
of simplicity, lets assume that each operand or operator will be of one character only.
In an actual implementation you can assume them to be tokens of multiple characters.
So, in my pseudo code here, the first thing I will do is, to create a stack of characters
named S. Now, I will run a loop starting 0 till length of expression -1. So, I am looking
at each character that can either be an operand or operator. If the character is an operand
we can append it to the postfix expression. Well, actually I should have declared and
initialized a string before this loop. This is the result string in which I shall be appending
else if exp[i] is operator, we need to look for operators in the stack having higher precedence.
So, I will say while stack is not empty, and top of stack has higher precedence. And let's
say this function HasHigherPrecedence will take two arguments, two operators. So, if
the top of Stack has higher precedence than the operator that we are looking at. We can
append the top of stack to the result which is the variable that will store the postfix
string. And, then we can pop that operator. I am assuming that this S is some class that
has these functions top and pop and empty to check whether it's empty or not. Finally,
once I am done with the popping, Outside this while loop I need to push the current operator.
S is an object of some class that will have these functions top, pop and empty. Okay,
so this is the end of my for loop. At the end of it, I may have some operators left
in the stack. I will pop these operators and append them to the postfix string. I will
use this while loop. I will say that while the stack is not empty, append the operator
at top and pop it. And, finally after this while loop I can return the result string
that will contain the postfix expression. So, this is my pseudo code for whatever logic
I have explained so far. In the logic, I have not taken care of parentheses. What if my
infix expression would have parentheses like this? Their will be slight change from what
we were doing previously. With, parentheses any part of the expression within parentheses
should be treated as independent complete expression in itself. And, no element outside
the parentheses will influence its execution. In this expression, this part A + B is within
one parentheses. It's execution will not be influenced by this multiplication or this
subtraction which is outside it. Similarly, this whole thing is within the outer parentheses.
So, this multiplication operator outside, will not have any influence on execution of
this part as a whole. If parentheses are nested, inner parentheses is sorted out or resolved
first,and then only outer parentheses can be resolved. With parentheses, we will have
some extra rules: we will still go from left to right and we will still use stack. And,
let's say I am going to write the postfix part in write here as I create it. Now, while
parsing, a token can be an operand or operator or an opening or closing of parentheses. I
will use some extra rules. I will first tell them and then I will explain. If it's an opening
parentheses, we can push it onto the stack. The first token here, is an opening parentheses.
So, it will be pushed onto the stack. And, then we will move on. We have an opening parentheses
once again, so once again we will push it. Now, we have an operand. There is no change
in rule for operand. It will simply be appended to the postfix part. Next, we have an operator.
Remember, what we were doing for operator earlier. We were looking at top of stack and
popping as long as we were getting operator of higher precedence. Earlier when we were
not using parentheses, we could go on popping and empty the stack. But, now we need to look
at top of stack and pop only till we get an opening parentheses. Because, if we are getting
an opening parentheses, then it's the boundary of the last opened parentheses and this operator
does not have any influence after that, outside that. So, this + operator does not have any
influence outside this opening parentheses. I will explain this scenario, with some more
examples later. Let's first understand the rules. So, the rule is, if I am seeing an
operator, I need to look at the top of stack. If it's an operator of higher precedence,
I can pop and then I should look at the next top. If it's once again an operator of higher
precedence, I should pop again. But, I should stop when I see an opening parentheses. At,
this stage we have an opening parentheses at top, so we do not need to look below it.
Nothing will be popped anyway. Addition however, will go on to the stack. Remember, after the
whole popping game, we push the operator itself. Next, we have an operand. It will go on, we
will move on. Next, we have a closing of parentheses. When I am getting a closing of parentheses,
I am getting a logical end of the last opened parentheses. For part of the expression, within
that parentheses, it's coming to the end. And, remember what we were doing earlier,
when we were reaching the end of infix expression, we were popping all the operators out and
placing them. So, this time also we need to pop the operators out, but only those operators
that are part of this parentheses that we are closing. So, we need to pop all the operators
until we get an opening parentheses. I am popping this + and appending it. Next, we
have an opening of parentheses, so I will stop. But, as last step I will pop this opening
also. Because, we are done for this parentheses. Okay, so the rule for closing a parentheses,
is pop until you are getting an opening parentheses and then finally pop that particular opening
parentheses also. Let's move on now. Next, we have an operator. We need to look at top
of stack. It's an opening of parentheses. This operator will simply be pushed. Next,
we have an operand. Next, we have an operator. Once again, we will look at the top. We have
multiplication, which is of higher precedence. So, this should be popped and appended. We
will look at the top again. Again, it's an opening of parentheses, so we should stop
looking now. '-' will be pushed now. Next, we have an operand. Next we have a closing
of parentheses. So, we need to pop until we get an opening. '-' will be appended. Finally,
the opening will also be popped. Next, we have an operator and this will simply go.
Next, we have an operand. And, now we have reached the end of expression. So, everything
in the stack will be popped and appended. So, this final is my postfix expression.
I will take one more example and convert it to make things further clear. I want to convert
this expression. I will start at the beginning. First, we have an operand. Then this multiplication
operator, which will simply go onto the stack. The stack right now, is empty. There is nothing
on the top to compare it with. Next, we have an opening parentheses which will simply go.
Next, we have an operand it will be appended and now we move on to this addition operator.
If this opening parentheses was not there, the top of stack would have been the multiplication
operator which has higher precedence. So, it would have been popped. But, now we will
look at the top and it's an opening parentheses. So, we cannot look below. And we will simply
have to move on. Next, we have C. I missed pushing the addition operator last time. Okay,
after C, I have this closure. So, we need to pop until we get an opening. And, then
we need to pop one opening also. Finally we have reached the end of expression. So, everything
in the stack will be popped and appended. So, this finally is my postfix part, postfix
form. In my pseudo code, that I had written earlier, only the part within this for loop
will change to take care of parentheses. In case we have an operator, we need to look
at top of the stack and pop but only till we are an getting opening parentheses. So,
I have put this extra condition in the while loop, this condition will ensure that we stop
once we get an opening parentheses. Right now, in the for loop we are dealing with operand
and operators, we will have two more conditions: If its an opening of parentheses, we should
push. Else, if it's a closure, we can go on popping and appending. Let's say this function
IsOpeningParentheses will check whether a character is opening of parentheses or not.
In fact we should use this function here also when I am checking whether current token is
opening or not. Because, it could be an opening curly brace or opening bracket also, this
function will then take care. Let's say this function will take care. And, similarly for
this last else if, we should use this function IsClosingParentheses. Okay, things are consistent
now. After this while loop in the last else if, we should do one extra pop. And, this
extra pop will pop the opening parentheses. And, now we are done with this else if. And,
this is closure of my for loop. Rest of the stuff will remain same. After the for loop,
we can pop the left overs and append to the string. And, finally we can return. So, this
is my final pseudo code. You can check the description of this video for a link to the
real implementation, actual source code. Okay, So, I will stop here now. This is it for this
lesson. Thanks, for watching.

Algorithms

The following content is
provided under a Creative Commons license. Your support will help
MIT OpenCourseWare continue to offer high quality
educational resources for free. To make a donation or
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: So, which
of the following is a good and valid definition
for a class representing a car? OK, most of the people
I think have gotten it, which is the red, which
says class car object, and that's perfect. OK. So this defines a function,
this defines a class correctly, but A is just not
descriptive name at all, and this is kind of weird, and
this is the right one here.

Statistics

In the previous example, we had
a model where the result of the first coin toss did not
affect the probabilities of what might happen in
the second toss. This is a phenomenon that we
call independence and which we now proceed to define. Let us start with a first
attempt at the definition. We have an event, B,
that has a certain probability of occurring. We are then told that event A
occurred, but suppose that this knowledge does not affect
our beliefs about B in the sense that the conditional
probability remains the same as the original unconditional
probability. Thus, the occurrence of A
provides no new information about B. In such a case, we
may say that event B is independent from event A. If this is indeed the case,
notice that the probability that both A and B occur, which
is always equal by the multiplication rule to the
probability of A times the conditional probability of B
given A. So this is a relation that's always true. But if we also have this
additional condition, then this simplifies to the
probability of A times the probability of B. So we can find the probability
of both events happening by just multiplying their
individual probabilities. It turns out that this relation
is a cleaner way of the defining formally the
notion of independence. So we will say that two
events, A and B, are independent if this
relation holds. Why do we use this definition
rather than the original one? This formal definition has
several advantages. First, it is consistent with
the earlier definition. If this equality is true, then
the conditional probability of event B given A, which is the
ratio of this divided by that, will be equal to the probability
of B. So if this relation holds, then this
relation will also hold, and so this more formal definition
is consistent with our earlier intuitive definition. A more important reason is that
this formal definition is symmetric with respect to the
roles of A and B. So instead of saying that B is independent
from A, based on this definition we can now say
that events A and B are independent of each other. And in addition, since this
definition is symmetric and since it implies this condition,
it must also imply the symmetrical relation. Namely, that the conditional
probability of A given B is the same as the unconditional
probability of A. Finally, on the technical
side, conditional probabilities are only defined
when the conditioning event has non-zero probability. So this original definition
would only make sense in those cases where the probability of
the event A would be non-zero. In contrast, this new definition
makes sense even when we're dealing with zero
probability events. So this definition is indeed
more general, and this also makes it more elegant. Let us now build some
understanding of what independence really is. Suppose that we have two events,
A and B, both of which have positive probability. And furthermore, these two
events are disjoint. They do not have any
common elements. Are these two events
independent? Let us check the definition. The probability that both A and
B occur is zero because the two events are disjoint. They cannot happen together. On the other hand, the
probability of A times the probability of B is positive,
since each one of the two terms is positive. And therefore, these two
expressions are different from each other, and therefore this
equality that's required by the definition of independence
does not hold. The conclusion is that these
two events are not independent. In fact, intuitively, these two
events are as dependent as Siamese twins. If you know that A occurred,
then you are sure that B did not occur. So the occurrence of A tells you
a lot about the occurrence or non-occurrence of B. So we see that being independent
is something completely different from
being disjoint. Independence is a relation
about information. It is important to always keep
in mind the intuitive meaning of independence. Two events are independent if
the occurrence of one event does not change our beliefs
about the other. It does not affect the
probability that the other event also occurs. When do we have independence
in the real world? The typical case is when the
occurrence or non-occurrence of each of the two events A
and B is determined by two physically distinct and
non-interacting processes. For example, whether my coin
results in heads and whether it will be snowing on New Year's
Day are two events that should be modeled
as independent. But I should also say that there
are some cases where independence is less obvious and
where it happens through a numerical accident. You can now move on to answer
some simple questions where you will have to check for
independence using either the mathematical or intuitive
definition.

Calculus

The following content is
provided under a Creative Commons license. Your support will help
MIT OpenCourseWare continue to offer high-quality
educational resources for free. To make a donation or
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. ANA BELL: So I have
this variable name once, which is umbr-- looks like
people are getting it. We have a variable called
repeat, which is ella, and I'm creating a
variable u, which is the concatenation of that
with the repeat multiplied by 4. So I think people are on the
right track, seeing by the 90% here. It's going to give me
this result right here. Great.

Probability

The following
content is provided under a Creative
Commons license. Your support will help MIT
OpenCourseWare continue to offer high quality
educational resources for free. To make a donation or
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: So we're going
to do rolling caches then, we're going to go a little
bit over amortized analysis and if we have a
lot of time left, we're going to talk about
good and bad hash functions. So can someone remind me what's
the point of rolling hashes? What's the problem? What are we trying
to solve in lectures? Be brave. AUDIENCE: Gets faster,
I think, because like-- PROFESSOR: So what are
we trying to solve? You don't need to
go ahead, tell me what's the big problem
that we're trying to solve. AUDIENCE: I don't remember/ PROFESSOR: OK, so
let's go over that. So we have a big document,
AKA a long string, and we're trying to find a
smaller string inside it. And we're trying to
do that efficiently. So say the big document is--
you might have seen this before. And we're trying to
look for the here. How do I do that
with rolling hashes? So the slow, nice
solution is I get this the and then I overlap with the
beginning of the document, I do a string comparison. If it matches, I say
that it's a match. It's not, I overlap it here. String match, I overlap it here. String match, so
on and so forth. The problem is this does a lot
of string matching operations, and the string matching
operation is how expensive? What's the running time? AUDIENCE: Order n. PROFESSOR: Order n, where n
is the size of the string. So if we have a string,
say this is the key that we're looking for and
n is the document size then this is going to
be order n times k. We want to get to
something better. So how do I do this
with rolling hashes? AUDIENCE: We take
the strings up, and you come up with
a hash code for it. PROFESSOR: OK so we're
going to hash this. And let's say this
is the key hash. OK, very good. AUDIENCE: And then once
you know that, then you'll need to compute the
next letter hash, or just add it on
to that pairing. PROFESSOR: OK, so
next letter for-- AUDIENCE: Yeah. So you compute the hash of the
entire string, n, capital n-- PROFESSOR: Let's not do that. Let's compute the hash of
the first key characters in the string. AUDIENCE: Are we separating
them by space [? inside? ?] PROFESSOR: Yeah, so this
is going to be a character. AUDIENCE: The space will? PROFESSOR: Sorry? AUDIENCE: The space
will be a character? PROFESSOR: Yeah. So let's take the
first three characters and compute the hash of that. And let's call this
the our sliding window. So we're going to say
that window has the and then we're going to compute
the hash of the characters in the window, and
we're going to see that this matches
the hash of the key. And then we'll figure
out what to do. That aside, we're going
to slide the window to the right by one
character so take out key and put in the space. And now the window
has HE space, we're going to compute the
hash of the window, see that it's not the same
as this hash of the key. What do we know in this case? Different hashes means-- AUDIENCE: Not the same string. PROFESSOR: For sure
not the same string. So this is not the. OK, now suppose I'm
sliding my window so after this I will
slide my window again, and I would have e space f. Right, so on and so forth. Suppose I'm happy sliding my
window and then I get here and I have my window be IN
space, and the hash the window happens to match
the hash of the key. So we're in the same
situation as here. Now what? AUDIENCE: [INAUDIBLE]. PROFESSOR: Very good. We have to check if the
string inside the window is the same as the
string inside the key. And if it is, we found a match. If it isn't, we keep working. All right? And we have to do
the same thing here. So this is our string
matching algorithm. AUDIENCE: Can we somehow
make sure that we make a hash function such that it
will never [INAUDIBLE]-- PROFESSOR: Excellent question. Thank you, I like that. Can we make a hash
function so that we don't have any false
positives, right? Let's see. How do hash functions work? What's the argument
to a hash function and what's the return value? AUDIENCE: The
argument is something that you want to hash. PROFESSOR: So in this case we're
working with three character strings. But let's say we're
looking for a one megabyte string inside the
one gigabyte string. Say we're a music
company and we're looking for our mp3 file
inside the big files off a pirate server
or something. So this is 1 million
character strings, because that's the window size. And it's going to return what? What do hash functions
return for them to be useful? Integers. Nice small integers, right? Ideally, the integer would
fit in a word size, where the word is the register
size on our computer. What are popular word sizes? Does anyone know? AUDIENCE: [INAUDIBLE]. AUDIENCE: Excellent. 32-bits, 64-bits integers. OK, so what's the universe
size for this function? How many one million
character strings are there? AUDIENCE: How many
characters are there? PROFESSOR: Excellent. Let's say we're old school
and we're doing [? SG ?]. We don't care about
the rest of the world. AUDIENCE: 256 characters? PROFESSOR: OK. AUDIENCE: To the
one millionth power. PROFESSOR: Cool. Let's say we're working
on an old school computer, since we're old school and
we have a 32-bit word size. How many possible values
for the hash function? AUDIENCE: 2 to the 32. PROFESSOR: The
[? other ?] is bigger. You're messing with me, right? OK so this is 2 to the 8th. AUDIENCE: 2 to the
8 million, right? PROFESSOR: Yup. So this is a lot
bigger than this. So if we want to make a
hash function that gives us no false positives,
then we'd have to be able to-- if we have the
universe of possible inputs and the universe of
possible outputs, draw a line from every
input to a different output. But if we have-- 2 to the 32 by
the way is about four billion. If we have four billion
outputs and a lot of inputs as we draw our lines,
we're eventually going to run out of
outputs and we're going to have to use the
same output again and again. So for a hash function,
pretty much always the universe size is bigger
than the output size. So hash functions will
always have collisions. So a collision for
a hash function is two inputs, x1, x2, so
that's x1 is not x2, but h of x1 equals h of x2. So this will always happen. There's no way around it. What we're hoping for
is that the collisions aren't something dumb. So we're hoping that
the hash function acts in a reasonably randomly and
we talked about ideal hash functions that would pretty
much look like they would get a random output for every input. And we're not going to
worry too much about that. What matters is that as long as
the hash function is reasonably good, we're not going to have
too many false positives. So say the output set is
O, so O is 2 to the 32. Then we're hoping to have
false positives about one every O times. So 1 out of 2 to the
32 false positives. So what's the running time
for when you slide the window and we're doing this logic here. What's the running time if
the hashes aren't the same? AUDIENCE: What's the running
time of the hash function-- PROFESSOR: Of the whole
matching algorithm. AUDIENCE: No, no no, of
the hash function itself. Can we make any
assumptions about that? Very good. What's the running time
of the hash function? So we're going to
have to implement-- if we implement the
hash function naively, then the running time for
hashing a key character string is order key. But we're going to come up
with magic way of doing it in order one time. So assume hashing is order 1. What's the running time
for everything else? So if the hashes don't match,
we know it's not a candidate. So we're going to keep going. So this is order 1. What if the hashes do match? AUDIENCE: [INAUDIBLE]
characters. PROFESSOR: Order-- AUDIENCE: I mean, but it
depends on how many ones match, but it will be-- PROFESSOR: So for
one match, what's the running time for one match? AUDIENCE: Order k-- PROFESSOR: Order k. Excellent. So the total running time is
the number of matches times order k plus the number of
non matches times order 1. So as long as the number
of false positives here is really tiny,
the math is going to come out to be roughly
order 1 per character. AUDIENCE: So the whole
thing is order in. PROFESSOR: Everything
should be order n, yeah, that's what we're hoping for. OK so let's talk about the
magic because you asked me what's the running time
for the hash function and this is the
interesting part. How do I get to
compute these hashes and order 1 instead of order k? We have this data structure
called rolling hash. So rolling hash--
does anyone remember from lecture what it is? AUDIENCE: Isn't that what
we're doing right now? PROFESSOR: So this
is a sliding window. And the data structure
will compute fast hashes for the strings inside
the sliding window. So how does it work? I mean not how does it
work functionally, what are the operations for a
rolling hash, let's try that. AUDIENCE: Oh [INAUDIBLE]. PROFESSOR: OK, so
we have two updates. One of them is pop. For some reason, our
notes call it skip, but I like pop
better, so I'm going to write skip and think pop. And the other one is? AUDIENCE: Always [INAUDIBLE]. PROFESSOR: A pen with
a new character, OK? Cool. So these are the updates. Now what's the point
of those updates? What's the query
for a rolling hash? AUDIENCE: [INAUDIBLE]. You just grab the
next character, append that, and then
skip [INAUDIBLE]. PROFESSOR: OK, so this is
how I update the rolling hash to contain to reflect the
contents of my sliding window. And what I do after that? What's the reason for that? AUDIENCE: You skip
your [INAUDIBLE]. PROFESSOR: So don't
think too hard, it's a really easy question. I moved a sliding window here. What do I want to get? AUDIENCE: You want to get
the hash of those characters. PROFESSOR: The hash of
those characters, very good. So this is the query. So a rolling hash has a sequence
of characters in it, right? Say t, h, e. And it allows us to append the
character and pop a character. Append a character,
pop a character. And then it promises
that it's going to compute the hash
of whatever's inside the rolling hash really fast. Append goes here,
skip goes here. How fast do these
operations need to be for my algorithm
to work correctly? AUDIENCE: Order 1. PROFESSOR: I promised
you that computing hash there is order 1, right? So I have to-- OK. Let's see how we're going
to make this happen. So these are letters. These make sense
when we're trying to understand string matching. But now we're going to switch
the numbers, because after all, strings are sequences
of characters, and characters are numbers. And because I know how
to do math on numbers, I don't know how to
do math on characters. So let's use this list. Let's say that instead of
having numbers in base 256 which is [INAUDIBLE],
we're going to have numbers in base 100,
because it's really easy to do operations in base 100 on paper. So 3, 14, 15, 92,
55, 35, 89, 79, 31. So these are all
base 100 numbers. And say my rolling
window is size 5. One, two, three, four, five. So I want to come up with a way
so that I have the hash of this and then when I slide my window,
I will get the hash of this. What hashing method do we
use for rolling hashes? Does anyone remember? AUDIENCE: [INAUDIBLE]. PROFESSOR: Mod, you said--
I heard mod something. AUDIENCE: Yeah,
that's what I said. PROFESSOR: OK, so? So? So the hash is? The hash of a key is? AUDIENCE: It's k mod m or m k. [INAUDIBLE] PROFESSOR: OK. I'm going to say
k mod something, and I'm going to
say that something has to be a prime number
and we'll see why in a bit. Let's say our
prime number is 23. So let's compute the value of
the hash for the sliding window of the first sliding
window and then we'll compute the hash for the
second sliding window. Oh, there is some at
the computer, sweet. 314159265 modulo 23 is how much? OK, while you're doing
that, can someone tell me what
computation will he need to do for the second
sliding window? AUDIENCE: 1519265359. PROFESSOR: 159265359. AUDIENCE: That's a third sign. AUDIENCE: There's
a 1-4 before that. AUDIENCE: The first one is 11. PROFESSOR: OK. And what's the second one? AUDIENCE: [INAUDIBLE] adding-- PROFESSOR: 1415926335 modulo 23. AUDIENCE: 5. PROFESSOR: I heard a 5 and a 7. OK. AUDIENCE: Hold on, hold on. PROFESSOR: I'll take
the average of those two and we can move on, right? AUDIENCE: Three
five, and arguably 6. PROFESSOR: All right,
so let's implement an operation called slide. And slide will take the new
number that I'm sliding in. And the old number
that I'm sliding out, making my life really easy. So in this case, the
numbers would be-- AUDIENCE: The new one is 35. PROFESSOR: And the old one? AUDIENCE: 3. PROFESSOR: Excellent. And I want to have an internal
state called hash that has 11 and I want to get 6 after
I'm done running slide. This is still too hard for me,
so before we figure out hash, let's say that we have an
internal state called n. And n is this big number here. So I want to get from this
big number to this big number. What am I going to do? AUDIENCE: Mod 3,000 [INAUDIBLE]. PROFESSOR: OK, so you want
to take the big number 159265 and mod it. AUDIENCE: [INAUDIBLE] PROFESSOR: So if I mod
it to by a big number, that's going to be too slow. So I can't mod it. AUDIENCE: Can't
you just divide it? PROFESSOR: Division
is also slow, I don't like the division. I like subtraction,
someone said subtraction. So what I want to do is I want
to get from here to a number that-- to this number, right? So I want to get
rid of the 3 and I want to add 35 at the end. To get rid of the 3,
what do I subtract? AUDIENCE: 3 with a bunch of 0s. PROFESSOR: 3 with a bunch of 0s. Excellent. 1, 2, 3, 4, 5, 6, 7, 8. How many of them are there? AUDIENCE: 8. PROFESSOR: OK, how many
digits conveys 100? AUDIENCE: Oh, 2 right? AUDIENCE: 4. AUDIENCE: Oh, oh. PROFESSOR: 4. So base 100, so two numbers-- AUDIENCE: One base 100
number is two digits. Yep. PROFESSOR: So 8. yeah, OK, 4. Cool. So let's try to write this
in a more abstract way. So n is the old n
minus old, right, so that 3 is old times what
do I have to multiply it by to get all those zeros? k minus 1? [INAUDIBLE] to
that base whatever. PROFESSOR: OK, so-- base
to the size to something. K minus 1. So K is 5 in this case, right? My window is 5. And I see a 4 there, so I'm
going to add the minus 1 just because that's
what I need to do. OK, so then I get 14159265. What do I do to tack
on a 35 at the end? AUDIENCE: [INAUDIBLE] 35. PROFESSOR: OK, times
the base, so that's going to give me the zeroes. And then this is a minus here. And then I'm going to add 35. Right? 1415926535. Look, it's right. So what do I write here? AUDIENCE: The base first. PROFESSOR: Good point. OK. Let me play with this a little
bit before we go further. I'm going to distribute
the base here. So this is n times
base minus old times base to the k plus mu. And let's rename base to size
to be the size of the window, I don't like k. And I'm renaming it
because later on we're going to break our slide
into appends and skip and the size won't
be constant anymore. OK so does this make sense? It's all math. So this math here becomes
abstract math here. But nothing else changes. OK, so now I want
to get hash-- I want to get hash out
of n, how do I do that? AUDIENCE: Mod 23. PROFESSOR: Mod 23, very good. So in a general way,
I would say mod p. OK so hash is n times
base minus old times base to the size plus new mod p. Now let's distribute this. I know I can distribute
modulo across addition and subtraction, so I have n
mod p times base minus old times base to the size mod p plus new. And everything still
has to be a mod p. So can someone tell me
where did I add the mod p? Why did I put it here and here? AUDIENCE: [INAUDIBLE]
the original? PROFESSOR: OK, nmodp
is hash, let's do that. So what's true about both
n and base to the size? AUDIENCE: Constant. PROFESSOR: Constant? AUDIENCE: Like can
you please repeat it? AUDIENCE: You could [INAUDIBLE]
base to the size but you can't [INAUDIBLE] hash,
I mean [INAUDIBLE]-- PROFESSOR: Hm. OK, so keep this in mind
that we can compute this, because we're going to
want to do that later. But what I had in mind is
the opposite of constant, because n is huge. Right? And base to the size
is also huge, right? N is this number. Base to the size is
this number here. 1 followed by this many zeros,
so these numbers are big. All the other numbers are small. Base is small, old is small,
new is small, p is small. PROFESSOR: So I want to
get rid of the big numbers, because math with
big numbers is slow. So unless I get rid
of the big numbers, I'm not going to get
to order 1 operation. So we already got rid of
this one because it's hash and how do I get
rid of this one? AUDIENCE: [INAUDIBLE] AUDIENCE: There's
some 6042 algorithm that does that quickly. AUDIENCE: Well, we definitely
just went over this in class today. AUDIENCE: Which is why you
needed the prime number, right? PROFESSOR: Not quite. There is an algorithm
that does it quickly. That algorithm is
called repeated squaring and the quickest-- wait, I'm not
done, I promise I'm not done. So the quickest
that this guy can run if you do everything right
is order of [? log ?] size. If the window size
is 1 megabyte, 10 megabytes, if the
window size keeps growing, if the window size is
part of the input size, is this constant? Nope. So I can't do that. Someone else gave me
the right answer before. What did you say before? AUDIENCE: Pre-compute it? PROFESSOR: OK. It's a constant, so why
don't we pre-compute it? Take it out of here,
compute it once, and after that, we can
use it all the time. And unless someone has
a better name for it, I'm going to call this magic. The name has to be
short, by the way, because I'll be writing
this a few times. OK, so now we have hash
equals hash times base minus old times magic
plus new modulo p. Doesn't look too bad, right? Pretty constant time. Now let's write the pseudo
code for the rolling hash, and let's break this
out into an append and a skip at the same time. AUDIENCE: What if hash is
bigger than your word size? PROFESSOR: So hash is always
going to be something modulo p. AUDIENCE: Oh that's true, OK. PROFESSOR: So as
long as p is decent, it's not going to get too big. AUDIENCE: All right. What if old and
new [INAUDIBLE]-- PROFESSOR: So old and new-- AUDIENCE: P is a big number . 314159269 is possibly bigger
than your word size, right? PROFESSOR: Definitely. So that's why we're
getting rid of it. AUDIENCE: That is
true. [INAUDIBLE] PROFESSOR: So this is
k digits in base b. Too much. Not going to deal with it. Hash is one digit in base p,
because we're doing it mod p. Old and new are
one digit base b. So hopefully small numbers. OK, I haven't seen a
constructor in CLRS, so I'm going to say that
when you write pseudocode, the method name
for a constructor is in it because we've
seen this before. And let's say our constructor
for a rolling cache starts with the base
that we're going to use. And it builds an
empty rolling hash, so first there's nothing in it. And then you append and you
skip and you can get the hash. AUDIENCE: What about p? Shouldn't you also do p? PROFESSOR: Sure. Do that. So let's say base and p are set,
so somethings sets base and p. And we need to compute
the initial values for hash and magic. What's hash? Zero. There's nothing in there, right? The number is 0. What's magic? AUDIENCE: [INAUDIBLE]. Well, I mean, you can
calculate it, right? PROFESSOR: So magic is
based to the size mod p. What size? AUDIENCE: [INAUDIBLE] 0. Just one mod p. PROFESSOR: Yep. So when I start, I have
an empty sliding window. Nothing in there, size
is 0, base to the size is 1, whatever the size is. Very good. Let's write append. Hash is? So here, we're doing both
an append and the skip. We have to figure out
which operation belongs to the append, which
operations belong to the skip. So someone help me out. AUDIENCE: We know subtraction
would [INAUDIBLE]-- AUDIENCE: Multiply
mod base [INAUDIBLE]. PROFESSOR: Yup. So this is the append, right? And this is the skip. So hash equals hash. Times base plus new mod p. Very good. This is important. If you don't put
this in, Python knows how to deal with big numbers. So it will take your
code and it'll run it, and you'll get the
correct output. But hash will keep growing
and growing and growing because you're computing
n instead of hash. And you'll wonder why
the code is so slow. So don't forget this. What else do I need to update? OK, I don't have a
constant for that, but I have a constant
I for something else. Magic. So magic is base
to the size mod p. So what happened
to the window size? AUDIENCE: Oh. Times base [INAUDIBLE]. PROFESSOR: Excellent. The window size grows
by 1, therefore, I have to multiply this by base. Magic times base mod p. AUDIENCE: Does p always have
to be less then the base, or can it be anything? PROFESSOR: It can be
bigger than the base. So if I want to not have
a lot of false positives, then suppose my base
is 256, because that's an extra character. I was arguing earlier that
the number of false positives that I have is 1/P basically. So I want p to be as close
to the word size as possible. So p will be around
2 to the 4 billion. So definitely bigger. It can work either way. It's better if it's
bigger for the algorithm that we're using there. All right, good
question, thank you. Skip. Let's implement skip. Hash is? AUDIENCE: Hash minus old
[INAUDIBLE] then comes magic [INAUDIBLE]. PROFESSOR: OK, can I
write this in Python? What happens if I write this? AUDIENCE: [INAUDIBLE]
magic is, [INAUDIBLE] We won't be able to find it. PROFESSOR: OK so--
sorry, not in Python. So assume all these are instance
variables done the right way, but what happens if old times
magic is bigger than hash? I get a negative number. And in math, people
assume that if you do something like minus 3 modulo
23, you're going to get 20. So modulo is always positive
in modular arithmetic, but in a programming language,
if you do minus 3 modulo 20, I'm pretty sure you're
going to get minus 3. And things will go back. So we want to get
to a positive number here so that the
arithmetic modulo p will work just like in math. So we want to add something to
make this whole thing positive. AUDIENCE: That's something
times [INAUDIBLE]. PROFESSOR: OK, so if
we're working modulo p then we can add anything to
our number, any multiple of p, and the result modulo
p doesn't change. For example, here to get from
minus 3 to 20, I added 23. Right? OK, so I want to
add a correction factor of p times something. So what should that be? I want to make sure that
this whole thing is positive. AUDIENCE: [INAUDIBLE] PROFESSOR: So let's see. How big are these
guys, by the way? Magic is something mod p, right? So it's definitely
smaller or equal to p. How about old? AUDIENCE: [INAUDIBLE] PROFESSOR: OK. So smaller or equal than? AUDIENCE: Base. PROFESSOR: Base. Very good. So this whole
thing is definitely going to be smaller
than [INAUDIBLE]. So this is definitely going to
be smaller than base time p, right? So let's put that in here. You can get fancy and say
hey, this is smaller than p, and this is old, so you can put
old here instead, same thing. OK so we have hash. Now what do we do to magic? AUDIENCE: [INAUDIBLE] divide
it by the base and mod p. It seems base [? and p ?]
don't share factors. You're allowed to do that? PROFESSOR: OK, so skip part two. Magic equals-- So what if
my magic is something like 5 and my base is 100? How is this going to work? This is where we use fancy math. And I call it fancy
math because I didn't learn it in high school. So I'm assuming at least some of
you do not know how this works. So if we're working
modulo p, you can think 23 if you prefer
concrete numbers instead. For any number between
1 and p minus 1, there's something called
the multiplicative inverse, a to the minus 1,
that also happens to be an integer
between 1 and p minus 1. And if you multiply, say, a
times b, that's another number. And then you multiply
this by a minus 1, you're going to
get to b modulo p. So a minus 1 cancels
a in a multiplication. Now let's see if you guys
are paying attention. What's a times a to
the minus 1 modulo p? AUDIENCE: 1. PROFESSOR: OK. Sweet. So suppose I want to find the
multiplicative inverse of 6. What is it? AUDIENCE: Is that the mod 23? PROFESSOR: Yeah. Can someone think of
what it should be? AUDIENCE: 4. PROFESSOR: 4, wow, fast. So 6 times 4 equals 24,
which is 1 modulo 23. Now let's see if this magic
really works, this math magic. So 6 times 7 equals? AUDIENCE: 42. PROFESSOR: Which is what mod 23? Computer guys. AUDIENCE: Negative 4, so 5. Ah, just kidding. Yeah. PROFESSOR: OK now
let's multiply 19 by 4. What is this? AUDIENCE: 76. PROFESSOR: All
right, 76 modulo 23? AUDIENCE: 7 maybe. PROFESSOR: Are you kidding? Did you compute it,
or did you use-- AUDIENCE: 69 [INAUDIBLE] PROFESSOR: OK. Started with 7, ended with 7. So this works. So as long we're working
modulo a prime number, we can always compute
multiplicative inverses. And Python has a
function for that, so I'll let you Google
its standard library to find out what it is. But it can be done,
that's what matters as far as we're concerned. So we're going to say that magic
is magic times base minus 1 mod p, which is the multiplicative
inverse everything mod p. Now suppose this base
minus 1 modulo p, this multiplicative inverse
algorithm is really slow. What do we do to stay order 1? Pre compute it. Base is not going to change. Very good. So the inverse of base, I
base, is base minus 1 mod p. So here I replace
this with I base. OK so skip part one is
there, skip part two is here. Does this make sense so far? I see some confusion. AUDIENCE: A lot. PROFESSOR: A lot
to take in at once? AUDIENCE: Yes. PROFESSOR: OK. So remember this concept. So this is where
we started from. Then we computed
n, then after n, we worked modulo p
to gets to hashes. So by working module p,
we're able to get rid of all the big
numbers and we only have small numbers
in our rolling hash. And there's that
curveball there, there is that inverse,
multiplicative inverse, but Python computes
it for you, so as long as it's in
the initializer, here you don't need
to worry about it, because it's not part of
the rolling hash operations. By the way, what's the cost of
the rolling hash operations? What's the cost of new? Sorry, what's the
cost of append? Not thinking here. Constant. All these are small
numbers, so the arithmetic is constant, right? What's the cost of skip? Skip part 1 here,
skip part two there. What's the cost of skip? Constant. . All the numbers are small. We went through a lot
of effort to get that, so skip is order 1. We're missing hash. How would we implement
the hash operation? A hash query. It's easy. Sorry? AUDIENCE: [INAUDIBLE]
lookup [INAUDIBLE]. PROFESSOR: So a rolling hash
has append, skip, and hash. I want to implement
that hash function. Hash. We're computing
hash all the time. Return. Sorry, I didn't understand
what you meant by lookup. AUDIENCE: It's
one of our states. PROFESSOR: Yeah. Exactly. So the hash function
returns hash, right? What's the cost of that? Constant. So append is constant time, yes? Skip is constant time. Hash is constant time. We're done. This works. Any questions on rolling
hashes before you have to implement
one of your own? AUDIENCE: [INAUDIBLE]
wouldn't it be easier to use
a shift function? Then you don't have to
think about plus and minus. PROFESSOR: A shift function. AUDIENCE: Well I mean like,
you can shift bit-wise, right? PROFESSOR: OK. AUDIENCE: So you
can just use shift instead of thinking
about where to add this, where to subtract this. PROFESSOR: Well so
I do bit operations if I'm willing to work
with these big numbers. AUDIENCE: But then
you have to compute the mod of some
big number, right? Like just like that. For this one, you don't
have to, because you have the original
hash [INAUDIBLE]. AUDIENCE: Oh, you
mean the big number being the actual word
you're looking at? AUDIENCE: Yeah. PROFESSOR: So doing shift
is equivalent to maintaining a list and pushing and
popping things into the list. And then you have
to do a hash, it's equivalent to looking
over the entire list and computing the hash function. Because you'd have a big
number and you have to take it modulo 23. And that's order of the
size of the big number. But we're not
allowed to do that. Hash has to be constant time,
otherwise this thing is slow. AUDIENCE: Why do we
compute magic numbers then? PROFESSOR: Why do
we compute magic? We compute magic because
somewhere here, we had this base to the size
mod p and this could get big. So I can't afford to keep
it around and do math with it all the time. So I can't compute
base to the size every time I want to do append. AUDIENCE: Would it be worth
it if you're computing 100 different values for
matching and [INAUDIBLE], so all you'd have to do
is, when reassigning magic, just look up-- PROFESSOR: So if you
do that, then you have to compute values
for all the sizes, right? For all the window sizes. AUDIENCE: Right. So if we assume that window
sizes will be less than 100, it doesn't take very long. PROFESSOR: Well what if the
window size is 1 million? What if I'm looking
for a 1 million character in a 1
gigabyte string? AUDIENCE: But wouldn't after
all, wouldn't the size just be around the string, like plus
or minus the size of the base? So-- AUDIENCE: Only if [INAUDIBLE] So why would the
size change again? Why wouldn't it
just be-- I mean, if you're looking
at one character. PROFESSOR: So if I have a
sliding window like this, then it doesn't change. But if I want to
implement a rolling hash, that's a bit more general and
that supports append and skip. Whenever I append,
the size increases. Whenever I skip,
the size decreases. AUDIENCE: Oh, you're not doing
those at every time step. You're doing them as needed. PROFESSOR: So I'm
trying to implement that, that can do
them in any sequence. AUDIENCE: Oh. AUDIENCE: OK. I thought we were just
doing sliding window. PROFESSOR: So if we're just
doing sliding window, you can-- AUDIENCE: This is really
more caterpillar hash instead of rolling hash, like
it's more general. PROFESSOR: Yeah. It's a bit more general. So let's look at rolling
hash for the window. And what you're saying
is, hey, the window size is constant, so-- AUDIENCE: Why do we
repeat magic [INAUDIBLE]? PROFESSOR: Yeah, if the
window size is constant, then we wouldn't re compute it. It wouldn't change. But with this thing, it's not. OK. AUDIENCE: But I guess it
doesn't really matter, but even if you call
these in the same order, then isn't that wasting
a lot of computing cycles because just shrinking and then
growing every single operation? PROFESSOR: Oh, well it turns out
that a lot of computing cycles is still order one, right? Everything is order one. So as algorithms
people, we don't care. If you're doing it in a
system and you actually care about that, then OK. But you're still going
to have to compute the initial value at some point. AUDIENCE: But if you know
window's staying the same, you don't need to that
computation every time? PROFESSOR: If you-- sorry? AUDIENCE: If you
know you're actually just doing a window
rolling hash-- PROFESSOR: Yup. So then you would
initialize magic here to be whatever you
want it to be, right? But then when you add the first
few characters to the window, you have to figure
out how to add them. So the code gets more messy. It turns out that
this is actually simpler than doing it that way. AUDIENCE: [INAUDIBLE]
magic I guess I'm just confused because it
seems like we're still working with the large numbers
every time [INAUDIBLE]. PROFESSOR: Oh. Let's see. Mod p, mod p. AUDIENCE: That's not-- so even
though you're still multiplying magic times base,
it doesn't matter. PROFESSOR: After I'm going
that, I'm reducing it modulo p. Yeah. AUDIENCE: And then
because we're only working with the smaller values. PROFESSOR: Yup. So everything here stays
between 0 and base or 0 and p. Actually hash is between 0 and
p and magic is between 0 and p. OK. AUDIENCE: How big
does p usually get? PROFESSOR: How big
does p usually get. So And let me get back to this. So I was arguing that the
number of false positives here is one over O, right? is the number of values that
the hash function can output. How many hash functions can we
output using a rolling hash? AUDIENCE: P. PROFESSOR: P. OK. So the number of
false positives is 1/P. So what do we want for p? AUDIENCE: We want p to be the
word size, because but if p's the word size, then-- PROFESSOR: So p can't
be the word size, because it has to
be prime, right? But we want it to be big,
because as p becomes bigger, 1/P becomes smaller. So there are two constraints. We want p to be big
so that we don't have a lot of false positives. And we want p to be
small so that operations don't take a lot of time. So in engineering, this
is how things work. We call it a tradeoff
because there are forces pushing in
opposite directions, and it turns out that a
reasonable answer to the trade off is you make p fit in a word
so that all those operations are still implementable
by one CPU instruction. You can't have it
be the word size. So if we're working
on a 32-bit computer, I can't have this
be 2 to the 32. But I can have a prime number
that's just a little bit smaller than 2 to the 32. AUDIENCE: Wait, why can't
it be the word size? Or why can't it be 2 to the 32? PROFESSOR: So if p would
be this instead of a prime, then I can't do this. AUDIENCE: Oh, right right
right, yeah I knew that. PROFESSOR: There are a
lot of moving parts here and they're all interconnected. AUDIENCE: You could do that
for any prime number, right? PROFESSOR: Yup. So this works for
prime numbers, but it doesn't work for
non prime numbers. AUDIENCE: You could find
the multiplicative inverse for any prime number in base 32. Is that true? I mean any odd number is
what I'm trying to say. No, that's not true. PROFESSOR: I refuse to
answer hard math questions. AUDIENCE: They need to
be relatively prime. They need to share no factors. PROFESSOR: Yes,
it might be true. AUDIENCE: So an odd will
not share a factor with 2 to the 32? PROFESSOR: You're forcing
me to remember hard math. AUDIENCE: Yeah, I totally
just thought about this as [INAUDIBLE] number. PROFESSOR: So, no, it
turns out that there's no-- if you're working modulo
and non prime base, then there's no
multiplicity inverses. So some numbers have no
multiplicative inverses, and other numbers have more
than one multiplicative inverse. And then the whole
thing doesn't work. So let me see if I
can make this work without having an
example by hand. Let's say we're
working mod 8, right? Mod 8. So 2 to the minus 1 mod 8 is
not going to exist, right? AUDIENCE: Right, but 3 will. PROFESSOR: 3. Let's see what do we use? 3 times 3 is 9, right? So this is 1. How about 3 times 5? 15 mod 8 is 7. So 3 and 5, and then-- AUDIENCE: 11. PROFESSOR: OK. So 3 times 7 would be 21, 5. OK so 3 and-- 3 is the
multiplicative inverse of itself, and 5
and 7 are-- yeah. I have to build a more
complicated example, but this breaks
down in some cases. I'll have to get back to you. I will look at my notes
for modular arithmetic and I'll get back to
you guys over email for why and how that breaks. Yes. AUDIENCE: Sorry, can
you tell me again why we did the part 2 in skip? Like why did we do that? I'm not really sure [INAUDIBLE]. PROFESSOR: So we
started with magic 1 and then we-- in order
for this to work, we agree that magic will be
base to the size modulo p all the time. So this has to be [INAUDIBLE]
invariant for my rolling hash. When I do an append,
the size increases by 1. And then I multiply
by base to modulo p. When I do a skip, the
size decreases by 1. So I have to change magic,
because magic is always base times size, so
I have to update it. So this is why this happened. Because initially,
I wanted to update by dividing it by base, right? Magic divided by base. But if magic is 5
and base is 100, we're not going
to get an integer. And we want to stay
within integers, so that's when I pulled
out fancy math and-- OK. OK. So how are we doing
with rolling hashes? Good? AUDIENCE: All this math
will be in the notes, right? PROFESSOR: Everything. Oh, yeah, everything else
will be in the notes. Before we close out, I want
to show you one cute thing. Who remembers
amortized analysis? I know there's one person
that said they understood. All PROFESSOR: The growing,
shrinking thing is what we did in lecture. I want to show something else. I want to show
you a binary tree. A binary search tree, because
you've seen this on the PSAT and you already hate it. AUDIENCE: Why'd they
call it amortization? Because I looked it up
online, it means to kill, and so I'm like, why not say
like, attrition or something else that's a little bit less-- PROFESSOR: Amortization is
also used in accounting to mean you're-- [INTERPOSING VOICES] PROFESSOR: Let's use the
growing hash example, because that's good for
why this is the case. So when you're growing your
table, you're inserting. If you still have
space, that's order one. If not, you have to grow
your table to insert. And that is more expensive. That's order n where n is how
many elements you had before. So if you graph this
costs, if you start off with a table of size one, you
can insert the first element for a cost of one. For the second element, you
have to resize the table, so it's a cost of two. Now when you're trying to
insert the third element, you have to resize the
table again to a size of 4. But when you insert the
fourth element, it's free. Well, cost of one. When you insert
the fifth element, you have to resize a table
to the size of eight, right? So The table size is one, two,
four, four, and now it's eight. But because they
resized this to eight, the next three assertions
are going to be order one. And then the one after that is
going to make the table be 16. So I can do seven
insertions for free and then I'm going
to have to pay a lot more for the next one. Someone said dampening. I like dampening because
the idea behind amortization is that you can take--
you have these big costs and they don't occur very often. So you can think of it
as taking these big costs and chopping them up. For example, I'm going
to chop this up into four and I'm going to take this
piece and put it here. This piece and put it here. This piece and put it here. And then I'm going to
chop this guy into two, and then take this
piece and put it here. And the beginning's
a little bit weird, let's not worry about
that but this guy, if I chop this guy up into
eight, it's going to happen, is it? Well we can put-- so this guy
grows exponentially, right? Every time it's multiplied by 2. But the gap size here
also is multiplied by 2. So when I chop this up and
I re distribute the pieces, it turns out that the
pieces are the same size. So if I apply a dampening
function that does this, then the costs are going
to look-- they're not going to be on
one, they there are going to be three or something. And they look like this. Now, my CPU time is going
to look like this, right? That's not going to
change, because that's what's really happening. But what I can argue
for is that if I look for a bunch of operations,
say if I look at the first 16 insertions, the cost of those
is the sum of these guys. So it's not been
squared, which is what you would get if you
look at the worst cases here, but it's order n. So this is what's
being dampened, the amount of time
an operation takes. Does this make some sense? All right, I want to show you a
cute example for amortization. And I'll try to make it quick. So how do you list the keys in
a binary search tree in order? AUDIENCE: [INAUDIBLE] PROFESSOR: In order
traversal, right? OK, there's another
way of doing it that makes perfect
intuitive sense. Get the minimum key, right? And then output the
minimum key, then while you can get the
next largest, which is the successor--
so while this is not, now output that key, right? If you do the thing
within order traversal, you get order end running time. What's the running
time for this? AUDIENCE: For n. You're going through
all the keys, too. PROFESSOR: Yeah,
but next largest-- what's the running
time for next largest? AUDIENCE: Log n. PROFESSOR: So this
guy's log in, right? So I have n keys, so this
whole thing is O of n logn. So it's definitely not
bigger than n logn. But now, let's look at what
happens using the tree. When I call min, I
go down on each edge. And then I call successor
and it outputs this guy. Then I call successor
and it goes here. Than I call successor and
it goes up here and here and outputs this guy. Successor goes here. Successor goes here. Successor goes all the way down
here, successor goes up here, successor goes here,
and then successor goes all the way up to
the roots and gives up. AUDIENCE: [INAUDIBLE] PROFESSOR: So how many times
do I traverse each edge? Exactly twice, right? How many edges in the tree? If I have n nodes, how many
lines do I use to connect them? [INAUDIBLE] So 1 node, zero lines. 2 nodes, one line. Three nodes, two lines. So n nodes, n minus one. N asymptotically, good. Good answer. Order, n, edges. Right, each edge gets
traversed exactly twice. So amortized cost for n next
largest operations is order n. So you can do this instead. This code makes a lot more
sense than in order traversal. OK, and the last part
is remember that list query that was on the PSAT? Turns out you can do a
find for the lowest element and then call successor
until you see the highest element for the same argument. Well, I couldn't tell
you this for the PSAT because we hadn't learned
amortized analysis, so you wouldn't be able to
prove that your code is fast. But now if you
get the intuition, you can write it that way. And your code will
still be fast. Same running time. So the intuition for that
is a bit more complicated. The proof is more complicated. But the intuition is that
say this is l and this is h. Then I'm going to go
in this tree here. So the same edge magic
is going to happen, except there will be logn
edges that are unmatched here and logn edges that
aren't unmatched here. Because once I find the
node that's next to h, I'll stop, right? So some edges will
not be matched. So then I'll say that the total
running time is logn plus a. AUDIENCE: i being the number of
elements you pull out, right? PROFESSOR: Yup. So this is amortized analysis. The list is hard. The traversal is easy. Remember the traversal. That's easy to reason
about, so that's good. OK. Any questions on
amortized analysis? So the idea is that you
look at all the operations, you don't look at one
operation at a time. And you're trying to see
if I look at everything, is it the case that I have
some really fast operations and the slow
operations don't happen too much, because
if that's the case, then I can make an argument
for the average cost, which is better than the argument
that says this is the worst case of an operation,
I'm doing an operation, the total cost is n
times the worst cost. Make some sense? OK. Cool. All right. Have fun at the next p set.

Data Structures

The following
content is provided under a Creative
Commons license. Your support will help MIT
OpenCourseWare continue to offer high quality
educational resources for free. To make a donation or
view additional materials from hundreds of
MIT courses, visit MIT OpenCourseWare
at ocw.mit.edu PROFESSOR: How about
I propose this? We go through
knapsack because it's on the Pset, not knapsack
but a variation of it. If we have time, we do
a couple variations. If not, maybe not. And if we have time,
we do at a distance. Let's see what happens
after knapsack. AUDIENCE: [INAUDIBLE]? PROFESSOR: How many
people got the difference between polynomial
and pseudo polynomial? OK, so we should
definitely do knapsack. AUDIENCE: Constant
factors, they don't matter. Oh wait. [INAUDIBLE]. Yeah, they do. PROFESSOR: So the thing is
it's not constant factors. That s there is not a constant. AUDIENCE: [INAUDIBLE]. PROFESSOR: So the
knapsack problem. So the way I look at it. You're a thief. You somehow made your
way into a vault. That vault has n items. AUDIENCE: Way
better than camping. PROFESSOR: Each item has
a weight of si pounds, and after you get
out of the vault, assuming you make it
alive and everything, you can sell it for vi
dollars on the market, so this is how much money
you get out of it, vi. Well, now the problem is the
only thing you have with you as you enter that vault
is one knapsack that can carry at most s pounds. If you try to put
more stuff in it, so if you try to load it
up with more than s pounds, it's going to break as you
try to escape from the vault and stuff is going
to fall on the floor, and then many laser beams
will shred you to pieces, so that's undesirable. So we can only load the
knapsack with s pounds. Now, given this
restriction, we want to make as much
money as possible out of the whole
thing, so we want to load up the
knapsack optimally. Does this make sense? Everyone remembers? Good. So two ways to solve it,
graphs and dynamic programming. We're going to do both. Who wants to go
for graphs first? Who wants-- OK, never mind. Majority has been achieved. How do we represent this? First off, let's
represent the solution. Our solution is which
items we chose, right? So we can phrase
this as n decisions. Each decision is a
true/false decision, and it indicates if you take
that item with you or not. So di says, do I take item i? So now this looks
more like a game. You are on a TV,
there's a game show, and you get asked n questions. Do you want to take this
item with you, yes or no? If you somehow
manage to get items that are more than s
pounds heavy, you get shot, you don't make it to
the end of the game. Otherwise, when
you leave the game, you make some
money and the money that you make depends on
what items you took with you. So now this looks
like the game problems that we used to solve with
graphs in that you have decisions, those
decisions are moves, and they get you through
a graph of states. Now, let's see
what's in the state. What do we need
to keep track of? First, we need to keep track of
what item we're thinking about, right? And then there's something else
that we need to keep track of. AUDIENCE: How much
capacity we have. PROFESSOR: Yep, very good. So the reason I
need that is when I'm deciding whether I'm
taking an item or not, I want to know if I'm
going to get shot or not. If I take too much stuff,
I'm not going to make it, so I need to know if this item
fits in my backpack or not. That's equivalent to knowing
how much weight I have left. So s pounds. Let's say this is
capacity, how much weight left in my backpack's capacity. This is equivalent to how much
weight I have accumulated, the total weight of my items. Weight of the items
I have taken so far. So the sum of the weights
of the taken items. AUDIENCE: [INAUDIBLE] left. Isn't it the total minus? PROFESSOR: Well, I'm
saying they're equivalent, so if you know one, you
can compute the other. They're not equal. Knowing one lets you
know the other one, but this is more useful in terms
of putting the graph together, I claim. We'll see if that
is true or not as we try to put the graph together. So what's a node? What's an edge? AUDIENCE: An edge is
putting in one more item, and a node is a state. PROFESSOR: So what the weight
going to be on an edge? If the edge means
I'm taking an item, then the weight is
going to be what? AUDIENCE: The weight of
the item you're adding. AUDIENCE: The earning
associated with that. PROFESSOR: Yeah. I like this better
because in the end, my goal is to maximize
earnings, right? And I'm going to feed my graph
to a shortest path algorithm. Maximize earnings,
minimize path weight. They're the same
issue, flip sign. So I'm going to say this is
the value of the item almost. Does this work or not? Negative value of the item. I have to flip the sign to turn
it from a maximization problem to a minimization problem. So shortest path will
give me the shortest path. That's going to correspond
to making the least amount of money if I
don't add this minus sign. So what's a node? AUDIENCE: Is it a
unique set of items? PROFESSOR: Sorry? AUDIENCE: A unique set of items. PROFESSOR: OK. So the state in a
node is going to have i, which is the item that I'm
looking at, because this way, I'm going to have one
node for each item. And what else did I say
I need to keep track of? AUDIENCE: The weight
of the taken items. PROFESSOR: Yep. Weight of taken. So this is sort of
like the gas problem where you have to keep track
of how much gas you have as well as where
you are on the map. Sorry if it brings bad memories. Let's talk about an example so
that we can draw a graph for it and see what things look like. So say we have three
items, and say our backpack has five pounds. And my three items
are a golden statue, value $10, weight, four pounds. These are 1800 dollars
when money actually used to be worth something. Crystal ball, you
can sell this for $4, and it weighs two pounds, and
someone in the previous section wanted a fountain
pen, so we're going to use a fountain
pen that's worth $7. Culture is worth a
lot of money, right? And weighs three pounds. Really ancient fountain pen. I don't know how
people wrote with it. So how do we draw
the graph for this? AUDIENCE: We'd make a tree
where each level the tree is take item i, or
don't take item i. Make a big binary tree. Does that make sense? PROFESSOR: So if each
level of the tree says whether I'm
taking an item or not, and then I have two
descendants, then that's going to be 2 to
the number of items. So that's going
to be exponential in the number of
items, whereas we're going to end up in a solution
where the running time is proportional to the
number of items. AUDIENCE: [INAUDIBLE]. PROFESSOR: I mean, it
makes sense in some cases, if you have fractional
costs or something. By the way, all these
weights are integers. Sorry I didn't mention that. My bad. But I like the idea of having
some sort of levels based on items, so let's say I'm
going to have a starting node, and then I'm going to have some
sort of layer for item one, some sort of layer for item
two, and some sort of layer for item three, some
sort of vertical layer. How many nodes do
I have in a layer? One node for each
possible weight because I promised that in a
node, I keep track of the item that I'm considering and
the weight of the items I've taken so far. How many possible
weights do I have here? AUDIENCE: Three weights. PROFESSOR: So my backpack
holds five pounds, so what are the possible sums I can get? How many? AUDIENCE: 4, 2, 3, 5. PROFESSOR: So far, I
heard 2 3, 4, 5, 6. Who wants to bid more? AUDIENCE: Not more than 6. PROFESSOR: So the
answer is 6, right? The possible weights are from
0, which is an empty knapsack, until weight 5, which
is a full knapsack. So weight 0, 1, 2,
3, 4, 5, 6, and I'm going to start
drawing the nodes. AUDIENCE: You could
have weight 10. PROFESSOR: We're
never going to fill up a knapsack with more than 5. Otherwise, we're going to die. AUDIENCE: Why do
we have 6, then? PROFESSOR: Because
I can't thank. Thank you. So this node, the first node. Weight in the backpack, 0. We're looking at item one. It's connected to
the starting node. What outgoing edges do I have? What do I do with item one? I can take it or not take it. AUDIENCE: You connect
the edges 4, 2, 3. Am I answering a
different question? AUDIENCE: If it's item one,
shouldn't it have a weight? PROFESSOR: So those have two
numbers in them, i and j. Sorry. They're in the wrong order here. 1, 0. So I'm looking at item one. So far, I have zero
pounds in my backpack. Looking at item two with zero
pounds, looking at item three with zero pounds. Looking at item one with
one pound in my backpack, item two with one pound,
item three with one pound, so on and so forth. So if I'm looking
at item one, and I have zero pounds in my backpack
so far, what happens if I don't take item one? Where do I end up? AUDIENCE: 2, 0. PROFESSOR: Yeah. So I'm looking at item two
after I'm done deciding, what do I do with item one,
and if I don't take item one, then I'm not going to have
anything in my backpack. So this edge corresponds
to we don't just take one. What if I want to take item one? Where do I land? AUDIENCE: 2, 4. PROFESSOR: All right. 2, 3. 2, 4. So if I did take
item one, then I had zero pounds in
my backpack before. Now I have four pounds, and
I'm still considering item two. What about edge weights? What's the weight
of the edge that says I don't take item one? What's the weight of the edge
that says I do take item one? AUDIENCE: [INAUDIBLE]. AUDIENCE: Minus 10. PROFESSOR: All right. 0 and minus 10. That minus lets us
get the shortest path. Now, suppose I'm in 2, 0. What are the outgoing edges? AUDIENCE: It's the same as
making a giant binary tree, right? PROFESSOR: It's not going
to be a tree because I can have multiple paths
from a root to some node. AUDIENCE: But this looks like
it's taking the same complexity as building a tree if you were
to carefully build the tree so that you don't have unfeasible
solutions on the tree. PROFESSOR: Then you're actually
doing dynamic programming. You're not building the
tree if you're doing that. You're still converging to this. You're probably thinking
this and you said binary decision tree, or at least
that's what I understood. You're probably thinking
of the right thing. We can see if we're thinking
the same thing when we end up looking at the running time. Yes? AUDIENCE: The goal here is to
have the most valuable items possible in the bag
in terms of money? PROFESSOR: Yeah. PROFESSOR: So the
negative 10 weight that you have there was
because that item was $10. PROFESSOR: Yep. And in the end, we're going to
give the graph to the shortest path algorithm. So yes, speaking of the
goal, what's the answer here, by the way? AUDIENCE: 11. PROFESSOR: 11. How do I get 11? AUDIENCE: By getting
the last two items. PROFESSOR: So I take
the ball and the pen and I get 11, right? Everyone on the same page? So I'm at item two. I have zero pounds
in my backpack. What are my outgoing edges? AUDIENCE: To 3, 0 and to 3, 2. Wait, sorry. That's the third item. PROFESSOR: No, second item. You're good. AUDIENCE: Shouldn't it be
3, 3 because the third item has three pounds? PROFESSOR: I said I'm
looking at the second item and then I'm deciding
where I'm going. So this graph has
a problem in that when I get to the third
item, what do I do? So I need one more layer here,
which means that I'm done. AUDIENCE: I was saying that it's
the outgoing edge from 2, 0, it's the weight
of the taken item, so if you decide to
take the third item, shouldn't the outgoing
edge go to 3, 3, not 3, 2? PROFESSOR: But when
I'm at layer two, I'm only looking at item two. So I'm looking at the
items in sequence. First I have to decide,
am I taking item one? Then I decide, am I taking item
two, and then I'm deciding, am I taking item three? While I'm here, I
don't see item three. I only see item two. AUDIENCE: Aren't those weights
on the left side, though? PROFESSOR: Yeah. These are backpack weights. So down here, these are
pounds and these are items. What are the weights
on the edges? 0 and negative 1. PROFESSOR: OK. How about this other node, 2, 4? What are the edges
coming out of it. AUDIENCE: 2, 0. AUDIENCE: 3, 4. PROFESSOR: So if I decide I'm
not going to take item two, but I already have four
pounds in my backpack, I'm still going to end up with
four pounds in my backpack, and I don't get
anything out of it. And? AUDIENCE: That's it. PROFESSOR: And that's
it, because if I try to take the second item,
that would put me overweight, so the edge would
go out of the graph. Therefore, it doesn't exist. AUDIENCE: You can remove it,
and then you go back to 3, 0, right? Is that not allowed? PROFESSOR: No,
because when I'm here, I don't know how I got here. I don't know if I
had item one or not. The benefits of doing
it this way is here, I'm just looking at item two. I do not know how I got there. I do not care what
I'm going to do later. I'm just looking at
one item and making one decision based on that. Does this make sense? Where do I get my answer from? AUDIENCE: Can you put a
final node on the right side? PROFESSOR: OK, so
one way of doing it is that I'm going to have
a final note on the right side and connecting everything to it. AUDIENCE: Do you need the
done nodes right there, or could you have just skipped
done and connected 3 to that? PROFESSOR: So I can
do that, but then it's going to be hard to
reason about edges. This makes it easier
to reason about edges because when I'm
looking at this layer, I'm still going to
have edges deciding whether I take
item three or not. So it would be confusing to
have all the edges pointing to the same place. I can, it's just that the graph
would look more confusing. By the way, if I'm at 3, 2,
what are the outgoing edges from 3, 2? AUDIENCE: It's just
all horizontal. You can't take anymore. PROFESSOR: So I can be
done with two pounds, and that means I
get 0 or I can take item three, and then
where do I land? AUDIENCE: Negative 7. PROFESSOR: Does this
make sense now somewhat? AUDIENCE: So what about this
dynamic programming style? PROFESSOR: We'll
get there in a bit. Before, let's see what's
the running time for this. So one way of
finishing this up is we connect everything
to one destination node. Another way of doing it is that
we connect the source nodes to everything here with
edges of cost 0 and say, well, this is how much
weight we're going to waste. So if my solution path
goes here and then goes somewhere through
the graph, it means that I'm going to waste
one pound of capacity, so my backpack is going to
have four pounds when I'm done. AUDIENCE: Would it be
[INAUDIBLE] or infinity? PROFESSOR: What
should the weight be? Good question. AUDIENCE: This is
not possible, right? To get from s to 1, 1. PROFESSOR: So I'm saying
that if I get from s to 1, 1, this means that I'm
wasting a pound. My backpack will have
four pounds of stuff and then one pound of
capacity will go to waste. AUDIENCE: Isn't
there another way to waste it on the right
side if you end at the top? Or not at the bottom, basically. PROFESSOR: Yeah. AUDIENCE: So you're
not double counting in terms of losing stuff? You're representing
losing weight on the left side
and the right side. PROFESSOR: Well, if I represent
losing weight on the left side, then the advantage is that I
have a single destination node. What is it? AUDIENCE: That dot. PROFESSOR: So this
is the source. The destination is this guy. I can waste capacity
here, so that means I can say
that on this side, I don't need to waste stuff. On this side, I need
to arrive at done 5. That's the advantage of doing
it this way, aside from the fact that it's going to map
to dynamic programming. There are many ways of doing it. I can choose to connect
the source to 1, 0, and then look at all the paths
here and choose the best one. I can connect the
source to everything here with paths of some
weight, which you guys still need to tell me what it is, and
then I can look at the answer here. AUDIENCE: Cost 0. PROFESSOR: Thank you. So this is another way. And yet the third way
would be to only connect the source to the
first node, and then connect all these done nodes
to another node with cost 0, and this is equivalent to this. The reason we're
doing it this way is because this maps
to dynamic programming. AUDIENCE: Easier. PROFESSOR: Yeah. It maps closely to
how the DP runs. AUDIENCE: I mean is it
possible to do the DP such that you do it the
other way as well? PROFESSOR: Yeah. You can flip the DP around, too. How are we doing with this? So then I hope the running time
analysis will go really fast. How many vertices? AUDIENCE: v or three. PROFESSOR: OK, so v is? AUDIENCE: ns. PROFESSOR: n layers, right? n plus 1 layers, actually. AUDIENCE: n times weight mass. PROFESSOR: n times
capacity, right? So it's actually n plus
1 times capacity plus 1, but I don't want
to deal with that, so I'm going to use
order of so that I don't have to deal with that. How many edges going
out of a vertex? AUDIENCE: Two. PROFESSOR: Yep. So at most, two
edges per vertex. So how many edges total? AUDIENCE: ns. PROFESSOR: What shortest path
algorithm am I going to use? AUDIENCE: Bellman-Ford. AUDIENCE: Topological sort BFS. PROFESSOR: I'm going to
choose the second one because it runs faster. So Bellman-Ford's running
time is V times E. If I use the DAG algorithm, that
means topological sort and then a DFS. That's V plus E.
So please, whenever you have a dynamic programming
graph, this is the answer. It's never anything else. It's never Dijkstra. It's never Bellman-Ford. It's always this. And this means? AUDIENCE: ns. PROFESSOR: So this
is the running time of the graph solution. AUDIENCE: If you
use BFS, do you need to put in the dummy nodes
for the edge weights? PROFESSOR: If you use BFS. Yes. So then the running time
would be bigger than V plus E. AUDIENCE: Right. So then the running time
depends on [INAUDIBLE] multiply by s again? PROFESSOR: This isn't doing BFS. This is doing the
shortest path algorithm for direct acyclic graphs. It's the DAG shortest path. AUDIENCE: Right. PROFESSOR: Were you here
before Thanksgiving? AUDIENCE: No PROFESSOR: Right before? You missed the algorithm. It's in the Dijkstra
lecture notes. AUDIENCE: But
basically, you don't have to put in all
the dummy nodes? PROFESSOR: No. So you do a topological
sort, and then you only consider a
node once, and you look at all the edges
coming through it and make a decision
based on that. So this is the running
time, this is the algorithm, this is the solution. What if we do dynamic
programming instead? Instead of nodes, we
have sub-problems. A sub-problem maps to a node. The same things that we
decided about this data that we store in a
node, the same things are going to apply to the state
that we have in a sub-problem. So what are the sub-problems? First up, how many sub-problems
am I going to have? AUDIENCE: n. AUDIENCE: ns. PROFESSOR: I just said
that one sub-problem will map to a
vertex in the graph. ns vertices, ns
sub-problems, right? Come on, guys. Bear with me. Get more cookies. Are the cookies still there? Please pass them
around and eat them. Yes? AUDIENCE: One sub-problem would
be getting the most amount of money for that little weight. At one pound [INAUDIBLE],
how much [INAUDIBLE]. PROFESSOR: I like that,
so let's start writing. What is the maximum value that I
can get using a certain weight? If I label the nodes
indices i, j, I'm also going to label the
sub-problems using i, j. So I'm going to say
that sub-problem i, j is, what's the maximum
amount of money I can get by using a
weight of at most j, and you said
something about items, so we have to figure
out which items. Which items? Anyone else can help her. You already did the
hard work, so you have most of the
stuff filled in. Feel free to jump in. AUDIENCE: Greater
than or equal to i. PROFESSOR: OK. Sounds good. We're going to number the
items starting from 0 this time because we're going to have
a dynamic programming table, and that means we like
zero base indexing. So the items are going
to be 0, 1, and 2. 0 is the statue, 1 is the
ball, and 2 is the pen. This means we're going to
look at suffixes of items. First, the empty set,
no item, then only the pen, then the ball
and the pen, and then the statue, the
ball, and the pen. And we're also going to
consider how much weight we have in our backpack. So let me write this again. 0, 1, 2 here, and weights
0, 1, 2, 3, 4, 5, and this is going to be our DP table. Now, how are we going
to compute this? DP of i, j is? AUDIENCE: Maximum. PROFESSOR: All right,
a maximum of something. Good. Perfect way to start. How many decisions do we have? AUDIENCE: Two. PROFESSOR: Two. It's the same as in the
graph problem, right? Once we solve that, this
should be pretty easy because it's really
very similar. So the two decisions
are, do I take item i, do I not take item i? If I don't take item i,
what situation do I land in? So suppose I don't take item i. AUDIENCE: i plus 1. PROFESSOR: OK. i plus 1. So I have to make up my
answer using items i plus 1 all the way through n minus
1, and how much weight do they have to have? AUDIENCE: j. PROFESSOR: Yep. Now suppose I do take item i. What happens? AUDIENCE: i plus-- no, no, no. PROFESSOR: You're thinking
of the right thing. We're making some money, right? AUDIENCE: Value i. PROFESSOR: Value i. So this is how much
money we're making. Good. AUDIENCE: Plus dp i
plus 1 j minus weight i. PROFESSOR: Make sense? OK Now I only have to do
one more thing to make sure this code doesn't crash. AUDIENCE: I'm sorry. What's the third term for? PROFESSOR: This is I'm
making some money, right? This means I'm going to
have to look at the items starting from i plus
1 and i minus 1. And now I took item
i, so that means I have si pounds in my backpack. So the remaining items must
weigh j minus si pounds because I'm going to add
the si pounds for this item, and in total, I'm going
to have at most j pounds. Make some money,
lose some capacity. AUDIENCE: Did we [INAUDIBLE]? PROFESSOR: Yep. This is what happens when
you move from the graph to dynamic
programming, or you can draw the graph the
other way around. AUDIENCE: So could you
have also done plus si, and then done
something else as well? PROFESSOR: Sorry. Plus si-- AUDIENCE: If you did
plus si and then you did some other check
case that was different. PROFESSOR: Well, I do
need to make a check. AUDIENCE: Yes. So that's greater
than or equal to 0. PROFESSOR: This thing, right? Because otherwise, the
code is going to crash. So j minus si is
greater or equal to 0. I think this means j is greater
than or equal to si, right? AUDIENCE: Yes. PROFESSOR: OK. At least for me, the natural
order when I have the graph is to consider the moves
that by making in the game. I'm answering the
questions one by one. Do I take this item? Do I not take this item? When I'm in the DP,
when I make a decision, I want to have the full
information on that decision. Whether I take the
item or not depends on what would happen with
the items following it. Representing it
this way allows me to make the decision right here. AUDIENCE: If we'd done
prefixes rather than suffixes, we could have done it the same. PROFESSOR: It would have been
exactly the same as there. So we taught dynamic
programming over many years, and it turns out that
people understand suffixes better than prefixes,
and the graph format makes more sense
that way, so that's why we're doing it this way. It's for your own sake. Trust me. Or at least we think so. AUDIENCE: Where
does suffix come in? I know what it means. PROFESSOR: These guys. These are the sub-problems. Nothing, item two, items
one, two, zero, one, two, so it's a suffix of
the set of items. AUDIENCE: How come you're going
backwards rather than forwards? It doesn't actually matter if
you go backwards or forwards? PROFESSOR: Yep. In the end, we're going
to look at everything. So we need two things. We need to know where is
the answer going to be, and we need to know what
are the initial conditions. Actually, I lied. We also need a topological sort. So where do we want to start? AUDIENCE: First one. PROFESSOR: Where is the answer? Yes? AUDIENCE: The lower
right hand corner. I mean, if you have all
the [INAUDIBLE] nodes. Oh, we don't have a
Done column, do we? PROFESSOR: No. Well, so the answer will have
considered all the items, right? AUDIENCE: Top left. PROFESSOR: So which i? Almost. So the top left corner means
I looked at all the items and I have an empty knapsack. AUDIENCE: Oh, so the max
of all of the first column. PROFESSOR: So the
max of all columns would be the answer if the
weight here would be equal, but I said the weight has
to be smaller or equal, so the answer is easier. I don't have to do a max. AUDIENCE: Bottom left corner. PROFESSOR: I can only look here. So this means that I will
use a weight of at most s, because this is s, and I
will have considered items 0 and larger, so all the items. This is where the answer is. And in general terms, that
means DP of what and what? So what's the
bottom left corner? AUDIENCE: 0, s. PROFESSOR: Yep. AUDIENCE: Can you explain again
why it needs to be DP 0, s. PROFESSOR: I think it's
easier to look at 0, s and see how it maps
to the sub-problem, and then convince yourself
that this is the answer, so let's do that. DP of 0, s means i is 0, j is s. So this is going
to tell me what is the maximum amount
of dollars I can get by using a weight
of at most s-- agrees with the initial problem--
and using items i or greater than i, or 0 or more, so
items 0 through n minus 1. These are all the items. This is the maximum
capacity, so this maps to the initial problem. And that's why I did this. That's why I don't have
an equal sign here. AUDIENCE: And if you
did have an equal sign, then you would be changing
your sub-problems? PROFESSOR: The recursion
stays the same, but I have to look at all
these to get the maximum. Someone suggested that. AUDIENCE: If you
put an equals there, you're going to have to do
something to the DP such that the equals
actually is captivated, because you can't
just arbitrarily-- PROFESSOR: What do
you think of this? AUDIENCE: Something there is
going to change, isn't it? PROFESSOR: No. I think this works. This stays the same no
matter what the sign is. The only thing that changes
is where is the answer and what are the
initial conditions. AUDIENCE: Oh. So there's something
else that changes. PROFESSOR: Yep. Initial conditions. Good. I like that you're
thinking about that because the next problem
changes pretty much that. The sign changes and
then these change. What's a good topological sort? This should be pretty easy. If I want to generate
a topological sort, what variables do I iterate
over, and in what order? AUDIENCE: i. PROFESSOR: OK. And you're pointing at n, right? AUDIENCE: n down to 0. PROFESSOR: Yep. And then the only one left
is j, so i in, and then j goes from where to where? AUDIENCE: j goes to s. AUDIENCE: It can go
either direction. PROFESSOR: Can it? What do the
dependencies look like? When I'm computing i, j, I
want i plus 1 and j minus-- so I'm looking at lower j's,
higher i's and lower j's. AUDIENCE: But it's
always higher i's. Doesn't that mean
in this for loop that you're already going to
have calculated the next right column, so it
doesn't matter where the column-- I may be wrong. PROFESSOR: You're right. Fine. OK, both orders work. I just looked at
this because I wanted to have a nice, simple--
but fine, both work. AUDIENCE: Wait. You mean reversing j works? PROFESSOR: Yes, because
we're iterating over i before iterating over j. Of course, this
makes more sense. Just because they're
both good answers doesn't mean you should
choose the one that requires more thinking. I always like the answer
that requires the least amount of thinking to
prove that it's correct. AUDIENCE: Does that mean
you're starting with a full-- PROFESSOR: I start here. AUDIENCE: But if you're
going from s down to 0, then you're starting
down at the corner, which means you have a full knapsack,
so that doesn't make sense. PROFESSOR: So the argument
was when I compute this, I'm looking one
right, i plus 1j, and then I'm looking
somewhere here. And I've already
computed this column, so it doesn't matter
if I go up or down. And that argument is
correct because first I'm going to compute this column,
then this column, then this column, then this column. By the way, what's this column? This column is the
initial condition. Now that we figured out
the topological sort, let's try to compute
values here and see what the initial
condition should be. So if I want to
compute DP of 2, 0, that is the maximum of either
DP 3, 0 or something else. This is going to
evaluate to false, so it's going to be this. What should the answer be here? AUDIENCE: 0. PROFESSOR: 0. So we want DP of 3, 0 to
be 0 for this to work. So what's a reasonable
initial condition? AUDIENCE: Column
3 is of 0 height. PROFESSOR: DP of
nj is 0 for all j, pretty much what you
said in math mode. So I'm going to have
an extra column. This is like the Done
problem over there, and here I'm going
to say that whenever I'm looking at
the empty subset-- these are all suffixes. All three items, two items,
one item, empty suffix. Whenever I look at
the empty suffix, I can fill up a backpack with
any weight by using no items, and I'm going to make 0 money. So what are the values here? You guys nodded,
so you understood, so then you should
be able to dictate the values in the table. AUDIENCE: So is that one also
0 because you can't go up to anything, and then the next
one is 4 because you can go-- PROFESSOR: Sorry. Next one is-- so it's for
the fountain pen, right? Last item. AUDIENCE: Crystal ball. PROFESSOR: So we're
going statute, ball, pen. AUDIENCE: And this is weight
on the left side, right? PROFESSOR: So we're
filling them like this in the topological sort
order that we chose here. We said this is
the order, so this is what we're going to use
to calculate the table. AUDIENCE: Oh. So it's still 0, then. PROFESSOR: OK. AUDIENCE: 7. PROFESSOR: So here,
this condition is going to evaluate
to True finally, so this is going to be the
maximum of either DP i plus 1j, so DP of 3, 3, which is
0, or 7 plus DP of 3, 0. So 7 plus 0, which is 7. That's why it's 7. AUDIENCE: 7, 7. PROFESSOR: 7, 7. How about this? AUDIENCE: 0. 0. 4. PROFESSOR: The maximum
of 0 and 4 plus 0, right? AUDIENCE: 4. AUDIENCE: No. 7. PROFESSOR: So it's the maximum
of 7 or 4 plus 0, so this is 7. AUDIENCE: 7. 11. PROFESSOR: 7 or 4 plus 7. Does this make
sense for everyone? Last column. Let's do it. AUDIENCE: 0. Feels like 0. 0 again. Why not? 4. How much does this thing weigh? AUDIENCE: 4. AUDIENCE: 7. 10. PROFESSOR: Sorry
for my handwriting. AUDIENCE: And it needs
to be 11 because you said that'd be the answer. PROFESSOR: All right. So it's 11 because it's either
this guy or 10 plus DP of 1, 1. The first item
weighs 4, so I need to look one right and
four up when adding. Does this make sense? Please say yes. AUDIENCE: What's the
[INAUDIBLE] again? I get how if you're
looking horizontally, the top thing is the max. It's saying DP of i plus 1 j. PROFESSOR: So this is
looking horizontally, and this means I
did not take item i. AUDIENCE: You
didn't take item i, but if you are going to take
item i, you add the value of i, and then you look back
for DP of i plus 1, which is the previous column,
and the row number is whatever capacity you have left
if you did take it? PROFESSOR: Yep. AUDIENCE: And so in
the case of cell 1, 3, if you took item one, then
capacity you'd have left is 3. PROFESSOR: Right. So you said total
capacity three, right? So if I take item one, how
many pounds does it have? AUDIENCE: It has four. PROFESSOR: Statue is
0, ball is 1, pen is 2. AUDIENCE: Oh. You're not using-- PROFESSOR: So I'm
using those items, but I'm starting using
zero based indexing. AUDIENCE: Yeah, but we're
going from here to here. So I'm saying if you pick
the middle cell that's 1, 3, that one-- PROFESSOR: So this has item
one, so the ball, weight three. If I take the ball, how
much weight do I have left? AUDIENCE: The ball
weighs two pounds, so you'd have three
pounds left, and that's why you're in the three column. PROFESSOR: I have a
backpack of three pounds, and then I put a ball
of two pounds in it, and I have three pounds left? AUDIENCE: No, no, no. You have three pounds left if
you put a ball and two pounds in, right? PROFESSOR: I have a
backpack of three pounds. I put a ball of two pounds. How many pounds do I
have left of capacity? So backpack, three pounds total. I put a ball in
that has two pounds. How many pounds? AUDIENCE: We have none left. Oh, it can hold
three total pounds? PROFESSOR: Yeah. AUDIENCE: Then you
have one pound left. But we have a five
pound backpack. PROFESSOR: Well, you said
we were looking at 3, 1. You said we were looking here. AUDIENCE: Yeah,
we're looking there. PROFESSOR: So this means
three pound backpack, because the sub-problem says
your weight is at most j. AUDIENCE: Got it, OK. Oh, I see. So j minus si is saying-- PROFESSOR: It means you used up,
so you put up something there, so you ate some capacity. AUDIENCE: Well, you ate
only two pounds, though, so why is it in row three? PROFESSOR: You told
me to start here. AUDIENCE: I know. I'm trying to remember
how we got there. PROFESSOR: You said,
let's start here. That's why this arrow
points here to one. You have two arrows,
2, 3, and 2, 1. AUDIENCE: Because
it's j minus si. AUDIENCE: Oh, I see. Minus 3. AUDIENCE: So j is the amount
of weight you have left, right? PROFESSOR: So j is
the amount of weight I can have for this sub-problem. AUDIENCE: Yeah, the
amount of capacity I have still in my backpack. PROFESSOR: Let's talk about
pseudo polynomial time very quickly and what
does it mean, OK? And you guys will promise to
look at the recitation notes and look at the other
problems, right? So incentive for that. Problem two is
easier than knapsack, so if you get that, that should
be a good confirmation that you got knapsack. Problem three is a bit
harder than problem two, but it shows up on
interviews, so you want to understand
problem three. I got problem two
twice in four years, so there's a decent
chance that you'll get it. So you want to get
to problem three, so you should go
through problem two. AUDIENCE: How many times
have you got problem three? PROFESSOR: Twice in
four years, so that's the problem that
you want to get to. Problem two is a stepping stone. So running time for
dynamic programming. How many sub-problems? AUDIENCE: Same, ns. PROFESSOR: Yep. Sub-problems. How many sub-problems
do I look at when I compute the answer
of a sub-problem? Two, right? So order one. So this is how
much time it takes to compute the answer
to a sub-problem, because I have the
max of two elements, so it's constant time. So the total running time is? AUDIENCE: Order ns. PROFESSOR: All right. Order of ns. This polynomial, is
this the same kind of algorithm as Dijkstra? AUDIENCE: Pseudo polynomial. PROFESSOR: Pseudo polynomial. AUDIENCE: Don't know why. PROFESSOR: So
intuitively, the problem is s shows up in
your running here, and s is the property
of your input numbers. It's not how many elements
you have in the input. It's what's inside
one of those elements. So let's see why that matters. Let's look at the
practical example. AUDIENCE: What about
measuring in cubic inches or cubic centimeters? Is that a problem? It would take a lot longer if
we do it in cubic centimeters. PROFESSOR: Yeah,
but then you could argue that if it's an integer
amount of cubic inches, you should divide
everything by that. Where this really
matters is suppose you have a 100 item
input, so 100 items. What's the worst case
input on a 32-bit machine? An input looks like
this, by the way. How many elements you have,
so 3, and then for each item, what's the weight
and what's the value? So then we're going to have
three weights, which I believe are 4, 2, 3. You guys have to
take my word for it. And then I have three
values, which are 10, 4, 7. Let's not worry
about the values. I claim that they're irrelevant. You can convince
yourselves afterwards that that's the case. Let's only look at this. The weights have to be between
0 and what for the problem to make sense? AUDIENCE: 5. PROFESSOR: If I have a
weight bigger than this, I know I'm not going
to take that item. How many bits do I need to
represent these weights? AUDIENCE: Log s. PROFESSOR: It's log
s plus 1 technically, but if I write order
of log s, I'm good. AUDIENCE: Times the
number of items. PROFESSOR: Yes. So if I want to
represent all of them, I have order of n times
log s, plus the number of bits required
to represent this. This is log n. So log n is smaller than n. I'm not going to add it here. So this is my input size. This is how many bits
I need for the input. So the worst case input
on a 32-bit machine is going to have
roughly 3,200 bits. What's the worst
case s that I can represent on a 32-bit machine? These are all 32-bit numbers. AUDIENCE: You mean each of the
weights are 32-bit numbers? PROFESSOR: Yeah. So what's the worst
case s I can represent? AUDIENCE: 2 to 31 minus 1. PROFESSOR: Which is roughly
4 times 10 to the ninth. So the worst case
running time is? Roughly ns, right? So n times s, which means
roughly 100 times 10 to the ninth, which means 4
times 10 to the 11 operations. Now, suppose we're
looking at the worst case input on a 64-bit machine. Still 100 items. What's the input size? AUDIENCE: To the 21
total at the end. PROFESSOR: That's
how many operations. Let's go through
it step by step. How many bits of input? AUDIENCE: 6,400. AUDIENCE: Where
does that come from? AUDIENCE: 100 items
times 32 bits. PROFESSOR: 100 items. Each item weight has 64 bits. What's the worst case s? AUDIENCE: 2 to the 64. PROFESSOR: 2 to the 64 minus 1. Doesn't matter too much. It's 1 times 10 to the 19th,
1.6 times 10 to the 19th. So the worst case
running time is? AUDIENCE: 10 to the 21. PROFESSOR: 10 to the 21. So what happened? I increased my word size. The running time
increased quadratically. This is not a
polynomial increase. What's the problem here? Why is this the case? If I write log s as
b, so log s becomes b, what's the input size? AUDIENCE: nb. PROFESSOR: Yep. n times
log s equals n times b, so this is the input size. Now, what is the running time? AUDIENCE: The number
of operations? PROFESSOR: Yep. AUDIENCE: n times 2 to the b. PROFESSOR: Yep. n times s, and s is 2 to the b. So this is the problem. This is a polynomial
in n and b, but here, if I write the input this
way, I have b as an exponent. So if I double
the number of bits by doubling the field
size, my running time increases quadratically,
so this is not polynomial. However, if I write it
as order of n times s, this looks like a polynomial. So it looks like a
polynomial, but it's not truly a polynomial, so that's why
it's pseudo, as in fake. AUDIENCE: So it's
actually exponential. PROFESSOR: It's not exponential. So an exponential
algorithm for this means try all the possible
subsets, and that's order of 2 to the n times
n to compute the sum. This is exponential, right? You have n here. It's clear. AUDIENCE: I know, but
if you have 2 to the b, that seems pretty clear as well. PROFESSOR: Well, you have
to look inside this s and see what it means. That's the difference. So by increasing
the number of items, if you double the
number of items but you don't do anything
to the word size, then your running time is
going to increase polynomially, but if you change
the field size, it's going to increase
exponentially. So pseudo polynomial means
watch out, there's a trap. It's not really polynomial. If you increase the input size
by increasing the number width, bad stuff is going to happen. So think of Dijkstra. What's the running
time of Dijkstra? You count the number of
vertices, the number of edges, and you have a polynomial
in that, right? You don't have to look
at the number size. You don't have to look at
any weird stuff like that. Here, you do. That's the difference. Does it make more sense? So whenever you have an
input number that shows up in your running time, instead
of how many numbers you have, that means there's a trap. That's pseudo polynomial. AUDIENCE: So it's just
like having some constant that depends on something? PROFESSOR: It's not a constant. AUDIENCE: I mean coefficient. AUDIENCE: Once you set it,
it's a constant, right? PROFESSOR: I mean, that's
true for everything, right? You can say that you have 10 to
the 80 atoms in the universe, so all the numbers that you work
with are at most 10 to the 80. Therefore, your running
time is order one no matter what you do,
and then running times are no longer useful. You have to draw
a line somewhere. AUDIENCE: I think it's just like
s, if it depends on your field size and you can scale
it, it's kind of like-- PROFESSOR: So
asymptotic running time. What's the point of that? How do our algorithms scale? As our data becomes
bigger and bigger, what happens to
the running time? This pseudo polynomial
thing tells you that if you're shifting
to a larger number size, to a larger word size,
then your running time is going to explode. It's not going to
scale linearly. Still don't buy it? AUDIENCE: I trust you, I just-- AUDIENCE: So it's only dependent
on the field in this case. PROFESSOR: Do you guys
want to look over Dijkstra and see what the input
to Dijkstra looks like and why that's different? Do you think that's
worth your time, given that this
is what's standing between you and the weekend? AUDIENCE: What time is it? PROFESSOR: If you guys want
to, I'm willing to do it. AUDIENCE: It's 4:07. I'll stay. PROFESSOR: Well, if you
guys want to go, you can go. I will draw this
for the people who still want to know
what it looks like. Dijkstra. What's the input to Dijkstra? It's a graph, right? AUDIENCE: Yeah, nodes and edges. PROFESSOR: What does
the graph look like? It's some number
of nodes, so it's a single number-- that's
the number of nodes-- that has log v bits in it. And then for each edge, we have
three numbers-- first vertex, second vertex, and the weight. What are the sizes
of these numbers? Log v, log v, and the last one? AUDIENCE: Log w? PROFESSOR: OK. And what's w? AUDIENCE: Maximum weight. PROFESSOR: Yeah. So this is a property of
the field size, right? Let's look at v, actually. So we have E edges here, right? So the input size is going to
be log v bits plus E times log v plus log w. AUDIENCE: How did you get that? PROFESSOR: Because I have the
edges, so I have E of these. I only have one vertex count,
but then I have E edges, and each edge has three
numbers, and these are the widths of the numbers. AUDIENCE: Is the
adjacent edges to v? Is that right? You say you have E edges. PROFESSOR: I'm
assuming it's a list, so the most compact
representation of edges, I think-- or
it might be a reasonably compact representation is that
you have the list of edges. So you have a graph
that has five nodes, and then you have an edge
that goes from 1 to 2, and then weight 3, an edge that
goes from 1 to 5, weight 4. AUDIENCE: If you use
a number, it's not a-- PROFESSOR: Well, do I need
anything else for vertices? Not really, right? AUDIENCE: So v1 and v2 are
neighbors of capital V? PROFESSOR: So v1, v2, w,
these are the first edge. Each edge has these fields. AUDIENCE: This is the
entire graph in one thing. PROFESSOR: This is the entire
graph as a list of numbers, and this is how
many bits it takes to represent the
graph in a reasonably compact representation. Now let's say little v is
log v, little w is log w. So then this is order of how
many bits do I have here? E times v plus w plus v.
I just replaced the logs with these variables. This is how many bits. Now, how many operations
does Dijkstra take? What's the running time? AUDIENCE: Well, it depends on-- AUDIENCE: E log v. AUDIENCE: Wouldn't
it be E plus v? That's the fastest one, right? But I think practically,
it's only going to be-- PROFESSOR: So this
is E plus v log v, the fastest
theoretical limit. This is still smaller
than E log v. This is going to make my life easier. So this is smaller than this. If this thing is polynomial,
this is polynomial for sure. E log v is E times little v.
So how many bits in the input? E times v plus w. How many operations? E times v. Any
exponential anywhere here? AUDIENCE: Wouldn't you
change the size of v? That looks fine. PROFESSOR: Well, so there is
a trick that v is 2 to the v, right? So you can say
that this is order of-- so this is
definitely bigger than 2 to the little v
times v, but then you have the same
thing in the input. So the input also has at least
2 to the little v times v bits. But don't worry about that. That's too much. The point is if you're worrying
about this, don't worry. The math still works out. So whatever you have
here as an input, the running time is
going to be a polynomial in the size of the input. What happens if you
double the word size? What happens if you
have bigger weights? AUDIENCE: Everything like
v is multiplied by 2, and w is multiplied by 2 and
everything in this problem, right? PROFESSOR: So if you're
doubling the word size, then this is going to double,
this is going to double. Everything's fine. What if you double the
size of the weights? AUDIENCE: That only
adds an extra bit. PROFESSOR: Sorry. So if you double the size
of the weight numbers? So if you move from 32-bit
weights to 64-bit weights? AUDIENCE: That's still a
constant factor, right? PROFESSOR: What happens
to the running time? AUDIENCE: Nothing. PROFESSOR: Nothing. w does not show up in Dijkstra. If it would, then
we'd be trouble. It wouldn't be
polynomial anymore. AUDIENCE: But
practically, you will be accessing that number, right? PROFESSOR: Yeah,
but that shows up in the cost of one operation. That's why I'm saying this
is how many operations you do, how many
arithmetic operations. Then the model of computation
that we use is RAM, and that says that
you can do any math operation in order one. AUDIENCE: [INAUDIBLE]. PROFESSOR: So here, my
number in the input, s, which is the
size of a weight, showed up in the running time. AUDIENCE: Oh, the
size of the input showed up in the running time. PROFESSOR: So the size
of the input is OK, but one number in
the input showed up, whereas here,
that's not the case. The weights do not show
up in the running time. AUDIENCE: So why don't we just
always run Dijkstra, then? PROFESSOR: Actually for this
problem, for the knapsack problem, you can't find an
algorithm that is polynomial. If you do, there's a
$1 million prize for it because you just proved
that p equals mp. This is the best we
can do for knapsack. AUDIENCE: Is counting
sort pseudo linear, because you need to
have a maximum, a range? Does that make sense? PROFESSOR: Counting sort. AUDIENCE: Depends on your range. PROFESSOR: Yeah, but the
range shows up under a log. You're allowed to have logs. You're allowed to have log w. You're not allowed to have w. It's dependent on the
size of the input. If you double the
number size, then you're going to have twice
as many rounds, but you don't have an
exponential number of rounds. Sorry. You're thinking
of counting sort. I thought radix sort. Radix sort doesn't
matter because you assume we can do
everything in-- never mind. You're right for counting sort. Sorry. You're right for counting sort. Sorry. I was confusing counting
sort with radix sort. So for counting sort,
yeah, it's not linear. It's linear in your
range size, which is not linear in the
input size, which is why we don't
do counting sort. We do radix sort. AUDIENCE: We do counting
sort in radix sort? PROFESSOR: Yeah. But radix sort limits the
size of the range, right? That's the point of radix sort. AUDIENCE: Oh. I see what you're thinking. So doing counting
sort with each number? AUDIENCE: Right, with
really big numbers, taking a really long time. AUDIENCE: Yeah, that
would take a long time. PROFESSOR: Yep. That is very true. If you try to do pure counting
sort on 64-bit numbers, you're going to run out of RAM. Does this make some sense? OK. Promise to look over the
other problems, and in return, given that I didn't have
time to cover them here, I promise to answer
any emails you guys might ask me
over the weekend. AUDIENCE: Oh. Awesome.

Algorithms

The following content is
provided under a Creative Commons license. Your support will help
MIT OpenCourseWare continue to offer high quality
educational resources for free. To make a donation or to
view additional materials for hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: Last time we
left off with a question having to do with
playing with blocks. And this is supposed to give us
a visceral feel for something anyway, having to
do with series. And the question was whether
I could stack these blocks, build up a stack so that--
I'm going to try here. I'm already off
balance here, see. The question is
can I build this so that the-- let's
draw a picture of it, so that the first
block is like this. The next block is like this. And maybe the next
block is like this. And notice there
is no visible means of support for this block. It's completely to the
left of the first block. And the question is,
will this fall down? Or at least, or more precisely,
eventually we'll ask, you know, how far can we go? Now before you
answer this question, the claim is that
this is a kind of a natural, physical
question, which involves some important answer. No matter whether the answer
is you can do it or you can't. So this is a good
kind of math question where no matter what the
answer is, when you figure out the answer, you're going to
get something interesting out of it. Because they're
two possibilities. Either there is a limit to how
far to the left we can go -- in which case that's a
very interesting number -- or else there is no limit. You can go arbitrarily far. And that's also
interesting and curious. And that's the difference
between convergence and divergence,
the thing that we were talking about up to
now concerning series. So my first question
is, do you think that I can get it so that
this thing doesn't fall down with-- well you see I have
about eight blocks here or so. So you can vote now. How many in favor
that I can succeed in doing this sort of thing with
maybe more than three blocks. How many in favor? All right somebody
is voting twice. That's good. I like that. How about opposed? So that was really
close to a tie. All right. But I think the there was
slightly more opposed. I don't know. You guys who are in the
back maybe could tell. Anyway it was pretty close. All right. So now I'm going-- because
this is a real life thing, I'm going to try to do it. All right? All right. So now I'm going to tell
you what the trick is. The trick is to do it backwards. When most people are
playing with blocks, they decide to build
it from the bottom up. Right? But we're going to build
it from the top down, from the top down. And that's going
to make it possible for us to do the optimal
thing at each stage. So when I build it from the
top down, the best I can do is well, it'll fall off. I need to have it you
know, halfway across. That's the best I can do. So the top one I'm going
to build like that. I'm going to take it as
far to the left as I can. And then I'm going to
put the next one down as far to the left as I can. And then the next one as
far to the left as I can. That was a little too far. And then I'm going to do the
next one as far to the left as I can. And then I'm going
to do the next one -- well let's line it up first --
as far to the left as I can. OK? And then the next one as
far to the left as I can. All right. Now those of you who are in
this line can see, all right, I succeeded. All right, that's over the edge. All right? So it can be done. All right. All right. So now we know that we can
get farther than you know, we can make it overflow. So the question now
is, how far can I get? OK. Do you think I can get to here? Can I get to the end over here? So how many people think I
can get this far over to here? How many people think
I can get this far? Well you know,
remember, I'm going to have to use more
than just this one more block that I've got. I don't, right? Obviously I'm
thinking, actually I do have some more blocks at home. But, OK. We're not going to. But anyway, do you think
I can get over to here? How many people say yes? And how many people say no? More people said no then yes. All right. So maybe the stopping place
is some mysterious number in between here. All right? Well OK. So now we're going
to do the arithmetic. And we're going to figure out
what happens with this problem. OK? So let's do it. All right, so now
again the idea is, the idea is we're going to start
with the top, the top block. We'll call that
block number one. And then the farthest,
if you like, to the right that you can put a
block underneath it, is exactly halfway. All right, well, that's
the best job I can do. Now in order to make my
units work out easily, I'm going to decide to call
the length of the block 2. All right? And that means if I
start at location 0, then the first place where I
am is supposed to be halfway. And that will be 1. OK so the first step in the
process is 1 more to the right. Or if you like, if I
were building up -- which is what you would actually
have to do in real life -- it would be 1 to the left. OK now the next one. Now here is the
way that you start figuring out the arithmetic. The next one is based
on a physical principle. Which is that the farthest
I can stick this next block underneath is what's called the
center of mass of these two, which is exactly halfway here. That is, there's a
quarter of this guy, and a quarter of that
guy balancing each other. Right? So that's as far as I can go. If I go farther than
that, it'll fall over. So that's the absolute
farthest I can do. So the next block is
going to be over here. And a quarter of 2 is 1/2. So this is 3/2 here. All right so we went to 1. We went to 3/2 here. And then I'm going to keep on
going with this eventually. All right so we're
going to figure out what happens with this stack. Question? AUDIENCE: How do
you know that this is the best way to optimize? PROFESSOR: The
question is how do I know that this is the
best way to optimize? I can't answer that question. But I can tell you
that it's the best way if I start with
a top like this, and the next one like this. Right, because I'm
doing the farthest possible at each stage. That actually has a name
in computer science, that's called the greedy algorithm. I'm trying to do the best
possible at each stage. The greedy algorithm
starting from the bottom is an extremely bad strategy. Because when you do that,
you stack it this way, and it almost falls over. And then the next time
you can't do anything. So the greedy algorithm is
terrible from the bottom. This is the greedy algorithm
starting from the top, and it turns out
to do much better then the greedy algorithm
starting from the bottom. But of course I'm not addressing
whether there might not be some other incredibly clever
strategy where I wiggle around and make it go up. I'm not addressing
that question. All right? It turns out this is
the best you can do. But that's not clear. All right so now, here
we have this thing. And now I have to figure out
what the arithmetic pattern is, so that I can figure out what
I was doing with those shapes. So let's figure out a
thought experiment here. All right? Now the thought
experiment I want to imagine for
you is, you've got a stack of a bunch of blocks,
and this is the first N blocks. All right? And now we're going to
put one underneath it. And what we're
going to figure out is the center of mass
of those N blocks, which I'm going to
call C sub N. OK. And that's the place
where I'm going to put this very next block. I'll put it in a
different color here. Here's the new--
the next block over. And the next block over
is the (N+1)st block. And now I want you to think
about what's going on here. If the center of mass
of the first N blocks is this number, this new
one, it's of length 2. And its center of
mass is 1 further to the right than the center
of mass that we had before. So in other words, I've added to
this configuration of N blocks one more block,
which is shifted. Whose center mass
is not lined up with the center of mass
of this, but actually over farther to the right. All right so the
new center of mass of this new block-- And this is
the extra piece of information that I want to observe,
is that this thing has a center of mass at C_N + 1. It's 1 unit over because
this total length is 2. So right in the middle there is
1 over, according to my units. All right now this is
going to make it possible for me to figure out what
the new center of mass is. So C_(N+1) is the center
of mass of N+1 blocks. Now this is really only in the
horizontal variable, right? I'm not keeping track of the
center of mass-- Actually this thing is hard to build
because the center of mass is also rising. It's getting higher and higher. But I'm only keeping track of
its left-right characteristic. So this is the
x-coordinate of it. All right so now
here's the idea. I'm combining the white
ones, the N blocks, with the pink one, which
is the one on the bottom. And there are N
of the white ones. And there's 1 of the pink one. And so in order to get the
center of mass of the whole, I have to take the weighted
average of the two. That's N*C_N plus 1 times the
center of mass of the pink one, which is C_N + 1. And then I have to divide --
if it's the weighted average of the total of N +
1 blocks -- by N + 1. This is going to give me
the new center of mass of my configuration
at the (N+1)st stage. And now I can just
do the arithmetic and figure out what this is. And the two C_Ns combine. I get (N+1)C_N +
1, divided by N+1. And if I combine these two
things and do the cancellation, that gives me this
recurrence formula, C_(N+1) is equal to C_N
plus-- There's a little extra. These two cancel. That gives me the C_N. But then I also have 1/(N+1). Well that's how much gain I
can get in the center of mass by adding one more block. That's how much I can
shift things over, depending on how we're
thinking of things, to the left or the
right, depending on which direction we're building them. All right, so now I'm going
to work out the formulas. First of all C_1, that was
the center of the first block. I put its left end at 0; the
center of the first block is at 1. That means that C_1 is 1. OK? C_2 according to this
formula-- And actually I've worked it out, we'll check
it in a-- C_2 is C_1 + 1/2. All right, so that's
the case N = 1. So this is 1 + 1/2. That's what we already did. That's the 3/2 number. Now the next one is C_2 + 1/3. That's the formula again. And so that comes out
to be 1 + 1/2 + 1/3. And now you can see
what the pattern is. C_N-- If you just
keep on going here, C_N is going to be 1
+ 1/2 + 1/3 + 1/4... plus 1/N. So now I would like
you to vote again. Do you think I can-- Now
that we have the formula, do you think I can
get over to here? How many people think
I can get over to here? How many people think I
can't get over to here? There's still a lot
of people who do. So it's still almost 50/50. That's amazing. Well so we'll address
that in a few minutes. So now let me tell
you what's going on. This C_N of course, is the same
as what we called last time S_N. And remember that we actually
estimated the size of this guy. This is related to what's
called the harmonic series. And what we showed
was that log N is less than S_N, which
is less than S_N + 1. All right? Now I'm going to
call your attention to the red part, which
is the divergence part of this estimate, which
is this one for the time being, all right. Just saying that this
thing is growing. And what this is saying is
that as N goes to infinity, log N goes to
infinity, So that means that S_N goes to infinity,
because of this inequality here. It's bigger than log N.
And so if N is big enough, we can get as far as we like. All right? So I can get to here. And at least half of you,
at least the ones who voted, that was-- I don't know. We have a quorum here,
but I'm not sure. We certainly didn't have
a majority on either side. Anyway this thing
does go to infinity. So in principle, if
I had enough blocks, I could get it over to here. All right, and
that's the meaning of divergence in this case. On the other hand, I want
to discuss with you-- And the reason why
I use this example, is I want to discuss
with you also what's going on with this
other inequality here, and what its significance is. Which is that it's going to
take us a lot of numbers N, a lot of blocks, to get
up to a certain level. In other words, I can't do it
with just eight blocks or nine blocks. In order to get
over here, I'd have to use quite a few of them. So let's just see
how many it is. So I worked this out carefully. And let's see what I got. So to get across the
lab tables, all right. This distance here, I
already did this secretly. But I don't actually even have
enough of these to show you. But, well 1, 2, 3,
4, 5, 6, and 1/2. I guess that's enough. So it's 6 and a half. So it's two lab tables
is 13 of these blocks. All right. So there are 13 blocks,
which is equal to 26 units. OK, that's how far
to get across I need. And the first one is already 2. So it's really 26
minus 2, which is 24. Which that's what I need. OK. So I need log N to be equal
to 24, roughly speaking, in order to get that far. So let's just see
how big that is. All right. I think I worked this out. So let's see. That means that N
is equal to e^24-- and if you realize that these
blocks are 3 centimeters high-- OK let's see how many
that we would need here. That's kind of a lot. Let's see, it's 3
centimeters times e^24, which is about 8*10^8 meters. OK. And that is twice the
distance to the moon. So OK, so I could do it maybe. But I would need
a lot of blocks. Right? So that's not very
plausible here, all right. So those of you who
voted against this were actually sort
of half right. And in fact, if you
wanted to get it to the wall over there,
which is over 30 feet, the height would be
about the diameter of the observable universe. That's kind of a long way. There's one other thing
that I wanted to point out to you about this shape here. Which is that if you
lean to the left, right, if you put your
head like this -- of course you have to be on
your side to look at it -- this curve is the shape
of a logarithmic curve. So in other words, if you think
of the vertical as the x-axis, and the horizontal that
way is the vertical, is the up direction,
then this thing is growing very, very,
very, very slowly. If you send the x-axis all
the way up to the moon, the graph still hasn't gotten
across the lab tables here. It's only partway there. If you go twice the distance
to the moon up that way, it's gotten finally to that end. All right so that's how
slowly the logarithm grows. It grows very, very slowly. And if you look at it another
way, if you stand on your head, you can see an
exponential curve. So you get some sense as
to the growth properties of these functions. And fortunately these
are protecting us from all kinds of
stuff that would happen if there
weren't exponentially small tails in the world. Like you know, I could
walk through this wall which I wouldn't like doing. OK, now so this is
our last example. And the important
number, unfortunately we didn't discover another
important number. There wasn't an amazing number
place where this stopped. All we discovered again is
some property of infinity. So infinity is
still a nice number. And the theme here is just that
infinity isn't just one thing, it has a character which
is a rate of growth. And you shouldn't
just think of there being one order of infinity. There are lots of
different orders. And some of them have
different meaning from others. All right so that's
the theme I wanted to do, and just have a
visceral example of infinity. Now, we're going to move
on now to some other kinds of techniques. And this is going to
be our last subject. What we're going
to talk about is what are known as power series. And we've already seen
our first power series. And I'm going to
remind you of that. Here we are with power series. Our first series was this one. And we mentioned last time
that it was equal to 1/(1-x), for x less than 1. Well this one is known
as the geometric series. You didn't use the letter x
last time, I used the letter a. But this is known as
the geometric series. Now I'm going to show you
one reason why this is true, why the formula holds. And it's just the
kind of manipulation that was done when these
things were first introduced. And here's the idea of a proof. So suppose that this sum
is equal to some number S, which is the sum of
all of these numbers here. The first thing
that I'm going to do is I'm going to multiply by x. OK, so if I multiply by x. Let's think about that. I multiply by x on both the
left and the right-hand side. Then on the left side, I get x
+ x^2 + x^3 plus, and so forth. And on the right
side, I get S x. And now I'm going
to subtract the two equations, one from the other. And there's a very, very
substantial cancellation. This whole tail here
gets canceled off. And the only thing
that's left is the 1. So when I subtract, I get
1 on the left-hand side. And on the right-hand
side, I get S - S x. All right? And now that can be
rewritten as S(1-x). And so I've got my formula here. This is 1/(1-x) = S. All right. Now this reasoning has one flaw. It's not complete. And this reasoning
is basically correct. But it's incomplete because
it requires that S exists. For example, it doesn't make
any sense in the case x = 1. So for example in
the case x = 1, we have 1 + 1 +
1 plus et cetera, equals whatever we call S. And
then when we multiply through by 1, we get 1 + 1 + 1 plus... equals S*1. And now you see that
the subtraction gives us infinity minus infinity is equal
to infinity minus infinity. That's what's really going on
in the argument in this context. So it's just nonsense. I mean it doesn't give
us anything meaningful. So this argument, it's great. And it gives us the right
answer, but not always. And the times when it gives us
the answer, the correct answer, is when the series
is convergent. And that's why we care
about convergence. Because we want manipulations
like this to be allowed. So the good case,
this is the red case that we were
describing last time. That's the bad case. But what we want is the good
case, the convergent case. And that is the case
when x is less than 1. So this is the convergent case. Yep. OK, so they're much
more detailed things to check exactly
what's going on. But I'm going to just say
general words about how you recognize convergence. And then we're not going
to worry about-- so much about convergence, because
it works very, very well. And it's always easy
to diagnose when there's convergence
with a power series. All right so here's
the general setup. The general setup
is that we have not just the coefficients 1 all
the time, but any numbers here, dot, dot, dot. And we abbreviate that with
the summation notation. This is the sum a_n x^n,
n equals 0 to infinity. And that's what's known
as a power series. Fortunately there is
a very simple rule about how power series converge. And it's the following. There's a magic number R which
depends on these numbers here such that-- And
this thing is known as a radius of convergence. In the problem that we had,
it's this number 1 here. This thing works
for x less than 1. In our case, it's
maybe x less than R. So that's some symmetric
interval, right? That's the same as minus
R less than x less than R, and so where
there's convergence. OK, where the series converges. Converges. And then there's
the region where every computation that you
give will give you nonsense. So x greater than R is
the sum a_n x^n diverges. And x equals R is very
delicate, borderline, and will not be used by us. OK, we're going to stick inside
the radius of convergence. Now the way you'll be
able to recognize this, is the following. What always happens
is that these numbers tend to 0 exponentially
fast, fast for x in R, and doesn't even tend to 0
at all for x greater than R. All right so it'll
be totally obvious. When you look at
this series here, what's happening
when x less than R is that the numbers are getting
smaller and smaller, less than 1. When x is bigger
than 1, the numbers are getting bigger and bigger. There's no chance that
the series converges. So that's going to be the
case with all power series. There's going to be a cutoff. And it'll be one
particular number. And below that it'll be obvious
that you have convergence, and you'll be able
to do computations. And above that every
formula will be wrong and won't make sense. So it's a very clean thing. There is this very
subtle borderline, but we're not going to
discuss that in this class. And it's actually not used in
direct studies of power series. AUDIENCE: How can you tell
when the numbers are declining exponentially fast, whereas
just-- In other words 1/x [INAUDIBLE]? PROFESSOR: OK so,
the question is why was I able to tell
you this word here? Why was I able to tell you
not only is it going to 0, but it's going
exponentially fast? I'm telling you
extra information. I'm telling you it always
goes exponentially fast. You can identify it. In other words, you'll see it. And it will happen
every single time. I'm just promising you
that it works that way. And it's really
for the same reason that it works that way
here, that these are powers. And what's going on
over here is there are, it's close to powers,
with these a_n's. All right? There's a long discussion
of radius of convergence in many textbooks. But really it's not necessary,
all right, for this purpose? Yeah? AUDIENCE: How do you find R? PROFESSOR: The question
was how do you find R? Yes, so I just said,
there's a long discussion for how you find the radius
of convergence in textbooks. But we will not be
discussing that here. And it won't be
necessary for you. Because it will be obvious in
any given series what the R is. It will always
either 1 or infinity. It will always work for
all x, or maybe it'll stop at some point. But it'll be very
clear where it stops, as it is for the
geometric series. All right? OK, so now I need to
give you the basic facts, and give you a few examples. So why are we looking
at these series? Well the answer is we're
looking at these series because the role that
they play is exactly the reverse of
this equation here. That is -- and this is a theme
which I have tried to emphasize throughout this course -- you
can read equalities in two directions. Both are interesting, typically. You can think, I don't know
what the value of this is. Here's a way of evaluating. And in other words,
the right side is a formula for the left side. Or you can think
of the left side as being a formula
for the right side. And the idea of series is
that they're flexible enough to represent all
of the functions that we've encountered
in this course. This is the tool which is very
much like the decimal expansion which allows you to
represent numbers like the square root of 2. Now we're going to be
representing all the numbers, all the functions that we know:
e^x, arctangent, sine, cosine. All of those functions
become completely flexible, and completely available to us,
and computationally available to us directly. So that's what
this is a tool for. And it's just like
decimal expansions giving you handle
on all real numbers. So here's how it works. The rules for
convergent power series are just like polynomials. All of the manipulations
that you do for power series are essentially the
same as for polynomials. So what kinds of things
do we do with polynomials? We add them. We multiply them together. We do substitutions. Right? We take one function
of another function. We divide them. OK. And these are all really not
very surprising operations. And we will be able to do
them with power series too. The ones that are interesting,
really interesting for calculus, are the last two. We differentiate them,
and we integrate them. And all of these
operations we'll be able to do for
power series as well. So now let's explain
the high points of this. Which is mainly just
the differentiation and the integration part. So if I take a series
like this and so forth, the formula for its derivative
is just like polynomials. That's what I just said,
it's just like polynomials. So the derivative of
the constant is 0. The derivative of
this term is a_1. This one is plus 2 a_2 x 2 x. This one is 3 a_3
x^2, et cetera. That's the formula. Similarly if I
integrate, well there's an unknown constant
which I'm going to put first rather than last. Which corresponds sort
of to the a_0 term which is going to get wiped out. That a_0 term suddenly
becomes a_0 x. And the anti-derivative of
this next term is a_1 x^2 / 2. And the next term is a_2
x^3 / 3, and so forth. Yeah, question? AUDIENCE: Is that a
series or a polynomial? PROFESSOR: Is this a
series or a polynomial? Good question. It's a polynomial if it ends. If it goes on infinitely
far, then it's a series. They look practically the
same, polynomials and series. There's this little
dot, dot, dot here. Is this a series
or a polynomial? It's the same rule. If it stops at a
finite stage, this one stops at a finite stage. If it goes on forever,
it goes on forever. AUDIENCE: So I thought that the
series add up finite numbers. You can add up terms
of x in series? PROFESSOR: So an
interesting question. So the question
that was just asked is I thought that a series
added up finite numbers. You could add up x? That was what you said, right? OK now notice that
I pulled that off on you by changing the
letter a to the letter x at the very beginning
of this commentary here. This is a series. For each individual value
of x, it's a number. So in other words, if
I plug in here x = 1/2, I'm going to add 1
+ 1/2 + 1/4 + 1/8, and I'll get a
number which is 2. And I'll plug in a number over
here, and I'll get a number. On the other hand, I can do
this for each value of x. So the interpretation of this
is that it's a function of x. And similarly this
is a function of x. It works when you plug
in the possible values x between -1 and 1. So there's really no
distinction there, it's just I slipped it passed you. These are functions of x. And the notion of a
power series is this idea that you put
coefficients on a series, but then you allow
yourself the flexibility to stick powers here. And that's exactly
what we're doing. OK there are other
kinds of series where you stick other
interesting functions in here like sines and cosines. There are lots of other
series that people study. And these are the simplest ones. And all those
examples are extremely helpful for
representing functions. But we're only going to
do this example here. All right, so here
are the two rules. And now there's only one
other complication here which I have to explain
to you before giving you a bunch of examples to show you
that this works extremely well. And the last thing
that I have to do for you is explain
to you something called Taylor's formula. Taylor's formula is the way you
get from the representations that we're used to of functions,
to a representation in the form of these coefficients. When I gave you
the function e^x, it didn't look
like a polynomial. And we have to figure
out which of these guys it is, if it's going to
fall into our category here. And here's the formula. I'll explain to you how
it works in a second. So the formula is
f(x) turns out-- There's a formula in terms
of the derivatives of f. Namely, you
differentiate n times, and you evaluate it at 0, and
you divide by n factorial, and multiply by x^n. So here's Taylor's formula. This tells you what
the Taylor series is. Now about half of our job
for the next few minutes is going to be to
give examples of this. But let me just explain
to you why this has to be. If you pick out this number
here, this is the a_n, the magic number a_n here. So let's just illustrate it. If f(x) happens to be a_0 + a_1
x + a_2 x^2 + a_3 x^3 plus dot, dot, dot. And now I differentiate
it, right? I get a 1 + 2 a_2 x + 3 a_3 x. If I differentiate it another
time, I get 2 a_2 plus 3-- sorry, 3*2*a_3 x
plus dot, dot, dot. And now a third time, I
get 3*2 a_3 plus et cetera. So this next term is really in
disguise, 4*3*2 x a-- sorry, a_4 x. That's what really comes down if
I kept track of the fourth term there. So now here is my function. But now you see if
I plug in x = 0, I can pick off the third term. f triple prime of 0
is equal to 3*2 a_3. Right, because all the rest of
those terms, when I plug in 0, are just 0. Here's the formula. And so the pattern here is this. And what's really going on here
is this is really 3*2*1 a_3. And in general a_n is
equal to f, nth derivative, divided by n!. And of course, n!, I remind
you, is n times n-1 times n-2, all the way down to 1. Now there's one more
crazy convention which is always used. Which is that there's something
very strange here down at 0, which is that 0 factorial turns
out, has to be set equal to 1. All right, so that's
what you do in order to make this formula work out. And that's one of the
reasons for this convention. All right. So my next goal is to
give you some examples. And let's do a couple. So here's, well
you know, I'm going to have to let you see
a few of them next time. But let me just tell
you this one, which is by far the most impressive. So what happens with e^x -- if
the function is f(x) = e^x -- is that its derivative
is also e^x. And its second
derivative is also e^x. And it just keeps
on going that way. They're all the same. So that means that these
numbers in Taylor's formula, in the numerator--
The nth derivative is very easy to evaluate. It's just e^x. And if I evaluate it
at x = 0, I just get 1. So all of those
numerators are 1. So the formula here, is the sum
n equals 0 to infinity, of 1/n! x^n. In particular, we now have
an honest formula for e to the first power. Which is just e. Which if I plug it in, x = 1, I
get 1, this is the n = 0 term. Plus 1, This is the n = 1 term. Plus 1/2! plus 1/3! plus 1/4! Right? So this is our first
honest formula for e. And also, this is
how you compute the exponential function. Finally if you take a
function like sin x, what you'll discover is that
we can complete the sort of strange business that we did
at the beginning of the course -- or cos x -- where we took
the linear and quadratic approximations. Now we're going to get complete
formulas for these functions. sin x turns out to be
equal to x - x^3 / 3! + x^5 / 5! - x^7 / 7!, et cetera. And cos x = 1 - x^2 / 2! -- that's the same as this
2 here -- plus x^4 / 4! minus x^6 / 6!, plus et cetera. Now these may feel like they're
hard to memorize because I've just pulled them out of a hat. I do expect you to know them. They're actually extremely
similar formulas. The exponential here just has
this collection of factorials. The sine is all the odd
powers with alternating signs. And the cosine is all the even
powers with alternating signs. So all three of them form
part of the same family. So this will actually
make it easier for you to remember,
rather than harder. And so with that, I'll
leave the practice on differentiation
for next time. And good luck, everybody. I'll talk to you individually.

Diff. Eq.

  Hi. In this problem, we'll get some
practice working with PDFs and also using PDFs
to calculate CDFs. So the PDF that we're given
in this problem is here. So we have a random variable,
z, which is a continuous random variable. And we're told that the PDF of
this random variable, z, is given by gamma times 1 plus z
squared in the range of z between negative 2 and 1. And outside of this
range, it's 0. All right, so first thing we
need to do and the first part of this problem is we need to
figure out what gamma is because it's not really a
fully specified PDF yet. We need to figure out exactly
what the value gamma is. And how do we do that? Well, we've done analogous
things before for the discrete case. So the tool that we use
is that the PDF must integrate to 1. So in the discrete case, the
analogy was that the PMF had to sum to 1. So what do we know? We know that when you integrate
this PDF from negative infinity to infinity,
fz of z, it has to equal 1. All right, so what
do we do now? Well, we know what
the PDF is-- partially, except for gamma--
so let's plug that in. And the first thing that we'll
do is we'll simplify this because we know that the PDF is
actually only non-zero in the range negative 2 to 1. So instead of integrating from
negative infinity to infinity, we'll just integrate from
negative 2 to 1. And now let's plug in
this gamma times 1 plus z squared dc. And now the rest of the problem
is just applying calculus and integrating this. So let's just go through
that process. So we get z plus 1/3 z cubed
from minus 2 to 1. And now we'll plug
in the limits. And we get gamma, and that's 1
plus 1/3 minus minus 2 plus 1/3 times minus 2 cubed.   And then if we add this all up,
you get 4/3 plus 2 plus 8/3, which will give you 6. So what we end up with
in the end is that 1 is equal to 6 gamma. So what does that tell us? That tells us that, in this
case, gamma is 1/6.   OK, so we've actually figured
out what this PDF really is. And let's just substitute
that in. So we know what gamma is. So it's 1/6. So from this PDF, we can
calculate anything that we want to. This PDF, basically, fully
specifies everything that we need to know about this
random variable, z. And one of the things that
we can calculate from the PDF is the CDF. So the next part of the
problem asks us to calculate the CDF. So remember the CDF, we use
capital F. And the definition is that you integrate from
negative infinity to this z. And what do you integrate? You integrate the PDF. And all use some dummy variable,
y, here in the integration. So what is it really doing? It's basically just taking the
PDF and taking everything to the left of it. So another way to think about
this-- this is the probability that the random variable
is less than or equal to some little z. It's just accumulating
probability as you go from left to right.   So the hardest part about
calculating the CDFs, really, is actually just keeping track
of the ranges, because unless the PDF is really simple, you'll
have cases where the PDF cold be 0 in some ranges and
non-zero in other ranges. And then what you really have
to keep track of is where those ranges are and where you
actually have non-zero probability. So in this case, we actually
break things down into three different ranges because
this PDF actually looks something like this. So it's non-zero between
negative 2 and 1, and it's 0 everywhere else. So then what that means is
that our job is a little simpler because everything to
the left of negative 2, the CDF will be 0 because there's
no probability density to the left. And then everything to the
right of 1, well we've accumulated all the probability
in the PDF because we know that when you integrate
from negative 2 to 1, you capture everything. So anything to the right of
1, the CDF will be 1. So the only hard part is
calculating what the CDF is in this intermediate range, between
negative 2 and 1. So let's do that case first-- so the case of z is between
negative 2 and 1.   So what is the CDF
in that case? Well, the definition is to
integrate from negative infinity to z. But we know that everything
to the left of negative 2, there's no probably density. So we don't need to
include that. So we can actually change this
lower limit to negative 2. And the upper limit is
wherever this z is. So that becomes our integral. And the inside is
still the PDF. So let's just plug that in. We know that it's 1/6 1 plus-- we'll make this y squared-- by. And now it's just
calculus again. And in fact, it's more or less
the same integral, so what we get is y plus 1/3 y cubed
from negative 2 to z. Notice the only thing that's
different here is that we're integrating from negative 2 to
z instead of negative 2 to 1. And when we calculate this out,
what we get is z plus 1/3 z cubed minus minus 2 plus 1/3
minus 2 cubed, which gives us 1/6 z plus 1/3 z cubed plus plus
2 plus 8/3 gives us 14/3.   So that actually is our CDF
between the range of negative 2 to 1. So for full completeness, let's
actually write out the entire CDF, because there's two
other parts in the CDF. So the first part is that
it's 0 if z is less than negative 2. And it's 1 if z is
greater than 1. And in between, it's this
thing that we've just calculated. So it's 1/6 z plus 1/3 z cubed
plus 14/3 if z is between minus 2 and 1. So that is our final answer.   So the main point of this
problem was to drill a little bit more the concepts
of PDFs and CDFs. So for the PDF, the important
thing to remember is that in order to be a valid PDF, the
PDF has to integrate to 1. And you can use that fact to
help you calculate any unknown constants in the PDF. And then to calculate the CDF,
it's just integrating the PDF from negative infinity to
whatever point that you want to cut off at. And the tricky part, as I said
earlier, was really just keeping track of the ranges. In this case, we've broke it
down into three ranges. If we had a slightly more
complicated PDF, then you would have to keep track
of even more ranged. All right, so I hope that
was helpful, and we'll see you next time.  

Statistics

The following content is
provided under a Creative Commons license. Your support will help
MIT OpenCourseWare continue to offer high-quality
educational resources for free. To make a donation or
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. LING REN: All right. Let's get started. Today's topic would be
distributed algorithms. We will look at
two new algorithms. But they are similar to what
you have seen in the lectures. So it will also be a review
of some concepts in lectures. So our first example
would be, again, [INAUDIBLE], the
simplest example. But this time, the network
and topology would be a ring. So in the lectures,
the example we see is a click, meaning they
are fully connected. Right. Every node can talk
to every other node. There, if you remember,
our solution for everyone to generate a UID
or a random number. And if you are the maximum, then
you output, you're the leader You can do that because, yeah,
you are connected to everybody. So you immediately know
what random number everybody generates. And you can compare whether
you are the largest. Now, the idea is the
same if we have a ring. So you want everyone to
generate a ID or random number. I'll just say ID from now on. And you want to collect
everyone else ID so that you know
whether your ID is the largest among all of them. OK. AUDIENCE: [INAUDIBLE] LING REN: Pardon? AUDIENCE: [INAUDIBLE] LING REN: OK. The question is where did
the comparison happen? So we first need
some way to pass the numbers around
such that everyone has everyone else's number. Right. You have the number
of everyone, then you can compare whether
the largest equals yours. OK. If the largest equals
yours, then you know you are the largest. And you're going to
output, I'm the leader. OK. So the difficulty is just
how to pass the numbers around so that everyone
sees everyone else's number. Any ideas? Simple solution. Well, there's not
much you can do. You are just connect to your
two neighbors, the left one and the right one. So how do you propagate
the information? Go ahead. AUDIENCE: You can take
the maximum of your ID and neighbor's IDs and
kind of broadcast that. LING REN: OK. So what does
broadcast mean here? AUDIENCE: Like
tell your neighbors what the largest of
your value [INAUDIBLE] LING REN: OK. And yes. Let me call them A, B, C, E,
F. Then, yes, C can tell D or can tell B, but
how does C tell E? AUDIENCE: It waited around to
I guess propagate the maximum. LING REN: Yep. So it would be just,
say, everyone talks to its right neighbor. And then B has A's IDs, C
these B's ID, D has C's ID, and then continue and
pass around next time. So just to make
it perfect clear. Say they generate some
random IDs that's 5, 10, 20. And then in the next round,
A would send its ID to B, B would send its ID to C,
C would send its ID to D. And in the next round, B would
pass this information along to C. And C would pass
the information to D. And just continue. And eventually everyone will
have everyone else's ID. So how many rounds do we need? If they are in
nodes in the system. AUDIENCE: I think probably
in just one direction or two neighbors. LING REN: I think you
can do it either way. If you propagate both ways,
it probably 2x faster, yeah. AUDIENCE: [INAUDIBLE] LING REN: Yeah, correct. It's just O(n). But to keep it simple,
let's say we just propagate in one direction. That's also fine. Still O(n). So how many messages
are sent in total? AUDIENCE: n squared. LING REN: Pardon? AUDIENCE: n squared. LING REN: n squared. Yep. Is that obvious to everyone? OK. Give an explanation. AUDIENCE: Yeah, at
each round everyone sends a message
to its neighbors. LING REN: Yeah. Yeah. Every round, everyone
sends a message. Or you can think of every
message as propagated n times. And there are n
messages in total. OK. Yeah. That's definitely a solution. Well, you can imagine, it's
probably the naive solution. And can we do better than that? AUDIENCE: In this case,
are we assuming they know that there are n of them? LING REN: OK. Good question. So if they know
there are n of them, yep, then they know
how to terminate. So your question is how
to terminate, right? If they don't, eventually
you will receive your own ID on the other side. If we keep sending
left, eventually you will receive it
on the left port. And, yeah, that's an
indication of termination. Question? AUDIENCE: Are we
assuming the trajectory that these are unique? LING REN: Yeah. We usually assume that, yeah. Either it's a unique
user-- sorry, UID. What does U stand
for by the way? Or if you generate random
numbers in a large range, it's very unlikely
that they collide. OK. So can we do better than that? You have some idea? AUDIENCE: Once
something [INAUDIBLE] where you only pass it
to the one that's bigger. LING REN: Hm-mm. OK. I think you have two ideas. And binary search, we'll
see how that works later. So you said only forward
something that's larger? 0 AUDIENCE: Yeah, like only
forward things are larger. LING REN: OK, yeah,
that's on the right track. So one obvious
thing we can do is that so we actually don't care
what all the IDs are, right. We only care whether a
certain ID's the largest. So for example, in this case,
when A send its ID 5 to B, B knows this 5
won't be the leader. All right. Because 5 is too small. It's even smaller than his ID. So we can choose to
drop this message. There's no point in passing
that message further. Same thing for this message. C knows that 10 is too small. And it doesn't have
to pass it along. AUDIENCE: [INAUDIBLE] LING REN: Oh, the ID? It's just the integer each
node chooses at random. See. Yeah, the only purpose of
the ID is to break symmetry. So, yeah. Like we have seen the lecture,
if they don't do this, if they don't have
any unique identifier, they won't be able
to select the leader. And when they have
a unique number, then they can select, yeah,
the largest one or smallest one in some way. OK. Does this optimization
make sense? We can cut [INAUDIBLE]
messages that have no chance of
becoming the leader. AUDIENCE: What is the
upper bound of it? Is it still n squared? LING REN: Correct. Yeah. That's a very good question and
very good answer, by the way. But how effective is this? Well in average case, we may
be able to drop some messages. But there pathological
case where it actually doesn't help at all. Say this is the ID we choose. Then when there's
20, it send around. B cannot drop it. Right, because it
may be the largest. And when this 10 is sent
to C, C cannot drop it. And when 20 comes along,
it also cannot drop it. So no node can drop any message. Its worst case is
still n square. OK. So let's think about the
binary search equivalent, yeah, or how to binary
search in this case. AUDIENCE: So binary
is [INAUDIBLE] what? LING REN: Yeah, good question. It's not very obvious
binary searching to what. I Yeah, just some
binary idea that will give us a n log end bound. So actually this is
the better logarithm. But once you have a
log n, you can probably get-- OK, go ahead. AUDIENCE: How about
kind of merging, like instead of finding
the [INAUDIBLE], finding like log maximum,
then [INAUDIBLE]. LING REN: Yeah. That's on the right track, yeah. AUDIENCE: Like clusters
of some [INAUDIBLE]. LING REN: Yeah. That's definitely
the right idea. Let's detail it a little bit. How do you carry it out? Hm-mm. AUDIENCE: Divide the two parts. Like kind of the [INAUDIBLE]. LING REN: And, OK,
so then it's let them select their leader
respectively and compare which one is larger. It's interesting thought. AUDIENCE: First,
for example, A, B finds the maximum
between these two, the C, D finds the maximum
E [INAUDIBLE] maximum. Kind of merging, considering
them as one node. LING REN: I think
the first difficulty I see is that if
you cut it by half, it's no longer a ring, right. AUDIENCE: Also they can't
cut themselves in half. LING REN: Yeah. Yeah. Correct. Yeah, yeah. Yeah, it's definitely
not an easy problem, not a easy algorithm. And the idea is to, well,
you had the right idea. That we want to--
what's the word? Let the weak candidates
shut up early. So what I mean by that is say
we have several round, so this B and A and C. We will that B
only propagate to A and C first. If B is the local maximum
among A, B, C, then B will try to talk further. Increase its range. If B is not the local maximum
of A, B, C, then, yeah, it can be quiet. Doesn't need to send
messages anymore. If B succeeds in the
next round, then you go further increase its range
to try to talk to more people. OK. So how does it work in detail? Well so in round I,
we will let a node send this message up
to 2 raised to I hops. In this case, is 1 and in
next round is 2 and then 4. If at any point some mode
like along this range decides that you are
not the local maximum, then they will reply
that, yeah, you no longer need to send messages anymore. If this message successfully
reaches this endpoint that 2 raised to I hops,
then this guy will respond. And this guy, if it still thinks
you are the local maximum, then it will respond the message
while saying, yeah, continue. OK. And if the sender receives the
continue message on both sides, then they it continue
into the next round. Otherwise, it will go inactive. Is the algorithm clear? AUDIENCE: If the problem
reaches the next maximum, like next the node each
has to send a message. LING REN: Say that again. AUDIENCE: Like after receiving
like don't sending it, like do you have to
choose one node, which has to send the message? LING REN: Oh, OK. In the first round, everyone
send their messages. OK. Then some of them
will go inactive because they learn
they are not maximum. And the remaining,
the surviving ones, will, yeah, continue
sending messages. And then, yeah, half
of them probably will die in the next round. And the surviving ones
keep sending messages. Make sense? AUDIENCE: How do you
return messages through I? Like if you [INAUDIBLE] its
neighbor and not the node that do either. LING REN: Yeah. So we will send the
message of this form, say, well, some message. And then we will send the
hop and the direction, either left or right. And this hop will
initially be set to 2 raised to I,
the number of round. Then when this guy
receives the message, it will increment the hop
count and pass it along. And every node when
forwarding the message will decrease the that h by 1. And finally when it reaches
here, that number becomes 0. And when a node sees a
message with 0 hop count, it's going to reverse it, send
it in the other direction. And, again, set it
back to 2 raised to I. This message I should say ID. And at certain
point, a certain node may decide this ID's too small. It doesn't have a chance. Then the I can directly send
it in the opposite direction, replying a message saying,
yeah, you are too small. OK. So any more questions
on the algorithm itself? If not, what's the next step? AUDIENCE: Time. LING REN: Yeah, time complexity. I already claimed it's n log n. Is it? This is round. This is message. This is also message complexity. OK. So why is it n log n? AUDIENCE: The log n [INAUDIBLE]. LING REN: Hm-mm. So we have a certain
number of rounds. So how many rounds do we have? AUDIENCE: Two. LING REN: Yeah, maybe. But, yeah, I'll just say log n. OK. Because you are increasing
your hop length. And we're going to compute
like how many nodes are still active each around and how many
messages are sent in the round. So, well, the number
of nodes active will just be this number
if we start from 0. Why? Because the first time
everyone is active. In the next time, only 1/3
of them will be active. But we said we are conservative
here at we put 1/2. Right. Next round is actually 1/5. If it's local maximum,
it means like these two and those two will go inactive. But we put this
as a upper bound. So this is the number of
nodes that are still active. And they will send the
message up to this many hops. OK. And there are two directions. And you send a message back. And then, yeah-- sorry,
send the message forward and someone will reply. But in the end, this is n log n. Yeah, and I think
I got 8n log n. So my recitation note says
it's 4 log n, not entirely sure what's going on. Yeah. But you can double check
whether this is correct or the recitation
note is correct. AUDIENCE: What is I again? LING REN: I's the number
of round in the I-th round. Yeah. This many nodes
are still active. And each of them will send
a message of this many hops. And I didn't mention
whether the network is synchronous or asynchronous. And it turns out
it doesn't care. Some of them can work for both
synchronous and asynchronous. Apparently, it works for
synchronous networks. If it's asynchronous,
then what changes is that different nodes
are in different rounds. A certain node may be far
ahead than the others. But it's fine. Eventually, they will converge
to the correct result. OK. So let's look at
the second problem. Well, problem's
defined even simpler. We just want to count how
many nodes are out there. We want the algorithm to
work both synchronously and asynchronously. By that I just mean
we have a network. Well, say you have a
lot of nodes after that. Just want to count how
many nodes are there in this network. So I'll give you, say, one
minutes to let's first come up with a high-level plan. Is the problem clear? AUDIENCE: In the worst
case, [INAUDIBLE]. The worst case. LING REN: OK. I haven't defined that. Let's not worry about
complexity now for now. The complexity will
depend on number of nodes and the number of
edges, E. Let's just get it functionally correct. OK. Anyone share a
high-level strategy? Go ahead. AUDIENCE: So each
node will start like IDs of the other
devices [INAUDIBLE]. So basically like it's going
to send propagates [INAUDIBLE] sets of each node. LING REN: On what edge? AUDIENCE: Furthest one. LING REN: Yeah. It wasn't on both edges? AUDIENCE: Itself first. LING REN: OK. So then maybe send the 1 here
and send the 1 here there. Then this node will think it
has a children, it as a child, right, which is this one. Let's just say this
is the entire network. And what message does
it send to this guy? AUDIENCE: Reinforced it. LING REN: Well, you
probably need send two here because you have one and this
is possibly its child, right. But then we're double
counting this node. See the problem? AUDIENCE: Do nodes have
their IDs, like unique IDs? LING REN: Oh, yeah. They have their IDs. AUDIENCE: We can't send
the IDs instead of the IDs through the end node. LING REN: OK. We are going to
send all the IDs. AUDIENCE: Yeah, neighbors. And after n steps, it's going
to have the [INAUDIBLE]. LING REN: OK. Yeah, that's an
interesting thought. And then you may
still need the root. And that will have
everyone's ID. And then see how many
unique I's are there. OK. Interesting. Does this algorithm work? Yeah, I don't see any problem. OK. But let me still repeat
what our algorithm is because it's closer
to what's in the lecture. So we're going to find a
spanning tree of this network. A spanning tree
means, well, like I have to cut one of these edges. If I have a tree, then I
can have every child report to its parent. Like how many offsprings,
including myself do I have. And this node will sum
up all its children and report to its parent. Does everyone get that? So first, we'll find
a spanning tree. And second, we'll have
child reports to parent. AUDIENCE: How can we
find the spanning tree? LING REN: Good question. So in the lecture, we
have seen algorithm that find BFS spanning tree
for synchronous networks. OK. This is review of the lecture. How does it work? Each node will, say, we
need to first choose a root. And our root will just send
a message to its neighbor and, yeah, saying, you are
my child, you are my child, you are my child. And then every node upon
receiving this message from the parent will
search among its neighbors. All right. So the neighbors that
haven't got a parent will acknowledge this
sender as the parent. OK. That's a little messy. So this node will
search to the leaf node. But then it will also try
to search for this guy. But this guy already had
a parent, then, yeah, it will say I already got a
parent and blah, blah, blah. This will give us a
BFS spanning tree. What does it mean? It's a spanning
tree found by BFS. Does it work for
asynchronous network? Go ahead. AUDIENCE: This version doesn't. But there's a
different version that contains an edge for
relaxation technique. LING REN: Yeah. Correct. And so why does this version not
work for a synchronous network? AUDIENCE: Because
different nodes can be on a different
round number so that a longer path could
end up going to the node even. LING REN: Yeah, exactly. So let me give a
concrete example. What does asynchronous
network mean? Is that, well, messages
travel at different speed. Say this link for some
reason is temporarily down. And this node doesn't receive
the message from the root. And then this message
travels very fast. And this message also
travels very fast. So this message may
reach this node earlier. And then this node will think
of it as a child of this node. OK. And then this message
comes along, finally. But this node says, yeah,
I already have a parent. And I'm going to reject you. AUDIENCE: So it still
makes a spanning tree? LING REN: Yes. Good point. This is not a BFS spanning tree
but it's still a spanning tree. All right. So in our problem, we're
totally fine with it. Yeah. But you do have to know if
you really want a BFS spanning tree in an asynchronous
network, then you have to like record the
distance and do the relaxation and so on and so forth. OK. So we will just use this
algorithm, the BFS spanning tree algorithm, just
run it asynchronously. It doesn't find the
BFS spanning tree but it finds some spanning tree. OK. How does this algorithm work? We will have several variables. The first one is parent. We will initialize to
this undefined single. OK. Then every node will pass
around this search message. I'll use a slightly
more shorthand notation than from the lecture. OK. Let's say the code I
wrote is for a process u. OK. If we receive a message,
a search message, from v to u, that means u-- sorry, v
is trying to become a parent. OK. And if I do not
have a parent yet, I should set the
parent to v. OK. So what's the next step? Now, we got this search
message from our parent. And we have to pass it along. AUDIENCE: Should we
send it to the like add your child [INAUDIBLE]. LING REN: Oh, yeah. Great. Yeah, we first need to
respond by saying, OK, I'll use a shorter notation. Send these a cue that
that's the message that will be sent to v at some point. The message we send is parent 1. Parent 2. This is a response to v saying,
yeah, you are my parent. OK. Then else, we also have to
respond by saying parent 0. You are not my parent because I
already got some other parent. OK. Missing a step here. If we receive a search message. Go ahead. AUDIENCE: We send messages
to all the other nodes. LING REN: Yeah. We need to pass it to all
my potential children. So I use this comma u,
meaning the neighbors of u. And then I'll send
them a message. What message? Search. OK. OK. So, well, naturally since
we have a search message and we know how to
deal with it, now we are sending this parent message
with better deal with it. Right. So then the next
chunk of code should be if we receive this parent
message, I'll say parent b, meaning this message comes
with either true or false. I received this message
from some node w. OK. I'm still u here. Because I just send all
the message to all the w's, to all my neighbors, and they
should give me a response. OK. If b is 1 that means
this particular w take me as its parent. Make sense? OK. So I'd better have a
list of my children. I want to keep track of that. How? We will create a new
variable for children. It's going to be
initialized to what? Yeah, empty set. And now if this b
is 1, then I'm going to put w in this children list. OK. I'm leaving some space
here because our root should be slightly different. So every other node will
send search messages when it receive
a search message. We need someone to initiate. Make sense? So if u equals root,
we say the root is v0. So two things should happen. First, we should set its
parent to some special value. Just say root. And then I should copy
this blob of code to here. OK. It's a little crowded but
I hope it's still legible. OK. This is already almost
the correct algorithm, except that we don't
know how to terminate. If we wait long
enough, then everyone will receive all the responses
and everyone will know its parent and this child list. But how do we terminate? AUDIENCE: After n rounds. LING REN: Yeah, that's one
way to do, after n rounds. But the whole point is we
are trying to find what n is. We don't know how
many nodes are there. AUDIENCE: If we receive
reject for all, [INAUDIBLE]. LING REN: Say again. AUDIENCE: For all the search,
if we receive a reject. LING REN: Oh, yeah. If you receive a response,
either parent 0 or parent 1 from all your neighbors, then
your job is pretty much done. But the others have not. All right. So you send a message very fast. They responded to you. And they are still
working very hard. OK. So then we need to use the
technique from the lecture that's called converge cast. So everyone will send a
done signal when it is done and all its children
are also done. OK. To do that, I'm going to define
a new variable called searched. Searched means
someone has responded. OK. In this case, w has responded. I'll put it into
the search the list, no matter whether it accepts
me as parent or reject. OK. Then naturally I need to
define this variable here. OK. As a last step, If we search the
list equals my neighbor list, that means everyone
has responded and all my children
are done, then I need a new
variable called done. That's another list tracking who
has finished and who has not. OK. AUDIENCE: So what's
the first one? LING REN: Say again. AUDIENCE: What
was the first one? LING REN: This one? Searched means
someone has responded to the search message. OK. Done means all its
children are done. I haven't write how
done is defined. Give me a minute. In this case, I'm going to
send my parent a message. This is the step
of converge cast. What do I send? I'm going to send them I'm done. OK. Then whenever we
create a message, we should deal
with that message. If we receive a message
I'm done, what do we do? OK. From w, then we're going
to mark that node as done. OK. There's this other point. So someone has to
initiate the done signal. That's going to be
our leaves, right. Because when this
condition check, they don't have any children. Their children
list is empty set. And their done list
is also empty set. But they're going to send
the I'm done signal first. And then in the
median nodes, we'll send a done signal when
all its children are done. All right. So this is the
converged cast version. Only gives us a
termination point of our spanning tree search. We haven't count the number
of nodes in the network. But that's a small modification. We're just going to include that
number in the I'm done signal. So then I need to define
another variable called total that's initialized to 0. This variable will
attract how many nodes do I have in
my subtree, including me and all my children. Then when I send
the I'm done signal, I'm going to send this-- sorry. That's not right way. When I'm sending
the I'm done signal, I'm going to send my total
number of offspring with it. And when I receive,
one of my children reports that I do
have t children. I need to increment my
total by that amount. OK. I made a mistake again. Should be total plus 1. Right. Because I'm counting
all my children and then I should include myself. So that's the
complete algorithm. Yeah. One purpose is just to create
a different angle to look at distributed algorithm. Usually just draw
that network graphs. But sometimes it's
helpful to think about how the code actually works. OK. That's all for today.

CS

The following
content is provided under a Creative
Commons license. Your support will help MIT
OpenCourseWare continue to offer high quality
educational resources for free. To make a donation or
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: We established that,
essentially, what we want to do is to describe the
properties of a system that is in equilibrium. And a system in equilibrium
is characterized by a certain number
of parameters. We discussed
displacement and forces that are used for
mechanical properties. We described how when systems
are in thermal equilibrium, the exchange of heat requires
that there is temperature that will be the
same between them. So that was where the
Zeroth Law came and told us that there is another
function of state. Then, we saw that,
from the First Law, there was energy, which is
another important function of state. And from the Second Law,
we arrived at entropy. And then by
manipulating these, we generated a whole set of
other functions, free energy, enthalpy, Gibbs free energy, the
grand potential, and the list goes on. And when the system
is in equilibrium, it has a well-defined
values of these quantities. You go from one equilibrium
to another equilibrium, and these quantities change. But of course, we saw that the
number of degrees of freedom that you have to
describe the system is indicated through looking
at the changes in energy, which if you were only
doing mechanical work, you would write as sum over all
possible ways of introducing mechanical work into the system. Then, we saw that
it was actually useful to separate
out the chemical work. So we could also
write this as a sum of an alpha chemical
potential number of particles. But there was also
ways of changing the energy of the system
through addition of heat. And so ultimately,
we saw that if there were n ways of doing
chemical and mechanical work, and one way of introducing heat
into the system, essentially n plus 1 variables are
sufficient to determine where you are in
this phase space. Once you have n
plus 1 of that list, you can input, in
principle, determine others as long as you have
not chosen things that are really
dependent on each other. So you have to choose
independent ones, and we had some discussion
of how that comes into play. So I said that today
we will briefly conclude with the
last, or the Third Law. This is the statement
about trying to calculate the
behavior of entropy as a function of temperature. And in principle,
you can imagine as a function of some coordinate
of your system-- capital X could indicate pressure,
volume, anything. You calculate that at
some particular value of temperature, T, T the
difference in entropy that you would have
between two points parametrized by X1 and X2. And in principle,
what you need to do is to find some kind of a
path for changing parameters from X1 to X2 and calculate,
in a reversible process, how much heat you have to
put into the system. Let's say at this fixed
temperature, T, divide by T. T is not changing along the
process from say X1 to X2. And this would be a
difference between the entropy that you would have
between these two quantities, between
these two points. You could, in principle,
then repeat this process at some lower temperature and
keep going all the way down to 0 temperature. What Nernst observed was that as
he went through this procedure to lower and lower temperatures,
this difference-- Let's call it delta s of T going
from X1 to X2 goes to 0. So it looks like, certainly
at this temperature, there is a change in entropy
going from one to another. There's also a change. This change gets
smaller and smaller as if, when you get
to 0 temperature, the value of your entropy is
independent of X. Whatever X you choose, you'll have
the same value of entropy. Now, that led to, after a while,
to a more ambitious version statement of the Third Law
that I will write down, which is that the entropy of
all substances at the zero of thermodynamic temperature is
the same and can be set to 0. Same universal
constant, set to 0. It's, in principle,
through these integration from one point to another
point, the only thing that you can calculate is the
difference between entropies. And essentially, this
suggests that the difference between entropies goes to 0,
but let's be more ambitious and say that even if you
look at different substances and you go to 0 temperature,
all of them have a unique value. And so there's more
evidence for being able to do this for
different substances via what is called allotropic state. So for example, some materials
can exist potentially in two different
crystalline states that are called allotropes,
for example, sulfur as a function of temperature. If you lower it's
temperature very slowly, it stays in some foreign all
the way down to 0 temperature. So if you change its temperature
rapidly, it stays in one form all the way to 0 temperature
in crystalline structure that is called monoclinic. If you cool it
very, very slowly, there is a temperature
around 40 degrees Celsius at which it makes a transition
to a different crystal structure. That is rhombohedral. And the thing that
I am plotting here, as a function of temperature,
is the heat capacity. And so if you are, let's
say, around room temperature, in principle you can say there's
two different forms of sulfur. One of them is truly stable,
and the other is metastable. That is, in principle, if
you rate what sufficiently is of the order of
[? centuries ?], you can get the transition from
this form to the stable form. But for our purposes,
at room temperature, you would say that
at the scale of times that I'm observing things, there
are these 2 possible states that are both equilibrium
states of the same substance. Now using these two
equilibrium states, I can start to test this
Nernst theorem generalized to different substances. If you, again, regard
these two different things as different substances. You could say that if I want
to calculate the entropy just slightly above the transition,
I can come from two paths. I can either come
from path number one. Along path number
one, I would say that the entropy at
this Tc plus is obtained by integrating
degree heat capacity, so integral dT Cx
of T divided by T. This combination is
none other than dQ. Basically, the combination
of heat capacity dT is the amount of
heat that you have to put the substance to
change its temperature. And you do this all the
way from 0 to Tc plus. Let's say we go
along this path that corresponds to this
monoclinic way. And I'm using this Cm that
corresponds to this as opposed to 0 that corresponds to this. Another thing that I can
do-- and I made a mistake because what I really need
to do is to, in principle, add to this some
entropy that I would assign to this green state at 0
because this is the difference. So this is the
entropy that I would assign to the monoclinic
state at T close to 0. Going along the
orange path, I would say that S evaluated
at Tc plus is obtained by integrating from 0. Let's say to Tc
minus dT, the heat capacity of this rhombic phase. But when I get to just
below the transition, I want to go to just
above the transition. I have to actually be put in
certain amount of latent heat. So here I have to add
latent heat L, always at the temperatures Tc, to
gradually make the substance transition from
one to the other. So I have to add here L of Tc. This would be the
integration of dQ, but then I would have to add
the entropy that I would assign to the orange state
at 0 temperature. So this is something that
you can do experimentally. You can evaluate at these
integrals, and what you'll find is that these two
things are the same. So this is yet
another justification of this entropy
being independent of where you start
at 0 temperature. Again at this
point, if you like, you can by [INAUDIBLE]
state that this is 0 for everything
will start with 0. So this is a supposed new
law of thermodynamics. Is it useful? What can we deduce from that? So let's look at
the consequences. First thing is so what
I have established is that the limit
as T goes to 0 of S, irrespective of whatever
set of parameters I have-- so I pick T as one
of my n plus one coordinates, and I put some other
bunch of coordinates here. I take the limit
of this going to 0. This becomes 0. So that means, almost
by construction, that if I take the derivative of
S with respect to any of these coordinates-- if I take then
the limit as T goes to 0, this would be
fixed T. This is 0. Fine. So basically, this
is another way of stating that entropy
differences go through 0. But it does have a consequence
because one thing that you will frequently measure
are quantities, such as extensivity. What do I mean by that? Let's pick a displacement. Could be the length of a wire. Could be the volume of a gas. And we can ask if I were
to change temperature, how does that quantity change? So these are quantities
typically called alpha. Actually, usually
you would also divide by x to make them intensive
because otherwise x being extensive,
the whole quantity would have been extensive. Let's say we do this at fixed
corresponding displacement. So something that
is very relevant is you take the volume of
gas who changes temperature at fixed pressure, and
the volume of the gas will shrink or expand
according to this extensive. Now, this can be related to this
through Maxwell relationship. So let's see what I have to do. I have that dE is
something like Jdx plus, according to what I
have over there, TdS. I want to be able to write
a Maxwell relation that relates a derivative of x. So I want to make x
into a first derivative. So I look at E minus Jx. And this Jdx becomes minus xdJ. But I want to take a derivative
of x with respect not s, but with respect
to T. So I'll do that. This becomes a minus SdT. So now, I immediately see
that I will have a Maxwell relation that says dx
by dT at constant J is the same thing as
dS by dJ at constant T. So this is the same thing by
the Maxwell relation as dS by dJ at constant T. All right? This is one of these quantities,
therefore, as T goes 0, this goes to 0. And therefore, the
expansivity should go to 0. So any quantity that measures
expansion, contraction, or some other change as a
function of temperature, according to this law, as
you go through 0 temperature, should go to 0. There's one other quantity
that also goes to 0, and that's the heat capacity. So if I want to calculate the
difference between entropy at some temperature T
and some temperature at 0 along some particular
path corresponding to some constant
x for example, you would say that what
I need to do is to integrate from 0 to
T the heat that I have to put into the
system at constant x. And so if I do
that slowly enough, this heat I can write as CxdT. Cx, potentially,
is a function of T. Actually, since I'm indicating
T as the other point of integration, let me call
the variable of integration T prime. So I take a path in which
I change temperature. I calculate the heat
capacity at constant x. Integrate it. Multiply by dT to convert
it to T, and get the result. So all of these results that
they have been formulating suggest that the result that you
would get as a function of T, for entropy, is something that
as T goes to 0, approaches 0. So it should be a perfectly
nice, well-defined value at any finite temperature. Now, if you integrate
a constant divided by T, divided by dT, then
essentially the constant would give you a logarithm. And the logarithm would blow
up as we go to 0 temperature. So the only way that
this integral does not blow up on you-- so this is
finite only if the limit as T goes to 0 of the heat
capacities should also go to 0. So any heat capacity should
also essentially vanish as you go to lower
and lower temperature. This is something that you
will see many, many times when you look at different
heat capacities in the rest of the course. There is one other
aspect of this that I will not really explain,
but you can go and look at the notes or elsewhere, which
is that another consequence is unattainability of T equals
to 0 by any finite set of operations. Essentially, if you want
to get to 0 temperature, you'll have to do something
that cools you step by step. And the steps become
smaller and smaller, and you have to repeat
that many times. But that is another consequence. We'll leave that
for the time being. I would like to, however, end
by discussing some distinctions that are between
these different laws. So if you think
about whatever could be the microscopic
origin, after all, I have emphasized
that thermodynamics is a set of rules that you look
at substances as black boxes and you try to deduce a
certain number of things based on observations, such
as what Nernst did over here. But you say, these
black boxes, I know what is inside
them in principle. It's composed of atoms,
molecules, light, quark, whatever the
microscope theory is that you want to assign to
the components of that box. And I know the
dynamics that governs these microscopic
degrees of freedom. I should be able to get
the laws of thermodynamics starting from the
microscopic laws. Eventually, we will do
that, and as we do that, we will find the origin
of these different laws. Now, you won't be surprised
that the First Law is intimately connected to the fact that
any microscopic set of rules that you write down embodies
the conservation of energy. And all you have to make
sure is to understand precisely what heat is
as a form of energy. And then if we regard heat
as another form of energy, another component, it's
really the conservation law that we have. Then, you have the Zeroth
Law and the Second Law. The Zeroth Law and Second Law
have to do with equilibrium and being able to go in
some particular direction. And that always runs a fall of
the microscopic laws of motion that are typically things
that are time reversible where as the Zeroth Law and
Second Law are not. And what we will see later on,
through statistical mechanics, is that the origin
of these laws is that we are dealing with large
numbers of degrees of freedom. And once we adapt the
proper perspective to looking at properties
of large numbers of degrees of freedom, which
will be a start to do the elements of that
[? prescription ?] today, the Zeroth Law and
Second Law emerge. Now the Third Law, you
all know that once we go through this process,
eventually for example, we get things for the
description of entropy, which is related to some
number of states that the system
has indicated by g. And if you then want to
have S going through 0, you would require that
g goes to something that is order of 1-- of 1 if
you like-- as T goes to 0. And typically, you
would say that systems adopt their ground state, lowest
energy state, at 0 temperature. And so this is
somewhat a statement about the uniqueness of the
state of all possible systems at low temperature. Now, if you think about
the gas in this room, and let's imagine that
the particles of this gas either don't interact, which is
maybe a little bit unrealistic, but maybe repel each other. So let's say you have
a bunch of particles that just repel each other. Then, there is
really no reason why, as I go to lower and
lower temperatures, the number of configurations of
the molecules should decrease. All configurations that I
draw that they don't overlap have roughly the same energy. And indeed, if I look at say
any one of these properties, like the expansivity of a
gas at constant pressure which is given in fact
with a minus sign. dV by dT at constant pressure
would be the analog of one of these extensivities. If I use the Ideal Gas
Law-- So for ideal gas, we've seen that PV is
proportional to let's say some temperature. Then, dV by dT at
constant pressure is none other than V over
T. So this would give me 1 over V, V over T. Probably
don't need it on this. This is going to
give me 1 over T. So not only doesn't it
go to 0 at 0 temperature, if the Ideal Gas
Law was satisfied, the extensivity would
actually diverge at 0 temperature as
different as you want. So clearly the Ideal Gas
Law, if it was applicable all the way down
to 0 temperature, would violate the Third
Law of thermodynamics. Again, not surprising
given that I have told you that a gas of classical
particles with repulsion has many states. Now, we will see
later on in the course that once we include quantum
mechanics, then as you go to 0 temperature,
these particles will have a unique state. If they are bosons, they will be
together in one wave function. If they are fermions, they
will arrange themselves appropriately so that,
because of quantum mechanics, all of these laws would
certainly breakdown at T equals to 0. You will get 0 entropy, and
you would get consistency with all of these things. So somehow, the nature
of the Third Law is different from the other
laws because its validity rests on being able to
be living in a world where quantum mechanics applies. So in principle, you could have
imagined some other universe where h-bar equals
to 0, and then the Third Law of thermodynamics
would not hold there whereas the Zeroth Law
and Second Law would hold. Yes? AUDIENCE: Are there any known
exceptions to the Third Law? Are we going to
[? account for them? ?] PROFESSOR: For
equilibrium-- So this is actually an
interesting question. What do I know
about-- classically, I can certainly come up with
lots of examples that violate. So your question then amounts if
I say that quantum mechanics is necessary, do I
know that the ground state of a quantum
mechanical system is unique. And I don't know of a proof of
that for interacting system. I don't know of a case that's
violated, but as far as I know, there is no proof that I give
you an interacting Hamiltonian for a quantum system, and
there's a unique ground state. And I should say
that there'd be no-- and I'm sure you know of cases
where the ground state is not unique like a ferromagnet. But the point is not that
g should be exactly one, but that the limit
of log g divided by the number of
degrees of freedom that you have should go to
0 as n goes to infinity. So something like a ferromagnet
may have many ground states, but the number of
ground states is not proportional to the number of
sites, the number of spins, and this entity will go to 0. So all the cases that we
know, the ground state is either unique
or is order of one. But I don't know a theorem that
says that should be the case. So this is the last
thing that I wanted to say about thermodynamics. Are there any
questions in general? So I laid out the
necessity of having some kind of a description of
microscopic degrees of freedom that ultimately will
allow us to prove the laws of thermodynamics. And that will come through
statistical mechanics, which as the name implies, has
to have certain amount of statistic characters to it. What does that mean? It means that you have to
abandon a description of motion that is fully
deterministic for one that is based on probability. Now, I could have told you
first the degrees of freedom and what is the
description that we need for them to
be probabilistic, but I find it more
useful to first lay out what the language
of probability is that we will be
using and then bring in the description of the
microscopic degrees of freedom within this language. So if we go first
with definitions-- and you could, for example, go
to the branch of mathematics that deals with probability,
and you will encounter something like this that what probability
describes is a random variable. Let's call it X, which has a
number of possible outcomes, which we put together
into a set of outcomes, S. And this set can be discrete as
would be the case if you were tossing a coin, and
the outcomes would be either a head or a tail,
or we were throwing a dice, and the outcomes would
be the faces 1 through 6. And we will encounter
mostly actually cases where S is continuous. Like for example, if I want to
describe the velocity of a gas particle in this room, I need
to specify the three components of velocity that can
be anywhere, let's say, in the range of real numbers. And again, mathematicians
would say that to each event, which is a subset of
possible outcomes, is assigned a value which we
must satisfy the following properties. First thing is the
probability of anything is a positive number. And so this is positivity. The second thing is additivity. That is the probability
of two events, A or B, is the sum total of
the probabilities if A and B are
disjoint or distinct. And finally, there's
a normalization. That if you're event is
that something should happen the entire set, the probability
that you assign to that is 1. So these are formal statements. And if you are a mathematician,
you start from there, and you prove theorems. But from our perspective,
the first question to ask is how to determine this
quantity probability that something should happen. If it is useful and I want to do
something real world about it, I should be able to measure
it or assign values to it. And very roughly
again, in theory, we can assign probabilities
two different ways. One way is called objective. And from the perspective
of us as physicists corresponds to what would be
an experimental procedure. And if it is assigning p of e
as the frequency of outcomes in large number of
trials, i.e. you would say that the probability
that event A is obtained is the number of times you
would get outcome A divided by the total number of
trials as n goes to infinity. So for example, if you want to
assign a probability that when you throw a dice that face 1
comes up, what you could do is you could make a table
of the number of times 1 shows up divided by the number
of times you throw the dice. Maybe you throw it 100
times, and you get 15. You throw it 200
times, and you get-- that is probably too much. Let's say 15-- you get 35. And you do it 300 times, and
you get something close to 48. The ratio of these
things, as the number gets larger and
larger, hopefully will converge to something that
you would call the probability. Now, it turns out that
in statistical physics, we will assign things through
a totally different procedure which is subjective. If you like, it's
more theoretical, which is based on uncertainty
among all outcomes. Because if I were to
subjectively assign to throwing the dice and
coming up with value of 1, I would say, well, there's six
possible faces for the dice. I don't know anything about
this dice being loaded, so I will say they
are all equally alike. Now, that may or may not
be a correct assumption. You could test it. You could maybe
throw it many times. You will find that either the
dice is loaded or not loaded and this is correct or not. But you begin by
making this assumption. And this is actually, we
will see later on, exactly the type of assumption
that you would be making in
statistical physics. Any question about
these definitions? So let's again
proceed slowly to get some definitions established by
looking at one random variable. So this is the next section
on one random variable. And I will assume that
I'll look at the case of the continuous
random variable. So x can be any real number
minus infinity to infinity. Now, a number of definitions. I will use the term
Cumulative-- make sure I'll use the-- Cumulative
Probability Function, CPF, that for this one
random variable, I will indicate
by capital P of x. And the meaning of
this is that capital P of x is the probability
of outcome less than x. So generically,
we say that x can take all values
along the real line. And there is this
function that I want to plot that I will
call big P of x Now big P of x is a probability,
therefore, it has to be positive according
to the first item that we have over there. And it will be less than 1
because the net probability for everything toward
here is equal to 1. So asymptotically,
where I go all the way to infinity, the
probability that I will get some number along the
line-- I have to get something, so it should automatically
go to 1 here. And every element of
probability is positive, so it's a function that
should gradually go down. And presumably, it
will behave something like this generically. Once we have the Cumulative
Probability Function, we can immediately construct the
Probability Density Function, PDF, which is the
derivative of the above. P of x is the derivative of big
P of x with respect to the x. And so if I just take here
the curve that I have above and take its derivative,
the derivative will look something like this. Essentially, clearly by the
definition of the derivative, this quantity is
therefore ability of outcome in the
interval x to x plus dx divided by the size
of the interval dx. couple of things to
remind you of, one of them is that the Cumulative
Probability is a probability. It's a dimensionless
number between 0 and 1. Probability Density is obtained
by taking a derivative, so it has dimensions that are
inverse of whatever this x is. So if I change my variable
from meters to centimeters, let's say, the value
of this function would change by a factor of 100. And secondly, while
the Probability Density is positive, its
value is not bounded. It can be anywhere
that you like. One other, again,
minor definition is expectation value. So I can pick some
function of x. This could be x itself. It could be x squared. It could be sine x, x
cubed minus x squared. The expectation value
of this is defined by integrating the
Probability Density against the value
of the function. So essentially,
what that says is that if I pick some
function of x-- function can be positive,
negative, et cetera. So maybe I have a function
such as this-- then the value of x is random. If x is in this
interval, this would be the corresponding
contribution to f of x. And I have to look at
all possible values of x. Question? Now, very associated to this
is a change of variables. You would say that if x is
random, then f of x is random. So if I ask you what is
the value of x squared, and for one random
variable, I get this. The value of x
squared would be this. If I get this, the
value of x squared would be something else. So if x is random, f of x
is itself a random variable. So f of x is a random
variable, and you can ask what is the
probability, let's say, the Probability Density
Function that I would associate with the value of this. Let's say what's the
probability that I will find it in the interval between
small f and small f plus df. This will be Pf f of f. You would say that the
probability that I would find the value of the function
that is in this interval corresponds to finding a value
of x that is in this interval. So what I can do,
the probability that I find the value
of f in this interval, according to what I have here,
is the Probability Density multiplied by df. Is there a question? No. So the probability that
I'm in this interval translates to the probability
that I'm in this interval. So that's probability p of x dx. But that's boring. I want to look at the
situation maybe where the function is
something like this. Then, you say that f is
in this interval provided that x is either
here or here or here. So what I really need
to do is to solve f of x equals to f for x. And maybe there
will be solutions that will be x1,
x2, x3, et cetera. And what I need to
do is to sum over the contributions of
all of those solutions. So here, it's three solutions. Then, you would say
the Probability Density is the sum over i P
of xi, the xi by df, which is really the slopes. The slopes translate the
size of this interval to the size of that interval. You can see that here,
the slope is very sharp. The size of this
interval is small. It could be wider
accordingly, so I need to multiply by dxi by df. So I have to multiply by
dx by df evaluated at xi. That's essentially the value
of the derivative of f. Now, sometimes, it is easy
to forget these things that I write over here. And you would say,
well obviously, the probability of
something that is positive. But without being careful,
it is easy to violate such basic condition. And I violated it here. Anybody see where I violated it. Yeah, the slope
here is positive. The slope here is positive. The slope here is negative. So I am subtracting
a probability here. So what I really
should do-- it really doesn't matter whether the
slope is this way or that way. I will pick up
the same interval, so make sure you don't forget
the absolute values that go accordingly. So this is the
standard way that you would make change of variables. Yes? AUDIENCE: Sorry. In the center of that board,
on the second line, it says Pf. Is that an x or a times? PROFESSOR: In the
center of this board? This one? AUDIENCE: Yeah. PROFESSOR: So the
value of the function is a random variable, right? It can come up to be here. It can come up to be here. And so there is, as any other
one parameter random variable, a Probability Density
associated with that. That Probability Density
I have called P of f to indicate that
it is the variable f that I'm
considering as opposed to what I wrote
originally that was associated with the value of x. AUDIENCE: But what you have
written on the left-hand side, it looks like your
x [? is random. ?] PROFESSOR: Oh, this was
supposed to be a multiplication sign, so sorry. AUDIENCE: Thank you. PROFESSOR: Thank you. Yes? AUDIENCE: CP-- that function,
is this [INAUDIBLE]? PROFESSOR: Yes. So you're asking whether this--
so I constructed something, and my statement is that the
integral from minus infinity to infinity df Pf of f better be
one which is the normalization. So if you're asking
about this, essentially, you would say the
integral dx p of x is the integral dx
dP by dx, right? That was the definition p of x. And the integral
of the derivative is the value of the function
evaluated at its two extremes. And this is one minus 0. So by construction, it
is, of course, normalized in this fashion. Is that what you were asking? AUDIENCE: I was asking
about the first possibility of cumulative
probability function. PROFESSOR: So the
cumulative probability, its constraint is that
the limit as its variable goes to infinity,
it should go to 1. That's the normalization. The normalization here
is that the probability of the entire set is 1. Cumulative adds
the probabilities to be anywhere up to point x. So I have achieved being
anywhere on the line by going through this point. But certainly, the integral
of P of xdx is not equal to 1 if that's what you're asking. The integral of
small p of x is 1. Yes? AUDIENCE: Are we assuming
the function is invertible? PROFESSOR: Well, rigorously
speaking, this function is not invertible
because for a value of f, there are three
possible values of x. So it's not a function,
but you can certainly solve for f of x equals to
f to find particular values. So again, maybe it
is useful to work through one example of this. So let's say that you
have a probability that is of the form e to the minus
lambda absolute value of x. So as a function of x,
the Probability Density falls off exponentially
on both sides. And again, I have to ensure that
when I integrate this from 0 to infinity, I will get one. The integral from
0 to infinity is 1 over lambda,
from minus infinity to zero by symmetry
is 1 over lambda. So it's really I have to divide
by 2 lambda-- to lambda over 2. Sorry. Now, suppose I change variables
to F, which is x squared. So I want to know what
the probability is for a particular value of x
squared that I will call f. So then what I have to
do is to solve this. And this will give me x is minus
plus square root of small f. If I ask for what f of-- for
what value of x, x squared equals to f, then I have
these two solutions. So according to the
formula that I have, I have to, first of
all, evaluate this at these two possible routes. In both cases, I will get
minus lambda square root of f. Because of the absolute
value, both of them will give you the same thing. And then I have to look
at this derivative. So if I look at this, I can
see that df by dx equals to 2x. The locations that I have to
evaluate are at plus minus square root of f. So the value of the slope is
minus plus to square root of f. And according to that
formula, what I have to do is to put the inverse of that. So I have to put for one
solution, 1 over 2 square root of f. For the other one, I have to
put 1 over minus 2 square root of f, which would be a disaster
if I didn't convert this to an absolute value. And if I did convert that
to an absolute value, what I would get is lambda
over 2 square root of f e to the minus lambda root f. It is important to note
that this solution will only exist only if f is positive. And there's no solution
if f is negative, which means that if I
wanted to plot a Probability Density for this
function f, which is x squared as a function
of f, it will only have values for positive
values of x squared. There's nothing for
negative values. For positive values,
I have this function that's exponentially decays. Yet at f equals to 0 diverges. One reason I chose
that example is to emphasize that these
Probability Density functions can even go
all the way infinity. The requirement,
however, is that you should be able to integrate
across the infinity because integrating across the infinity
should give you a finite number less than 1. And so the type of divergence
that you could have is limited. 1 over square root of f is fine. 1/f is not accepted. Yes? AUDIENCE: I have a doubt
about [? the postulate. ?] It says that if you raise
the value of f slowly, you will eventually get to--
yeah, that point right there. So if the prescription that
we have of summing over the different roots, at
some point, the roots, they converge. PROFESSOR: Yes. AUDIENCE: So at some point,
we stop summing over 2 and we start summing over 1. It just seems a
little bit strange. PROFESSOR: Yeah. If you are up here, you have
only one term in the sum. If you are down here,
you have three terms. And that's really just
the property of the curve that I have drawn. And so over here, I
have only one root. Over here, I have three roots. And this is not surprising. There are many situations
in mathematics or physics where you encounter
situations where, as you change some parameters,
new solutions, new roots, appear. And so if this was really some
kind of a physical system, you would probably
encounter some kind of a singularity of phase
transitions at this point. Yes? AUDIENCE: But how does the
equation deal with that when [INAUDIBLE]? PROFESSOR: Let's see. So if I am approaching
that point, what I find is that the f by
the x goes to 0. So the x by df has some kind
of infinity or singularity, so we have to deal with that. If you want, we can
choose a particular form of that function and
see what happens. But actually, we have
that already over here because the function f
that I plotted for you as a function of x
has this behavior that, for some range of
f, you have two solutions. So for negative values
of f, I have no solution. So this curve, after
having rotated, is precisely an example
of what is happening here. And you see what the
consequence of that is. The consequence of that
is that as I approach here and the two solutions merge,
I have the singularity that is ultimately
manifested in here. So in principle, yes. When you make these
changes of variables and you have functions
that have multiple solution behavior like that, you
have to worry about this. Let me go down here. One other definition
that, again, you've probably seen, before
we go through something that I hope you
haven't seen, moment. A form of this expectation
value-- actually, here we did with x squared,
but in general, we can calculate the expectation
value of x to the m. And sometimes, that is called
mth moment is the integral 0 to infinity dx x
to the m p of x. Now, I expect that
after this point, you would have seen everything. But next one maybe
half of you have seen. And the next item,
which we will use a lot, is the characteristic function. So given that I have some
probability distribution p of x, I can calculate
various expectation values. I calculate the expectation
value of e to the minus ikx. This is, by definition
that you have, I have to integrate
over the domain of x-- let's say from minus infinity
to infinity-- p of x against e to the minus ikx. And you say, well, what's
special about that? I know that to be the
Fourier transform of p of x. And it is true. And you also know how to
invert the Fourier transform. That is if you know the
characteristic function, which is another name for the Fourier
transform of a probability distribution, you
would get the p of x back by the integral
over k divided by 2pi, the way that I chose the things,
into the ikx p tilde of k. Basically, this is the
standard relationship between these objects. So this is just a
Fourrier transform. Now, something that appears a
lot in statistical calculations and implicit in lots
of things that we do in statistical mechanics
is a generating function. I can take the characteristic
function p tilde of k. It's a function of this
Fourrier variable, k. And I can do an
expansion in that. I can do the expansion
inside the expectation value because e to the minus ikx I can
write as a sum over n running from 0 to infinity minus ik
to the power of m divided by n factorial x to the nth. This is the expansion
of the exponential. The variable here is x, so I can
take everything else outside. And what I see is that
if I make an expansion of the characteristic
function, the coefficient of k to the n up to some
trivial factor of n factorial will give me the nth moment. That is once you have
calculated the Fourrier transform, or the characteristic
function, you can expand it. And you can, from out
of that expansion, you can extract all the
moments essentially. So this is expansion
generates for you the moments, hence the generating function. You could even do
something like this. You could multiply e to the
ikx0 for some x0 p tilde of k. And that would be the
expectation value of e to the ikx minus x0. And you can expand
that, and you would generate all of the moments
not around the origin, but around the point x0. So simple manipulations of
the characteristic function can shift and give you
other set of moments around different points. So the Fourier transform,
or characteristic function, is the generator of moments. An even more
important property is possessed by the cumulant
generating function. So you have the
characteristic function, the Fourier transform. You take its log, so
another function of k. You start expanding this
function in covers of k. Add the coefficients of
that, you call cumulants. So I essentially repeated the
definition that I had up there. I took a log, and all I did
is I put this subscript c to go from moments to cumulants. And also, I have to start the
series from 1 as opposed to 0. And essentially, I can find the
relationship between cumulants and moments by
writing this as a log of the characteristic
function, which is 1 plus some n
plus 1 to infinity of minus ik to the n over n
factorial, the nth moments. So inside the log,
I have the moments. Outside the log, I
have the cumulants. And if I have a log
of 1 plus epsilon, I can use the expansion
of this as epsilon minus epsilon squared over 2 epsilon
cubed over 3 minus epsilon to the fourth over 4, et cetera. And this will enable me to
then match powers of minus ik on the left and powers
of minus ik on the right. You can see that the first
thing that I will find is that the expectation
value of x-- the first power, the first term that I have
here is minus ik to the mean. Take the log, I will get that. So essentially, what I get
is that the first cumulant on the left is the
first moment that I will get from the
expansion on the right. And this is, of course, called
the mean of the distribution. The second cumulant, I will
have two contributions, one from epsilon, the other from
minus epsilon squared over 2. And If you go through
that, you will get that it is expectation
value of x squared minus the average of x,
the mean squared, which is none other than the
expectation value of x around the mean squared, which
is clearly a positive quantity. And this is the variance. And you can keep going. The third cumulant is x cubed
minus 3 average of x squared average of x plus 2
average of x itself cubed. It is called the skewness. I don't write the
formula for the next one which is called
a [? cortosis ?]. And you keep going and so forth. So it turns out that this
hierarchy of cumulants, essentially, is a hierarchy
of the most important things that you can know about
a random variable. So if I tell you that the
outcome of some experiment is some number x,
distribute it somehow-- I guess the first thing that
you would like to know is whether the typical
values that you get are of the order of 1, are of
the order of million, whatever. So somehow, the
mean is something that tells you something that is
most important is zeroth order thing that you want to
know about the variable. But the next thing that
you might want to know is, well, what's the spread? How far does this thing go? And then the variance will tell
you something about the spread. So the next thing
that you want to do is maybe if given
the spread, am I more likely to get things
that are on one side or things that are on the other side. So the measure of its
asymmetry, right versus left, is provided by the third
cumulant, which is the skewness and so forth. So typically, the
very first few members of this hierarchy of
cumulants tells you the most important
information that you need about the probability. Now, I will mention
to you, and I guess we probably will deal
with it more next time around, the result that is in some
sense the backbone or granddaddy of all graphical expansions
that are carrying [INAUDIBLE]. And that's a relationship
between the moments and cumulants that I
will express graphically. So this is graphical
representation of moments in
terms of cumulants. Essentially, what I'm
saying is that you can go through the
procedure as I outlined. And if you want to calculate
minus ik to the fifth power so that you find the description
of the fifth cumulant in terms of the moment, you'll
have to do a lot of work in expanding the log and powers
of this object and making sure that you don't make any
mistakes in the coefficient. There is a way to
circumvent that graphically and get the relationship. So how do we do that? You'll represent nth cumulant
as let's say a bag of endpoints. So let's say this entity will
represent the third cumulant. It's a bag with three points. This-- one, two, three,
four, five, six-- will represent the
sixth cumulant. Then, the nth moment
is some of all ways of distributing end
points amongst bags. So what do I mean? So I want to calculate
the first moment x. That would correspond
to one point. And really, there's
only one diagram I can put the bag
around it or not that would correspond to this. And that corresponds
to the first cumulant, basically rewriting
what I had before. If I want to look at the second
moment, the second moment I need two points. The two points I can either
put in the same bag or I can put into two separate bags. And the first one
corresponds to calculating the second cumulant. The second term
corresponds to two ways in which their first
cumulant has appeared, so I have to squared x. if I want to calculate the
third moment, I need three dots. The three dots I can
either put in one bag or I can take one of them out
and keep two of them in a bag. And here I had the
choice of three things that I could've pulled out. Or, I could have all of them in
individual bags of their own. And mathematically, the first
term corresponds to x cubed c. The third term corresponds
to three versions of the variance times the mean. And the last term is
just the mean cubed. And you can massage
this expression to see that I get the expression
that I have for the skewness. I didn't offhand
remember the relationship that I have to write down
for the fourth cumulant. But I can graphically,
immediately get the relationship for
the fourth moment in terms of the fourth
cumulant which is this entity. Or, four ways that I
can take one of the back and maintain three in the bag,
three ways in which I have two bags of two, six ways in
which I can have a bag of two and two things that are
individually apart, and one way in which there
are four things that are independent of each other. And this becomes x to the fourth
cumulant, the fourth cumulant, 4 times the third
cumulant times the mean, 3 times the square
of the variance, 6 times the variance
multiplied by the mean squared, and the mean raised
to the fourth power. And you can keep going. AUDIENCE: Is the variance not
squared in the third term? PROFESSOR: Did I forget that? Yes, thank you. All right. So the proof of this is really
just the two-line algebra exponentiating these expressions
that we have over here. But it's much nicer to
represent that graphically. And so now you can go
between things very easily. And what I will show next time
is how, using this machinery, you can calculate any
moment of a Gaussian, for example, in just a
matter of seconds as opposed to having to do integrations
and things like that. So that's what we
will do next time will be to apply this machinery
to various probability distribution, such
as a Gaussian, that we are likely to
encounter again and again.

Calculus

The following content is
provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer
high quality educational resources for free.
To make a donation or to view additional materials from
hundreds of MIT courses, visit MIT OpenCourseWare at
ocw.mit.edu. OK, so remember we left things
with this statement of the divergence theorem.
So, the divergence theorem gives us a way to compute the
flux of a vector field for a closed surface.
OK, it says if I have a closed surface, s,
bounding some region, D, and I have a vector field
defined in space, so that I can try to compute
the flux of my vector field through my surface.
Double integral of F.dS or F.ndS if you want,
and to set this up, of course, I need to use the
geometry of the surface depending on what the surface
is. We've seen various formulas for
how to set up the double integral.
But, we've also seen that if it's a closed surface,
and if a vector field is defined everywhere inside,
then we can actually reduce that to a calculation of the
triple integral of the divergence of F inside,
OK? So,
concretely, if I use coordinates,
let's say that the coordinates of my vector field are,
sorry, the components are P, Q, and R dot ndS,
then that will become the triple integral of,
well, so, divergence is P sub x plus Q sub y plus R sub z.
OK, so by the way, how to remember this formula
for divergence, and other formulas for other
things as well. Let me just tell you quickly
about the del notation. So,
this guy usually pronounced as del,
rather than as pointy triangle going downwards or something
like that, it's a symbolic notation for an
operator. So, you're probably going to
complain about putting these guys into a vector.
But, let's think of partial with respect to x,
with respect to y, and with respect to z as the
components of some formal vector.
Of course, it's not a real vector.
These are not like anything. These are just symbols.
But, so see for example, the gradient of function,
well, if you multiply this vector by scalar,
which is a function, then you will get partial,
partial x of f, partial, partial y of f,
partial, partial z, f, well, that's the gradient.
That seems to work. So now, the interesting thing
about divergence is I can think of divergence as del dot a
vector field. See, if I do the dot product
between this guy and my vector field P, Q, R,
well, it looks like I will indeed get partial,
partial x of P plus partial Q partial y plus partial R partial
z. That's the divergence.
and of course, similarly, when we have two variables
only, x and y, we could have thought of the
same notation, just with a two component
vector, partial, partial x,
partial, partial y. So, now, this is like of
slightly limited usefulness so far.
It's going to become very handy pretty soon because we are going
to see curl. And, the formula for curl in
the plane was kind of complicated.
But, if you thought about it in terms of this,
it was actually the determinant of del and f.
And now, in space, we are actually going to do del
cross f. But, I'm getting ahead of
things. So, let's not do anything with
that. Curl will be for next week.
Just getting you used to the notation, especially since you
might be using it in physics already.
So, it might be worth doing. OK, so the other thing I wanted
to say is, what does this theorem say physically?
How should I think of this statement?
So, I think I said that very quickly at the end of last time,
but not very carefully. So, what's the physical
interpretation of a divergence field?
So, I want to claim that the
divergence of a vector field corresponds to what I'm going to
call the source rate, which is somehow the amount of
flux generated per unit volume. So, to understand what that
means, let's think of what's called an incompressible fluid.
OK, so an incompressible fluid is something like water,
for example, where a fixed mass of it always
occupies the same amount of volume.
So, guesses are compressible. Liquids are incompressible,
basically. So, if you have an
incompressible fluid flow -- -- well, so, again,
what that means is really, given mass occupies always a
fixed volume. Then, well, let's say that we
have such a fluid with velocity given by our vector field.
OK, so we're thinking of F as the velocity and maybe something
containing water, a pipe, or something.
So, what does the divergence theorem say?
It says that if I take a region in space,
let's call it D, sorry, D is the inside,
and S is the surface around it, well, so if I sum the
divergence in D, well, I'm going to get the flux
going out through this surface, S.
I should have mentioned it earlier.
The convention in the divergence theorem is that we
orient the surface with a normal vector pointing always outwards.
OK, so now, we know what flux means.
Remember, we've been describing, flux means how much
fluid is passing through this surface.
So, that's the amount of fluid that's leaving the region,
D, per unit time. And, of course,
when I'm saying that, it means I'm counting
everything that's going out of D minus everything that's coming
into D. That's what the flux measures.
So, now, if there is stuff coming into D or going out of D,
well, it must come from somewhere.
So, one possibility would be that your fluid is actually
being compressed or expanded. But, I've said,
no, I'm looking at something like water that you cannot
squish into smaller volume. So, in that case,
the only explanation is that there is something it here that
actually is sucking up water or producing more water.
And so, integrating the divergence gives you the total
amount of sources minus the amount of syncs that are inside
this region. So, the divergence itself
measures basically the amount of sources or syncs per unit volume
in a given place. And now, if you think about it
that way, well,
it's basically the divergence theorem is just stating
something completely obvious about all the matter that is
leaving this region must come from somewhere.
So, that's basically how we think about it.
Now, of course, if you're doing 8.02,
then you might actually have seen the divergence theorem
already being used for things that are more like force fields,
say, electric fields and so on. Well, I'll try to say a few
things about that during the last week of classes.
But, then this kind of interpretation doesn't quite
work. OK, any questions,
generally speaking, before we move on to the proof
and other applications? Yes?
Oh, not the gradient. So, yeah, the divergence of F
measures the amount of sources or syncs in there.
Well, what makes it happen? If you want,
in a way, it's this theorem. Or, in another way,
if you think about it, try to look at your favorite
vector fields and compute their divergence.
And, if you take a vector field where maybe everything is
rotating, a flow that's just rotating
about some axis, then you'll find that its
divergence is zero. If you, sorry?
No, divergence is not equal to the gradient.
Sorry, there's a dot here that maybe is not very big,
but it's very important. OK, so you take the divergence
of a vector field. Well, you take the gradient of
a function. So, if the gradient of a
function is a vector, the divergence of a vector
field is a function. So, somehow these guys go back
and forth between. So, I should have said,
with new notations comes new responsibility.
I mean, now that we have this nice,
nifty notation that will let us do gradient divergence and later
curl in a unified way, if you choose this notation you
have to be really, really careful what you put
after it because otherwise it's easy to get completely confused.
OK, so divergence and gradients are completely different things.
The only thing they have in common is that both are what's
called a first order differential operator.
That means it involves the first partial derivatives of
whatever you put into it. But, one of them goes from
functions to vectors. That's gradient.
The other one goes from vectors to functions.
That's divergence. And, curl later will go from
vectors to vectors. But, that will be later.
Let's see, more questions? No?
OK, so let's see, so how are we going to actually
prove this theorem? Well, if you remember how we
prove Green's theorem a while ago, the answer is we're going
to do it exactly the same way. So, if you don't remember,
then I'm going to explain. OK, so the first thing we need
to do is actually a simplification.
So, instead of proving the divergence theorem,
namely, the equality up there, I'm going to actually prove
something easier. I'm going to prove that the
flux of a vector field that has only a z component is actually
equal to the triple integral of, well, the divergence of this is
just R sub z dV. OK, now, how do I go back to
the general case? Well, I will just prove the
same thing for a vector field that has only an x component or
only a y component. And then, I will add these
things together. So, if you think carefully
about what happens when you evaluate this,
you will have some formula for ndS,
and when you do the dot product,
you'll end up with the sum, P times something plus Q times
something plus R times something.
And basically, we are just dealing with the
last term, R times something, and showing that it's equal to
what it should be. And then, we the three such
terms together. We'll get the general case.
OK, so then we get the general case by summing one such
identity for each component. I should say three such
identities, one for each component, whatever.
Now, let's make a second simplification because I'm still
not feeling confident I can prove this right away for any
surface. I'm going to do it first or
what's called a vertically simple region.
OK, so vertically simple means it will be something which I can
setup an integral over the z variable first easily.
So, it's something that has a bottom face, and a top face,
and then some vertical sides. OK, so let's say first what
happens if the given region, D, is vertically simple.
So, vertically simple means it looks like this.
It has top. It has a bottom.
And, it has some vertical sides. So, if you want,
if I look at it from above, it projects to some region in
the xy plane. Let's call that R.
And, it lives between the top face and the bottom face.
Let's say the top face is z equals z2 of (x,
y). Let's say the bottom face is z
equals z1(x, y). OK, and I don't need to know
actual formulas. I'm just going to work with
these and prove things independently of what the
formulas will be for these functions.
OK, so anyway, a vertically simple region is
something that lives above a part of the xy plane,
and is between two graphs of two functions.
So, let's see what we can do in that case.
So, the right-hand side of this equality, so that's the triple
integral, let's start computing it.
OK, so of course we will not be able to get a number out of it
because we don't know, actually, formulas for
anything. But at least we can start
simplifying because the way this region looks like,
I should say this is D, tells me that I can start
setting up the triple integral at least in the order where I
integrate first over z. OK, so I can actually do it as
a triple integral with Rz dz dxdy or dydx,
doesn't matter. So, what are the bounds on z?
See, this is actually good practice to remember how we set
up triple integrals. So, remember,
when we did it first over z, we start by fixing a point,
x and y, and for that value of x and y,
we look at a small vertical slice and see from where to
where we have to go. Well, we start at z equals
whatever the value is at the bottom, so, z1 of x and y.
And, we go up to the top face, z2 of x and y.
Now, for x and y, I'm not going to actually set
up bounds because I've already called R the quantity that I'm
integrating. So let me change this to,
let's say, U or something like that.
If you already have an R, I mean, there's not much risk
for confusion, but still.
OK, so we're going to call U the shadow of my region instead.
So, now I want to integrate over all values of x and y that
are in the shadow of my region. That means it's a double
integral over this region, U, which I haven't described to
you. So, I can't actually set up
bounds for x and y. But, I'm going to just leave it
like this. OK,
now you see, if you look at how you would
start evaluating this, well, the inner integral
certainly is not scary because you're integrating the
derivative of R with respect to z,
integrating that with respect to z.
So, you should get R back. OK, so triple integral over D
of Rz dV becomes, well, we'll have a double
integral over U of, so, the inner integral becomes
R at the point on the top. So, that means,
remember, R is a function of x, y, and z.
And, in fact, I will plug into it the value
of z at the top, so, z of xy minus the value of
R at the point on the bottom, x, y, z1 of x,
y. OK, any questions about this?
No? Is it looking vaguely
believable? Yeah? OK.
So, now, let's compute the other side because here we are
stuck. We won't be able to do anything
else. So, let's look at the flux
integral. OK, we have to look at the flux
of this vector field through the entire surface,
S, which is the whole boundary of D.
So, that consists of a lot of pieces, namely the top,
bottom, and the sides. OK, so the other side -- So,
let me just remind you, S is bottom plus top plus side
of this vector field, dot ndS equals,
OK, so what do we have? So first, we have to look at
the bottom. No, let's start with the top
actually. Sorry.
OK, so let's start with the top. So, just remind you,
let's do all of them. So, let's look at the top first.
So, we need to set up the flux integral for a vector field dot
ndS. We need to know what ndS is.
Well, fortunately for us, we know that the top face is
going to be the graph of some function of x and y.
So, we've seen a formula for ndS in this kind of situation,
OK? We have seen that ndS,
sorry, so, just to remind you this is the graph of a function
z equals z2 of x, y.
So, we've seen ndS for that is negative partial derivative of
this function with respect to x, negative partial z2 with
respect to y, one, dxdy.
OK, and, well, we can't compute these guys,
but it's not a big deal because if we do the dot product with dot ndS,
that will give us, well, if you dot this with
zero, zero, R, these terms go away.
You just have R dxdy. So, that means that the double
integral for flux through the top of R vector field dot ndS
becomes double integral of the top of R dxdy.
Now, how do we evaluate that, actually?
Well, so R is a function of x, y, z.
But we said, we have only two variables that
we're going to use. We're going to use x and y.
We're going to get rid of z. How do we get rid of z?
Well, if we are on the top surface, z is given by this
formula, z2 of x, y.
So, I plug z equals z2 of x, y into the formula for R,
whatever it may be. Then, I integrate dxdy.
And, what's the range for x and y?
Well, my surface sits exactly above this region U in the xy
plane. So, it's double integral over
U, OK? Any questions about how I set
up this flux integral? No?
OK, let me close the door, actually.
OK, so we've got one of the two terms that we had over there.
Let's try to get the others. [LAUGHTER] No comment.
OK, so, we need to look, also, at the other parts of our
surface for the flux integral. So, the bottom,
well, it will work pretty much the same way,
right, because it's the graph of a function,
z equals z1 of x, y.
So, we should be able to get ndS using the same method,
negative partial with respect to x, negative partial with
respect to y, one dxdy.
Now, there's a small catch. OK, we have to think of it
carefully about orientations. So,
remember, when we set up the divergence theorem,
we need the normal vectors to point out of our region,
which means that on the top surface,
we want n pointing up. But, on the bottom face,
we want n pointing down. So, in fact,
we shouldn't use this formula here because that one
corresponds to the other orientation.
Well, we could use it and then subtract, but I was just going
to say that ndS is actually the opposite of this.
So, I'm going to switch all my signs.
OK, that's the other side of the formula when you orient your
graph with n pointing downwards. Now, if I do things the same
way as before, I will get that &lt;0,0,
R&gt; dot ndS will be negative R dxdy.
And so, when I do the double integral
over the bottom, I'm going to get the double
integral over the bottom of negative R dxdy,
which, if I try to evaluate that,
well, I actually will have to integrate.
Sorry, first I'll have to substitute the value of z.
The value of z that I will want to plug into R will be given by,
now, z1 of x, y.
And, the bounds of integration will be given,
again, by the shadow of our surface, which is,
again, this guy, U.
OK, so we seem to be all set, except we are still missing one
part of our surface, S, because we also need to look
at the sides. Well, what about the sides?
Well, our vector field, ,
is actually vertical. It's parallel to the z axis.
OK, so my vector field does something like this everywhere.
And, why that makes it very interesting on the top and
bottom, that means that on the sides, really not much is going
on. No matter is passing through
the vertical sides. So, the side -- The sides are
vertical. So, &lt;0,0,
R&gt; is tangent to the side, and therefore,
the flux through the sides is just going to be zero.
OK, no calculation needed. That's because, of course,
that's the reason why a simplified first things so that
my vector field would only have a z component,
well, not just that but that's the main place where it becomes
very useful. So, now, if I compare my double
integral and, sorry, my triple integral and
my flux integral, I get that they are,
indeed, the same. Well, that's the statement of
the theorem we are trying to prove.
I shouldn't erase it, OK? [LAUGHTER]
So, just to recap, we've got a formula for the
triple integral of R sub z dV. It's up there at the very top.
And, we got formulas for the flux through the top and the
bottom, and the sides. And, when you add them
together, you get indeed the same
formula, top plus bottom -- -- plus
sides of, OK, and so we have, actually,
completed the proof for this part.
Now, well, that's only for a vertically simple region,
OK? So, if D is not vertically
simple, what do we do? Well, we cut it into vertically
simple pieces. OK so, concretely,
I wanted to integrate over a solid doughnut.
Then, that's not vertically simple because here in the
middle, I have not only does top in this bottom,
but I have this middle face. So, the way I would cut my
doughnut would be probably I would slice it not in the way
that you'd usually slice the doughnut or a bagel,
but at it's probably more spectacular if you think that
it's a bagel. Then, a mathematician's way of
slicing it is like this into five pieces, OK?
And, that's not very convenient for eating,
but that's convenient for integrating over it because now
each of these pieces has a well-defined top and bottom
face, and of course you've introduced
some vertical sides for two reasons.
One is that we've said the flux through them is zero anyway.
So, who cares? Why bother?
But, also, if you sum the flux through the surface of each
little piece, well, you will see that you
will be integrating twice over each of these vertical cuts.
Once, when you do this piece, you will be taking the flux
through this red guy with normal vector pointing to the right,
and once, when you take this middle little piece,
you will be taking the flux through that cut surface again,
but now with normal vector pointing the other way around.
So, in fact, you'll be summing the flux
through these guys twice with opposite orientations.
They cancel out. Well, and again,
because of what you are doing actually, the integral was just
zero anyway. So, it didn't matter.
But, even if it hadn't simplified, that would do it for
us. OK, so that's how we do it with
the general region. And then, as I said at the
beginning, when we can do it for a vector
field that has only a z component,
well, we can also do it for a vector field that has only an x
or only a y component. And then, we sum together and
we get the general case. So, that's the end of the proof.
OK, so you see, the idea is really the same as
for Green's theorem. Yes?
Oh, there's only four pieces, thank you.
Yes, there's three kinds of mathematicians:
those who know how to count, and those who don't.
Well, OK. So, OK, now I hope that you can
see already the interest of this theorem for the divergence
theorem for computing flux integrals just for the sake of
computing flux integrals like might happen on the problem set
or on the next test. But let me tell you also why
it's important physically to understand equations that had
been observed empirically well before they were actually
understood in terms of how things go.
So, let's look at something called the diffusion equation.
So, let me explain to you what it does.
So, the diffusion equation is something that governs,
well, what's called diffusion. Diffusion is when you have a
fluid in which you are introducing some substance,
and you want to figure out how that thing is going to spread
out, the technical term is diffuse,
into the ambient fluid. So, for example,
that governs the motion of, say, smoke in the air,
or if you put dye in the solution or things like that.
That will tell you, say that you drop some ink into
a glass of water. Well, you can imagine that
obviously it will get diluted into there.
And, that equation will tell you how exactly over time this
thing is going to spread out and start filling the entire glass.
So, what's the equation? Well, we need,
first, to know what the unknown will be.
So, it's a partial differential equation, OK?
So the unknown is a function, and the equation will relate
the partial derivatives of that function to each other.
So, u, the unknown, will be the concentration at a
given point. And, of course,
that depends on the point where you are.
So, that depends on x, y, z, the location where you
are. But, since the goal is also to
understand how things spread over time, it should also depend
on time. Otherwise, we're not going to
get very far. And, in fact,
what the equation will give us is the derivative of u with
respect to t. It will tell us how the
concentration at a given point varies over time in terms of how
the concentration varied in space.
So, it will relate partial u partial t to partial derivatives
with respect to x, y, and z. [APPLAUSE]
OK, [LAUGHTER] so what's the equation?
The equation is partial u partial t equals some constant.
Let me call it constant k times something I will call del
squared u, which is also called the Laplacian of u,
and what is that? Well,
that means, OK, so just to scare you,
del squared is this, which means it's the divergence
of gradient u that we've used this notation for gradient.
OK, so if you actually expand it in terms of variables,
that becomes partial u over partial x squared plus partial
squared u over partial y squared plus partial squared u over
partial z squared. OK, so the equation is this
equals that. OK, so that's a weird looking
equation. And, I'm going to have to
explain to you, where does it come from?
OK, but before I do that, well, let me point out actually
that the equation is not just the diffusion equation.
It's also known as the heat equation.
And, that's because exactly the same equation governs how
temperature changes over time when you have,
again, so, sorry I should have been actually more careful.
I should have said this is in air that's not moving,
OK? OK, and same thing with the
solution. If you drop some ink into your
glass of water, well, if you start stirring,
obviously it will mix much faster than if you don't do
anything. OK, so that's the case where we
don't actually, the fluid is not moving.
And, the heat equation now does the same, but for temperature in
a fluid that's at rest, that's not moving.
It tells you how the heat goes from the warmest parts to the
coldest parts, and eventually temperatures
should somehow even out. So, in the heat equation,
that would just be, this u would just measure the
temperature for temperature of your fluid at a given point.
Actually, it doesn't have to be a fluid.
It could be a solid for that heat equation.
So, for example, say that you have a big box
made of metal or something, and you do some heating at one
side. You want to know how quickly is
the other side going to get hot? Well, you can use the equation
to figure out, you know, say that you have a
metal bar, and you keep one side at 400 because it's in your
oven. How quickly will the other side
get warm? OK, so it's the same equation
for both phenomena even though they are, of course,
different phenomena. Well, the physical reason why
they're the same is actually that phenomena are different,
but not all that much. They involve,
actually, how the atoms and molecules are actually moving,
and hitting each other inside this medium.
OK, so anyway, what's the explanation for
this? So, to understand the
explanation, and given what we've been doing,
we should have a vector field in there.
So, I'm going to think of the flow of, well,
let's imagine that we are doing motion of smoke in air.
So, that's the flow of the smoke: that means at every
point, it's a vector whose direction tells me in which
direction the smoke is actually moving.
And, its magnitude tells me how fast it's moving,
and also what amount of smoke is moving.
So, there's two things to understand.
One is how the disparities in the concentration between
different points causes the flow to be there,
how smoke will flow from the regions where there's more smoke
to the regions where there's less smoke.
And, that's actually physics. But, in a way,
it's also common sense. So, physics and common sense
tell us that the smoke will flow from high concentration towards
low concentration regions. OK, so if we think of this
function, U, that measures concentration,
that means we are always going to go in the direction where the
concentration decreases the fastest.
Well, what's that? That's negative the gradient.
So, F is directed along minus gradient u.
Now, how big is F going to be? Well, they are,
you have to come up with some intuition for how the two are
related. And, the easiest relation I can
think of is that they might be just proportional.
So, the steeper the differences in concentration,
the faster the flow will be, or the more there will be flow.
And, if you try to think about it as molecules moving in random
directions, you will see it makes actually complete sense.
Anyway, it should be part of your physics class,
not part of what I'm telling you.
So, I'm just going to accept that the flow is just
proportional to the gradient of u.
So, if you want, the differences between
concentrations of different points are very small,
then the flow will be very gentle.
And, if on the other hand you have huge disparities,
then things will try to even out faster.
OK, so that's the first part. Now, we need to understand the
second part, which is once we know how flow is going,
how does that affect the concentration?
If smoke is going that way, then it means the concentration
here should be decreasing. And there, it should be
increasing. So, that's the relation between
F and partial u partial t. At that part is actually math,
namely, the divergence theorem. So, let's try to understand
that part more carefully. So, let's take a small piece of
a small region in space, D, bounded by a surface,
S. So, I want to figure out what's
going on in here. So, let's look at the flux out
of D through S. Well, we said that this flux
would be given by double integral on S of F dot n dS.
So, this flux measures how much smoke is passing through S per
unit time. That's the amount of smoke
through S per unit time. But now, how can I compute that
differently? Well, I can compute it just by
looking at the total amount of smoke in this region,
and seeing how much it changes. If I'm gaining or losing smoke,
it means it must be going up there.
Well, unless I have a smoker in here, but that's not part of the
data. So,
that should be, sorry, that's the same thing as the
derivative with respect to t of the total amount of smoke in the
region, which is given by the triple
integral of u. If I integrate the
concentration of smoke, which means the amount of smoke
per unit volume over d, I will get the total amount of
smoke in d, except,
well, let's see.
This flux is counted positively if we go out of d.
So, actually, it's minus the derivative.
This is the amount of smoke that we are losing per unit
time. OK, so now we are almost there.
Well, let me actually -- Because we know yet another way
to compute this guy using the divergence theorem.
Right, so this part here is just common sense and thinking
about what it means. The divergence theorem tells me
this is also equal to the triple integral, d, of div f dV.
So, what I got is that the triple integral over d of div F
dV equals this derivative. Well, let's think a bit about
this derivative so, see, you are integrating
function over x, y, and z.
And then, you are differentiating with respect to
t. I claim that you can actually
switch the order in which you do things.
So, when we think about it, is, here, you are taking the
total amount of smoke and then see how that changes over time.
So, you're taking the derivative of the sum of all the
small amounts of smoke everywhere.
Well, that will be the sum of the derivatives of the amounts
of smoke inside each little box. So, we can actually move the
derivatives into the integral. So, let's see,
I said minus d dt of triple integral over d udV.
And, now I'm saying this is the same as the triple integral in d
of du dt dv. And the reason why this is
going to work is you should think of it as d dt of a sum of
u of some values. You plug in the values of your
points at that given time times the small volume.
You sum them, and then you take the
derivative. And now, you see,
the derivative of this sum is the sum of the derivatives.
yi, zi, t, so, if you think about what the
integral measures, that's actually pretty easy.
And it's because this variable here is not the same as the
variables on which we are integrating.
That's why we can do it. OK, so now, if we have this for
any region, d. So, we have a function of x,
y, z, t, and we have another function here.
And whenever we integrate them anywhere, we get the same
answer. Well, that must mean they're
the same. Just think of what happens if
you just take d to be a tiny little box.
You will get roughly the value of div f at that point times the
volume of the box. Or, you will get roughly the
value of du dt at that point times the value of a little box.
So, the values must be the same. Well, let me actually explain
that a tiny bit better. So, what I get is that one
over, let me divide by the volume of D, sorry.
I promise, I'm done in a minute. Is the same thing as one over
volume D of negative du dt, dV.
So, that means the average value,
OK, maybe that's the best way of telling it,
the average of div f in D is equal to the average of minus
partial u partial t in D. And, that's true for any
region, D, not just for some regions, but for,
really, any region I can think of.
So, the outcome is that actually the divergence of f is
equal to minus du dt. And, that's another way to
think about what divergence means.
The divergence, we said, is how much stuff is
actually expanding, flowing out.
That's how much we're losing. And so, now,
if you combine this with that, you will get that du dt is
minus divergence f, which is plus K del squared u.
OK, so you combine this guy with that guy,
and you get the diffusion equation.

Math for Eng.

  In this problem, we're going
to look at the probability that when you take a stick and
break it into three pieces randomly that these three pieces
can actually be used to form a triangle. All right, so we start
out with a stick of unit length, so-- length 1. And we'll choose a point along
the stick to break. And we'll choose that point
uniformly at random. So let's say that we chose it
here, that was the point where we'll break it. And then independently of this
first choice we'll again choose a second point
to break it. Again, uniformly at random
along the entire stick. So let's say the second point
we chose was here. So what we have now is, we'll
break it here, here, and so we'll have three pieces-- the first one, the left
one and the right one. And we want to know, what's the
probability that when you take these three pieces you
could form a triangle? So the first thing we
should ask ourselves is, what do you need-- what conditions must be
satisfied in order to actually be able to form a triangle
with three pieces? So you could think about, what
would stop you from being able to do that? Well, one possibility is
that you have pieces that look like this.   So in that case you would
try to form something that looks like this. But you can't get a triangle
because these two pieces are too short and they can't
touch each other. So actually the condition that
must be satisfied is that when you take any two of the three
pieces, their combined length has to be greater than
the length of the remaining third piece. And that has to be true
for any two pieces.   And really that's just so that
any two pieces, they can touch and still form a triangle. So let's try to add some
probability to this. So we have a unit
length stick. So let's actually give
a coordinate system. The stick goes from 0 to 1. And let's say that we break
it at these two points.   So the first point where we
choose, we'll call that x. So that's the first point that
we choose to break it. And then the second point we
choose, we'll call that y. Now note that I've drawn it so
that x is to the left of y. But it could actually be the
case that the first point I chose is here and the
second point that I chose is to the left. But for now, let's
first assume that this scenario holds. That the first point is to the
left of the second point. So under this assumption,
we can see that-- from the definition of these
random variables-- we can actually see that the
lengths are given by these three lengths.   So the lengths are x, the left
most piece has length x. The second, middle piece
has length y minus x. And the last piece has
length 1 minus y.   And now let's recall our
three conditions. So the conditions were that any
two of these, the sum of any two lengths, has
to be at least-- has to be greater than the
length of the third piece. So let's do these together. So x plus y minus x has to be
greater than 1 minus y. So with these two pieces you
can cover this third piece. We also need that with the first
and third pieces, we can cover the middle piece. And we need with the second and
third pieces, we can cover the first piece. Now this looks kind of messy,
but in fact we can actually simplify this. So this actually simplifies. x minus x, that disappears. And so this actually simplifies
to 2y has to be at least 1. Or even more simply, y has
to be greater than 1/2.   What about this one? This one, we can rearrange
things again. x we can move over. y we can move over here. And we get that 2x plus 1 has
to be greater than 2y. Or put in other words, y is
less than x plus 1/2.   And for the last one, again
we can simplify. The y's cancel each other out. And we're left with
2x is less than 1.   Or x is less than 1/2.   So these are our three
conditions that need to be satisfied. So now we just have to figure
out what's the probability that this is actually
satisfied? Now let's go back to original
definition and see what are the actual distributions
for these random variables, x and y. Remember, we defined them to
be x is the location of the first break and y is the
location of the second break. And as we said in the problem,
these are chosen uniformly at random and they're
independent. And so we can actually draw
out their joint PDF. So x and y, you can cover
any point in the square. And moreover, it's actually
uniform within the square. Because each one is chosen
uniformly at random and they're independent. So it's anywhere in here. And so what do we need to do? We just need to identify, what
is the probability that these three conditions hold?   Rewrite this, line these up. So these are our three
conditions that we need. And now remember, we're still
working under the assumption that the first point that we
chose is actually to the left of the second point. So what does that mean? That means that we are
actually in this top triangle, top half-- x is less than y. All right, so what do we need? We need y to be at least
1/2, so here's 1/2. So we need y to be
above this line. We need x to be less than 1/2. So we need x to be to
the left of here. So now so far we're stuck
in this upper square. And the last thing we need is y
to be less than x plus 1/2. What is y? The line y equals x and a 1/2,
x plus 1/2, is this one. So y has to be less than that,
so it would have to be in this triangle here. So these three conditions tell
us that in order for us to have a triangle we need to for
x and y to fall jointly in this small triangle here. Now because the joint
distribution is uniform, we know that the density
is just 1, right? Because the area
here is just 1. So the height is
just 1 as well. And so the density, or the
probability of falling within this small triangle, is just
going to be also the area of this triangle. And what is the area
of this triangle? Well, you can fit 8 of these
triangles in here, or you could think of it as 1/2
times 1/2 times 1/2. So the area is just 1/8. So assuming that x is less than
y, then the probability of forming a triangle is 1/8. Now, that's only half
this story, though. Because it's possible that when
you chose these two break points that we actually had
the opposite result. That x, the point that you
chose first, falls to the right of the point that
you chose second. In which case everything
kind of flips. Now we assume that y is less
than x, which means that now we're in this lower triangle
in the square. Now we can go through this
whole exercise again. But really, what we can see is
that all we've really done is just swap the names. Instead of having x and y we now
call x-- we call x y and we call y x. And so if we just swap names,
we can see that-- let's just fast forward through
all these steps and see that we could just swap
names here, too, as well, in the three conditions. So instead of needing y to be
greater than 1/2, we just need x to be greater than 1/2. Instead of having x less than
1/2, we need y less than 1/2. We also swap this. So we need x to be less than y
plus 1/2 or y is greater than x minus 1/2. All right, now let's figure out
what this corresponds to. We need x to be greater than
1/2, so it needs to be to the right of here. We need y to be less than
1/2, so we need it to be below this line. And we need y to be greater
than x minus 1/2. What is the line y equals
x minus 1/2? That is this line here. And we need y to be greater than
that, so it needs to be above this line. And so we get that this is the
triangle, the small triangle that we need in this case. And notice that it's exactly
the same area as this one, right? And so we get another
contribution of 1/8 here. So the final answer is
1/8 plus 1/8 is 1/4. So the probability of forming
a triangle using this three pieces is exactly 1/4. And so notice that we've done
is, you've set things up very methodically in the beginning
by assigning these random variables. And you consider different
cases. Because you don't actually know
the order in which x and y might fall, let's just assume
that one particular order and work from there. And then do the other
case, as well. And it just so happened that
because of the symmetry of the problem the second case was
actually very simple. We could just see that it is
actually symmetric and so we get the same answer. So this is kind of an
interesting problem because it's actually a practical
application of something that you might actually do. And you can see that just by
applying these probability concepts you can actually--

Probability

The following content is
provided under a Creative Commons license. Your support will help
MIT OpenCourseWare continue to offer high quality
educational resources for free. To make a donation or
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. ERIK DEMAINE: Today,
we continue our theme on integer data structures. And we're going to cover
one data structure called fusion trees, which introduces
a bunch of cool concepts using, you might call bit tricks,
using the word level parallelism to do lots of great things. To do those great things we
need to do something called sketching, which is essentially
taking w-bit integers and making them
less than w-bits, but still having enough
stuff that we care about. And then we can actually
compute those sketches using integer multiplication
in a very clever way. And given a bunch of
these slightly less than w bit numbers we can
compare several for the price of one operation
as a parallel comparison. And then a particularly
nifty thing, which is new this time I
haven't covered it before, is how to compute
the leftmost 1 bit in a w-bit integer in
constant time, all very cool. And so we're going to
combine all these techniques and get fusion trees. What do fusion trees
give us in the end? Basically, the goal is to get
log base w of n, predecessor and successor. So we're again talking about
the predecessor problem. We did log of w last time with
van Emde Boas and y-fast tries. So then with the two
of them together, we get log w and log base w of n. The min of those
two things is always at most root log n, but
sometimes much better than that. So fusion trees are
great when w is big, van Emde Boas was good
when w was smallish. Like poly log n, van
Emde Boas is optimal. Here, we're thinking
about w being closer to n, or maybe n to the
epsilon or something. Then we get constant time,
if it's n to the epsilon. Anyway, the version we're
going to cover is static. And it's linear space, and
it runs on the good old word RAM, which you may
recall is regular C operations on w-bit words.
w is at least log n, so you can do random access. And anything else? I think that's the
version we will cover. And that is the original
version of Fredman and Willard, and it was invented
in 1990, which was one year after the
cold fusion debacle. So this may be where
this name came from. There's other reasons it might
be called fusion trees, which we'll get to. Cool. So now, there's other
versions of fusion trees which I will not cover, but
just so you know about them. And in some sense we
will pseudo cover them. There's a version
for an AC0 RAM. This is a model we
haven't talked about. It's another version of
a trans-dichotomous RAM, somewhere off to the
side next to word RAM. AC0 is a circuit model. And it's basically any constant
depth circuit of unbounded fan in and fan out. And so in particular
what AC0 forbids are operations like
multiplication. I think this used to be a
bigger deal than it is today. I think multiplication used
to be in order of magnitude slower than integer addition. I checked on current
Intel architecture. It's about 3 times
slower than an addition, because of pipelining a
lot of things get cheaper. But in some theoretical
sense, multiplication is worse than a lot
of other operations, because you need to have
a log and depth circuit. So you can't quite get
as much parallelism as you can with
integer addition. If you don't know
about circuit models, don't worry about it too much. But AC0 means no multiplication,
sort of simpler operations. The weird thing
about the AC0 RAM is it lets you do weird things,
which are AC0, but are not in C. So you could look
at the intersection of AC0 RAM and word RAM, and that
would basically be word RAM without multiplication. But AC0 RAM allows
other operations as long as there's some
circuit that can compute them. Sometimes they're
reasonable operations like most significant set
bit is an AC0 operation. So, you don't have to do any
work to get this operation. So, in some sense this
makes your life easier. In other ways it makes
your life harder, because there is
no multiplication and we're going to
use multiplication to compute sketches
and other things. So it's both better and worse. This is by Anderson
and others a few years after the original fusion trees. More interesting
is that there are-- so this is a version
of fusion trees. More interesting are the dynamic
versions of fusion trees. So there's one that
the first version-- it's actually published later. There's a general trick
for dynamizing static data structures. We actually saw one. You may recall weight balanced
search trees was a way to dynamize a static
data structure. It turns out it doesn't work
so great on fusion trees. Because the time to build a
fusion tree is polynomial. It's not linear. And so that's kind of annoying. You need polynomial time
to build this thing. So weight balance is not enough
to slow down the rebuilds. There's another structure,
which we won't cover, called exponential search trees. It has this log log n overhead. But other than that, it gives
you a nice time dynamization. So these are updates. There's another version
which uses hashing, and achieves log base w
expected time for updates. This is by Raman. And so this gives you matching
the fusion tree query bound, you can do inserts and deletes,
the same amount of time if you allow expected. And essentially the idea is to
do sketches more like hashing. I mean, think of sketching
as just hashing down to a smaller universe. And in expectation
that will work well, although it's open,
actually, whether you can achieve this bound
with high probability. So it's an interesting
open question. So that's the various
versions of fusion trees. But we're going to just
cover the static ones, because they're
interesting enough. Dynamic ones are not that
much harder than the regular. So how do we achieve
log base w of n? Well we've already seen B-trees
which achieve log base b of n. So presumably it's
the same idea, and indeed that's what we'll do. What we want is a B-tree with
branching factor ideally w. We can't quite achieve w though. So it's going to be w to
some small constant power. And 1/5 is the one that
I'll use in this lecture. You can improve it or
make it worse, up to you. But any constant
up here will do. Because then log
base w to the 1/5 is going to be five
times log base w of n. So we have a node. So it has branching
factor w to the 1/5. Then the height of
the tree, of course, is theta log base w of n. So that's all good. But now what we need to
do is, doing a predecessor search given a node, we need to
decide in constant time which branch has our answer. So normally in a B-tree you
would read in all these keys, and then compare your
item to all of them, and then decide which way to go. Because in a B-tree we can
read all of these items in one operation. Now here, is that possible? Think about it. You've got each of these
keys is w bits long. There's w to the 1/5 of them. So the total number
of bits in the node to store all those keys is w to
the 1 plus 1/5, which is a lot. There's no way you can read all
those bits in constant time. You can only read order
w bits in constant time. So we can't look at them all. Somehow, we still have to figure
out which way to go correctly in constant time. So this is the idea
of a fusion node. We need to be able to store k,
which is order w to the 1/5. Keys, I'm going to
give them some names, x0 up to xk minus 1. Assume that they've
been presorted. We can preprocess and
do all those things. I'm going to go constant time,
predecessor and successor, and it's going to
be linear space, and it's going to require
polynomial preprocessing. And this is the annoying part. If this was just
k preprocessing, it would be easy to make
fusion trees dynamic. But it's k to squared,
or k cubed or something, depending on how fancy you are. It's not known how to
do it in linear time. So that's really our goal. If we can implement
fusion tree nodes and do constant time predecessor
on this small value of n basically, when n is
only w to the 1/5. If we can do constant
time for that, then by plugging in B-trees
we get the log base w of n for arbitrary values of n. So it's really all
about a single mode and doing that fast. So, the rest of the
lecture will be about that. So, I want to introduce
this idea of sketching. And to do that
I'm going to think about what it takes
to distinguish this small number of keys. High level ideas, well
we've got w to the 1/5 keys. Each of them is w bits. Do you really need all
w bits for each of them? It seems a little excessive. If there's only w to the 1/5,
you should only need about w to the 1/5 bits of each
to distinguish them all. So that's the intuition. And indeed, you can
formalize that intuition by viewing each of the keys
as a path in a binary tree. So this represents the
bit string 01011101. 0 means left. 1 means right. This is a transformation
we'll use a lot. So maybe that's in your set. Maybe this other bit
string is in your set. Maybe this bit string-- oh, I've got to make these
the same height which is a little challenging. So maybe those are the three. Suppose you just have these
three bit strings, w bit strings in your set. So this is a tree of height w. Because each of the keys
has w bits, so maybe k is 3. And those are your three keys. OK. The idea is, look at
the branching nodes. Where's a color? So, you've got a branching node
here and a branching node here. Because there's
three leaves, there's only going to be
two branching nodes. So the idea is, well I really
only care about these two bits. Or it would be enough to
think about these two bits. OK. Well, we'll look at this
more formally in a moment. But by storing this bit,
I know whether the key is over here on the left
or over here on the right. And then by storing
this bit, I don't really care about it for this word. But it will distinguish
these two words. So if you just look at the bits
that contain branching nodes in this tri-view, then it's
enough to distinguish all of the xi's. So this is x0, x1, x2. OK. Let me formalize
that a little bit. So we have k minus 1 branching
nodes in this height w tree of the k keys. Because there's
k leaves, there's going to be k minus 1 branching
nodes, because the k leaves are distinct. So this means there are at most,
k minus 1 levels containing a branching node. It might be fewer. Maybe it's nice to add
in another key over here on the left. I mean if I was lucky, there'd
be another key over here, and then I'd be using
this bit and getting two for the price of one. If I'm less lucky, it
will be more like this. So here's another x value. And in this case, I care
about this branching node. So I care about
another bit here. OK. But if I have four keys, it
will be at most three bits corresponding to these levels. So call these-- these
levels correspond to bits. This is the first bit, second
bit, third bit, and so on. This is the most
significant bit, next, and then the least
significant is at the bottom. So these levels correspond
to important bits. That's the definition. And we're going to give
these bits a name, b0, b1, up to br minus 1. Those are bit indices saying
which bits we care about. And we know that r is less than
k, and k is order w to the 1/5. So there are only w to
the 1/5 important bits overall among these k keys. So the idea is don't store
all w bits for all the keys. I mean you have to store them. But don't look at them. Just look at these
important bits for the keys. And then life is good. Because there's only w
to the 1/5 bits per key. There's only w to the 1/5 keys. And so the total number of
important bits among all k keys is small. It's only w to the 2/5,
which is less than w. So it fits in a single
word, and we can look at this in constant time. So that seems like a good thing. Let me tell you what
properties this has. Let me also define the notion
of a perfect sketch of a word. x is going to be what
you get when you extract bits b0 to br minus 1 from x. So in other words, this is a bit
string, an r-bit string who's i-th bit equals bit bi of x. So you've got a bit
string which is x. You say, oh, the important
ones are this one, this one, this one, and this one. Inside here is
either a 0 or a 1. And there's other bits
which we don't care about, because they're not important. And we just compress
this to a 4-bit string. 0110. OK. This is sketch of x. And to be a little bit
more explicit about how I'm labeling things, this is b0. This is b1, b2, and b3. Because you number bits-- I think this is right,
we'll see later-- we're going to number bits
from the right-hand side. This is 0-th bit, first
bit, second, third, fourth; which is the opposite of this
picture, unfortunately, sorry. This is a bit 0, bit 1, anyway. This will be convenient. So, that's perfect sketch. For now, I'm going
to assume that we can compute this in constant time. This one answer is
it's an AC0 operation. That's not so obvious,
but it's true. So on an AC0 RAM, you can just
say, oh, this is an operation. Right? It's given one word. And, well OK. It's given these
description of bit numbers, but those will also
fit in one word. And then does this
bit extraction. We're going to see a
reasonable way to do it. But for now, take that as an
unreasonable way to do this. So perfect sketch is
good, because it implies the following nice property. If you look at the
sketch of x0 that's going to be less than the
sketch of x1, and so on, which is going to be less
than the sketch of xk minus 1. Sketch preserves order. We assume that--
where do we have it? Over here, x0 is less than
x1, is less than xk minus 1. And because we're
keeping all the bits where are these xi's
get distinguished, this one it doesn't matter
whether we kept here, it doesn't matter whether
we kept these guys. But in particular,
we keep the bits that have all the branching nodes. That will preserve
the order of the xi's. So we know that the
order of the xi's is preserved under sketching. The trouble is the following. Suppose you want to do a
search, a predecessor search. So, you're given some query q,
and you want to know where does q fit among the xi's. Because that will tell you
which child to visit from here. So, OK. You compute a sketch
of q, seems reasonable, and move into sketch world. And now you try to
find where sketch of q fits among these guys. So you can do that. And I claim you can do
that in constant time. It's again, an AC0 operation. But the nice thing is the
sketches all fit in one word. Also this single sketch fits
in one word, no big surprise there. So let's say you can
find where sketch of q fits among these items
in constant time. The trouble is were the sketch
of q fits is not necessarily the same as where q
fits among the xi's. Because q was not involved
in the definition of sketch. q is an arbitrary query. They come online, I mean
any word could be a query, not just the xi's. So you've set everything up
to distinguish all the xi's. But q is going to fall off
this tree at some point. And that kind of messes you up. Because if q fell off here,
you don't have that bit. You won't notice
the q fit there. So we have to do some work. And this is what I
call de-sketchifying. And I like a big board. OK, let mean draw a some more
methodical and smaller example. I need to make it the
right number of levels. A little bigger than my usual
tree, and I'll get my red, actually maybe use two colors. So here's a real example. Now it has four keys. And here I'm in the lucky case,
where this is an important bit. And I get two for
the price of one. I cared about this
branching node, I cared about this
branching node, and so I only have to
door two bits in my sketch for these four nodes. In general, it
might be three bits. But this will just
make the point. So it's actually,
life is in some ways harder in this situation. OK. So what are my bit strings here? Over here I've got 0000, which
corresponds to always going left. And I've got 0010. Over here I've
got 1100 and 1111. We drew these pictures
for van Emde Boas, right? The idea is we're
going to use some of the similar
perspectives at least. OK, but the important bits were
the very first, the leftmost bit I should say. And then two bits after
that, so these guys. And so the sketch here
is 11, 10, 01, and 00. And you can see this is
the minimal number of bits I need to keep them in order. But it does. It works. You can check. This works in general. OK. Now comes the query. I have a problematic
query I'd like to draw. And it is 0101, so 0-1-0-1. So here's my query queue. Let me draw these as white. Query is 0101. 1 If we take the
sketch, we get 00. Those are the important bits. So if I search for
the query of 00, I will find that it
happens to match this key, or it matches the
sketch of this key. But that key is neither the
predecessor, nor the successor of that query. So this is bad news. I find the predecessor in sketch
world, which is the red stuff, I get the wrong answer. In general, they could be
very far away from each other. Here, I've got it 1
away, but that's as big an example as I can draw. So, how do we deal with this? This is the de-sketchification. So when I do this query, I
end up finding this guy, x0. I claim that I can still use
that for something interesting. OK, let's say we
have a sketch of xi as the predecessor
of the sketch of q. And so sketch of q is
sandwiched between a sketch of xi and sketch of xi plus 1. First of all, we're
assuming that I can compute this
in constant time, I can find where sketch of
q fits among these guys. Because it just fits into words. And for now, let's just
assume all operations on a constant number of
words are at constant time. We will see how to do this. This is parallel comparison. So you figure out
sketch of q fits here. I want to learn something about
where q fits among the xi's. It's obviously, these
may be the wrong answer. But I claim I can do
something useful by looking at the longest common
prefix of those words. So I want to compare q, not
sketch of q but the real value q, and either xi or xi plus 1. And what I want is the longest. So I look at the longest
common prefix of q and xi. I look at longest common
prefix of q and xi plus 1. Whichever of those
is the longest that's my longest common prefix. In the tree, it's the
longest common ancestor, or lowest common ancestor. OK. So let's do it. We found that sketch of
q fit between, I guess, these two guys, the
way I've written it with the inequalities. It's between x0 here and x1. So in this case, the
lowest common ancestor of this node and q
is going to be here. Also this node and q
happens to also be here. So this is the lowest we can go. And what this means
is that these guys, they share the bit
string up to here. We were on the blue
substructure up till here. This was the node
where q diverged. We followed a pointer here
along a non-blue edge. That's where we made a mistake. So this lets us
find, in some sense, the first mistake, where
we fell off the tree. So that's where we
fell off the blue tree. That's useful information. Because now we know, well,
we went to the right, whereas all the actual data is
over here in the left subtree. There is no blue
stuff in the right. So that tells us a lot. If we want to now find
the predecessor of q, it's going to be whatever
is the max in this subtree. So, I just need to be able
to find the max over here. So this is the idea. Now there's two cases, depending
on whether we were in the right or in the left from that node. So let me write
this, find the node y where q fell off the blue tree. So this node y, we can
think of as a bit number. Here the leftmost
bit was still on, but then the next bit was off. And so we look at-- I'll call that bit y plus 1,
or maybe size of y plus 1. If that bit equals 1,
that's the picture we have. Then what I'm going to do
is set a new quantity e, which is going to be-- this is a new word. It's going to be the bit
string y, followed by a 0, followed by lots of 1's. Whereas our bit string q had a
1 here, and fell off the tree. What we're instead
going to do is identify this node, the
rightmost node in this subtree. That's not necessarily an xi. But it's a thing. And then we're
going to do, again, this search and sketch space,
but now using e instead of q. If we do that,
what is this node? Let's label it, 0011. If you look at the sketch
bits, this has a sketch of 01. So if I did a search
here, I would actually find that this is
the answer, and that actually is a predecessor of q. In general, this is
going to work well. Because essentially
some of these bits are going to be sketch bits. This one was not, and
we made a mistake there. We went right. We should have got left. These ones, some of them
are going to be sketch bits. Some of them are not. But whichever ones
get underlined, it's going to be a
1, which means we're going to do the right thing. We want the very rightmost
item in this tree. So if we always go right
whenever there's a sketch bit, and then do a search
in sketch space, we will find the rightmost
item in this tree. So if we then do a
search on e, we're always going to get
the right answer. So in the end, we're going do
two searches in sketch space, once with q to find this
place where we fell off, then once with e where we
actually find the right answer. And there's a symmetric case,
which is if we went left and we should have gone right,
then we go right, and then we put a lot of 0's. Because then we want to
find the min in that tree. So, back to search, we
compute sketch of q. We find it among the
sketch of the xi's. This gives us this y. So we find the
longest common prefix, y equals longest common prefix
of q and xi or xi plus 1. Then we compute e, and then we
find sketch of e among sketch of xi's. And the claim is that the
predecessor and successor of sketch of e among sketch
of xi's equals the predecessor and successor of q, our
actual query, among the xi's. So this is a claim. It needs proof. But it's what I've been
arguing that e gives us the right structure. It fixes all the sketch bits
that are potentially wrong. We found the first sketch
bit that was wrong. We fixed that one. And then the remainder, as
long as we go all the way to the right, we'll find the
max, or in the other case we want to go all
the way to the left because we want to find the min. So that's this claim. We find the predecessor
of sketch of e. Run the sketch of the xi's,
which is just this thing again. So again, we can do
it in constant time. Then we find-- I mean I have to be a little
bit more precise here. Of course, we find the
predecessor and successor, we get a sketch of the xi. We have to undo that
sketch operation. Really the way to think of it
is predecessor and successor are really returning a rank. I want to know the
i that matters. So if it fits between sketch
of xi and sketch of xi plus 1, if sketch of e
fits between those, then I know that q will fit
between xi and xi plus 1, in terms of that rank, i,
the index in the array. So, that makes sense. This is the end of the-- what's the right way to put it? This is the big picture
of fusion trees. At this point you should
believe that everything works. And overall, what are we doing? We're building a
w to the 1/5 tree. It's not yet clear
why w to the 1/5. And so we have to
implement these nodes that only have w to the 1/5 keys. So we're looking
at a single node. And say, hey look,
there's a bunch of keys. Let's just look at
the important bits. That defines the
sketch operation. Now if we want to do a search,
we do this double search. We compute the sketch, find
the sketch among the sketches, find our mistake,
compute our proper query, compute the sketch of that, find
that sketch among the sketches, and then that index in the array
of sketches will be the correct index of our actual
query q among the xi's. Now, there are
several things left to be done in a reasonable way. One is how do we
compute sketches. How do we do this
kind of operation of taking the bits we care
about and bringing them all next to each other? Second thing is, how
do we do this find? This parallel comparison. So it's basically all
the bullets up here. We have how do we
do a sketch, how do we do parallel comparison
to find where one sketch fits among many sketches, and there's
also a most significant set bit. Where did we do that? In computing the
longest common prefix. So if you have two
bit strings and you want to know where
did they first differ, the natural way to
do that is compute the XOR, which gives you
all the differing bits. And then find the first
one bit from the left. So this is really most
significant set bit. So, we need that operation. So, we have our
work cut out for us. But the overall
picture of fusion trees should now be clear. It just remains to do
these three things. And this is where the
engineering comes in, I would say. Any questions about
the big picture? So, the first thing I'm
going to do is sketch. And as I've hinted at
in the outline here, we're not going to
do a perfect sketch. We're going to do an
approximate sketch. This will probably
be the most work among any of these operations. Parallel comparison is
actually quite easy. Sketching is, I think,
the biggest insight in Fusion trees. So, perfect sketch
takes just the bits you care about that we need. We only want to look at
the bits we care about. But it's easy to look at
the bits we care about. We can apply a mask, and just
AND out the bits we care about. Everything else we can zero out. So that's easy. The hard part is compression,
taking these four bits and making them four
consecutive bits. But they don't really
need to be consecutive. If I added in some 0's here
in a consistent pattern that would still work. I'd still preserve the
order among the sketches. And that's all I care about. And this is where I'm going
to use the slop I have. Because right now I
have w to the 1/5 keys. If I did perfect sketch,
the total number of bits would only be w to the 2/5. But I can go up to w. So what I'm going
to do is basically spread out the bits,
the important bits, in a predictable pattern
of length w to the 4/5. Predictable just means it
doesn't depend on what x is. So when there are extra 0's
here, you know that's fine. But there's always going to
be two 0's here, one 0 here, three 0's here, no
matter what x was. As long as it's predictable,
I'm going to preserve order. And as long as it's
length order w to the 4/5, if I take w to the
1/5 of them, that will still fit in a
constant number of words. Because it will be
order w bits total. So that's what I can afford. And now I'm going to do it. So here's how. First thing, as I
said, is we're going to mask the important bits. I just want the important bits. I should throw away
all the others. And so this is going to be x
prime equals x bit-wise AND. And here's where I'm going to
use the notation that the bits count from the right. I want the bi-th bit to
correspond to the value 2 to the bi. This thing is just a bit string. It has 1's wherever
the important bits are. So if this is the
b0, b1, b2, and b3; I just want this bit string. I mean, you can think of this
as 1 shifted left bi times. So I get 1's in exactly
the positions I care about. And if I bit-wise AND
that with x it zeros out all the other bits. This is what we call masking. So that's the
obvious thing to do. And then the second
idea is multiplication. And it's just like, well, maybe
we could do it with a multiply, and then we'll just work it out. And the answer is yes, you
can do it with a multiply. So that I imagine
was the big insight was to see that multiplication
is a very powerful operation. So we're just going to do x
prime times some number m. And we're going to prove
that there exists a number m that does what we need. So I'm going to write this out
a little bit algebraically. So we can think about
what m might be. Now x prime only has
the important bits. So we can write that as a sum i
equals 0 to r minus 1 of xbi 2 to the bi. So I am introducing
some notation here. xbi, is that important bit bi 1 or 0? This is just a de-reference of
the bit vector or a bit string. And so you multiply
that by that position. I mean this the definition
of binary notation, right? But we only care about
the important bits. Because only those are set. So that's x prime. And then we're
multiplying that by m. Now m could have any bit set. So I'm going to-- but I'm going to
write it like this. I'm going to assume that
m only has r bits set, same as the number
of important bits, r is a number of important bits. But I don't know where they are. So I'm just going to suppose
they're at positions m0, m1, up to mr minus 1. I've got to find what these
mi's should be, or mj's. And now just taking
this product, so we can expand out the
product algebraically and see what we get. So, what's this product? Sum i equals 0 to
r minus 1 sum j equals 0 to r minus 1 of xbi-- I mean just the
product of these. So 2 to the bi plus mj. That's the algebraic
product of those two things. That's why I wrote
it out this way. So I can see what's going on. The point is when you
do multiplication, you're doing these
pairwise products. Now the guys that
are going to survive are the ones where the
xbi's are 1, of course. But they survive
in multiple places. Essentially the mj's
shift all of those bits by various amounts. So it used to be at this
position, 2 to the bi. But now we're shifting
it by mj for all j. So some of those bits
might hit each other. Then they add up. That's really messy. We're going to avoid
that, and design the mj's so that all of
these values are unique. Therefore, bits
never hit each other. That's step one. And then furthermore, what we
care about or what we're trying to do is to get the xbi's to
appear in a nice little window, consecutive interval
of w to the 4/5 bits, somehow by setting the mj's. So let me tell you
the claim, which we will prove by induction. So we're given these bi's
that we can't control. Those are the important bits. And the claim is we can
choose the mi's such that three properties hold. First one is that bi plus mj
are distinct for all i and j. So that was that these bits
don't collide with each other. So there's no actual
summation here. These sums could
then be replaced by ORs, which makes
it very easy to keep track of where the
bits are going, if we can achieve this property. Property b is that it
turns out that the bits I'm going to end up
caring about our b0 plus m0, b1 plus m1,
and general bi plus mi. In general, we have bi plus
mj for different values of i and j. I claim the ones I care
about are the ones where i and j are equal. So I'm going to
look at these bits, and in particular I want
them to appear in order in the bit string. And then third property-- I need some more space-- is that if I look at
the span of those bits, so I look at br minus
1 plus mr minus 1 minus b0 plus m0 that is the
interval that these bits span. I want that to be order
r to the fourth power. Because r was w to the 1/5. So this would be
order w to the 4/5. That's what I need for
everything to fit in. So this is guaranteeing
that these bits are the sketch that I need. They appear in
order, and they don't span a very large interval,
just w to the 4/5. This is what I need to prove. If I can prove this, I
have approximate sketching. So let's prove it. Proof happens in two steps. First thing I'm
going to worry about is just getting
these guys distinct. Then I'll worry about
the order property. So here's how we
get them distinct. And these are going
to be the mi primes, not quite the mi's that we want. They're all going to be integers
less than r cubed, greater than or equal to 0, and they're
going to have the property that the bi's plus mj primes
are distinct mod r cubed. So this is a stronger
version of a. We really just need
them to be distinct. But to make it easier
for the other steps, we're going to force them
to be distinct mod r cubed. How do we do this? By induction. So let's suppose that we've
picked m0 up to mt minus 1. So suppose by induction
that we've done that. And now our goal is
to pick mt prime. So how do we choose mt prime? Well, what can't it be? mt prime has to avoid
basically mi prime, and believe minus bj plus b-- we're going to call it k? I guess so. If it avoids all
expressions like this, then mt prime plus bj
will be different from mi prime plus bk. In other words, all of these
things will be distinct. So it has to avoid
this modulo r cubed. If I can avoid all
of these things-- so this is for all ijk-- if I can choose mt prime
to avoid all those, then I'm happy. Because then these things
will continue to be distinct, and then I apply induction. Well, how many choices
are there for i, j, and k? For i, there's I
guess t choices. Because mi can be any
of the previous values. For j, let's call it r choices
for k, there's r choices. That's how many
important bits there are. So total number of
choices is tr squared. But t here is
always less than r. So this is going to
be less than r cubed. So that means there is
less than r cubed things we have to avoid. But I have r cubed
allowable choices on working modulo r cubed. So I just pick any one
that avoids the collision. This is basically deterministic
hashing, in a certain sense. We are choosing these
values deterministically to avoid collisions in
this simple hash function. OK. It takes time. It's going to take polynomial
time to compute this thing. And you can imagine if
you just plug in hashing, this will work with
some probability, and blah, blah, blah. But I want to make it always
work deterministically. Because we know what
the xi's are here. All right. So we've avoided collisions. There's enough space. That's all. That was step one. Step two, and this
will solve property a, even modulo r cubed. Now we have a
little bit of space. We're allowed to go
up to r to the fourth. And now we just need to
spread out these bits. So that's step two. Basically we're going to set
mi to be these values that we chose plus this weird thing,
w minus bi plus ir cubed rounded down to a
multiple of r cubed. So I guess you could put this
in parentheses if you want. Rough idea is, we want to
take mi prime plus ir cubed. Because these mi primes,
they're all values between 0, and r cubed minus 1. We got everything
working modulo r cubed. If we could just add ir
cubed to each of these values that we'll spread them out. Because each of these
values used to fall just in this tiny range ir cubed. So we can move the next
one to the next position, move the next one to the
next position, and so on. Spread them out to
the left by adding on multiples of r cubed, then
that will achieve property b. The annoying issue here is
we don't want to mess things up modulo r cubed. So we need to round
things down to be a multiple of r cubed so that
this is congruent to mi prime. That's what we want. We want it to stay
congruent to mod r cubed. Well, why do we need to round
down to a multiple of r cubed? We were adding on ir cubed. Well, it's not quite
mi that we care about. It's mi plus bi. Those are the bits that we
want to be nicely ordered. And so we kind of
need a minus bi here, so that when we take mi
plus bi, those cancel. But then bi is not a
multiple of r cubed. So you've got to do this
rounding down to r cubed. Also negative bi is
a negative number. And we can't really deal
with negative numbers. Because you can't go left of 0. So we have to add on this w
just to make things work out. So it's a little
messy, and I don't want to spend too much time
on why this formula works. But I think you have the
essence of what's working. This is just to avoid
negative numbers. This negative bi is so that when
you add it to mi that cancels. And so you get these
r cubes separations. In the end, let me draw
a picture, perhaps. In the end, if you look at the
bit space, so this is w bits. And you divide it up into
multiples of r cubed. All of the mi primes
are over here. So these are mi primes. You don't know in what
order or anything. They're just kind of randomly
in there, and chosen pretty much arbitrarily on the low
end of the spectrum, from 0 to r cubed minus 1. And then what we want
is for x0 plus m0 to fall somewhere in this
range, and then x1 plus m1 to fall somewhere in this
range, and x2 plus m2 to fall somewhere in this range. If I do that, and it's
weird because the bits are numbered from
0 to the left here. Then I will have this property. I claim this
assignment does that. It's an exercise you can check
that indeed xi plus mi will fall in this range. So this gives us property b. It also gives us property c. Because we've been
fairly tight here. There's r of these guys, and
r of these intervals of size r cubed. And so the total
range of these bits is going to be r to the fourth. We started at x0 plus m0. We end at xr minus
1, plus, mr minus 1. That's going to
be somewhere here. But if you look at just
that interval of bits-- so there are more bits actually
over here, in particular, because of this w bit part. This whole picture
basically starts at bit w. Then there's all
this stuff to 0. So this is a more
accurate picture. You're doing this
multiplication. Garbage happens here. We have no idea. Garbage happens here. We have no idea. Actually, garbage
happens all over here. But what we know
is that these bits are the bits we care about. These are the xi plus mi bits. If you look at xi plus
mi, they give you xbi. They exist in other places. But these bits will
have the important bits. Now the bits are also
all over everywhere else. But none of the
bits hit each other. So these bits remain
correct, because nothing else collides with it. And so if I just mask
out those bits, again, so I have to do another mask. I did one mask here. I did a multiplication, and
then I have to do another mask. So why don't I
write it over here? So we AND with sum i
equal 0 to r minus 1 of 2 to the bi plus mi. Those are the circled bits. So if we grab those things,
and then we shift right by-- why am I writing x? Sorry. These are all b's. Too many letters. We shift right by b0 plus m0. Because we don't care about
all those leading bits. So we shift this
over to the left. We did the mask, then we will
just have the important bits and they will occupy over here
an interval of size at most-- I'll say order r to the fourth. Clear? So this is
approximate sketching. This is definitely a bit
complicated, but it works. Let me review briefly. So, our algorithm was simple. We have a bit string, x. We just want to get
the important bits and compress them to a thing
of size r to the fourth. So first of all, we threw away
all the non-important bits with this mask. That was easy. Then we just did an
arbitrary multiplication, and we proved that there
was a multiplication that avoided collision. So the sums basically
turned into ORs or XORs. I mean you never get two
1 bits hitting each other, so you don't have
to worry about that. And we did that with the
simple inductive argument. And then we also wanted
the bi's plus mi's to be linearly ordered. Because we need to preserve the
order of the important bits. We can't just permute them. And we needed them to
occupy a small range. And we did that
basically by adding ir cubed to each of them. But it was a little messy
and we had to add w, and blah-blah-blah. But in the end, we
got our important bits to be nicely spaced out
here by pretty much putting an r cubed in between each one. So those were our
bi plus mi bits. They occupied this range
of r to the fourth. We'll mask out all the
rest of the garbage. Because this multiplication
made a quadratic number of bits. We only want these r bits,
the r squared 1 bits in here. We'll mask away all the others. Take these bits,
shift them over. Now they occupy a nice interval
at the beginning size order r to the fourth. And that's our
approximate sketch. So sketch should only take
r, but we're being sloppy. With this multiplication
trick, the best we know is to get down to
r to the fourth. And that's good enough. And that's why I set everything
to w to the one fifth. Because this is w to the 4/5. We're going to have
w to the 1/5 of them. And so if you take
these sketches and you concatenate them, fuse
them together if you will, and that's fusion trees. Then the sketches of all of
the keys x0 up to xk minus 1 will occupy order 1 words. Because it's order w bits, w
to the 4/5 times w to the 1/5. Which brings us to
parallel comparison. I have all of these
approximate sketches. So you could start forgetting
approximate sketching. Somehow, we get these
w to the 4/5 bits. We want to concatenate them
together, and then in parallel compare all of them
to the sketch of q. The sketch of the xi's
we can preprocess. We can actually
spend a lot of time finding the sketch function. But then we have to fix
the sketch function. We have to be able to compute
a sketch of q in constant time. That's what we just did. Sketch of q is one AND one
multiplication and another AND. So computing sketches is fast. That's the steps of
computing sketch of q. Now, next step is find it
among the sketch of the xi's. So this is the next thing
we want to make fast. It's actually pretty easy. You probably know you
can compare two integers by subtracting one
from the other. So we're just going to do
that, but in a clever way, so we can do k subtractions
for the price of one. I'm going to define the sketch
of a node to be 1 bit followed by the sketch of x0 dot, dot,
dot, 1 sketch of xk minus one. And I'm going to define a
sketch of q to the k-th power, so to speak, to be a 0 bit
followed by sketch of q, dot, dot dot, zero bit sketch of q. This is aligning
things, so that if I did this subtraction
and this one, I would basically be comparing
q with all the xi's at once. The point is these sketches--
this is the thing that fits in order 1 words. These sketches are
w to the 4/5 bits, and there's w to
the 1/5 of them. So this whole thing
is order w bits. So it fits in one word. This thing also. It happens to be the same
bits repeated many times but also it fits in one word. How do I compute this thing? I can do it with multiplication. It's sketch of q times
0000001, 000000001. So, ahead of time, I'll just
pre-compute this bit string that has 1's at the rightmost
slot for each of these k fields. If I just take that and
multiply it by sketch of q, then I get this. So this is easy to do
in one multiplication. Now, I take this thing
minus this thing. I take the difference. And the key thing is because
I put these 1 bits here, I'm taking this minus this. The point is either this 1 bit
will get borrowed when I do binary subtraction, or it won't. It gets borrowed when
this is bigger than this, otherwise it doesn't
get borrowed. So I'm going to
get either a 0 or 1 here, and then
some garbage which I don't care about, and a 0 or
a 1 here, and then some garbage. And I'll just mask that out. I'm ANDing with 10000, 100000. And so I end up just with
01 bits and the rest 0's. And these bits,
if I get it right, it's 1 if the sketch
of q is less than or equal to the sketch of xi. And it's 0 if the sketch of q is
greater than the sketch of xi. Because when it's greater
that's when the borrow happens. And then the 1 turns into a 0. So 1's indicate the query is
too small or they're just right. And 0's indicate
that they're greater. Now the xi's were in order. So probably x0 is too small. And so this bit will
end up being a 0. Probably xk this
plus 1 is too big. So this bit will be a 1. In general, it's going to be
a monotone sequence of bits. If you look at these
bits, these 01 bits, they are going to be monotone. They'll be 0 for a while,
and then at some point they'll switch to being 1's. And that transition from 0 to
1 that's what we want to find. These keys are too small. These keys are too big. This key is just right. So we fit between-- this would be position
i and position i plus 1. And we fit between
xi and xi plus 1. Well, not actually
xi and xi plus 1. We fit between sketch of
xi and sketch of xi plus 1. That's what we need to find. Now that is again the
problem of finding the most significant 1 bit. But in this case, I don't
need that operation. I can do it in a simpler way. But we're almost done, right? We've done all of this
parallel comparison. We just need to find that
transition between 0's and 1's. Turns out there's a
cool way to do it. The cool way is multiply that
word times our good friend, this thing, 000001, 000001. This is a little
harder to think about. But take this bit string
and multiply it by this. What that does is it
takes this string. It includes it. Because there's a 1 right there. It shifts it over by one field,
and includes it, shifts it over by another field, includes it. So this repeats this thing. And now collision happens,
because they're perfectly aligned. If these 1 bits ever hit each
other, they'll be summing. Now, some of them are
0, some of them are 1. Instead of computing
the position of the 0 to 1 transition, we could
equivalently just count how many 1's are there. I mean that's counting
from the right, whereas this is counting of from
the left, whatever, same thing. So if I could count how
many 1's I'd be all set. And in this case, if
you look at right here, this will be the
number of 1's I claim. Because if this one was
there, it will stay there. And then all the other bits
get shifted over and fall right here on top of this bit. So as they get added up,
you'll get some carries and things will move over. But this is not very big. Because we're
talking about k bits. So this is only going
to be with log k. I mean there's tons
of room here before we get to the next shift. So I just look at these bits. I mask them out. I shift them over. And that gives me
the number of 1's. This is a cute way to count the
number of ones in a bit string when the bits are
spread out nicely. They have to be at least
log k away from each other. Otherwise you get collision. It doesn't work for an
arbitrary bit string. But for a bit string
like this, we're all set. We can count how
many 1's there are. Then we figure out where
this transition is. That is parallel comparison. One more thing to do, which
is most significant set bit. The place we needed this, was
we were taking the XOR of q with xi. And then we wanted
to find the first bit where they were differing. So after you take
the XOR, you've got some bit string
that looks like this. And you want to find
this bit, because that's the place you diverged. Then we would turn that to 0
and change the rest to 1's. That's easy to do if we
know where this bit is. And this is a generally
useful operation. It's used all over computer
science, I would say. So much so that most CPUs
have it as an instruction, so on Intel it's called CLZ. And it has many names. They're in the notes. Most compilers
provide this to you as an operation on
architectures that have it, otherwise they simulate it. They probably don't
simulate it as well as I'm going to tell you. Because we're going to
do this in constant time on a regular word RAM,
just C operations, which does not seem to have
made it into popular culture. It's slightly
complicated, which is why. But what's cool is
we're going to use-- I'm going to do this
relatively quickly. Because I don't
have a ton of time. We're going to use all the
things that we just did again, quickly. Most of them just
as black boxes. All right. So, here's what
we're going to do. Maybe I should go somewhere new. So, I'm going to use sketches,
not approximate sketches, but I'm going to sketches. I'm going to use multiplication. I'm going to use
parallel comparison. And in some sense I'm going to
use most significant set bit. All of these things
I'm going to use to solve the most
significant set bit problem. So here's what we do. We split the word into root
w clusters of root w bits. Sound familiar? This is exactly what we
did in van Emde Boas. So van Emde Boas did
this recursively. We're going to do it once. We can only afford
constant time. So here's an example. x is 0101, 0000, 1000, 1101. So each of these is root w bits. There's root w of them. It's approximate. It doesn't it to be exactly. But we'll assume x is
a nice power of two, so that works cleanly. So the first thing, so what
the high level idea is I need to find the first
non-empty cluster. Here it happens to
be the first cluster. And then I need to find the
first 1 bit within the cluster. Hard part is finding the
first non-empty cluster. Actually, the hard
part or the messy part is finding which
clusters are empty and which clusters are not. This cluster is not empty. This cluster is empty. These are non-empty. So I want the summary
vector which is 1011. I claim if I can do that,
everything else is easy. So let's spend some time on
identifying non-empty clusters. First thing I do is I take x,
ANDed with this thing, which I'm going to call F, 1000,
1000, 1000, 1000; F for first. So I'm just seeing which
of these first bits in each cluster are set. So the result is I get
0000, 0000, 1000, and 1000. So in particular that tells me
this cluster and this cluster are non-empty, because they
have the first bit set. What about all those other bits? Well, the other bits I'm going
to do in a different way. Just the first bits, I
need a little bit of room. I need this bit of room. I want to put these 1's in. So I've got to get rid of some
bits to make room for that. So this deals with
the first bits. Now I'm going to
clear those out. So I'm going to take x XOR this. And that will give me everything
with the first bits cleared. So I've got 0101,
0000, 0000, and 0101. These are the rest of the
bits I've got to figure out. This one is non-empty
and this one's non-empty. How do you do it? With subtraction. I take F minus that thing. This F has 1's, and they're
going to get borrowed. When I take F minus this,
this 1 will get borrowed because there's something here. This one will not get
borrowed because this is 0. This one will not get
borrowed because this is 0. This one will get borrowed
because there's something here. That's it. We're comparing
with 0 everything. So we're going to get, in
this case, 0 and some garbage, 1 and 0's, 1, and 0
with some garbage. I just care about these bits. These are the bits that tell
me which ones were empty. The 0's are empty. The 1's are non-empty. So I do a mask. I get 0, and some 0's, 1 and
some 0's, 1 and some 0's 0 and some 0's. OK. Then I do an XOR with F. Because
I really want 1 for these guys, and 0 for these guys. 1 means it's not empty. 0 means it's empty. I got that right. So I'm just inverting the 0
bits to 1 bits, and vice versa. So 1 means this one's not empty. 1 means this one's not empty. Those are the non-empty guys. I take this and I OR
it with this thing. This was the thing
that told me which ones had that first bit set. So if I take the OR of those
two I learn, or any bit set. Because this was dealing
with all of the other bits. I threw away this bit,
but I had to remember that it was non-empty. OK. So I take that OR. Now, this tells me those
three blocks were not empty. This one was empty. So now here I have the
bits that I care about. Sadly they're spread out. I'd really like them compressed. So I do that with sketch. I want to compress them to 1011. It would fit in one
little thing here. Because this is root w. There's root w of them. Sadly, I can't use
approximate sketch. Because I don't
have enough space. This is w to the 1/2. If I used approximate
sketch I get w-- I'd lose this factor of
4 and be bigger than w. I really need it to
be perfectly sketched. Conveniently, you can do
perfect sketch in this regime. Before the bi's were
arbitrary things. We had no idea how
they were spread out. Here bi is root w minus
1-- that's the first one-- plus i times root w. They're nicely uniformly
spaced by i root w. In this case-- I'm running out of time-- I claim you can use mj equal to
w minus root w minus 1 minus j root w plus j. And I won't go to the proof. There's a sketch in the notes. If you do this, this is
a nice setting of mj. It turns out you will get bi-- if we look at bi plus mi,
this cancels, this cancels, because i equals j. You're left with w plus j. So in other words, if
you look at bi plus mi, you get from bit w to bit
w plus root w minus 1. These bits will be exactly
the bits you care about. So you take those. You mask out the others. You shift it over to
the right, and you have exactly your perfect sketch. The thing you need to prove
here is that bi plus mj are all distinct. So there's no collisions. But in this case it's
easy to avoid collisions. You've got all your bits
nice and consecutive. Now you've got it
down to this thing. OK, not quite done though. Only one more minute. Let's say-- well,
that was step one. Identify non-empty clusters. Step two was sketch. Step three is find the
first non-empty cluster. I claim this is easy. So I take this sketch vector. It only has root w bits. So I use parallel comparison. What do I compare to? I'm going to compare many copies
of this thing to 0001, 0010, 0100, 1000; the powers of 2. So I take this. I put them in a vector
like the sketch of a node. And I take the k, or I guess
root w copies of the sketch of the summary vector. That's this 1011. So I compare four copies
of this to each of these, and I learn which power
of 2 it is greater than. In other words, what is the
most significant set bit. That's why when I told you
how to do over here, when I told you how to do
parallel comparison, I didn't want to use
most significant bit as a subroutine. Because this is a subroutine
to most significant bit. Over here, we could just do
this multiplication and boom, we found what the
most significant set bit was as long as
there was room to fit all this stuff in a word. And because I've reduced
everything to size root w, and then only there's
w of these things to compare to, because
that's the width of one of these fields. This all fits in a word. I can do this
parallel comparison. Boom, I find the first 1 bit
in this bit string, which happens to be the first bit. That tells me that this cluster
is a cluster I care about. So I take those bits out. I mask them out,
shift them over, and I find the first
1 bit in that cluster. How do I do it? In exactly the same way,
clusters again, root w bits. I can use parallel
comparison to compare it to all these things
in constant time. I find where the
first 1 bit is there. And then I take this cluster
C, I take this bit D, and my answer is
C root w plus D. That is the final index of
the most significant 1 bit in constant time, using all
those fusion tricks once again. And that in the end
gives you fusion trees on a word RAM static. It's complicated, probably
impractical, but pretty cool. And we're going to use
these bit tricks again.

CS

The following content is
provided under a Creative Commons license. Your support will help
MIT OpenCourseWare continue to offer high quality
educational resources for free. To make a donation or
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: All right,
let's get started. Today we have another
data structures topic which is, Data
Structure Augmentation. The idea here is we're going
to take some existing data structure and augment it
to do extra cool things. Take some other data
structure there we've covered. Typically, that'll be
a balanced search tree, like an AVL tree or a 2-3 tree. And then we'll modify it to
store extra information which will enable additional kinds
of searches, typically, and sometimes to
do updates better. And in 006, you've seen an
example of this where you took AVL trees and augmented AVL
trees so that every node knew the number of nodes in
that rooted subtree. Today we're going to see
that example but also a bunch of other examples,
different types of augmentation you could do. And we'll start out with
a very simple one, which I call easy tree augmentation,
which will include subtree size as a special case. So with easy tree
augmentation, the idea is you have a tree, like
an AVL tree, or 2-3 tree, or something like that. And you'd like to
store, for every node x, some function of the
subtree, rooted at x. Such as the number
of nodes in there, or the sum of the
weights of the nodes, or the sum of the
squares of the weights, or the min, or the max, or the
median maybe, I'm not sure. Some function f of x which
is a function of that. Maybe not f of x,
but we want to store some function of that subtree. Say the goal is to store
f of the subtree rooted at x at each node x in a
field which I'll call x.f. So, normally nodes have a left
child, right child, parent. But we're going to
store an extra field x.f for some function
that you define. This is not always
possible, but here's a case where it is possible. That's going to
be the easy case. Suppose x.f can be
computed locally using lower information,
lower nodes. And we'll say,
let's suppose it can be computed in constant time
from information in the node x from x's children
and from the f value that's stored in the children. I'll call that children.f. But really, I mean left
child.f, right child.f, or if you have a
2-3 tree you have three children, potentially. And the .f of each of them. OK. So suppose you can
compute x.f locally just using one level down
in constant time. Then, as you might expect,
you can update whenever a node ends up changing. So more formally. If some set of nodes
change-- call this at s. So I'm stating a very
general theorem here. If there is some set of nodes,
which we changed something about them. We change either
their f field, we change some of the data
that's in the node, or we do a rotation,
loosen those around. Then we count the total number
of ancestors of these nodes. So this subtree. Those are the nodes
that need to be updated because we're assuming we
can compute x.f just given the children data. So if this data is
changing, we have to update it's
parents value of f because it depends
on this child value. We have to update
all those parents, all the way up to the root. So however many nodes there are
there, that's the total cost. Now, luckily, in an AVL tree, or
2-3 tree, most balanced search structures, the updates
you do are very localized. When we do splits in
a 2-3 tree we only do it up a single
path to the root. So the number of ancestors
here is just going to be log n. Same thing with an AVL tree. If you look at the
rotations you do, they are up a single
leaf to root path. And so the number
of ancestors that need to be updated is
always order log n. Things change, and there's an
order log n ancestors of them. So this is a little more
general than we need, but it's just to point out if
we did log n rotation spread out somewhere in the tree,
that would actually be bad because the total number of
ancestors could be log squared. But because in the
structures we've seen, we just work on a single path
to the root, we get log n. So in a little more
detail here, whenever we do a rotation in an AVL tree. Let's say A, B, C, x, y. Remember rotations? Been a while since
we've done rotations. So we haven't changed any
of the nodes in A, B, C, but we have changed
the nodes x and y. So we're going to have to
trigger an update of y. First, we'd want to
update y.f and then we're going to trigger
the update to x.f. And as long as this one can
be computed from its children, then we compute y.f, then we
can compute x from its children. All right. So a constant number
of extra things we need to do whenever
we do rotation. And because the rotations lie
on a single path, total cost that-- once we stop doing the
rotations, in AVL insert say, then we still have to keep
updating up to the root. But there's only log n at
most log n nodes to do that. OK. Same thing with 2-3 trees. We have a node split. So we have, I guess,
three keys, four children. That's too many. So we split to two nodes
and an extra node up here. Then we just trigger an
update of this f value, an update of this f value,
and an update of that f value. And because that just follows a
single path everything's log n. So this is a general
theorem about augmentation. Any function that's well
behaved in this sense, we can maintain in AVL
trees and 2-3 trees. And I'll remind you and state,
a little more generally, what you did in 006, which are
called order statistic trees in the textbook. So here we're going to--
let me first tell you what we're trying to achieve. This is the abstract data
type, or the interface of the data structure. We want to do insert, delete,
and say, successor searches. It's the usual thing we want
out of a binary search tree. Predecessor too, sure. We want to do rank of a
given key which is, tell me what is the index of that key
in the overall sorted order of the items, of the keys? We've talked about rank a few
times already in this class. Depends whether you
start at 0 or 1, but let's say we start at one. So if you say rank of the key
that happens to be the minimum, you want to get one. If you say rank of the key
that happens to be the median, you want to get n over
2 plus 1, and so on. So it's a natural thing
you might want to find out. And the converse
operation is select, let's say of i, which is,
give me the key of rank i. We've talked about select
as an offline operation. Given an array,
find me the median. Or find me the n over
seventh rank item. And we can do that in linear
time given no data structure. Here, we want a
data structure so that we can find the
median, or the seventh item, or the n over seventh key,
whatever in log n time. We want to do all of these
in log n per operation. OK. So in particular, rank of
selective i should equal i. We're trying to find
the item of that rank. So far, so good. And just to plug these
two parts together. We have this data structure
augmentation tool, we have this goal
we want to achieve, we're going to achieve
this goal by applying this technique where f
is just the subtree size. It's the number of
nodes in that subtree because that will
let us compute rank. So we're going to use easy tree
augmentation with f of subtree equal to the number of
nodes in the subtree. So in order for
this to apply, we need to check that given a node
x we can compute x.f just using its children. This is easy. We just add everything up. So x.f would be equal to 1. That's for x. Plus the sum of c.f
for every child c. I'll write this as a
python interpolation so it looks a little
more like an algorithm. I'm trying to be generic here. If it's a binary
search tree you just do x.left.f, plus x.right.f. But this will work
also for 2-3 trees. Pick your favorite
data structure. As long as there's a constant
number of children then this will take constant time. So we satisfied this condition. So we can do easy
tree augmentation. And now we know we
have subtree sizes. So given any node. We know the number of
descendants below that node. So that's cool. It lets us compute
rank in select. I'll just give you those
algorithms, quickly. We can check that
they're log n time. Yeah. So the idea is pretty simple. You have some key-- let's
think about binary trees now, because it's a
little bit easier. We have some item x. It has a left subtree,
right subtree. And now let's look up from x. Just keep calling x.parent. So sometimes the parent
is to the right of us and sometimes the parent
is to the left of us. I'm going to draw this
in a, kind of, funny way. But this funny way has
a very special property, which is that the
x-coordinate in this diagram is the key value. Or is the sorted order
of the keys, right? Everything in the left subtree
of x has a value less than x. If we say all the
keys are different. Everything to the right of x
has a value greater than x. If x was the left
child of its parent, that means this thing
is also greater than x. And if we follow a
parent and this was the right child of
that parent, that means this thing is less than x. So that's why I drew it all
the way over to the left. This thing is also less
than x because it was a, I'll call it a left parent. Here we have a right
parent, so that means this is something
greater than x. And over here we have
a left parent, so this is something less than x. Let's say that's the root. In general, there's
going to be some left edges and some right
edges as we go up. These arrows will go either
left or right in a binary tree. So the rank of x is just
1 plus the number of nodes that are less than x. Number of keys that
are less than x. So there's these guys,
there's these guys, and there's whatever's
hanging off-- OK. Here I've almost violated
my x-coordinate rule. If I make these really
narrow, that's right. All of these things,
all of these nodes in the left subtrees of
these less than x nodes will also be less than x. If you think about
these other subtrees, they're going to
be bigger than x. So we don't really
care about them. So we just want to count
up all these nodes and all of these nodes. So the algorithm to do
that is pretty simple. We're just going
to start out with-- I'm going to switch from
this f notation to size. That's a little more natural. In general, you might
have many functions. Size is the usual
notation for subtree size. So we start out by counting
up how many items are here. And if we want to
start at a rank of 1, if the min has rank
1, then I should also do plus 1 for x itself. If you wanted to start at zero
you just omit that plus 1. And then, all I do is walk up
from x to the root of the tree. And whenever we go left
from, say x to x prime. So that means we
have an x prime. It's right child is x. And so when we went from x to
its parent we went to the left. Then we say rank plus
equals x prime.left.size plus 1 for x prime itself. And maybe x
prime.left.size is zero. Maybe there's no
nodes over there. But at the very least we have
to count those nodes that are to the left of us. And if there's
anything down here we add up all those things. So that lets us compute rank. How long does it take? Well, we're just walking
up one path from a leaf to a root-- or not necessarily
a leaf, but from some node x to the root. And as long we're using
a balance structure like AVL trees. I guess I want binary here,
so let's say AVL trees. Then this will take log n time. So I'm spending
constant work per step, and there's log n steps. Clear? So that's good old rank. Easy to do once you
have subtree size. Let's do select for fun. This may seem like review,
but I drew out this picture explicitly because we're
going to do it a lot today. We'll have pictures like
this a bunch of times. Really helps to
think about where the nodes are, which
ones are less than x, which ones are greater than x. Let's do select first. This you may not
have seen in 006. So we're going to
do the reverse. We're going to start at the root
and we're going to walk down. Sounds easy enough. But now walking down is
kind of like doing a search but we don't have a key
we're searching for, we have a rank we're searching for. So what is that rank? Rank is i. OK. So on the other hand,
we have the node x. We'd like to know the rank
of x and compare that to i. That will tell us whether we
should go left, or go right, or whether we happen
to find the item. Now one possibility
is we call rank of x to find the rank of x. But that's dangerous because I'm
going to have a four loop here and it's going to
take log n iterations. If at every iteration
of computing rank of x, and rank costs
log n, then overall cost might be log squared n. So I can't afford to-- I want
to know what the rank of x is but I can't afford to
say rank, open paren, x. Because that recursive
call will be too expensive. So what is the rank
of x in this case? This is a little special. What's that? AUDIENCE: Number of
left children plus 1. PROFESSOR: Number
of left, or the size of the left subtree plus 1. Yep. Plus 1 if we're counting,
starting at one. Very good. I'm slowly getting better. Didn't hit anyone this time. OK. So at least for the
root, this is the rank, and that only takes us constant
time in the special case. So we'll have to
check that it's still holds after I do the loop. But it will. So, cool. Now there are three cases. If i equals rank. If the rank we're
searching for is the rank that we happen to have,
then we're done, right? We just return x. That's the easy case. More likely is that I will be
either less than or greater than the rank of x. OK. So if i is less than the
rank, this is fairly easy. We just say x equals x.left. Did I get that right? Yep. In this case, the rank. So here we have x. It's at rank, rank. And then we have the left
subtree and the right subtree. And so if the rank
were searching for is less than rank, that
means we know it's in here. So we should go left. And if we just said x
equals x.left you might ask, well what rank are we
searching for in here? Well, exactly the same rank. Fine. That's easy case. In the other situation, if
we're searching in here, we're searching for
rank greater than rank. Then I want to go
right but the new rank that I'm searching for
is local to this subtree. I'm searching for
i minus this stuff. This stuff is rank. So I'm going to let
i be i minus rank. Make sure I don't have
any off by 1 errors. That seems to be right. OK. And then I do a loop. So I'll write repeat. So then I'm going to
go up here and say, OK. Now relative to this thing. What is the rank of the
root of this subtree? Well, it's again going to be
that node .left.size plus 1. And now I have the new
rank I'm searching for, i. And I just keep going. You could write this recursively
if you like, but here's an iterative version. So it's actually very familiar
to the select algorithm that we had, like when we
did deterministic linear time median finding or
randomized median finding. They had a very similar
kind of recursion. But in that case, they
were spending linear time to do the partition
and that was expensive. Here, we're just spending
constant time at each node and so the overall
cost is log n. So that's nice. Any questions about that? OK. I have a note here. Subtree size is obvious once you
know that's what you should do. Another natural
thing to try to do would be to augment,
for each node, what is the rank of that node? Because then rank is
really easy to find. And then select would
basically be a regular search. I just look at the
rank of the root, I see whether the
rank I'm looking for is too big, or too small, and I
go left or right, accordingly. What would be bad about
augmenting with rank of a node? Updates. Why? What's a bad example
for an update? AUDIENCE: If you add
new in home element. PROFESSOR: Right. Say we insert a new
minimum element. Good catch, cameraman. That was for the
camera, obviously. So, right. If we insert, this is off to
the side, but say we insert, I'll call it minus infinity. A new key that is smaller
than all other keys, then the rank of
every node changes. So that's bad. It means that easy tree
augmentation, in particular, isn't going to apply. And furthermore, it would
take linear time to do this. And you could keep inserting,
if you insert keys in decreasing order from there, every
time you do an insert, all the ranks increase by one. Maintaining that's going to
cost linear time per update. So you have to be really
careful that the function you want to store actually
can be maintained. Be very careful about that,
say, on the quiz coming up, that when you're
augmenting something you can actually maintain it. For example, it's very hard to
maintain the depths of nodes because when you do a rotation
a whole lot of depths change. Depth is counting from the root. How deep am I? When I do a rotation
then this entire subtree went down by one. This entire subtree
went up by one. In this picture. But it's very easy to
maintain heights, for example. Height counting from
the bottom is OK, because I don't affect
the height of a, b, and c. I affect it for x and y
but that's just two nodes. That I can afford. So that's what you want to be
careful of in the easy tree augmentation. So most the time easy tree
augmentation does the job. But in the remaining
two examples, I want to show you cooler
examples of augmentation. These are things you
probably wouldn't be expected to come up with
on your own, but they're cool. And they let us do more
sophisticated operations. So the first one is
called level linking. And here we're going to do it
in the context of 2-3 trees, partly for variety. So the idea of level
linking is very simple. Let me draw a 2-3 tree. Not a very impressive 2-3 tree. I guess I don't feel
like drawing too much. Level linking is the
idea of, in addition to these child and
parent pointers, we're going to add links
on all the levels. Horizontal links,
you might call them. OK. So that's nice. Two questions-- can we do this? And what's it good for? So let's start with
can we do this. Remember in 2-3 trees all
we have to think about are splits and merges. So in a split, we have,
for a brief period, let's say three
keys, four children. That's too many. So we change that to-- I'm going to change
this in a moment. For now, this is the split
you know and love, maybe. At least know. And if we think about where
the leveling pointers are, we have one before. And then we just need to
distribute those pointers to the two resulting nodes. And then we have to create a
new pointer between the nodes that we just created. This is, of course, easy to do. We're here. We're taking this node. We're splitting it in half. So we have the nodes
right in our hands so just add pointers between them. And key thing is, there's some
node over here on the left. It used to point
to this node, now we have to change it to
point to the left version. The left half of the node. And there's some node
over on the right. We have to change it's
left pointer to point to this right half of the node. But that's it. Constant time. So this doesn't fall under
the category of easy tree augmentation because this is
not isolated to the subtree. We're also dealing with it's
left and right subtrees. But still easy to
do in constant time. Merging nodes is
going to be similar. If we steal a node from our
parents or former sibling, nothing happens in
terms of level links. But if we have, say, an empty
node and a node that cannot afford any stealing. So we have single child
here, two children, and we merge it into-- We're taking something
from our parent. Bringing it down. Then we have three
children afterwards. Again, we used to have
these level pointers. Now we just have
these level pointers. It's easy to maintain. It's just a constant
size neighborhood. Because we have
the level links, we can get to our left
and right neighbors and change where
the links point to. So easy to maintain
in constant time. I'll call it constant overhead. Every time we do
a split or merge we spend additional
constant time to do it. We're already spending
constant time. So just changes everything
by constant factor. So far, so good. Now, I'm going to have
to tweak this data structure a little bit. But let me first tell you why. What am I trying to achieve
with this data structure? What I'm trying to achieve is
something called the finger search property. So let's just think
about the case where I'm doing a
successful search. I'm searching for key x and I
find it in the data structure. I find it in the tree. Suppose I found one-- I
search for x, I found it. And then I search
for another key y. Actually I think
I'll do the reverse. First I found y, now
I'm searching for x. If x and y are
nearby in the tree, I want this to run
especially fast. For example, if x is
the successor of y I want this to
take constant time. That would be nice. In the worst case x and y
are very far away from me in the tree then I want
it to take log n time. So how could I interpolate
between constant time for finding the
successor and log n time for finding the
worst case search. So I'm going to call
this search of x from y. Meaning, this is a little
imprecise, but what I mean is when I call
search, I tell it where I've already found y. And here it is. Here's the node storing y. And now I'm given a key x. And I want to find
that key x given the node that stores key y. So how long should this take? Will be a good
way to interpolate between constant
time at one extreme. The good case, when
x and y are basically neighbors in sorted
order, versus log n time, in the worst case. AUDIENCE: Distance
along the graph. PROFESSOR: Distance
along the graph. That would be one
reasonable definition. So I have a tree which you
could think of as a graph. Measure the shortest
path length from x to y. Or we have a more
sophisticated graph over here. Maybe that length. The trouble with the
distance in the graph, that's a reasonable suggestion,
but it's very data structure specific. If I use an AVL tree
without level links, then the distance
could be one thing, whereas if I use a 2-3 tree,
even without level lengths, it's going to be a
different distance. If I use a 2-3 tree
with level lengths it's going to be yet
another distance. So that's a little unsatisfying. I want this to be an
answer to a question. I don't want to phrase the
question in terms of that data structure. AUDIENCE: Difference
between ranks of x and y? PROFESSOR: Difference between
ranks between x and y. That's close. So I'm going to look at the
rank of x and rank of y. Let's say, take the
absolute difference. That's kind of how far away
they are in sorted order. Do you want to add anything? AUDIENCE: Log? PROFESSOR: Log. Yeah. Because in the worst case
the difference in ranks could be linear. So I want to add a log out
here to get log n in that worst case. Add a big o for safety. That's how much time
we want to achieve. So this would be the
finger search property that you can solve this
problem in this much time. Again, difference in
ranks is at most n. So this is at most log n. But if y is the successor of
x this will only be constant and this will be constant. So this is great if you're
doing lots of searches and you tend to search for
things that are nearby, but sometimes you search
for things are far away. This gives you a nice bound. On the one hand, we
have, this is our goal. Log difference of ranks. On the other hand, we
have the suggestion that what we can
achieve is something like the distance in the graph. But we have a problem with this. I used to think that data
structure solved this problem, but it doesn't. Let me just draw-- actually
I have a tree right there. I'm going to use that one. Suppose x is here and y is here. OK. This is a bit of a small tree
but if you think about it long enough, this node is
the predecessor of this node. So their difference
in ranks should be 1. But the distance in
the graph here is two. Not very impressive. But in general, you have
a tree of height log n. If you look at the root, and
the predecessor of the root, they will have a rank
difference of one by definition of predecessor. But the graph distance
will be log n. So that's bad news, because if
we're only following pointers there's no way to get from
here to there in constant time. So we're not quite there. We're going to use another tweak
that data structure, which is store the data in the leaves. Tried to find a data structure
that didn't require this and still got finger search. But as far as I
know, there is none. No such data structure. If you look at, say,
Wikipedia about B-trees, you'll see there's a ton
of variations of B-trees. B+-trees, B*-trees. This is one of those. I think B+-trees. As you saw, B-trees or
2-3 trees, every node stored one or two keys. And each key only
existed in one spot. We're still only going to put
each key in one spot, kind of. But it's only going
to be the leaf spots. OK. Good news is most nodes
are leaves, right? Constant fraction of the
nodes are going to be leaves. So it doesn't change too
much from a space efficiency standpoint. If we just put data down
here and don't put-- I'm not going to put any
keys up here for now. So this a little weird. Let me draw an example
of such a tree. So maybe we have 2, and 5,
and 7, and 8, 9, let's say. Let's put 1 here. So I'm going to have a node
here with three children, a node here with two
children, and here's a node with two children. So I think this mimics
this tree, roughly. I got it exactly right. So here I've taken
this tree structure. I've redrawn it. There's now no keys
in these nodes. But everything else is
going to be the same. Every node is going
to have 0 children if it's a leaf, or two, or
three children otherwise. Never have one child
because then you wouldn't get logarithmic depth. All the leaves are going
to be at the same depth. And that's it. OK. That is a 2-3 tree with the
data stored in the leaves. It's a useful trick to know. Now we're going to do a
level linked 2-3 tree. So in addition to
that picture, we're going to have links like this. OK. And I should check that I can
still do insert and delete into these structures. It's actually not too hard. But let's think about it. I think, actually,
it might be easier. Let's see. So if I want to
do an insert-- OK. I have to first search
for where I'm inserting. I haven't told you
how to do search yet. OK. So let's first
think about search. What we're going to do is
data structure augmentation. We have simple
tree augmentation. So I'm going to do
it and each node, what the functions
I'm going to store are the minimum
key in the subtree, and the maximum
key in the subtree. There are many ways to
do this, but I think this is kind of the simplest. So what that means
is at this node, I'm going to store 1 as
the min and 7 as the max. And at this node it's
going to be 1 at the min and 9 at the max. And here we have 8 as
the min and 9 as the max. Again min and max of
subtrees are easy to store. If I ever change a
node I can update it based on its children,
just by looking at the min of the
leftmost child and the max of the rightmost child. If I didn't know 1
and 9, I could just look at this min and
that max and that's going to be the min and the
max of the overall tree. So in constant time
I can update the min and the max of a
node given the min and the max of its children. Special case is at the leaves. Then you have to actually
look at keys and compare them. But leaves only have,
at most, two keys. So pretty easy to compare
them in constant time. OK. So that's how I do
the augmentation. Now how do I do a search? Well, if I'm at a node and
I'm searching for a key. Well, let's say
I'm at this node. I'm searching for a key like 8. What I'm going to do is
look at all of the children. In this case, there's two. In the worst case there's three. I look at the min and max
and I see where does 8 fall? Well it falls in this interval. If I was searching for 7
1/2 I know it's not there. It's going to be
in between here. If I'm doing a successor
then I'll go to the right. If I'm doing predecessor
I'll go to the left. And then take either the maximum
item or the minimum item. If I'm searching
for 8 I see, oh. 8 falls in the interval
between 8 and 9, so I should clearly
take the right child among those two children. In general, there's
three children. Three intervals. Constant time. I can find where my key
falls in the interval. OK. So search is going to take
log n time again, provided I have these mins and maxs. If you stare at it
long enough, this is pretty much the same thing
as regular search in a 2-3 tree. But I've put the data
just one level down. OK. Good. That was regular search. I still need to do finger
search, but we'll get there. And now, if I want to do
an insert into this data structure, what happens. Well I search for the key
let's say I'm inserting 6. So maybe I go here. I say because 6. Is in this interval. 6 is in neither of
these intervals. But it's closest to the interval
2, 5, or the interval 7. Let's say I go down to 2, 5. And well, to insert 6 I'll
just add a 6 on there. Of course, now that
node is too big. So there's still going to be a
split case at the leaves where I have let's say,
a,b,c, too many keys. I'm going to split
that into a,b and c. This is different from before. It used to be I would
promote b to the parent because the parent
needed the key there. Now parents don't have keys. So I'm just going to split
this thing, roughly, in half. It works. It's still the case that
whoever was the parent up here now has an
additional child. One more child. So maybe that node
now has four children but it's supposed
to be two or three. So if I have a node with four
children, what I'm going to do, I'm suppose to use
these fancy arrows. What do I do in this case? It's just going to split
that into two nodes with two children. And again this used
to have a parent. Now that parent has
an additional child, and that may cause
another split. It's just like before. Was just potentially split
all the way up to the root. If we split the root then
we get an additional level. But we could do all this and
we can still maintain our level links, if we want. But everything will take log n. I won't draw the
delete case, as delete is slightly more annoying. But I think, in
this case, you never have to worry about where
is the key coming from, your child or your parent? You're just merging nodes so
it's a little bit simpler. But you have to deal
with the leaf case separately from
the nonleaf case. OK. So all this was to
convince you that we can store data in the leaves. 2-3 trees still work fine. Now I claim that the graph
distance in level link trees is within a constant factor
of the finger search bound. So I claim I can get the finger
search property in 2-3 trees, with data in the leaves,
with level links. So lots of changes here. But in the end, we're going
to get a finger search bound. Let's go over here. So here's a finger
search operation. First thing I want to
do is identify a node that I'm working with. I want to start from y's node. So we're supposing that we're
told the node, a leaf, that contains y. So I'm going to
let v be that leaf. OK. Because we're supposing
we've already found y, and now all the data
is in the leaves. So give me the leaf
that contains y. So that should
take constant time. That's just part of the input. Now I'm going to do a
combination of going up and horizontal. So starting at a leaf. And the first thing I'm
going to do is check, does this leaf
contain what I want? Does it contain the key I'm
searching for, which is x? So that's going to be the case. At every node I store
the min and the max. So if x happens to fall
between the min and the max, then I'm happy. Then I'm going to do a
regular search in v's subtree. This seems weird in
the case of a leaf. In the case of a
leaf, this is just to check the two
keys that are there. Which one is x. OK. But in general I gave
you this search algorithm which was, if I decide
which child to take, according to the ranges,
that's a downward search. So that's what I'm calling
regular search here. Maybe downward would
be a little better. This is the usual
log n time thing. But we're going to claim
a bound better than log n. If this is not the
case, then I know x either falls before
v.min or after v.max. So if x is less than v.min
then I'm going to go left. v equals v. I'll call it
level left to be clear. You might say left
is the left child. There's no left child
here, of course. But level left is clear. We take the horizontal
left pointer. And otherwise x is
greater than v.max. And in that case
I will go right. That seems logical. And in both cases
we're going to go up. x equals x.parent Whoops. v equals v.parent. X is not changing here. X is a key we're searching
for. v is the node. V for vertex. So we're always going
to go up, and then we're going to go
either left or right, and we're going to keep
doing that until we find a subtree that contains
x in terms of key range. Then we're going
to stop this part and we're just going
to do downward search. I should say return
here or something. I'm going to do a
downward search, which was this regular algorithm. And then whatever it finds,
that's what I return. I claim the algorithm
should be clear. What's less clear is that
it achieves the bound that we want. But I claim that this will
achieve the finger search property. Let me draw a picture
of what this thing looks like kind of generically. On small examples it's hard
to see what's going on. So I'm going to draw a
piece of a large example. Let's say we start here. This is where y was. I'm searching for x. Let's suppose x is to the right. 'Cause otherwise I go
to the other board. So x is to the right. I'll discover that the range
with just this node, this node maybe contains one other key. I'll find that
range is too small. So I'm going to go follow
the level right pointer, and I get to some other node. Then I'm going to
go to the parent. Maybe the parent was the
parent of those two children so I'm going to
draw it like that. Maybe I find this
range is still too low. I need to go right to get to x,
so I'm going to follow a level pointer to the right. I find a new subtree. I'll go to its parent. Maybe I find that this subtree,
still the max is too small. So I have to go to
the right again. And then I take the parent. So this was an example
of a rightward parent. Here's an example of
a leftward parent. This is maybe the parent of
both of these two children. Then maybe this subtree
is still too small, the max is still smaller than x. So then I go right
one more time. Then I follow the parent. Always alternating
between right and parent until I find a node
whose subtree contains x. It might have actually, x
may be down here, because I immediately went to the
parent without checking whether I found where x is. But if I know that x is
somewhere in here then I will do a downward search. It might go left and then down
here, or it might go right, or there's actually
potentially three children. One of these searches
will find the key x that I'm looking
for because I'm in the case where x is
between v.min and v.max, so I know it's in
there, somewhere. It could be x doesn't exist, but
it's predecessor or successor is in there somewhere. And so one of these
three subtrees will contain the x range. And then I go follow that path. And keep going down until I
find x or it's predecessor or successor. Once I find it's predecessor I
can use a level right pointer to find its
successor, and so on. So that's kind of the general
picture what's going on. We keep going rightward
and we keep going up. Suppose we do k up steps. Let's look at this
last step here. Step k. How high am I in the tree? I started at the leaf level. Remember in a 2-3 tree all the
leaves have the same level. And I went up every step. Sorry. I don't know what this
is, like the 2-step dance where, let's say every
iteration of this loop I do one left or right step,
and then a parent step. So I should call
this iteration k. I guess there's
two k steps, then. Just to be clear. So in iteration k, that
means I've gone up k times and I've gone either
right or left k times. You can show if you start going
right you keep going right. If you initially go left
you'll keep going left. Doesn't matter too much. At iteration k I am at
height k, or k minus 1, or however you want to count. But let's call it k. So when I do this
right pointer here I know that, for
example, I am skipping over all of these keys. All the keys down-- the
keys are in the leaves, so all these things down
here, I'm jumping over them. How many keys are down there? Can you tell me,
roughly, how many keys I'm skipping over when I'm
moving right at height k? It's not a unique answer. But you can give me some bounds. Say again. Number of children
to the k power. Yeah. Except we don't know
the number of children. But it's between 2 and 3 Closer
one should be easy but I fail. So it's between two
and three children. So there's the number-- if
you look at a height k tree, how many leaves does it have? It's going to be between
2 to the k and 3 to the k. Because I have between 2 and
3 children at every node. And so it's exponential in k. That's all I'll need. OK. When I'm at height k here,
I'm skipping over a height k minus 1 tree or something. But it's going to be-- So in iteration k I'm skipping,
at least, some constant times 2 to the k. Maybe to the k minus
1, or to the k minus 2. I'm being very sloppy. Doesn't matter. As long as it's exponential
in k, I'm happy. Because I'm supposing that
x and y are somewhat close. Let's call this
rank difference d. Then I claim the
number of iterations I'll need to do in this loop
is, at most, order log d. Because if, when I get
to the k-th iteration, I'm jumping over 2
to the k elements. How large does k have
to be before 2 to the k is larger than d? Well, log d. Log base 2 The number of
iterations is order log d, where d is
the rank difference. d is the absolute value between
rank of x and rank of y. And I'm being a
little sloppy here. You probably want
to use an induction. You need to show that
they're really, these items here that you're skipping
over that are strictly between x and y. But we know that there's
only d items between x or y. Actually d minus 1, I guess. So as soon as we've skipped over
all the items between x and y, then we'll find a
range that contains x, and then we'll go do
the downward search. Now how long does the
downward search cost? Whatever the height
of the tree is. What's the height of the tree? That's the number of iterations. So the total cost. The downward search
will cost the same as the rest of the search. And so the total cost is
going to be order log d. Clear? Any questions about finger
searching with level linked data at the
leaves, 2-3 trees? AUDIENCE: Sir, I'm not sure
why [INAUDIBLE] d, why is that? PROFESSOR: I'm defining d to be
the rank of x minus rank of y. My goal is to achieve
a log d bound. And I'm claiming that because
once I've skipped over d items, then I'm done. Then I've found x. And at step k I'm skipping
over 2 to the k items. So how big is k going to be? Log d. That's all. I used d for a notation here. Cool. Finger searching. It's nice. Especially if you're doing
many consecutive searches that are all relatively
close to each other. But that was easy. Let's do a more
difficult augmentation. So the last topic for
today is range trees. This is probably the coolest
example of augmentation, at least, that you'll
see in this class. If you want to see
more you should take advanced data structure 6851. And range trees solve
a problem called orthogonal range searching. Not orthogonal search ranging. Orthogonal range search. So what's the problem? I'm going to give you
a bunch of points. Draw them as fat dots so
you can actually see them. In some dimension. So this is, for
example, a 2D point set. OK. Over here I will
draw a 3D point set. You can tell the
difference, I'm sure. There. Now it's a 3D point set. And this is a static point set. You could make this
dynamic but let's just think about the static case. Don't want the 2D points
and the 3D points to mix. Now, you get to preprocess
this into a data structure. So this is a static
data structure problem. And now I'm going to come along
with a whole bunch of queries. A query will be a box. OK. In two dimensions, a
box is a rectangle. Something like this. Axis aligned. So I give you an x min, x
max, a y min, and a y max. I want to know what
are the points inside. Maybe I want you to list them. If there's a lot
of them it's going to take a long
time to list them. Maybe I just want to know
10 of them as examples. Maybe this is a Google
search or something. I just get the first 10
results in the first page, I hit next then want the
next 10, that kind of thing. Or maybe I want to know how
many search results there are. Number of points
in the rectangle. Bunch of different problems. In 3D, it's a 3D box. Which is a little
harder to draw. You can't really tell which
points are inside the box. Let's say these three points
are all inside the box. I give you an interval
in x, an interval in y, and an interval in
z, and I want to know what are the points inside. How many are there? List them all. List 10 of them, whatever. OK. I want to do this in
poly log time, let's say. I'm going to achieve today
log squared for the 2D problem and log cubed for
the 3D problem, plus whatever the
size output is. So let me just write that down. So the goal is to preprocess
n points in d dimensions. So you get to spend a
bunch of time preprocessing to support a query which is,
given a box, axis aligned box, find let's say the number
of points in the box. Find k points in the box. I think that's good. That includes a special case of
find all the points in the box. So this, of course, we have
to pay a penalty of order k for the output. No getting around that. But I want the rest of the
time to be log to the d. So we're going to
achieve log to the d n plus size of the output. And you get to control how
big you want the output to be. So it's a pretty
reasonable data structure. In a certain sense we will
understand what the output is in log to the d time. If you actually
want to list points, well, then you have to
spend the time to do it. All right. So 2D and 3D are great,
but let's start with 1D. First we should
understand 1D completely, then we can generalize. 1D we already know how to do. 1D I have a line. I have some points on the line. And I'm given, as a
query, some interval. And I want to know how many
points are in the interval, give me the points in
the interval, and so on. So how do I do this? Any ways? If d is 1. So I want to achieve
log d, sorry, log n, plus size of output. I hear whispers. Yeah? AUDIENCE: Segment trees? PROFESSOR: Segment tree? That's fancy. We won't cover segment trees. Probably segment trees do it. Yeah. We know lots of ways to do this. Yeah? AUDIENCE: Sorted array? PROFESSOR: Sorted array
is probably the simplest. If I store the items in a sorted
array and I have two values, I'll call them x1
and x2, because it's the x min and x max. Binary search for x1. Binary search for x2. Find the successor of x1
and the predecessor of x2. I'll find these two guys. And then I know all
the ones in between. That's the match. So that'll take log n
time to find those points and then we're good. So we could do a sorted array. Of course, sorted array is
a little hard to generalize. I don't want to do a 2D
array, that sounds bad. You could, of course,
do a binary search tree. Like an AVL tree. Same thing. Because we have log n
search, find successor, and predecessor, I guess
you could use Van Emde Boas, but that's hard to
generalize to 2D. You could use level links. Here's a fancy version. We could use level linked 2-3
trees with data in the leaves. Then once I find x
min, I find this point, I can go to the successor
in constant time because that's a finger search
with a rank difference of 1. And I could just keep
calling successor and in constant time per item
I will find the next item. So we could do that easily
with the sorted array. BST is not so great
because successor might cost log n each time. But if I have the
level links then basically I'm just
walking down the link list at the bottom of the tree. OK. So actually level
linked is even better. BST would achieve something
like log n plus k log n, where k is the size of the output. If I want k points in the
box I have to pay log n. For each level linked I'll
only pay log n plus k. Here I actually only need
the levels at the leaves. Level links. OK. All good. But I actually want to
tell you a different way to do it that will
generalize better. The pictures are
going to look just like the pictures
we've talked about. So these would actually
work dynamically. My goal here is just to achieve
a static data structure. I'm going to idealize this
solution a little bit. And just say, suppose I
have a perfectly balanced binary search tree. That's going to be
my data structure. OK. So the data structure is not
hard, but what's interesting is how I do a range search. So if I do range query of the
interval, I'll call it ab. Then what I'm going to do
is do a binary search for a, do a binary search for
b, trim the common prefix of those search paths. That's basically finding
the lowest common ancestor of a and b. And then I'm going
to do some stuff. Let me draw the picture. So here is, suppose here's
the node that contains a. Here's the node that contains b. They may not be at the
same depth, who knows. Then I'm going to look
at the parents of a. I just came down from some path
here, and some path down to b. I want to find this
branching point where the paths to a and
the paths to b diverge. So let's just look
at the parent of a. It could be a right
parent, in which case there's a subtree here. Could be a left parent in
which case, subtree here. I'm going to follow
my convention again. That x-coordinate
corresponds roughly to key. Left parent here. Maybe right parent here. Something like that. OK. Remember it's a perfect tree. So, actually, all the leaves
will be at the same level. And, roughly here, x-coordinate
corresponds to key. So here is a. And I want to return all the
keys that are between a and b. So that's everything
in this sweep line. The parents of the LCA don't
matter, because this parents either going to be way over
to the right or way over to the left. In both cases, it's outside
the interval a to b. So what I've tried
to highlight here, and I will color it in
blue, is the relevant nodes for the search between a and b. So a is between a and b. This subtree is greater
than a and less than b. This node, and these nodes. This node, and these nodes. This node and these nodes. The common ancestor. And then the corresponding
thing over here. All the nodes in all
these blue subtrees, plus these individual
nodes, fall in the interval between a and b, and that's it. OK. This should look super familiar. It's just like when
we're computing rank. We're trying to figure out
how many guys are to our left or to our right. We're basically doing
a rightward rank from a and a
leftward rank from b. And that finds all the nodes. And stopping when those
two searches converge. And then we're finding all
the nodes between a and b. I'm not going to write down
the pseudocode because it's the same kind of thing. You look at right
parents and left parents. You just walk up from a. Whenever you get a
right parent then you want that node, and
the subtree to its right. And so that will
highlight these nodes. Same thing for b, but
you look at left parents. And then you stop when
those two searches converge. So you're going to
do them in lock step. You do one step for a and b. One step for a and b. And when they happen to hit the
same node, then you're done. You add that node to your list. And what you end
up with is a bunch of nodes and rooted subtrees. The things I circled in blue
is going to be my return value. So I'm going to return all
of these nodes, explicitly. And I'm also going to
return these subtrees. I'm not going to have
to write them down. I'm just going to return
the root of the subtree, and say, hey look. Here's an entire
subtree that contains points that are in the answer. Don't have to list
them explicitly, I can just give you the tree. Then if I want to know how
many results are in the answer, well, just augment to store
subtree size at the beginning. And then I can
count how many nodes are down here, how many
nodes are down here, add that up for
all the triangles, and then also add one for
each of the blue nodes, and then I've counted the size
of the answer in how much time? How many subtrees and how many
nodes am I returning here? Log. Log n nodes and log n rooted
subtrees because at each step, I'm going up by one for
a, and up by one for b. So it's like 2 log n. Log n. So I would call this an implicit
representation of the answer. From that implicit
representation you can do subtree size. Augmentation to count
the size the answer. You can just start walking
through one by one, do an inter traversal of the trees, and
you'll get the first k points in the answer in order k time. Question? AUDIENCE: Just a clarification. You said when we
were walking up, you want to get
all the ancestors in their right subtrees. But you don't do that for
the left parent, right? PROFESSOR: That's right. As I'm walking up the tree,
if it's a right parent then I take the right subtree
and include that in the answer. If it's a left parent
just forget about it. Don't do anything. Just keep following parents. Whenever I do right
parent then I also add that node and
the right subtree. If it's a left parent I
don't include the node, I don't include
the left subtree. I also don't include
the right subtree. That would have too much stuff. It's easy when you
see the picture, you would write
down the algorithm. It's clear. It's left versus right parents. AUDIENCE: Would you include
the left subtree of b? PROFESSOR: I would
also-- thank you. I should color the
left subtree of b. I didn't apply
symmetry perfectly. So we have the right subtree
of a and the left subtree of b. Thanks. I would also include b if
it's a closed interval. Slightly more general. If a and b are not
in the tree then this is really the successor of a and
this is the predecessor of b. So then a and b don't
have to be in there. This is still a well
defined range search. OK. Now we really understand 1D. I claim we've almost
solved all dimensions. All we need is a little
bit of augmentation. So let's do it. Let's start with 2D. But then 3D, and 4D,
and so on will be easy. Why do I care about
4D range trees? Because maybe I have a database. Each of these points
is actually just a row in the database which has
four columns, four values. And what I'm trying to do
here is find all the people in my database that have a
salary between this and this, and have an age
between this and that, and have a profession
between this and this. I don't know what that means. Number of degrees between
this and this, whatever. You have some numerical data
representing a person or thing in your database, then this
is a typical kind of search you want to do. And you want to know how
many answers you've got and then list the first
hundreds of them, or whatever. So this is a practical
thing in databases. This is what you might call
an index in the database. So let's start. Suppose your data is
just two dimensional. You have two fields
for every item. What I'm going to do is store
a 1D range tree on all points by x. So this data structure makes
sense if you fix a dimension. Say x is all I care about. Forget about y. So my point set. Yeah. So what that corresponds to is
projecting each of these points onto the x-axis. And now also
projecting my query. So my new query is
from here to here in x. And so this data
structure will let me find all these
points that match in x. That's not good because
there's actually only two points that
I want, but I find four points in this picture. But it's half of the answer. It's all the x matches
forgetting about y. Now here's the fun part. So when I do a search
here I get log n nodes. Nodes are good because they
have a single key in them. So I'll just check for
each of those log n nodes. Do they also match in y? If they do, add
it to the answer. If they don't forget about it. OK. But the tricky part is I also
get log n subtrees representing parts of the answer. So potentially it could be that
your search, this rectangle, only has like five points. But if you look at this
whole vertical slab there's a billion points. Now, luckily, those
billion points are represented succinctly. There's just log
n subtrees saying, well there's half
a billion here, and a quarter billion here, and
an eighth of a billion here. Now for each of that
big chunk of output, I want to very quickly find
the ones that match in y. How would I find the
ones matching in y? A range tree. Yeah. OK. So here's what
we're going to do. For each node, call it x. x is overloaded. It's a coordinate. So many things. Let's call it v.
In the, this thing I'm going to call the x-tree. So for every node
in the x-tree I'm going to store
another 1D range tree. But this time using
the y-coordinate on all points in these rooted subtree. At this point I really
want to draw a diagram. So, rough picture. Forgive me for not
drawing this perfectly. This is roughly what
the answer looks like for the 1D range search. This is the x-tree. And here I've searched between
this value and this value in the x-coordinate. Basically I have log n nodes. I'm going to check
those separately. Then I also have
these log n subtrees. For each of those
log n sub trees I'm going to have
a pointer-- this is the augmentation-- to another
tree of exactly the same size. On exactly the same
data that's in here. It's also over here. But it's going to
be sorted by y. And it's a 1D range tree by y. Tons of data duplication here. I took all these points and I
copied them over here, but then built a 1D range tree in y. This is all preprocessing. So I don't have to pay for this. It's polynomial time. Don't worry too much. And then I'm going
to search in here. What does the search
in there look? I'm going to get, you know,
some more trees and a couple more nodes. OK. But now those items, those
points, match in x and y because this whole
subtree matched in x and I just did a y search, so I
found things that matched in y. So I get here
another log n trees that are actually in my answer. And for each of these nodes I
have a corresponding other data structure where I
do a little search and I get part of the answer. Every one. Sounds huge. This data structure sounds
huge, but it's actually small. But one thing that's clear is
it takes log squared n time, because I have log n
triangles over here. For each of them I spend log
n to find triangles over here. The total output is log squared
n nodes, for each of them I have to check manually. Plus, so over here,
there's log n, different searches I'm doing. Each one has size log n. So I get log squared
little triangles that contain the results
that match in x and y. How much space in
this data structure? That's the remaining challenge. Actually, it's not that hard,
because if you look at a key. So look at some
key in this x-tree. Let's look at a leaf
because that's maybe the most interesting. Here's the x-tree. x-tree has linear size. Just one tree. If I look at some key value,
well, it lives in this subtree. And so there's going to be a
corresponding blue structure of that size that
contains that key. And then there's the parent. So there's a structure here. That has a corresponding
blue triangle. And then its parent,
that's another triangle. That contains-- I'm
looking at a key k here. All of these triangles
contain the key k. And so key k will be
duplicated all this many times, but how many sub trees is k in? Log n. Each key, fundamental fact
about balanced binary search trees, each key lives
in log n subtrees. Namely all of its ancestors. Awesome. Because that means the
total space is n log n. There's n keys. Each of them is duplicated
at most log n times. In general, log
to the d minus 1. So If you do it in 3D,
each of the blue trees, every node in it has a
corresponding pointer to a red tree
that's sorted by z. And you just keep doing this,
sort of, nested searching, like super augmentation. But you're only losing a log
factor each dimension you add.

Diff. Eq.

Okay, that's,
so to speak, the text for today.
The Fourier series, and the Fourier expansion for f
of t, so f of t, if it looks like
this should be periodic, and two pi should be a period.
Sometimes people rather sloppily say periodic with
period two pi, but that's a little ambiguous.
So, this period could also be pi or a half pi or something
like that as well. The an's and bn's are
calculated according to these formulas.
Now, we're going to need in just a minute a consequence of
those formulas, which, it's not subtle,
but because there are formulas for an and bn,
it follows that once you know f of t,
the an's and bn's are determined.
Or, to put it another way, a function cannot have two
different Fourier series. Or, to put it yet another way,
if f of t, if two functions are equal,
you'll see why I write it in this rather peculiar form.
Then, the Fourier series for f is the same as the Fourier
series for g. And, the reason is because if f
is equal to g, then this integral with an f
there is the same as the integral with a g there.
And therefore, the an's come out to be the
same. In the same way,
the bn's come out to be the same.
So, the Fourier series are the same, coefficient by
coefficient, for f and g. Now, my ultimate goal-- let's
all put down the argument since there are formulas,
since we have formulas for an and bn.
Now, a consequence of that is, well, let me first say,
what I'm aiming at is you will be amazed at how long it's going
to take me to get to this. I just want to calculate the
Fourier series for some rather simple periodic function.
It's going to look like this. So, here's pi,
and here's negative pi. So, the function which just
looks like t in between those two, so, it goes up to,
it's a function, t, more or less,
goes up to pi here, minus pi there.
But, of course, it's got to be periodic of
period two pi. Well, then, it just repeats
itself after that. After this, it just does that,
and so on. It's a little ambiguous what
happens at these endpoints. Well, let's not worry about
that for the moment, and frankly,
it won't really matter because the integrals don't care about
what happens in individual points.
So, there's my f of t. Now, I, of course,
could start doing it right away.
But, you will quickly find, if you start doing these
problems and hacking around with them, that the calculations seem
really quite long. And therefore,
in the first half of the period, the first half of the
period I want to show you how to shorten the calculations.
And in the second half of the period, after we've done that
and calculated this thing successfully,
I hope, I want to show you how to remove various restrictions
on these functions, how to extend the range of
Fourier series. Well, one obvious thing,
for example, is suppose the function isn't
periodic of period two pi. Suppose it has some other
period. Does that mean there's no
formula? Well, of course not.
There's a formula. But, we need to know what it
is, particularly in the applications,
the period is rarely two pi. It's normally one,
or something like that. But, let's first of all,
I'm sure what you will appreciate is how the
calculations can get shortened. Now, the main way of shortening
them is by using evenness and oddness.
And, what I claim is this, that if f of t is an
even function, remember what that means,
that f of negative t is equal to f of t.
Cosine is a good example, of course, cosine nt;
are all these functions are even functions.
If f of t is even, then its Fourier series
contains only the cosine terms. In other words,
half the calculations you don't have to do if you start with an
even function. That's what I mean by
shortening the work. There are no odd terms,
or let's put it positively. All the bn's are zero.
Now, one way of doing this would be to say,
well, y to the bn zero, well, we've got formulas,
and fool around with the formula for the bn,
and think about a little bit, and finally decide that that
has to come out to be zero. That's not a bad way,
and it would remind you of some basic facts about integration,
about integrals. Instead of doing that,
I'm going to apply my little principle that if two functions
are the same, then their Fourier series have
to be the same. So, the argument I'm going to
give is this, so, I'm going to try to prove
this statement now. And, I'm going to use the facts
on the first board to do it. So, what is f of minus t? Well, if that's equal to f of
t, then in terms of the Fourier series,
how do I get the Fourier series for f of minus t?
Well, I take the Fourier series for f of t, and substitute t
equals minus t. Now, what happens when I do
that? So, the Fourier series for this
looks like a zero over two plus summation what?
Well, the an cosine nt, that does not change
because when I change t to negative t,
the cosine nt does not change, stays the same
because it's an even function. What happens to the sine term?
Well, the sine of negative nt is equal to minus the
sine of nt. So, the other terms,
the sine terms change sign. So, all that's the result of
substituting t for negative t and f of t. On the other hand,
what's f of t itself? Well, f of t itself is what
happened before that. Now it's got a plus sign
because nothing was done to the series.
Well, if the function is even, then those two right hand sides
are the same function. In other words,
they're like my f of t equals g of t. And therefore, the Fourier series on the left
must be the same. In other words,
if these are equal, therefore, these have to be
equal, too. Now, there's no problem with
the cosine terms. They look the same.
On the other hand, the sine terms have changed
sign. Therefore, it must be the case
that bn is always equal to negative bn for all n.
That's the only way this series can be the same as that one.
Now, if bn is equal to negative bn,
that implies that bn is zero. Zero is the only number which is equal to its negative.
And so, by this argument, in other words,
using the uniqueness of Fourier series, we conclude that if the
function is even, then its Fourier series can
only have cosine terms in it. Now, you say,
hey, that's obvious. The cosine, that's just a point
of logic. But, this is a mathematics
course, after all. It's not just about
calculation. Many of you would say,
yeah, of course that's obvious because cosines are even,
and the sines are odd. I say, yeah,
and so why does that make it true?
Well, the cosine's even. Plus t into minus t,
and what you are proving is the converse.
The converse is obvious. Yeah, obvious,
I don't care. If the right-hand side is the
sum of the functions, well, so is the left.
But I'm saying it the other way around.
If the left is an even function, why does the
right-hand side have to have only even terms in it?
And, this is the argument which makes that true.
Now, there is a further simplification because if you've
got an even function, oh, by the way,
of course the same thing is true for the odd,
I ought to put that down, and so also,
if f of t is odd, then I think one of these
proofs is enough. The other you can supply
yourself. That will imply that all the
an's are zero, even including this first one,
a zero, and by the same reasoning. So, an even function uses only
cosines for its Fourier expansion.
An odd function uses only sines.
Good. But, we still have to,
suppose we got an even function.
We've still got to calculate this integral.
Well, even that can be simplified.
So, the second stage of the simplification,
again, assuming that we have an even or odd function,
and by the way, [LAUGHTER].
Totally unauthorized. So, if f of t is even,
what we'd like to do now is simplify the integral a little.
And, there is an easy way to do that, because,
look, if f of t is an even function, then so is f of t
cosine nt, is also even.
Imagine, we could make little rules about an even function
times an even function is an even function.
There are general rules of that type, and some of you know them,
and they are very useful. But, let's just do it ad hoc
here. If I change t to negative
t here, I don't change the function
because it's even. And, I don't change the cosine
because that's even. So, if I change t to negative
t, I don't change the function. Either factor that function,
and therefore I don't change the product of those two things
either. So, it's also even.
Now, what about an even function when you integrate it?
Here's a typical looking even function, let's say,
something like, I don't know,
wiggle, wiggle, again.
Here's our better even function.
All right, so, minus pi to pi,
even, even though the t-axis is somewhat curvy.
So, there is an even function. The point is that if you
integrate an even function from negative pi to pi,
I think you all know even from calculus you were taught to do
this simplification. Don't do that.
Instead, integrate from zero to pi, and double the answer.
Why should you do that? The answer is because it's
always nice to have zero as one of the limits of integration.
I trust to your experience, I don't have to sell that.
Minus pi is a particularly unpleasant lower limit of
integration because you are sure to get in trouble with negative
signs. There are bound to be at least
three negative signs floating around.
And, if you miss one of them, you'll get the wrong signs of
answer. The answer will have the wrong
sign. So, the way the formula from
this simplifies is that an, instead of integrating from
negative pi to pi, I can integrate only from zero
to pi, and double the answer. So, our better formula is this.
If the function is even, this is the formula you should
use: zero to pi, f of t cosine nt dt. Of course, I don't have to tell you what bn should be because bn
will be zero. And, in the same way,
if f is odd, the same reasoning shows that
bn-- of course, an will be zero this time.
But it will be bn that will be two over pi times the integral
from zero to pi of f of t sine nt dt. Maybe we'd better just a word about that since,
why is that so? If it's odd,
doesn't that mean things become zero?
If you integrate an odd function like that,
the integral over minus pi to pi, you get zero.
Well, but this is not an odd function.
This is an odd function, and this is an odd function.
But the product of two odd functions is an even function.
Odd times odd is even. I said I wasn't going to give
you those rules, but since this is the one which
trips everybody up, maybe I'd better say it just
justbecause it looks wrong. Right, this is odd.
That's odd. Think about it.
If I change t to negative t, this multiplies by
minus one. This multiplies by minus one.
And therefore, the product multiplies by minus
one times minus one. In other words,
it multiplies by plus one. Nothing happens,
so it stays the same. Why does nobody believe this,
even though it's true? It's because they are thinking
about numbers. Everybody knows that an odd
number times an odd number is an odd number.
So, I'm not multiplying numbers here, which also I'll put them
in boxes to indicate that they are not numbers.
How's that? Brand-new invented notation.
The box means caution. The inside is not a number,
it's the word odd or even. It's just a symbolic statement
that the product of an odd function and an odd function is
an even function. Even times even is even.
What's odd times even? Yes, it has to get equal time.
Obviously, something must come out to be odd,
right. Okay, so, now that we've got
our two simplifications, we are ready to do this
problem. Instead of attacking it with
the original formulas, we are going to think about it
and attack it with our better formulas.
So, now we are going to calculate the Fourier series for
f of t. The first thing I see,
so f of t is our little thing here.
Well, first of all, what kind of function is it:
odd, even, or neither? Most functions are neither,
of course. But, fortunately in the
applications, functions tend to be one or the
other. Or, they can be converted into
one to the other. Maybe if I get a chance,
I'll show you a little how, or the recitations will.
So, this function is odd. Okay, half the work just
disappeared. I don't have to calculate any
an's. They will be zero.
So, I only have to calculate bn, and I'll calculate them by
my better formula. So, it's two over pi times the
integral from zero to pi, and what I have to integrate,
well, now, finally you've got to integrate something.
From zero to pi, this is the function,
t. So, I have to integrate t times
sine of nt dt. Okay, so this is why you learned
integration by parts, one of many reasons why you
learned integration by parts, so that you wouldn't have to
pull out your little calculators to do this.
Okay, now, let's do it. So, it's two over pi. Let's solve that away so we can
forget about it. And, what's then left is just
the evaluation of the integral between limits.
So, if I integrate by parts, I'll want to differentiate the
t, and integrate the sign, right?
So, the first step is you don't do the differentiation.
You only do the integration. So, that integrates to be
cosine nt over n, more or less.
The only thing is, if I differentiate this,
I get negative sine nt instead of,
so, I want to put a negative sign in front of all this.
And, I will evaluate that between the limits,
zero and pi, and then subtract what you get
by doing both things, both the differentiation and
the integration. So, I subtract the integral
from zero to pi. I now differentiate the t,
and integrate. Well, I just did the
integration. That's negative cosine nt over
n. You see how the negative signs
pile up? And, if this is negative pi
instead of zero, it's at that point when it
starts to lose heart. You see three negative signs,
and then when you substitute, you're going to have to put in
still something else negative, and you just have the feeling
you're going to make a mistake. And, you will.
Okay, now all we have to do is a little evaluation.
Let's see, at the lower limit I get zero, here.
Let's right away, as two over pi.
At the lower limit, I get zero.
That's nice. At the upper limit,
I get minus pi over n times the cosine of n pi. Now, once and for all,
the cosine of n pi-- If you like to make
separate steps out of everything, okay,
I'll let you do it this time, --
-- but in the long run, it's good to remember that
that's negative one to the n'th power
The cosine of pi is minus one . The cosine of two pi is plus
one, three pi, minus one,
and so on. So, at the upper limit,
we get minus pi over n, oh, I didn't finish the
calculation, times the cosine of n pi,
which is minus one to the n'th power.
And now, how about the other guy?
Shall we do in our heads? Well, I can do it in my head,
but I'm not so sure about your heads.
Maybe just this once we won't. What is it?
It's plus sine nt, right?
So, I combined the two negative signs to a plus sign by putting
one this way and the other one that way.
And then, if I integrate that now, it's sine nt divided by n
squared, right?
And that's evaluated between zero and pi.
And of course, the sign function vanishes at
both ends. So, that part is simply zero.
And so, the final answer is that bn is equal to,
well, the pi's cancel. This minus combines with those
n to make one more. And so, the answer is two over
n times minus one to the n plus first power. And therefore,
the final result is that our Fourier series,
the Fourier series for f of t, that funny function
is, the Fourier series is summation bn,
which is two, put the two out front because
it's in every term. There's no reason to repeat it,
minus one to the n plus first power over n times the sign of
nt. That's summed from one to infinity.
Let's stop and take a look at that for a second.
Does that look right? Okay, here's our function. Here's our function.
What's the first term of this? When n is one,
this is plus one. So, the first term is sine t. What's the next term?
When n is two, this is negative.
So, it's minus one to the third power.
So, that's negative one over two.
So, it's minus one half sine two t,
and then it obviously continues in the same way plus a third
sign three t. Now, watch carefully because
what I'm going to say in the next minute is the heart of
Fourier series. I've given you that visual to
look at to try to reinforce this, but it's really very
important, as you go to the terminal yourself and do that
work, simple as it is, and pay attention now.
Now, if you think old-fashioned,
i.e. if you think taylor series,
you're not going to believe this because you will say,
well, let's see, these go on and on.
Obviously, it's the first term that's the important one.
That's two sine t. Now, the derivative,
two sine t, sine t would exactly follow the pink curve.
Sine t would look like this. Two sine t goes up
with the wrong angle. The first term,
in other words, does this.
It's going off with the wrong slope.
Now, that's the whole point of Fourier series.
Fourier series is not trying to approximate the function at zero
at the central starting point the way Taylor series do.
Fourier series tries to treat the whole interval,
and approximate the function nicely over the entire interval,
in this case, minus pi to pi,
as well as possible. Taylor series concentrates at
this point, does it the best it can at this point.
Then it tries, with the next term,
to do a little better, and then a little better.
The whole philosophy is entirely different.
Taylor series are used for analyzing what a function of
looks like which you stick close to the base point.
Fourier series analyze what a function looks like over the
whole interval. And, to do that,
you should therefore aim to, so the first approximation is
going to look like that, going to have entirely the
wrong slope. But, the next one will subtract
off something which sort of helps to fix it up.
I can't draw this. That's why I'm sending you to
the visual because the visual draws them beautifully.
And, it shows you how each successive term corrects the
Fourier series, and makes the sum a little
closer to what you started with. So, the next guy would,
let's see, so it's 2t. So, I'm subtracting off,
probably I'm just guessing, but I don't dare draw this.
I haven't prepared to draw it, and I know I'll get it wrong.
So, okay, your exercise. But, it'll look better.
It'll go, maybe, something like,
let's see, it has to end up... some of it gets subtracted
off... I don't know what it looks
like. When you use the visual at the
computer terminal, I've asked you to use it three
times on a variety of functions. I think this is maybe even one
of them. Notice that you can set the
parameter, you can set the coefficients independently.
In other words, you can go back and correct
your works, improving the earlier coefficients,
and it won't affect anything you did before.
But, the most vivid way to do it is to try to get,
visually, by moving the slider, to try to get the very best
value for the first coefficient you can, and look at the curve.
Then get the very best value for the second coefficient and
see how that improves the approximation,
and the third, and so on.
And, the point is, watch the approximations
approaching the function nicely over the whole interval instead
of concentrating all their goodness at the origin the way a
Taylor series would. Now, there is still one
mathematical point left. It's that equality sign,
which is wrong. Why is it wrong?
Well, what I'm saying is that if I add that the series,
it adds up to f of t. Now, it almost does but not
quite. And, I'd better give you the
rule, the theorem. Of all the theorems in this
course that aren't being proved, this is the one that would be
most outside the scope of this course, the one which I would
most like to prove, in fact, just because I'm a
mathematician but wouldn't dare. The theorem tells you when a
Fourier series converges to the function you started with.
And, the essence of it is this. If f is continuous,
is a continuous function, let's give the point,
it's confusing just to keep calling it t.
If you like, call it t, but I think it would
be better to call it t zero just to indicate I'm
looking at a specific point. So, if the function is
continuous there, the value of f of t is
equal to, the Fourier series converges, and it's equal to its
Fourier series, the sum of the Fourier series
at t zero. And, the fact that I can even
use the word sum means that the Fourier series converges.
In other words, when you add up all these guys,
you don't go to infinity or get something which just oscillates
around crazily. They really do add up to
something. Now, if f is not continuous at
t zero, this emphatically will not be
the case. It will definitely not,
but by far, the kinds of discontinuities which occur in
the applications are ones like in this picture,
where the discontinuities are jump discontinuities.
They are almost always jump discontinuities.
And, in that case, in other words,
they are isolated. The function looks good here
and here, but there's a break. Typically, electrical engineers
just don't leave a gap because they like, I don't know why.
But electrical engineer, and others of his or her ilk
would draw that function like this, like a rip saw tooth.
Even those vertical lines have no meaning whatever,
but they make people look happier.
So, if f has a jump discontinuity at t zero,
and as I said, that's the most important kind,
then f of t, then the Fourier series adds up
to, converges to, it converges,
and it converges to the mid point of the jump.
Let me just write it out in words like that,
the midpoint of the jump. That's the way we'll be using
it in this course. There's a notation for this,
and it's in your book. But, those of you who would be
interested in such things would know it anyway.
So, let's just call it the midpoint of the jump.
So, if I ask you, to what does this converge?
In other words, this series,
what this shows is that the series, I'll write it out in the
abbreviated form, summation minus one to the n
plus one over n sine nt, what's the sum of the series? What is it?
Let's call this not  little f of t.
Let's call it capital F of t. I want to know, what's the graph of capital F
of t? Well, the initial thing is to
say, well, it must be the same as the graph of the function you
started with. And, my answer is almost,
but not quite. In fact, what will its graph
look like? Well, regardless of what
definition I made for the endpoints of those pink lines,
this function will converge to the following.
From here to here, I'll draw it.
I won't put in minus pi's. I'll leave that to your
imagination. So, there's a hole at the end
here. In other words,
the end of the line is not included.
And, the end of this line, regardless of whether it was
included to start with or not, it's not now.
And here, similarly, I start it here with a hole,
and then go down parallel to the function,
t, slope one. And now, how do I fill in,
so the missing places, this is the point,
pi. This is the point,
negative pi, and there are similar points as
I go out. Well, since the function is
continuous here, the Fourier series will
converge to this orange line. But here, there's a jump
discontinuity, and therefore,
the Fourier series, this function converges to the
midpoint of the jump, in other words,
to here. This function,
in other words, converges to this very
discontinuous looking function, and rather odd how these points
are, I say, but in this case, I can prove to you that it
converges here by calculating it.
Look, this is the point, pi.
What happens when you plug in t equals pi?
You get everyone of these terms is zero, and therefore the sum
is zero. So, it certainly converges,
and it converges to zero. Now, that's a general theorem.
It's rather difficult to prove. You would have to take,
again, an analysis course. But, I don't even get to it in
the analysis course which I teach.
If I had another semester I'd get to it, but I can't get
everything. Anyway, we're not going to get
to it this semester to your infinite relief.
But, you should know the theorem anyway.
People will expect you to know it.
Well, that was half the period, and in the remaining half,
you're going to stay a long time today.
Okay, no, don't panic. I have to extend the Fourier
series. Okay, let me give you the hurry
up version indicating the two ways in which it needs to be
extended. Extension number one -- The period is not two pi,
but two times, I'll keep the two just to make
the formulas look as similar as possible to the old ones.
The period, let's say, instead of two pi,
is two times L. Now, I think you know enough
mathematics by this point to sort of, I hope you can sort of
shrug and say, well, you know,
isn't that just kind of like changing the units on the
t-axis? You're just stretching.
Yeah, right. All you do is make a change of
variable. Now, should we make it nicely?
I think I'll give you the final answer, and then I'll try to
decide while I'm writing it down how much I'll try to make the
argument. First of all,
the main thing to get is, if the period is not pi but L,
what are the natural versions of the cosine and sine to use?
Use the natural functions. Natural has no meaning,
but it's psychologically important.
In other words, what kind of function should
replace that? I'll certainly have a t here.
What do I put in front? I'll keep the n also.
The question is, what do I fix?
What should I put here in between in order to make the
thing come out, so that it has period 2L?
You probably should learn to do this formally as well as just
sort of psyching it out, and taking a guess,
or memorizing the answer. If this is the t-axis,
here is t and L, zero and L.
What you want to do is make a change of variable to the u-axis
where the axis is the same. This is still the point.
But, L, now, on the u coordinate,
has the name pi. Now, so I'm just describing a
change of variable on the axis. What's the one that does this?
Well, when t is L, u should be pi.
So, t should be L over pi. When u is pi, t is L, and vice versa.
How about expressing u in terms, well, then u is equal to
pi over L times t. That's the backwards form of writing it, or the forward form,
depending upon how you like to think of these things.
Okay, so the cosine should be pi over L times t,
in order that when t be L, it should be like cosine of n
pi, which is what we would have
had. So, if t is equal to L,
in other words, where is this from?
What am I trying to say? That's the function.
This one is probably a little easier to see.
Where is this one zero? The sine functions that we used
before was zero at zero pi, two pi, three pi.
Where is this one zero? It's zero at zero.
When t is equal to L, it's zero.
When t is equal to 2L, so, this is the right thing. So, it's zero.
It's periodic, and it's zero plus or minus L
plus or minus 2L. And, in fact,
formally you can verify that it's periodic with period 2L.
So, in other words, we want a Fourier expansion to
use these functions as the natural analog of what would be
up there. So, the period of our function
is 2L, and the formula is, I'll give you the formula.
It's f of t equals identical summation,
an, except you'll use these as the natural functions instead of
cosine nt and sine nt. So, n pi t over L plus bn, okay,
I'm tired, but I'll put it in anyway, n pi t over L. Yeah, but of course,
what about the formulas for an? Somebody up there is watching
over us. Here are the formulas.
They are exactly what you would guess if somebody said produce
the formulas in ten seconds, and you'd better be right,
and you didn't have time to calculate.
You say, well, it must be, let's do the cosine
series. Okay, let's not do a cosine.
So, it's one over L times the integral from negative
L, in other words, wherever you see an L,
wherever you see a pi, just put an L times the f of t
cosine, and now we'll use our new function,
not the old one. I submit that's an easy,
if you know the first formula, then this would be an easy one
to remember. All you do is change pi to L
everywhere. Except, you got to remember
this part. Make it a function periodic of
period 2L, not 2pi. And similarly,
bn is similar. It looks just the same way.
And, how about, and the same even-odd business
goes, too, so that if f of t, for example,
is even, and has period 2L, then the function,
then the best formula for the an will not be that one.
It will be two over L, and where you integrate only
from zero to L, f of t cosine. So, now, the bn's will be zero,
and you'll just have positive, etc.
for L. As I say, this is important
case, particularly if the period is two, in other words,
if the half period is one because in the literature,
frequently one is used as the standard normal reference,
not pi. Pi is convenient mathematically
because it makes the cosines and sines look simple.
But, in actual calculation, it tends to be where L is one.
So, usually you have a pi here. You don't have just nt.
Well, I should do a calculation, but instead of
doing that, let me give you the other extension.
Fortunately, there are plenty of
calculations in your book. So, let me give you in the last
couple of minutes the other extension.
This is going to be a very important one for us next time.
Typically, in applications, well, I mean,
the first thing, periodic functions are nice,
but let's face it. Most functions aren't periodic,
I have to agree. So, all this theory is just
about periodic functions? No.
It's about functions. Really, it's about functions
where the interval on which you are interested in them is
finite. It's a finite interval,
not functions which go to infinity.
For those, you will have to use Fourier transforms,
Fourier transforms, not Fourier series.
But, if you are interested in a function on a finite interval,
then you can use Fourier series even though the function isn't
periodic because you can make it periodic.
So, what you do is, if f of t is on,
let's take the interval from zero to L.
That's a sample finite interval.
I can always change the variable to make the interval
from zero to L. I can even make it from zero to
one, but that's a little too special.
It would be a little awkward. So, if a function is defined on
a finite interval, the way to apply the Fourier
series to it is make a periodic extension.
Now, since I have so little time, I'm just going to get away
with murder by just drawing pictures.
So, let me give you a function. Here's my function defined on
zero to L, colored chalk if you please.
Let's make it the function t squared,
and let's make L equal to one. That function is not periodic.
If I let it go off, it would just go off to
infinity and never repeat its values, except on the left-hand
side. But, I'm not even going to let
it be on the left hand side. It's only defined from zero to
one as far as I'm concerned. Okay, that function has an even
periodic extension. And, its graph looks like this
extended to be an even function. Okay, now, that means from zero
to negative L, you've got to make it look
exactly as it looked on the right-hand side.
Otherwise, it would be even. And now, what do I do?
Well, now I've got, from minus L to L.
So, all I'm allowed to do is keep repeating the values.
In other words, apply the theory of Fourier
series to this guy, use a cosine series because
it's an even function, and then everything you want to
do, you say, okay, all the rest of this is
garbage. I only really care about it
from here to here. And, that's what you will plug
into your differential equation on the right-hand side,
just that part of it, just this part of it.
How about the odd extension? What would that look like?
Okay, the odd extension, here I start like this.
And now, to extend it to be an odd function,
I have to make it go down in exactly the same way it went up.
And, what do I do here? I have to make it start
repeating its values so it will look like this.
So, the odd extension is going to be discontinuous in this
case. And, what's the Fourier series
going to converge to? Well, in each case,
to the average, to the midpoint of the jump,
and the odd extension looks like this, and this will give me
assigned series. Okay, you've got lots of
problems to do.

Data Structures

So we're talking about
joint distributions, right? And there's a lot more to do with that,
so to just continue. So last time, we calculated the expected
distance between two iid uniforms, okay? So I wanted to do this analagous
problem for the normal. Because I think that's another nice
related example that has a different approach that makes it easier, okay? So last time,
we did expected absolute difference. This is just an example, but
I think it's a nice example. Expected absolute difference
between two uniforms, and what if we wanna do the same
thing with normals? So we wanna find
the expected value of say, let's call them Z1 and Z2. So, we did this with uniform last time,
now assume these are iid standard normal. Okay, wo last time we did this for
uniform, using the 2D version of LOTUS, right? Completely analogous to LOTUS, except we had a double integral
instead of a single integral. So these are iid standard normal. So, we could write down the 2D LOTUS here,
and try to do that integral. And because they're iid,
the joint PDF of Z1 and Z2 is just the product of
the two marginal PDFs. And well,
we could just try to do that integral, and we could probably get it with some effort. But that's not a good way to do this
problem, it's better to stop and think about the structure of the problem,
okay? So in the case of the uniforms,
we've never particularly studied, what are the properties of
the difference of two uniforms? On the other hand, the difference of normals is
something we've talked about before. So instead of jumping right into
this two-dimensional thing, let's see if we can actually
simplify the problem first. So in fact, we've mentioned before that
the sum of independent normals is normal. We haven't proven that yet, but we have all the tools to
be able to prove that now. So let's just do that quickly
to verify what I said before about the sum of normals,
so just a little theorem. This is gonna be easy now,
because we know MGFs. The sum of normals, so
we stated this before. If X is, let's say N(mu 1,
sigma 1 squared), and y is N(mu 2, sigma 2 squared) and
they're independent, X has to be independent of Y,
otherwise this won't work. Then the sum, we talked about this before, by linearity the means just add, and also the variances add. And we talked about the fact
that if we took a difference, we would take the difference of means. But we would still add the variances,
not subtract. Because if this were -Y, you would
just think of it as plus -Y, okay? So anyway, let's just prove this fact now,
which we haven't done yet, and this is just an easy MGF calculation. So we just use the MGFs, So
let's get the MGF of X + Y. Since they're independent, we talked
about the fact that since they're independent ,we can just multiple
the MGF of X times the MGF of Y. The MGF of a normal, well, we derived
the MGF of a standard normal before. But it's very easy to get from
a standard normal to any normal, right? If we do this thing, mu + sigma z, we can
immediately get the MGF of any normal. And that's just gonna be e to the mu 1 t, this is the MGF of x, mu 1 t, + one-half sigma 1 squared t squared. That's the MGF of X. We multiply by the MGF of Y,
which is the same thing, you just change the subscripts. Mu 2 t + one-half sigma 2
squared t squared equals, Now let's just write this as
one exponential and factor. So that's e to the mu 1 + mu 2 t,
just factor out the t + one-half,
this is all up in the exponent. One-half t squared (sigma 1
squared + sigma 2 squared), right? Sigma 1 squared plus sigma
2 squared t squared. Okay well, I ran out of space on this
board, but that's the end of the proof. Because all we have to do is just say,
look, that's the MGF. I have little more space,
that's the MGF of N(mu 1 + mu 2, sigma 1 squared + sigma 2 squared),
All right, so since the MGF determines the distribution, then that's the end,
we don't have to do anything else. So, it's a very easy calculation,
using MGFs. Okay, so now that we've proven that fact,
and we see this thing, z1- z2. Rather than jumping into the 2D LOTUS,
let's just say, what is that? Well note that Z1- Z2 is N(0, 2), just add the variances. So really all we're asking is for
the expected value of, Expected value of absolute value of,
now when we say N(0, 2), let's once again think about that
as location and scale, right? We could take a standard normal, and
multiply by the square root of 2, and that would give us variance 2. So the easiest way to think
of this is as square root of 2 times Z,
where Z is standard normal, right? That's just the scale,
that gives it variance 2. Now this is just square root of 2 E|Z|. Now its just a one-dimensional LOTUS. And this is a LOTUS that
you've actually seen. If you studied strategic practice five,
we did this. But whether you remember
ever looking at that or not, doesn't matter, this is a easy LOTUS. Whereas here, you have to do a double
integral, here I just write down LOTUS. So I'll do this quickly,
cuz on the strategic practice, it's just write down LOTUS. Integral minus infinity, to infinity |z| 1 over root 2 pi e to -z squared over 2 dz, And notice that this is an even function, That is, if we replace z by -z,
we get the same thing. So we can just multiply by 2 and
go from 0 to infinity. And once we go from zero to infinity,
we can drop the absolute values. Then it's just z e to
the minus z squared over 2. That's a really easy
u-substitution integral, right, cuz you can just let u equals z squared,
or u equals z squared over 2 if you like. And then you get exactly what you want,
so that's then an easy integral. And if you simplify it,
you get square root 2 over pi, which should be an easy calculation. It's also on the strategic practice, so I
won't write out more of that calculation. So then that becomes just
a simple one-dimensional LOTUS, that's a much better way to think of it. All right, so just an example that you
don't always have to jump into the 2D LOTUS, just cuz you have this
function of two variables. Okay, so that's a continuous example. I wanted to do some more discrete stuff. In particular,
to introduce the multinomial distribution. Which is by far the most important
discrete multivariate distribution, and I'll tell you multivariate
distribution means. So this is gonna be
called the multinomial. A multivariate distribution just
means that's a joint distribution for more than one random variable, right? So we have all these normals and
Poisson and geometric, and so on. Those are all univariate distributions,
cuz we have one random variable. Now we're working with more than
one random variable at once. And for this course, there's really only
two multivariate distributions that you need to know by name. One is the multinomial,
which we are about to do. The other one is the multinomial which
is the generalization of the normal distribution to higher dimensions,
and we'll get to that one later, okay? So multinomial as the name might suggest
it's a generalization of the binomial, right? Bi becomes multi, okay? So it's like a higher dimensional
version of the binomial, and let's just introduce it by its story. So this is the definition and
story, Of the multinomial, which I'll sometimes just
abbreviate to mult of np. It has to parameters,
n and p like the binomial, except in this case,
this p is actually a vector. So p = vector, let's say P1 through Pk, where we assume that that's
a probability vector. And by probability vector all,
I mean is that these are non negative and add up to 1. Cuz we're gonna think of them as
probabilities for disjoint cases, so that encompasses all possibilities. So we want pj greater than or equal to 0, and the sum of all pj's = 1. That's the assumption, okay? So the binomial would just be
if this is one dimensional and then we just have binomial np,
but now we have k of them. So the intuition is that in the binomial, we just talked about success and
failure, right? There are two possible outcomes,
there are two categories. Multinomial means instead of two
categories, we have k categories, okay? So it's a natural extension, right? And binomial, we have to classify
everything as either success or failure for each trial. Here we have more than two
possible result, okay? So we say that x is multinomial np, We think of that as saying that, so in this case, X is also a vector. This is a multivaried distribution so X = (X1 to Xk), if we can think of X. So like in the binomial,
we have n independent trials. But I'll just call them objects
instead of trials and each object, objects could be people, could be trials,
could be anything, so just very general. We have n objects that we
are categorizing, okay? We have n objects, which we are
independently putting into k categories. So there are k possible categories, and
the binomial is just success or failure, but now we have k categories. And there, each object is independently determined
which category it falls into, okay? Just like in the binomial,
we had independent Bernoulli trials. And if Pj is the probability
of category j, by P of category j,
I mean the probability that any one of these objects is in category j,
has probability Pj. And we interpret Xj is just the count, is the number of objects in category j. All right, so that was a lot of writing,
but the concept is really simple. We just have n things that we're
breaking them into categories, and then we just see how many objects
are in each category, right? So it's very natural, you can make up
as many examples of this as you want, really easily, right? Just anytime you're classifying
things into categories. It's very, very general. Okay, so let's find the PMF. So this is gonna be a joint PMF,
cuz it's a joint distribution. So we want the probability that x1 = n1, blah, blah, blah xk = nk., right? That's a joint PMF, we just need to say
what's the probability that there are n1 objects in the first category and
to in the second category and so on? And we can immediately write
down the answer just by thinking back to how do we derive the binomial PMF. All we have to do is imagine
any particular sequence, it's gonna be P1 to the n1, P2 to the n2,
blah, blah, blah, Pk to the nk. Just to have a little
intuitive example in mind, let's just suppose this is very
similar to how we did the binomial. But just to quickly review and
generalize that. Suppose we just have three categories, just to have a little
mental picture in mind. We had three categories,
lnd let's just say our sequence, and let's just write one, two, three. Where one means category one and so on. So we might have a sequence like 23311112,
for example, okay? So let's put a couple more 2s, there
are four 2s two 3s four 1s for example. This says that the first
object is category 2, right? We're just categorising
the objects one by one. So any particular sequence like this, the probability would be P1 is
the probability of category one. Multiple to the power of how
many ones there are, right? I need to put another one there. P2 to the power of the number of twos and
so on. That will be the probability
of any specific sequence that has the desired counts,
right? But then we can permute
this however we want, then it's just going back
to those counting problems. How many ways are there to permute
the letters in the word pepper, or the letters in the word Mississippi or
something like that. Where you start with n factorial, but that overcounts because the twos
could have been in any order. The threes could have been in any order, the ones could have been in any order,
and so on. So you have to adjust for
that overcounting. Exactly like we did for the binomial,
so we just divide by n1 factorial, n2 factorial, blah, blah,
blah n k factorial to account for all the ways you could permute the 3s,
permute the 1s, permute the 2s. Of course, there's a constraint here, this is if n1 plus blah,
blah, blah plus nk = n. Otherwise, it doesn't make sense, right? Cuz we have n objects. We're assuming that every object
is in exactly one category. So it wouldn't make sense if
we added up these counts and they had too many or
too few, makes no sense. So it's 0, otherwise. That is if the sum of
these ns is not this n, then it's impossible, so it's 0. So that didn't require a calculation. It just required thinking
about an example like that and just so
different ways to promote things, right. So that's the joint PMF. It looks a lot like the binomial,
all right. So it's a generalization of the binomial
when you have more than two categories. So we'll come back to some other
properties of the multinomial later, but just to do a couple quick
properties to think about. We could ask about the marginal
distribution, conditional distribution, things like that. So let's think about
the marginal distribution first. Okay so we're letting X be multinomial. N, p. Sometimes I'll subscript a k,
just to indicate what the dimension is, so the number of categories. And suppose we want the marginal,
find the marginal distribution of just one of these component,
let's say Xj. So Xj is just how many people or
how many objects are in category j. We want its marginal distribution. What do you think that is? Yeah, binomial, why did you say binomial? Exactly, it's either nk or it isn't nk. So I mean if you said if
you look at your notes, how do you get from joint distribution
to marginal distribution? I would say if you take this thing and do k- 1 sigma sign sum over all
possible things, do a lot of algebra. But that's not thinking about it, right? To marginalize we'd sum up the joint or
we integrate in the continuous case. We sum in the discrete case,
sum of everything we don't want, okay? But instead let's just think about
the story, think about what it means. As you just said, each of these objects,
either it's in category j or it isn't. We're assuming they're
all independent trials. So if we define success to
mean being in category j, the probability of success
is pj in our object. So that's just immediate. I didn't write justification for this but
that just proved itself from the story you know it's a complete truth
just to say because the binomial, it's independent Bernoulli trial. That's the probability of success, okay? So we can get that immediately and in
particular that also gives us the mean and the variance without having to
do a calculation, E(Xj) = npj. And the expected value of the variance
because we derived the variance of the binomial before we
don't need to re-derive that. We already know the variance of a binomial
is np(1- p) so this npj (1- pj), no additional work needed
because we know it's binomial. Okay, so that's just immediate from
thinking about what this means. So that's one property. That's the marginals. And let's think about kind
of something similar. Let's call this, well,
I call this the lumping property. What happened, the question is,
we have all these categories, well what happens if we decide to merge
certain categories together, right? Okay, so just to have an example in mind, let's let K = 10, so
we're thinking of X as a vector. X1 through X10 and
just to have a concrete example in mind. So this is multinomial, let's say this
is multinomial, and, P1 through P10. And to have a concrete example in mind, well let's imagine we're in a country
that has ten political parties. Okay, and you take n people and
assume that the people are independent of each other, and
you wanna know how many people are in, and assume that everyone in this country is
a member of one of these ten parties. Okay, and then you take all
these people and you say, okay. Ask each person which party they're in. X1 is the number of people in
the first political party, X2 is the number in the second one,
and so on, right? So that that would be multinomial if these are the probabilities of the
different party memberships, all right? So now, what I call the lumping property
is what if it's a country where it's, there are only two dominant parties, and
all the other parties are much smaller? And so it might be kind of unwieldy to
deal with this ten dimensional vector. Maybe we wanna compress
all the third party, so suppose that the first two are kind
of the two dominant major parties and the rest of them are kind of minor, so
we may wanna just lump them together. So that's why I call it
the lumping property, lump all the other parties together. So what if we considered,
let's see, let Y = X1, X2, and
then group all these ones together. So I'll just add them up,
X3 plus blah, blah, blah, plus X10, right. So this would be like party one,
party two, and then other third party grouped together. Without doing any calculation or
algebra whatsoever, we can immediately write
down the distribution of Y. Y is just gonna be multinomial. Same n. And then all we've done is group
these categories together, but then it's the same problem again, it's just it has a larger probability
just lump together all those Ps. Okay, so this should be
obvious from the story, right. It's the same problem again. So just like we emphasized with
the binomial we can define success and failure however we want. Here we can rearrange the categories and
whatever, the only thing we need to make sure of is that each
object is in exactly one category. So it would not work if you could
be in more than one category or be in no categories. But if you define your
categories such that it's true that each object isn't
exactly one, then you get multinomial. Didn't need to do any algebra or
calculus to show that. So that's pretty nice. Similarly, let's get
the conditional distribution. So what if we wanted, so
again x is multinomial. What if we want a conditional distribution
where we got to learn what X1 is and we want the conditional distribution
of the rest given that we now know X1. So we want a conditional, you might call it a conditional
joint PMF because you're given X1. Let's say that we're given that X1 = n1,
okay? And then we want the conditional joint
distribution of everything else. So we know exactly how many
people are in the first category. But we don't know about the rest of them. Well, given that X1 = n1, we want the joint PMF of
the rest X2 through Xk. Still gonna be multinomial, but we have to be a little bit careful
with getting the parameters right. So now this is gonna be k- 1 dimensional, cuz we know how many people
are in the first category, but we're looking at the remaining
k- 1 categories. And the number of people, well, n- n1 have been allocated
into the first party, okay? So we have n- n1 people left. And then we just have to get
the probability vector, right? Now if we just wrote p2 through pk,
that would be a common mistake, but it should be easy to see that that's
a mistake because those don't add up to 1. So it can't just be p2 through pk, right? I'm imagining that I've taken, and
it doesn't matter which people. I can imagine,
I'm conditioning on the count. But then I could further condition on
which specific people are in category one, and then use symmetry. So I guess, so I may as well just assume
that the first n1 people are in category one, okay, but to get these ps, well, then
we have to think conditionally, right? So let's call this vector, let's call it p2 prime through
pk prime where somehow we have to figure out what's p2 prime and
so on. Because without the primes it doesn't
add up to one makes no sense. So let's find p2 prime for example. Intuitively, I want this to
be proportional to P2, right? Cuz I know how many people
are in the first party, but that shouldn't kind of affect the relative
distribution of the rest of the party. So basically you just have to renormalize. If I want to write that out, mathematically I would say P2
equals the probability of being in category 2, given a random object being in category 2 given,
that it's not in category 1. Because we've already thrown out
the ones that are in category 1. So just by the definition of conditional
probability, being in category 2 I take the intersection of this and this,
but once you say you're in category 2, you know you're not in category 1,
so that's redundant. So the numerator is just P2. And the denominator is 1- P1, that is just the probability
of not being in category 1. Or we could also write it as P2
over P2 + blah, blah, blah + Pk. So all this says is we've taken these and
you know similarly for the other ones Pj prime equals Pj
over P2 + blah, blah, blah + Pk. All this says is that we've renormalized
this, it's still multinomial. Okay, so multinomials have really
nice properties like this and you can see these things just by thinking about
what it means without doing a calculation. So that's a very useful distribution for
lots of applications. Okay, so we'll say more about
the multinomial in the next lecture or the lecture after. But I wanna do one more
continuous example as well, an example where we actually
do need to do a calculation. And this is another kind of famous one. Good example of how do
we work with joint PDFs, which I think we need more practice with
or at least you need more practice with, then I'll try to help with that,
so this is a good example. I call this the Cauchy Interview Problem. I call the Cauchy Interview Problem
not because Cauchy ask this as an interview problem, but because it sounds more interesting to
call it that than the Cauchy Problem. But actually for some reason, this doesn't seem like it should
be a common interview problem, but I've actually seen this on several
occasions asked as an interview problem just I think to test whether you can do
work with joint PDFs and things like that. So okay, it is an interview problem, though it sort of
shouldn't be in some sense. Anyway, I have to tell you what
the Cauchy, I mean Cauchy was a famous mathematician, but in this context, Cauchy
is referring to a specific distribution. The Cauchy distribution. It's a famous distribution
that has a lot of kind of weird, scary properties. I just got some of these distribution
plushies that I found online. I might bring them, but they're
a little bit small to show you here. But if you come to my office hours,
you can see them in my office. But little pillows illustrating different
distributions, I have them in my office. The Cauchy is called the evil Cauchy and it looks pretty evil. And so let me first tell you
what the distribution is, and then tell you a little
bit about why is it evil. And then we'll try to find its PDF,
which as I said, has been a common interview problem,
find the PDF of a Cauchy. That's the problem, okay? So the Cauchy is
the distribution of let's say, X over Y with x and y iid standard normal. So it's a simple definition, just take a ratio of two iid standard
normals and we call that a Cauchy. And you can see why that could
be a useful distribution for a lot of different applications where
ratios is a pretty natural thing. So that's a Cauchy, and
the problem is find the PDF. Of this random variable, okay? Let's call this thing T. Find PDF of T. So we're defining T to be
the ratio of iid standard normals. We want to find its PDF. All right, so that doesn't yet
answer why this is evil. Well, some properties of the Cauchy
that we're not gonna prove right now, but just to kind of foreshadow
why is this thing so evil. First of all,
it does not have an expected value. If you try to compute e,
expected value, it'll blow up. No, no, no, that's not that evil. There are a lot of distributions where if
you try to compute the expected value, it blows up. So it does not have a mean,
it doesn't have a variance. The thing that's really evil about the
Cauchy is that If you take iid cauchys, so let's say don't just have T,
we have T1 through TN. They're just iid ratios of normals. When we get to the law of large numbers
later in the course, we'll see that when we average a bunch of iid random
variables, we What happens if we average a lot of them is that
should be close to their mean, right? You average a lot of IID things
that should be close to the mean. In this case there is no mean. But the weird fact is that if you
average all these Cauchy, IID Cauchy the distribution of that average is still
a Cauchy, doesn't change the distribution. You can average a million IID
cosye it's still gonna be Cauchy. So in some sence you that's kind of you're
hoping soil as you collect more and more data you're hoping to converge
to the truth in some sense. In this case if all you do
is average then you just not getting anywhere
the distribution doesn't change. There are other ways to work with, if you had Cauchy data there
are other ways to work with it. It would be a bad idea to just take
the naively average everything, there are other things you could do. Okay, so
any way that's the Cauchy Distribution. Now let's find the PDF, just for
practice with our joint distributions. And there are several
ways we could do this. One way would be to use
the law of total probability, and condition on y to make things easier. And that's a perfectly good way to do it. But I think I wanna just start by
practicing just more directly how to just directly get the CDF. Let's find the CDF, and
take the derivative and get the PDF. So with the CDF we could use
the law of total probability, but let's just directly write down. It's going to be a double
integral because we have an X and a Y and let's just write down that double
integral and see if we can do it, okay? So let's find the CDF. So the probability that
x over y is less than or equal to some number, t,
that's what we need for the CDF. This is practice with, this is an event, it's an event that the ratio
is less than or equal to t. We want to find some probability of
an event where it's based on x and y, so unless we can think
of some clever trick for simplifying this we basically
have to do a double integral. Or else, we can use the law of total
probability and do a single integral, but I actually don't think
that's any easier here. So my first impulse would be to
multiply both sides by y here. But you have to be careful in doing
that because y could be negative, so we can simplify this
a little bit by using symmetry first, And putting absolute values. This follows from the symmetry
of of the normal. And you can think through for yourself
exactly how I'm using symmetry here, but the basic idea is with the normal. If I have a standard normal and multiply
it by minus 1 it's still standard normal, if I multiply it, if I randomly chose say
with probably one half multiply it by minus 1, probably one half do
nothing It's still standard normal. Have the same symmetry in the denominator,
so sort of have two symmetric things. And we might as well just kind
of absorb the plusses and minuses and write it this way,
follows from symmetry. The reason I wanted to do that is just so
that I could write this as x less than or equal to t absolute y, without having to flip the inequality or worrying
about whether the inequality flips. Now let's just write this down
as a double integral, okay? We are saying that x so we can either dxdy or dydx, but let's suppose that we are doing dx dy. And to get a probability well what we do, we integrate the joint PDF over whatever region we want, okay? So Y goes from minus infinity to infinity. And, the main thing again to be careful
about, is the limits of integration. X, the inner limits can depend on Y, and X, we're looking at the region
that goes up to t absolute y. So x goes from minus infinity
to t absolute y, and then what we're integrating
is just the joint PDF, right? So the joint PDF is 1 over root 2 pie,
e to the minus x squared over 2. And then same thing for the y,
1 over root 2 pi e to the minus y square, because they're IID standard normal. So the other term, e to the minus y
squared over 2, doesn't depend on x. So I could write it here but
I could immediately then pull it out here. So I may as well write it here so
that it's not interfering with this part. Sets e to the minus y square over two and there is another 1/2 pi
this just stick over there. So all I did is write
down the normal PDF for x and the normal PDF for y and
I took the y part cuz that depends on x. That looks pretty ugly so
let's see if we can do it, well, one thing that we could simplify is just
recognizing what do we actually have here. So we have this integral,
minus infinity to infinity, e to the minus y squared over 2, and
then we have this inner integral. Okay?
Now in one sense we can't do this integral. Because that's the normal PDF and you can
prove that you can't do that integral. And in another sense,
not only can you do that integral, you already know what
that integral is right? That's just capital phi evaluated here,
that's just the normal CDF. So actually it's just phi,
so depending on whether you consider that doing the integral or
not, it's just that, dy. That's just the definition of
the standard, normal CDF, okay? Now these absolute value signs
are a little bit annoying. So, let's notice that we
have an even function, because y squared, absolute value y,
this is an even function. So we may as well go from 0 to
infinity instead, and multiply by 2. So then we'd have a square
root of 2 over pi. I just multiplied by 2, and then we're going from 0 to infinity, e to the -y squared over
two capital phi of ty dy. All right, and then you know, the clock
is ticking on our job interview and we've get here and
it's sort of possibly start to panic. And that capital phi is
an intractable integral, that's why we call it capital phi,
it's cuz we couldn't do it. Now, you are being asked in
your interview to integrate an integral that you couldn't do, sounds pretty bad, However. One thing that might help,
is that on the interview, we were asked to find the PDF,
not to find the CDF, that's the CDF. And we know that the PDF is
the derivative of the CDF, so the PDF is the derivative of the integral
of an integral that we can't do. So somehow maybe that will save us. So let's take the derivative. So here's the PDF,
PDF is the derivative of the CDF. This thing is capital F(t),
if we call the CDF capital F. The PDF is the derivative, F'(t). So we're taking the derivative
with respect to t, not with respect to y
which would make no sense. Notice that this y is a dummy variable,
okay? This is a function of t we're taking
the derivative with respect to t. Okay, now there's a theorem
in calculus that says, under some pretty mild conditions, if you
have a reasonably well-behaved thing that you're integrating, you can exchange
the derivative and the interval. This is a very,
very well behaved function. Capital Phi is just a continuous
differentiable thing between 0 and 1. Either the -y squared over 2,
that's infinitely differentiable. It decays to 0 very fast, so
this is a very, very nice function. So there's gonna be no technical problem
whatsoever with swapping the derivative and the integral. We're gonna take the derivative
of this with respect to t, and then we're gonna try to simplify it. So we take the derivative,
bring the derivative inside, okay? So we have the integral 0 to infinity,
e to the -y squared over 2. We're differentiating with respect to t,
we're bringing in a d with respect to dt. So we're treating either the -y square
derivitive which behaves as a constant when we're differentiating
with respect to t. Then we take the derivative of
capital Phi of ty, by the chain rule, y is gonna come out, because we
are differentiating with respect to t. So y is going to come out
from the chain rule, y. And then we just need
the derivative of this, but the derivative of
the standard normal CDF is the standard normal PDF,
which is 1 over root 2pi, e to the -z squared over 2
in general where z is ty. So it's e to the -t squared,
y squared over 2. I just squared this thing divided by 2dy. Now let's see if we can do it. So the square root of 2 here
cancels this square root of 2. We have square root of pi, square root
of pi, so we're gonna get 1 over pi. And then we just need to integrate from 0 to infinity of ye to the -t squared y squared over 2dy. Now this looks like an integral we can do. &gt;&gt; [INAUDIBLE]
&gt;&gt; The other what? &gt;&gt; [INAUDIBLE]
&gt;&gt; Say that again. &gt;&gt; [INAUDIBLE]
&gt;&gt; There's another e to the -y squared over 2. Yeah, I forgot that one, thank you. There's another,
we'll just combine that one with this one. So that would be 1. Uh-oh, I guess don't get hired,
that's sad. There's another,
e to the -y squared over 2 that I forgot. But now, I thank you, I put it back, okay? I haven't interviewed for
any jobs since I came here five years ago, so I'm kinda rusty. So I put back either the -y squared
over 2 that you helped me with, and now that should be okay, right? Now, this is an integral we can do, because we know that the derivative
of y squared is gonna be 2y, and that's gonna be taken care of there,
now it's an easy u substitution again. So we can just let u = let's say 1 + t squared y squared over 2. Just make that substitution, so
then this just becomes e to the -u, okay? So du = 1 + t squared times, now we're treating t as constant again. We're changing the variable y,
transforming it to u. So the derivative y squared over 2 is y,
so we have y times 1 + t squared dy. So we have the Ydy, we're just
missing the 1 + t squared, okay? So I'll just multiply and
divide by 1 + t squared. Then we're just integrating e to the -u
du, which is a very, very easy integral. We know that that's 1,
either just by doing it or because it's the integral of
the exponential PDF again. Okay, so then we immediately now have the
answer, 1 over pi 1 + t squared for all t. So that's the PDF. If we wanted the CDF,
all we would have to do is integrate this, then it's gonna do some arc tangent thing. All right, so that's the Cauchy. And let me just quickly just show you
how you would start the other method, which would be the probability. I'm not gonna do the whole thing,
because at some point, that's just gonna reduce
back to this method. But just to show you
what it would look like. Just as a quick alternative without
going through the whole thing, cuz it's gonna be similar. But it's useful to have both methods. So this would be the method using the
double integral, okay, and that's the PDF. Which by the way,
we should check thati that's a valid PDF. Does it integrate to 1? Well, if you integrate that thing,
you'll get an arctangent thing. And you can check that when you
evaluate the arctangent thing, you will get 1, okay? So just quickly, the alternative
using the law of total probability. x less than or
equal to t absolute value of y. You kind of just think to yourself,
what do we wish that we knew here? We could decide to condition on x, or
we could decide to condition on y. This is gonna be the integral,
let's say we condition on y. The probability x less than or equal to t, absolute value of Y,
given, let's say, Y = y. This would be the law of total
probability, right, just conditioning. We can choose whether to condition on x or
to condition on y, but I think I wanna condition on y. Okay, law of total probability,
remember the discrete case we just seen, sum over all cases, p of a given,
b, whatever, p of v, whatever. We have a partition. And in this case,
we're integrating instead of summing, so we're conditioning on y and
then we're multiplying by 5y. Lower case 5y is the standard normal PDF. All right, well,
let's see if this helps at all. This is saying to treat Y as just
being known to equal little y, okay? So I can plug in little y there. And then the tricky part here is that we need to use the fact
that x is independent of y. Because if x are not independent of y,
you could plug this thing in, but then you still have this condition, okay? But since they're independent,
you can plug in Y = y and then get rid of the condition,
because they're independent. So when we do that,
that's just gonna be phi. The probability that x is less than or
equal to t absolute value of y, is just phi of tf to the value of y,
just by definition, right? Because we're plugging in y, that's just a constant probability
of x less than some constant. It's just the standard
normal CDF evaluated there. Phi of (y)dy, which I think is
the same as the integral we had. Does that look the same? Yeah, so over there, I just wrote out
what this is, but it's the same thing. And then proceed in the same way. So that would be a second way to do this. We'll see a third way later on, just because this is
a common interview question. So it's good to have more than three or
more than two ways to do it. All right, so I'll stop for now. I'll see you Wednesday.

Statistics

The following content is
provided under a Creative Commons license. Your support will help
MIT OpenCourseWare continue to offer high quality
educational resources for free. To make a donation or to
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR WILLIAMS: OK,
so today's lecture-- we're going to be talking about
probabilistic planning later, and in these cases
where you're planning a large state spaces
is very difficult. You do the MVP planning. It could be stress that
activity planning, or the likes. But you have to be
able to figure out how to deal with
these state spaces. So Monte Carlo tree searches
is one of the techniques that people can identify,
over last five years, is having an amazing performance
improvement over other kinds of sample-based approaches. So entity is very interesting
from that standpoint. And then if we [? link it to ?]
the last lecture, then the combination
of something, we just learn about [INAUDIBLE]
and combine it with search, is very powerful, in this case,
through the state-of-the-art techniques for that, as much as
tree search [INAUDIBLE] later [INAUDIBLE] PROFESSOR 2: Good
morning, everyone. As Professor Williams
just said, we are going to be talking about
Monte Carlo tree search today. My name is Eann
and I'll be leading the introduction and motivation
of this presentation. By the end of this
presentation, you will know not only why we
care about Monte Carlo tree searches. As Professor Williams said,
there's so many algorithms out there. Why do we care about
this specific one? And second, we'll be
going through the pros and cons of MCTS, as well
as the algorithm itself. And then lastly, we will
have a pretty cool demo on how it's applied to Super
Mario Brothers and the latest Alpha Go AI that built
the second best leading Go player in the world. So the outline for
today's presentation is, first, we're going to talk
about pre-MCTS algorithms. There are other algorithms
that currently exist out there, and just a few of them to lead
into why we do care about MCTS and why these other
algorithms fail. And second, we'll talk about
Monte Carlo tree searches itself with Yo. And lastly, Nick will tell you
more about the applications of Monte Carlo tree searches. So the motivation of
these kind of algorithms is we want to be
able to play games and we want to be able to create
programs to play these games, but we want to play
them optimally. We want to be able
to win, but we also want to be able do this in
a reasonable amount of time. So these three can
train itself leads to different kinds
of algorithms, and different algorithms
with different complexities and time, or times to search. And so that's why
today we're going to be talking about Monte
Carlo tree searches. And you'll figure out in a
few slides why we do care. So these are the types
of games we have. You have this
chart where there's fully observable games,
partially observable games, determinstic, and
games of chance. And so today, the games
that we care about are the games that are fully
observable and deterministic. And these games are games like
chess and checkers and Go. And we'll also be talking
about another example with Tic-tac-toe. So these pre-MCTS
algorithms include deterministic, fully observable
games, like we said earlier. And the idea of this, and the
nice thing about these games, is that they have
perfect information, and that you have
all of the states that you need and there's
no opportunity for chance. And so the idea is
that we can construct a tree that contains
all possible outcomes because everything
is fully determined. And so one of these
algorithms, to address this, is the algorithm Minimax, which
you might have heard before. And the idea of
Minimax to minimize the maximum possible loss. That sounds a little
weird in the beginning, but if you take a
look at this tree, this red dot, for
example, is the computer. And so in the computer's eyes,
it wants to beat its opponent. And we're assuming the
opponent wants to win also, so they're playing
their best game as well. And so the computer wants to
maximize his or her points, but also knowing that the
opponent, or the human, wants to maximize
their own win as well. And so in the
computer's eyes, it wants to minimize the
maximum possible lost. Does that make
sense to everyone? Yes? OK. And so in the
example of Minimax, we're going to start
with a connect, or a Tic-tac-toe board,
where the computer is this board right here, and
the blue Tic-tac-toe boards are the states that the
computer finally chooses. It's anticipating the
moves a human could play. So if you take a
look up here, here's the current state of the board. The current state of the board. And the possible options for the
human are this guy, this guy. Nope. Possible options
for the computer, we have three different options. And so you'll notice that this
is clearly the obvious winner. But in the state
of Minimax, it goes through the entire
tree, which is different from
depth-first search. It goes through the entire
tree until it finds the winning move and the minimize of
the maximum possible points it could win. So is there a way we
can make this better? Yes. I'm sure you've
heard about pruning, where, in our human
intuition, it makes sense. Well, why don't we
just stop when we win, or when we know
we're going to have a game that allows us to win? And so this idea is the
idea of simple pruning. And so when we combine Minimax
and simple pruning, we have-- anyone know? AUDIENCE: Alpha, beta. PROFESSOR 3: Yes. Our 6.034 head TA
knows about this. We have alpha-beta pruning,
where we prune away any branches that cannot
influence the final decision. So in other words, you wouldn't
keep exploring the tree if you already knew that
a previous term would allow you to win. And so this idea in
alpha-beta pruning, we have an alpha and a beta. And so the details
aren't important for you to know right
now, but the idea is that we stop whenever
we know we don't need to go on any further. So in the games that
have Tic-tac-toe and Connect 4 and chess,
we have relatively low branching factor. So in the case of
Tic-tac-toe, we have 2 to the fourth branching factor. But what if we have really
large branching factors, like Alpha Go? In Alpha Go, we
have 2 to the 250. Do you see that Mini Max,
or even alpha-beta pruning, would be an optimal
algorithm for this? The answer is? AUDIENCE: No. PROFESSOR 3: No. And this leads us
to out next section. Our goal is going to talk about
how we can use the Monte Carlo tree search algorithm for
games with really high branching factors, and using
the random extension to allow us to see, ultimately, how Alpha
Go, which is Google's AI, was able to beat the leading
Go player in the world. PROFESSOR 3: All right, guys. So this is the part
where we re-explain the algorithm itself. And before we dive
into this, I want to make something
really clear, which is that because these
are technical details and because we actually
want you to understand them, and because I definitely didn't
understand this the first three times I read the paper. I really want you to feel
free to ask any questions on your mind, with the knowledge
that, in my experience, it is very rare that someone
asks a question in class that's [INAUDIBLE] OK, so really,
whenever you have one. OK. So why are we doing this? Well, the ideal
goal behind MTCS is that we want to
selectively build up different parts of the tree. So the depth-first search
way, the exhaustive search, would have us exploring
the entire koopa tree, and that our depth
is limited by looking at all the possible
nodes of that level. But what we want is we want-- because the amount of
computation required for that explodes really quickly. With the number of moves
that you're basically looking into the
future, we wanted to be able to search selectively
in certain parts of the tree. And so for example, if there are
less promising parts over here, then we care less about looking
into the future of those areas. But if we have a certain move-- in chess, for example,
there's a certain move where in two moves, you're
going to be able to take the opponent's queen. You're really want
to search that region and figure out
whether that's going to end up being a significantly
positive group for me. And so the whole
goal of our algorithm is going to be growing
this asymmetric tree. How does that sound? OK, great. So how do we actually do this? We're going to go over
a high-level outline, but before we do
that, let's talk about our tree,
which you're going to get very familiar with. Can people see that this
is red and this is blue? So this is our game state
when we start our game. We can be given a Tic-tac-toe
board with a [INAUDIBLE] place, a game of chess with the lose
configured a certain way. And so our player,
which is the computer, has three separate
moves that it can take. And so each of those moves
are presented by a node. And each of those moves have
response moves by the opponent. So you can imagine
that if one of these is a Tic-tac-toe board with
just a circle, that one of these is with that circle and
the next place right by it. And as you go down
the this tree, you start understanding
basically, it's the way that humans think
about playing these games. If I go here, then
what if they go there, and then what if
I go right here. You try to think through
the set of future moves and try to evaluate
whether your move will be good in the long term sense. They way that are going to
expand our tree, as we said, to create an asymmetric
tree is first of all, we're going to descend
through the tree. We're going to start at the
top and we're basically, jump down some sequence of
branches until we figure out where we're going to place
our new node, which seems like a key operation here. To create an asymmetric
tree it's all about how you [INAUDIBLE]. For example, in this
case, we're going to pick this sequence of nodes. And once we get to the bottom
and find every location, we're going to
create a new node. It's not very hard. Then we're going to simulate
a game from this new node. And this is the
key part of MCTS. Once you get to new
a location, what you're going to
be doing then, is you're going to be simulating
a game from that new location. We're going to
talk about how you go about simulating a game from
this more advanced game state that what we started out with. Does anyone have any
questions right now? We will be going in depth
into all of these steps, but just in a high level sense. AUDIENCE: Just a quick question. PROFESSOR 3: Yeah. AUDIENCE: To create
the new node, is it probabilistic, just
creating a new node as the most probable [INAUDIBLE] PROFESSOR 3: No, no. You're creating some new node. We'll talk about how
we pick that new node, but we're just making a new node
and we're not thinking anything about probability. The next thing is that we're
going to update the tree. So whatever the value of
the simulation delta was-- delta, remember-- we're going to
propagate that up and basically add that to all
of the nodes that are in that parent of
that node in the tree and update some information
that goes in there and that they're storing. This is going to be good because
it's going to mean that-- it's a lot like in
search algorithms where you have trees that then
the entirety of the tree remains up to date with the
information from every given simulation. And we're just
going to repeat this over and over and over again. And slowly, our
tree will grow out until whenever we
feel like stopping. This is actually one
of the nice things about MCTS, is that whenever
we decide that we're out of time, like for example, if
you're in a competition playing a champion Go player, you
can stop the simulation. And then all you
have to do is pick between one of the
best first moves that you're going to make. Because an the end of
the day, after you're doing all the simulation,
we're still right here. And we're still only picking
between the movies that go immediately where we started. Yeah. AUDIENCE: Could this
[INAUDIBLE] good tree? And then on some initial
region of interest, or is it arbitrary how
you get to create it? PROFESSOR 3: We'll go
through how you pick where to descend right now. I guess, it's any
possible move that starts at your starting game state. Does that make-- great. Before we move on to
the algorithm itself, let's talk about what we store
in each one of these nodes. So now we've added
these numbers. And these numbers
represent is that nk, as in the value of the
right, is the number of games that have been played that
involve a certain node. So for example, if
I look this node, that means that
four games have been played that involve this node. A game that has been played
that involves the node just means that
one of the states of the board at some
point in the game was the state of the board
that this represents. For example, if I have a
game that was played here, if I know that I've
played this once, then that guarantees
to me that I played this game
once because this is a precursor state to this one. Make sense? Yeah. AUDIENCE: How can the two
n's below that node not add up to a value of [INAUDIBLE] PROFESSOR 3: That will come when
we start expanding our game. But that's a great question. And intuitively
speaking, it should. AUDIENCE: You're saying you're
storing data from past games about what we've-- PROFESSOR 3: Yes. AUDIENCE: --done before. AUDIENCE: If past game's outside
of the script simulation? PROFESSOR 3: No, no, no. Past game's in the
script simulation. And then the other
value is the number of wins associated
with a certain node. And these are going to be
wins for player one, which is red in this case. It would get confusing
if we put both of them, but they're complementary. So for example, three
out of the four times that the red player visited this
node, they won in that node. And these are the two numbers
that we're going to store. And we're going
to see why they're significant to store later. So first, descending the
key part of our algorithm that we're talking about. And when descending,
there are these two counterbalanced
desires that we have. The first of them is that
we want to explore really deeply into our tree. We want to think about, OK, if
they do this then I'll do this. And then, well, then I'll do
that unless I want it to forth. And we want to think through
a long term strategy. But at the same time, we don't
want to get caught in that. We want to make
sure that we're not missing a really promising
other movie that we weren't even considering because we
were really going down this certain rabbit
hole of the move that we had thought
about before. This is illustrated by the
x case [INAUDIBLE] SMBC. The SMBC comic about academia
and how someone tells you that a lot of really
great work has been done in an area,
that means nothing about how promising
the future will be. It's all about expansion
and exploration. And the way that we're
going to balance expansion and exploration
in order to create our really nice asymmetric
tree is the following formula. And it's fine if that looks
really confusing and messy. But actually, it breaks down
quite nicely into two parts. This formula is
known as the UCB. You don't need to know why it's
the Upper Confidence Bound. Let's just talk about
what's inside it. So first of all, you have
this term on the left. And this term on the left
is the extension term. It's basically proportional
to the likelihood that the expected number of
times that you're going to win, given that you are
in a certain node and that you were
a certain player. It's basically the
quality of your state in some abstract level. If we knew this
perfectly, then we would be doing
great because that's the thing we're looking for on
some grand level, The expected likelihood of winning
from a certain state. On the other hand, you
have this exploration term. And you may not be able
to read the font there. But what this is
basically saying is that it looks at
the number of games that I have been played through,
and it was the number of games that my parent has
been played through. And it tries to preserve those
numbers at a certain ratio, at a log ratio. And what that effectively means,
is that the number of times that I have been-- if I have been visited
relatively few times, and the denominator is small. Whereas my parent has been
visited many times, which means that my siblings have
gotten much more attention, then the likelihood that I
will be visited again actually increases. So this is biased
on the one hand, towards nodes that
are really promising, and on the other
hand, towards nodes that haven't been explored
yet, where there's a gold mine and all you need to do is dig
a little bit, potentially. We don't actually have an
analytical expression for this. But we can approximate
it because you can think that the expected
value from a certain node is, roughly speaking,
approximately the ratio of wins at that node to
the ratio of times that that node has
been visit at all. Let's talk about actually
applying this statement. Because what the statement
is going to give you, is it's going to give you some number
for here and some number here, and some number
for here, and so on. When we start descending
through the tree, we're going to start
at the top node. And then we're going
to look at the three children of that node. And we're going to
compute this UCB value for each of
these children and pick whichever one is the highest. So just as a thought
for a moment, what if we ignore this one? And what if we're just
computing the UCB of these two? Does anyone have any intuition
on whether the UCB would be higher for this
node or for this node? AUDIENCE: The left node. PROFESSOR 3: The left node? OK. So why is that? AUDIENCE: It has
a win [INAUDIBLE] PROFESSOR 3: Yeah. It has a win. AUDIENCE: And they both
have a [INAUDIBLE].. PROFESSOR 3: Exactly. And so clearly, you think the
exploration term is the same because you know it's not that
one child has been loved less than the other, but
the expansion term is going to be different. And so it's definitely
going to pick this one. In this case, what
we're going to say is actually that this is so much
more promising than the others that it's actually going
to pick this left node. And so it's going to expand,
and it's going to look down. And then when it
looks down, it's going to compare
between these two. And this time, remember,
that this is a parent. A parent want to minimize the
number of wins that we have. Which means that our
opponent is going to want to pick the one that
were less likely to win in and they're more
likely to win in. This is the idea of
mini-max, minimizing how well my enemy does in this game. Although again,
the expiration term might counterbalance it a little
bit because, technically, this has been explored more. We're going to pick the
one on the left again. And we're going to
get to that location that we got to originally. Now when we're comparing
between these two, between a node that
has been visited once and a node that has
never been visited, can anyone guess which one
of these it is going to pick? Yeah. AUDIENCE: Never
has been visited. PROFESSOR 3: Yeah, exactly. Because this number is zero. And so if the
parent has ever been visited but the node hasn't,
this is going to be infinite and it's going to have to pick
the node that it has never seen before. So that's how we descend
through the tree. Does anyone have any
questions on that. Really, it's totally fine. We're going to be talking
about this for a while. Yeah. AUDIENCE: With the left node
that has the four for n sub k, wouldn't that be three because
there's two and one below? PROFESSOR 3: No
because of the way that we're going to
be updating the tree. Next, we'll talk about
some [INAUDIBLE].. AUDIENCE: I like the concept. But if it's a deterministic
game, why couldn't it hold it's [INAUDIBLE]
pretty strictly? PROFESSOR 3: That's
a great question. That's really up to
computer memory limits. As I think that Leah
mentioned, the number of stakes in the game of Go-- it's a 19 by 19 board,
and you can play something at every state. It's only like 2 to the-- PROFESSOR 2: [INAUDIBLE] PROFESSOR 3: What? PROFESSOR 2: 250. PROFESSOR 3: 250. You could never explore
the entire search tree. AUDIENCE: [INAUDIBLE]
over the first few layers or are we going polite. We try to do this real
time where you could have done something offline. PROFESSOR 3: It's
definitely true. If you know a state
that you're going to arrive at ahead of time,
then you can totally do that. But in a game
that's large enough that to do that for
all the possible states would take that much more time
and take that much more memory. It doesn't end up
making that much sense. Also, something
to point out here, is that for most of the games
that we're talking about, simulating a run through
of the game is really fast. So if you think about it-- let's actually get to
that in next piece. But the point is
that building up this many levels of
a tree for a computer takes probably on the order
of less than millisecond. So doing this for a
really, really huge tree, it's peanuts because their
such simple operations. But it won't get expensive
when we start building up the tree to serious depths. AUDIENCE: But a game like Go,
how many nodes would you have? PROFESSOR 3: On each
level, in the beginning, we have something on
the order of 400 nodes. And we have a depth
of about, I think most games have up to 250
steps, or something like that. AUDIENCE: So just to build,
if you go in there blank, without any nodes built,
you have to in the computer, like you said, it
hasn't visited a node, it has to go there before
it descends further. Basically, like breadth first. PROFESSOR 3: It's sort of like
breadth first but not quite. There's an important
distinction here, which is that it doesn't have
to build up this or this node. It doesn't have to build
up all of the nodes at a certain level. All it has to do is, if it
branches down to a certain sub region, then can't
descend in that sub region below one of its siblings
without having at least looked once at all its siblings. After it looks once it
can do whatever it wants. And the point is,
that it doesn't mean the tree has to be
kept at an even level. All it means is that
the tree, in order to descend on a specific
part of the tree, it has to have at least visited
direct neighbors once before. Any more questions
on this before-- Yeah. AUDIENCE: What's the
advantage necessarily of having to visit every single? PROFESSOR 3: The
advantage of having to visit every single--
the way that I think of it, is that you don't
want to be missing out on potentially being interested
in some of the things and not others. It comes back to the exploration
versus expectation distinction. We do want to descend into
the region of the tree that is really valuable to us. But at least have
explored a little bit, at least maintaining
some baseline, which really isn't
that costly compared to the size of the tree. 400 moves is not that bad
compared with 400 and 250. AUDIENCE: Are these
simulations, they're just random simulations? PROFESSOR 3: We're going to
talk about that in a minute. Any more questions
before I move onto that? Next step is expanding. And this is very simple. You just create a node and you
set the two initial values. And the initial
values are the number of times it's been
visited is zero, and then number of times that
someone has won from there is zero. AUDIENCE: [INAUDIBLE] So
the easy part is solving it. PROFESSOR 3: Now, simulating. Simulating is really hard. You can imagine that if
you get to a single node and you've never seen
that node before, and you don't know what to
do from this node onward, that if we knew how the
game was going to play out, that is exactly what
were searching for, and we would be done. But we don't. And in fact, we have no idea
how to go about simulating a realistic game,
and a game that will tell us something
meaningful about the quality of a certain state. And so, as you
correctly guessed, we're going to do it randomly. We're going to be
at a certain state. And then from that
state, we're just going to pick random nodes
for each of the players until the game ends. And if we, as player one, win
then we're going to add one. Then we're going to say
delta equals plus one. And if we don't win,
or if we tie or lose, then we're going
to call it a zero. You can in this graph, we're
descending randomly and not thinking about it. And it turns out that
this is actually great because it's really, really
computationally efficient. If you have a board, even
if it has 400 open squares, populating it by a
bunch of random moves doesn't take you very
long, on the order of not that many machine can. AUDIENCE: That's why
does you don't score-- if you go down a tree randomly,
you already have a simulation. So the node's going
to get to someplace. But you don't store it because
it would lose the randomness? PROFESSOR 3: You're totally
right, actually, in this case. I've thought through this, and
I can't come up with a reason why you wouldn't
store it, that's it's temporary values that you
find all the way down the tree. But they don't in most of
the literature [INAUDIBLE] But you're totally
right about that. Does everyone understand
that distinction? The fact that we only
hold onto the result here and don't
theoretically make nodes for every place down in
the tree just because we could, just because we've
seen them before. We don't, and it doesn't
really matter in this case. But it's theoretically a slight
speed up that you could do. AUDIENCE: But you reduce that
question to generalities? PROFESSOR 3: Yeah, a little bit. So we can look at an example of
simulating out a running game. We get some intuition for
why a random game would be correlated with how good
your board position is. For example, here we
have a Detecto game. Circle is going to move next. But as hopefully you can
see, because you have played Detecto before, this is not a
particularly promising board for x. Because no matter
what circle does, if x is an intelligent
player x can win right now. It has two different
options for winning. And so, if you simulated this
forward randomly, what you'll get is that 2/3 of the
time, x will in fact win, even if the players
aren't really thinking of it ahead of time. Yeah. AUDIENCE: Then why
not do n simulations at a node instead of
just a single simulation? PROFESSOR 3: You
totally can do that. That's in fact, something
that make sense to do and that some people do. Although what you'll
find somewhat soon, is that considering that
we're going down the tree, and that sometimes
soon we're going to explore all of
its children, there's a good question of why
you end simulations now when you could just descend
through the tree n times and thereby do n simulations
by going through the thing and also building
out the children? This case is-- yeah. AUDIENCE: This gives
more importance to why you do randomness. Because if you're doing
random simulations you would ignore the
possibility of the best one. When you first ran a simulation
here was that o wins. If I ignore this node-- PROFESSOR 3: Absolutely. Which is why it matters that we
do this so many times that we drown out all the noise that
is associated with playing a game out randomly. Let's talk about that. If there's a lot of distance
between where we are right now and our end result-- For example, in
this game, if I were to tell you how good is this
board position, if you are one of those people who played
out every game of Detecto, you'll know that this is
great if you want it to be [INAUDIBLE] Anyway, the point
is, that is not easy to do if you are doing
random simulations from where you start. The correlation between
your friend's board state and the quality of that state
actually drops precipitously. And this for me is one
of the hardest parts to study about Monte
Carlo Tree Search. Although, as Nick
will explain to you, it actually works quite well. And one of the reasons that it
works quite well in practice for more complicated
applications is they do away
with the assumption of random simulation. Because even the
random simulations does allow you to explore
all the states, if you have some idea of where a reasonable
quality approach would be, then using that, as long as it's
not that much more expensive computationally, can help
you with your simulation. Right now we're still talking
about total randomness. How are people doing
with that idea? Now we're going to update
the tree with the results of our simulation. So given that we had
some result lambda, we're going to try to
get up the parents. And for each parent
we're going to add that the game has been
played there once, and that the result
of that simulation gets added if it was a one. So for example, if there
was a win in this game, than this becomes one, one
because now it's won once and it's been visited once. And these two get
incremented by one, and these two get
incremented by one. That in itself comprises
a complete iteration, the complete single iteration
of running Monte Carlo Tree Search, which means that
now we can keep doing this over and over again,
building up the tree and slowly making it
deeper, and making it deeper in selective areas. And having these numbers
increase and increase. And be more and
more proportional to the actual expected value
of the quality of the state, until-- does anyone have any
questions about this idea?-- until we terminate. And we have to come up
with a way to terminate it. Now again, we said we're going
to pick what the best child is going to be, what the best
immediate move from the start state is going to be. That's the move that were
actually going to play. And so, how do we
determine what the best is? Well, the trivial solution
is just the highest expected win given k. What that, in our
case, is going to be is the ratio of number
of times that I've win from a given early
state to the number of times that I visited. However, this doesn't actually
work as well as we might hope. Let's suppose the
following scenario, which is that you have the
Detecto game like this. And you have been exploring
the tree for a while. And you're really mostly
looking at these two nodes. One of these nodes, if
you think it through, this node is quite
promising and you've been exploring it for a while. There is a winning
strategy from this node. It's that circle goes
here, and then x goes here, and then circle loses because
x has two options to win. However, if you explore
this a bunch of times, and for some reason,
due to the randomness, this is at 11 out of 20. Whereas this state, which
is inherently inferior, is at three out of five because
of a bunch of randomness and because it hasn't
been explored as much. And if we had looked at
this one as exhaustively we had at this one,
that you probably would actually say that this
state is actually better. And so, you can create
an alternative criteria, which is that it's the
highest expected win value of one of the children. But also, that value
has to be the node that has been most visited
so that they aren't explored by different amounts. What this sacrifice
is however, is that this means that we
can't terminate on demand. This is not always
going to be true, and therefore, we're going
to have to let the algorithm run until that's true for
some start state, which means that maybe is not
a criteria that we want to apply even though we know
that it would be wise to do so. Are there any
questions about how we pick the terminating guide? That was the whole thing. And now we're going to do
it lots and lots of times until you guys are sick of
Monte Carlo Tree Search. So this our tree. It's more or less
what we've had before. The first thing
we're going to do is we're going to
look at the top. And then we're going to
pick one of these children. Now let's say that
we looked at this, and it turns out that the one
on the left is really valuable. I think it's the one. Nope, yeah. Never mind. It's wrong. The one on the left
has been explored a whole bunch of times. Remember, this term
starts becoming larger than the ones that haven't
been visited as much. And so we're going to
descend from this one. And now we're going to descend,
and we have these two options. Given what you know,
would you expect that this is going
to pick is going to be the one on the right
or the one on the left? AUDIENCE: [INAUDIBLE] PROFESSOR 3: On the
right because it's never been visited before. And so, this term
is going to explode. And so, we're going
to build a node there. And then we're going
to simulate a game. And the result is a win,
which is bad for this player. That means that he probably
didn't want to make that move. And so we're going to
propagate that value up. And we're going to start
the algorithm again. And it's going to compare
between these three. And now it's going to
pick the one on the left. Now that it picked
the one on the left, it going to compare
between these two states. Which of the two is going to
have a higher expansion factor? AUDIENCE: The left. AUDIENCE: Don't you
invert it, though, because this is the opponent. PROFESSOR 3: Exactly. Because two out of three
is actually better. Because it's one out of
three for the opponent that's currently
making the move. So the one on the left is going
to have a higher expansion factor, and the
one on the right is going to have a higher
exploration factor. Does that make sense for people? It's OK if it doesn't. So we're actually going to
pick the one on the right because the other one was
is doing three and has lots of it's mother's
love than that one's. Anyone else need a drink? We're going to expand that node. It doesn't matter. They are both equally
likely to be expanded. We're going to simulate forward,
and it's going to be one. Which means that that was
probably a wise countermove. Yeah. AUDIENCE: So when it's
the opponent's turn versus your turn, the
exploration factor is the same but we complement
the expansion factor, right? PROFESSOR 3: Yes. So the key here
being that this takes in both the state that
you're talking about and the player that
you're talking about. AUDIENCE: But regardless
of the player, the exploration factor will
always be like this is. PROFESSOR 3: Because it's only
the number of visits it's. It has nothing to do with
results of exploration. AUDIENCE: If you win and
you have the plus one, double plus one, and
you've propagated out, but I'm wondering-- so if the opponent wins
do you also propagate out the win increment itself? If the opponent's
winning, wouldn't you want to [INAUDIBLE] node here? PROFESSOR 3: If the
opponent wins then what you do is you propagate up a zero. Which means that wk is not
incremented, but nk is. Have we seen a zero yet? There's one soon. But the idea is that rather
than subtract or anything, all you do is propagate
up the result of the game, which in this case is zero. Which means that
all of those states seems to become more valuable
to the blue and less valuable to the red. Because these numbers are
lower than the other ones were. AUDIENCE: OK. PROFESSOR 3: So we
propagate this up and this becomes better. What we've done here
is we've figured out a theoretical countermove
to blue moving here. That's how you should think
about this whole tree. It's really a lot like
the way the humans think about these things. If I do this, then
what if they do this? Well, then I'll do this. And I see that I'm
successful when I do that. We're going to look
again at the top. And we're going to pick
the one on the left because it's really promising. Five out of six
is a good number. And we're going to
look at both sides. And which one is blue
going to pick now? Well, it's going to
pick the one that it's going to be more successful
in, which is two out of three. I realize that this is
actually not the kind of thing where I could
necessarily ask people because I'm the one who's
decided which node to stop. Then we go down here. And there's an equal
likelihood of picking either of those nodes. And so we're going to
pick one at random. So that's going to
be the left one. And we're going to
create an empty node. Then we're going to play it out. And it was a success
for blue, which is amazing because what this
means now is that suddenly, in this tree of this really
good move that red could make the blue wasn't find a
response to, suddenly there's hope because we're
going to propagate this back. And that means
that blue actually has a response move to that
sequence of red's moves. And so it's going
to propagate up. And this state's going to be
more promising to blue and less promising of red. That region of the tree
that we had dug into is a little less promising. We're going to look back up. And this time,
instead, we're going to evaluate the thing
that is both promising from the expansion
factor, and also promising because
we haven't looked at it very much [INAUDIBLE]
exploration factor. We're going to pick
between these two. Which one is going
to be picked here? AUDIENCE: [INAUDIBLE] PROFESSOR 3: Because the
exploration factor is the same but the expansion factor is
higher for the one on the left. And it's going to
show us a node. And the result is going to
be a win for a red, which means that red has found a good
countermove to the thing that was previously
promising for blue. And we propagate it back up. And finally, we're going to pick
the one furthest on the right. Because even though
it's terrible for red, and even though it's never
won when it's tried it, it has to obey his idea
of the exploration mode to find out whether maybe there
isn't something possible there. So it explores,
and it goes down, and it has to pick
the one on the right. And so it does. And it plays this game out. And it's a loss, again. Which goes to show
you, that blue has found yet
another superior move to this really bad
move of red, where probably this move of red,
if this is a game of chess, is like putting
my queen directly in front of the
opponent's row of pawns, and I just leave it there. There's nothing good that's
ever going to come of it but we have to explore
it just to find out whether there isn't some magical
way that I should protect. And as you can see,
we've built up this tree over and over and over again. And it's starting
to look asymmetric. And we're starting to
see that there's really this disparity between exploring
the regions that are crossing this tree and exploring
the regions that are not and that don't really
matter to us very much. And that this is exactly what we
wanted from Monte Carlo trees. That was why we started
the whole endeavor in the first place. The next thing I'm going to
talk about is the pros and cons. But before I do
that, does anyone have any more questions
about the algorithm? Yeah. AUDIENCE: It's still not
clear how we're getting nodes with different denominators-- [INAUDIBLE] PROFESSOR 3: The reason for
that is because of the way that we're simulating through. We're actually not holding
onto to the results of the simulation as we're
going farther down the tree than the lowest node we expand. For example, when you
simulate from here, you're going to propagate that
value here and here, and so on. But then when we
expand below, even if in the course of
this guy's simulation it happened to go
through one of the states that we expanded
below, it will not have incremented the
values of that state because we weren't
keeping track of it. Theoretically, if
we were to keep track of all of the simulations
that we have in fact run, the numbers beneath these
things would be higher. AUDIENCE: If you've already
run a simulation from that-- if you've already
run a simulation from that red node when
you first built it, and then when you created those
two ones, each of those have [INAUDIBLE] PROFESSOR 3: OK. I see. AUDIENCE: So would
the denominator always be one more than the
sum of the children? PROFESSOR 3: Yeah,
in [INAUDIBLE] Yeah. AUDIENCE: I understand
how you built that. Is there a rule of thumb, like
it's time to choose a move? And it seems like you
have very low numbers here to make a [INAUDIBLE] Is there a rule of
thumb on giving games like it's 2 to the 4 or 2
to the 350, whatever it is. What kind of numbers do
you need for that first row before you [INAUDIBLE]? PROFESSOR 3: What we'll get
to soon is that isn't one. That's one of the
problem with MCTS. But in terms of which of
the moves you will choose, there are actually variants of
MCTS that suggest that you more selectively age or
insert new children based on something more than just
the blind look right now. In terms of, if I'm here and
it's creating my next children as the equivalent, then there
are some intelligent guesses that you can make in
terms of which one you should score first. Although it doesn't
particularly matter. AUDIENCE: I'm just
saying computational time being what it is,
you might say, OK, if this is the timeline
of this game I can expect to do a million simulations,
which will give me if there's 400 nodes, I'm
going to have so much use. In other words, is
that enough time to say that I can
play through a game? I couldn't play through
a game with 400 options if I've gotten five out
of seven [INAUDIBLE] three out of four [INAUDIBLE] PROFESSOR 3: Absolutely. And I would say that
so far as I know, that's something
that's basically very high experimentally. They don't have
good balance on it. [INAUDIBLE] So let's get on
the first comment because that is a
computer element. So why should you
use this algorithm? Even though we've seen
tremendous breakthroughs in this algorithm,
and you're going to have to ignore
everything that I tell you and remember that
this does actually work quite well in
certain scenarios. Should we use it or not? The pros are that it
actually does the thing that we want it to do. It grows the tree
asymmetrically. It means that we do
not have to explore. And it doesn't
explode exponentially with the number of moves that
we're looking into the future. And that it selectively grows
the tree towards the areas that are most promising. The other huge
benefit, if you'll notice from what we've
just talked through, is that it never
relies on anything other than the strict
rules of the game. What that means is that the
only weight of the game that's factored in is that the
game is what tells us what the next moves we can
take from a given state are, and whether a given state
is a victory or a defeat. And that's kind of
amazing because we had no external heuristic
information about this game. Which means that if I
took a completely new game that someone had just invented,
and I plugged MCTS into it, MCTS would be a slightly or
someone competitive player for this game, which
is a powerful idea. It leads to our next two pros. The first of which is that it's
very easy to adapt to new games that it hasn't seen
before, or even that people haven't seen before. This is clearly valuable. But the other nice
thing about it is that even though
heuristics are not required to make MCTS
work [INAUDIBLE],, it can work [INAUDIBLE]. There are a number
of [? advanced ?] places in the algorithm
that you can actually incorporate heuristics into. Nick is going to talk about how
AlphaGo uses this very heavily. AlphaGo is not vanilla Go. It has a lot of
external information that's built into the
way that it works. But MCTS is a framework-- you
can imagine your heuristics you can apply in the
simulation, there are heuristics you can
apply in the UCB in the way that we choose the next node. There are places
that it can fit in. And this services as a nice
infrastructure to do so. The other benefit is that it's
an on demand algorithm, which is particularly valuable when
you're under some sort of time pressure, when you're competing
against someone that's a mathematician, or when
something is about to explode and you have to make a decision
on which reactor to shut down. And lastly-- or not
lastly, actually, it's complete, which is
really nice because you know that if you run
this game for long enough it's going to start looking
at a lot like a BFS tree. No, it's actually
going to start looking like an alpha-beta tree, if
it is what it is converted to. It's a nice property to have. Although, this
property does slightly get compromised if you
remove the red in this idea, and if only simulate
these [INAUDIBLE].. Yeah. PROFESSOR: You made
an interesting comment when you said, oh, it
looks like -beta tree. So it looked like
a mini-max tree. But have they also
incorporated notions of pruning in the
MCTS, which would make it look like an -beta tree? PROFESSOR 3: Sorry,
you're completely right. It does look like
a mini-max tree. I think I've seen variants
where they do pruning, but I haven't looked
into it as much. But I would imagine
that they would converge to whatever
you know pruning a certain tree [INAUDIBLE]. AUDIENCE: But people have
explored incorporating pruning into MCTS? PROFESSOR 3: I think so. I can't say [INAUDIBLE]
And then lastly, it's really parallelizable. You'll notice, none of
the regions of this tree, other than the
original choice, ever have to interact
with each other. So if you have 200
processors and you decide, OK, I'm going to break up this
tree in the first 200 decisions and then have each
one of those flesh out one of those decisions, that
actually means that they can all combine information
right at the end and make a decision
[INAUDIBLE],, which is a really nice, powerful
principle as you [INAUDIBLE].. It does have its fair
share of problems. The first problem being
that it does breakdown under extreme tree depth. The main reason for this
being that as you increase more moves between you
and the end of the game, you're increasing
the probability-- you are decreasing the
correlation between your game state and whether a
random playoff would suggest that you're in a good
position or a bad position. The same goes for
branching factors. One of the things that people
sometimes talk about it as if MCTS AI's cannot
play first-person shooters because the distance between the
number of things that you can do at every given moment, and
what would be a successful approach in the long term
after meeting many, many, many moves that each have
many branching factors, is that never begins to explore
the size of the search tree. For the most part, it's
not really coming up with a long term policy. It's really thinking about what
are the next sequence of moves that I should [INAUDIBLE]. Another problem is
that it requires simulation to be very
easy and very repeatable. So for example, if we
wanted to tell our AI, how do I take over Ontario? There's not a
particularly good way that you can simulate
taking over Ontario? If you try it once,
you're not going to have an opportunity
to try it again, at least with the same
set of configurations. And actually, one of the things
that we really took advantage of, if that random simulation
happens really quickly, on the order of microseconds. On other hand, the
bigger your computational resources that you
have access to, the better the algorithm works. That means that I can't run it
off my Mac particularly well. It would be like large games. It relies on this tenuous
assumption of random play be weakly correlated with the
quality of our game state. And this is one of the
first assumptions that is going to be thrown out the
window for a lot of the more advanced MCTS approaches,
which are going to have more intelligent play outs. But those are going to
lose some of the generality that we had before. Something that goes off of that
is that MCTS is a framework. But in order to actually make
it effective for a lot of games it does require a lot of
tuning, in the sense that there are a whole bunch of variants. And that you need to be
able to implement whatever flavor is best suited for you. Which means that it's not
quite as nice and black boxy as we would want it to be
as far as give it the rules and have it magically come up
with a strategy [INAUDIBLE].. And then lastly,
as you mentioned, there is not a great amount
of literature right now about the properties of
MCTS and its convergence, and what the actual
proportion of time to quality of your solution is. This is true of all modern
machine learning things, is that there is certainly a lot
more work that could be done. But right now,
that's a gap in terms of using this for a simulation
that's supposed to be reliable. Anyone have any questions
on the Pros and Cons? Before we jump dive
into applications, let's talk through
a few examples of what games could be
solved and could not be solved by MCTS. Do you guys think that
checkers is a game that could be solved by MCTS? AUDIENCE: Yes. PROFESSOR 3: It's
completely deterministic. It's two-player. It satisfies all of the criteria
that we've laid out before. Checkers is
definitely a game that can and has been solved by
MCTS, although not solved to the extent that you can
defeat the thing that actually has the solution [INAUDIBLE]. How about "Settlers of Catan?" This one's a little
bit trickier. Do you guys think that MCTS
is likely to be able to play "Settlers of Catan?" If not, let's throw out reason
why or why not it would be [INAUDIBLE]. Yeah. AUDIENCE: No because
there's randomness. PROFESSOR 3: So yes, that
is absolutely the criticism. And that's why we
can't apply it vanilla. I put this on here
as a trick question, though, because it
turns out that MCTS is robust to randomness. That you can actually play-- and I realize that's
just me and we do. [LAUGHTER] You can actually
play through games. If you think about
the simulation, the simulation is
actually applicable even if the game is
not deterministic because it does give you
a sense of the quality of your position. And the MCTS-based
AI to play "Settlers" is, I think, at least 49%
competitive with the best AI to play, at least in the
autonomous non-scale space. So it does work. Let's talk about the war
operations plan response. Who here has seen the
movie "War Games?" OK. Well, it should be more of you. The idea of "War
Games" is that one of the core characters
in this world is this computer
that has been put in charge of the national
defense strategy with respect to Russia. And that it needs to think
through the possible future scenarios and decide whether
it's going to launch the nukes or not. Do you think that WOPR
can be MCTS-based? AUDIENCE: No. PROFESSOR 3: No. AUDIENCE: It could, it
just wouldn't be very good. PROFESSOR 3: Absolutely. Once you fire the
nukes you're not going to get another chance. So you can't
particularly simulate through what the possible
scenarios are going to be like. Yeah. AUDIENCE: So what if you had-- I agree you can't simulate
it in the real world. But what if you had
a really good model and you just simulated
based on that model? PROFESSOR 3: In that
case, it probably depends on the quality of your model. If you have a good model for
how World War III is going to [INAUDIBLE]. [LAUGHTER] AUDIENCE: It is the case
that the military does have simulators and they
do war games in simulation. PROFESSOR 3: Yes, that's true. They could certainly try it
and run MCTS if they wanted. And that's what
happened in the movie. [INTERPOSING VOICES] AUDIENCE: And there
you're putting your money in the simulation not in the-- AUDIENCE: It's like having an
MCTS play SOCOM or something like that. PROFESSOR 3: Yeah. It's definitely about putting
money into the simulation and you get really
good simulation. If you have a really
good simulations then you [INAUDIBLE] to play WOPR. Yeah. AUDIENCE: Back to
"Settlers" for a second. I'm curious if there's a way
for the whole player training resources thing,
or would it have to be only purely
like using the ports. PROFESSOR 3: That's
a good question. I haven't looked closely at
whether they do that or not. If it's playing a
two-player game, then I would imagine that they
wouldn't because you don't really trade in to play a game. But if they weren't,
I bet that you can incorporate it with WOPR. AUDIENCE: Is it limited
to two-player games? PROFESSOR 3: No, not at all. In fact, there are
lots of purchases that do only
one-player games, where you think of what's the best
movie that you can make. AUDIENCE: I know. But I mean, couldn't MCTS handle
three- or four-player games? PROFESSOR 3: Yeah,
it absolutely could. I'm not sure how they
computed their head-to-head. That might be
completely flat cursors. I'm not even sure how
the settlers interact. Yeah. AUDIENCE: A quick question. So at first you know if I
reduce the chess board to only 4 by 4 or 5 by 5, and
I run MCTS versus the traditional algorithm that
AlphaGo offered as a tree. Do you think MCTS will
prefer theory and perform this computational requirement. PROFESSOR 3: The thing about
the way that Deep Blue is, which is the AI that
ended the Kasparov thing, a bunch of his
chess grand master, is that it has a tremendous
amount of heuristic information. There's a lot of
external stuff that's incorporated into the
system that makes it able to explore the best paths. What I would say is
that knoledgesless MCTS based on randomness,
would take a very long computational time to even
become competitive with those kinds of algorithms, and
probably feasibly never would. What if you incorporated
heuristic information, I think that there's a bunch of
hope in terms of getting MCTS to start performing better. And you can look at what
next I'm going to talk about, AlphaGo. It takes inspiration for how
we go about incorporating these new circuits. AUDIENCE: So only the
circuit you [INAUDIBLE] PROFESSOR 3: It definitely
seems like if you have a really good
heuristic model for what good states in the game are,
that if it's a smaller search space, that some other
models could perform better. Although, I'm probably
going to eat my foot here because this is going to be
on OCW some massive amount, massive chess
playing algorithms. Eat my shoe not my foot. [LAUGHTER] One last game. Does anyone know
what this game is? AUDIENCE: "Total War?" PROFESSOR 3: Yes. Nice. This is "Rome, Total War II." It's a simulator for this
tremendous real time strategy game, where you play, I
think, the Roman Empire. And you're controlling armies
and huge infrastructure systems that move and conquer
states and continents, and meet in the field, and
manage resources, and do all of these incredible
diplomacy feats. And so do you think that this
game can be solved by MCTS? AUDIENCE: Yes. AUDIENCE: Yes. PROFESSOR 3: Lets say no. But I guess I put it on here. So that's good on you. The way that the AI in
"Rome, Total War II" is built is that it's built
on an MCTS structure. And it in fact does
do resource allocation and a lot of its
political maneuvers based on Monte Carlo
Tree Search moves. There are a bunch of
reasons that they explain in the game for
why they do this, or in papers released
about the game. But one of the nice ones
is that it's random, which means that
you're never going to play against the same kind
of AI twice because every time the set of decisions that
it's going to think about is completely different. AUDIENCE: I have
a quick question. PROFESSOR 3: Yeah. AUDIENCE: So if I want to
model any game with MCTS, does it have to be that the
actions in playing a game has to be able to discretize. PROFESSOR 3: Yes. So far as I know, I haven't
seen many continuous variants in MCTS. And so, I think that it is about
choosing these reactions, which on it's most narrow level does
actually bring it down to here. I think one of the
reasons that this is nice is that there are so
many different decisions that could be made that MCTS is
really the only approach that could even begin to handle the
massive branching factor that's associated with the
game Rome, Total War. Yeah. AUDIENCE: This is
also the consequence of this year you get the play
off when this game comes. PROFESSOR 3: That's interesting. That's probably totally it. That's cool. That's everything about how
the algorithm actually works. I'm going to pass
it off to Nick, and he's going to talk to us
about some actual limitations for this game [INAUDIBLE]. PROFESSOR 3: So
as you have said, I'm going to start diving
into some applications here. And not only applications
but also some modifications or augmentations of MCTS. It should hopefully clarify
some of the side questions you all have been having
on slight tweaks to MCTS. Now let's get started. Wait for it. Now let's get started. [LAUGHTER] Part III, applications. First thing we're
going to look at is an MCTS-based
"Mario" controller. And "Mario" might seem like
some weird thing to test AI on, but there actually is a "Super
Mario Bros" AI benchmark, which it used to
test a lot of AI on how well they could
play this platform. In case any of you don't
know what "Super Mario Bros" is, this is a screenshot. Basically, you control
this one character. It's a single-player game. The ultimate goal is to
reach this flag at the end. But along the way
there's enemies, there's some bonus
shrooms you can get. If you break open some
boxes you might get coins, things like that. But first, let's just highlight
some of the modifications that need to be made, or some of
the differences between vanilla MCTS and an MCTS that's going
to be able to work for "Mario." First thing is that
it's single-player. The second is, we use a
slightly different simulation strategy than the initial
just vanilla simulation. And someone actually hinted at
doing more than one simulation because you, you're watching
us to n simulations, I think. We'll touch on that. Then this also introduces
what I would consider to be domain knowledge. Then finally, there's a 50 to
40 millisecond computation time. And that has to do with the
frames per second of the game. So you would think that
"Mario" is a continuous game, but if we discretize
time into these chunks, then we can use MTTS. Now let's just think about
how we could possibly formulate this problem. Can anyone think of
what each of these nodes would be if we're
playing "Super Mario?" AUDIENCE: Jump. PROFESSOR 3: Sorry? AUDIENCE: Jump. It would be like, first
node you're going to jump. PROFESSOR 3: That might
be a way to formulate it. But I think that could get-- AUDIENCE: Oh, it's not your
control at inputs [INAUDIBLE].. PROFESSOR 3: Right. So the node itself isn't
going to be an action. AUDIENCE: Equal frames. PROFESSOR 3: Yeah, basically. So it's going to be the
state of a game, what we'll call a state. So it's basically
just a screen grab. And it take it,
in this case, it's a 15 by 19 grid screen
grab of the game. And it will have
information about-- it knows Mario's position, it knows
the enemy's position, position of the blocks, et cetera. And then, as Yo
was saying, in MCTS we have values associated
with our nodes. And so it will
also have a value. But we'll get into the
value in the next slide because I can't really
fit it all in here. With that being said
for our node, that being the state of the game,
what makes sense for the edge? Does anyone know? How do we transition from
one state to another state? AUDIENCE: Jump. PROFESSOR 3: Yeah, exactly. So this is where the
jump and all the action have been played. So the actions that you take-- I didn't list all the actions. You can also have a jump left,
jump right, all those things. But basically, the
actions are what takes you from state to state. So I just drew out
what a node might look like if you
used the jump action. You might have Mario
go up in the sky. Are there questions? AUDIENCE: Does it just
run the rest of it? Because that little thing's
moving as they move on? PROFESSOR 3: Well, it's not
moving in this moment in time. We're discretizing
time right now. AUDIENCE: But I'm saying,
if your action is jump, just you would have 1,000
nodes because if you did plan out where that thing's
moving, left or right, then it could be-- PROFESSOR 3: Yeah, right. So in each state we
have the enemy position. And we know the
speed and direction. And so we know when we go from
this node to one time step later, we'll know where
the enemy's moving. Any other questions? Moving on. Sorry. Let me just preface
this part real quick. So in our other simulations,
at the end of the simulation we would get either a one or a
zero, if we'd won tic-tac-toe or we lost tic-tac-toe. But that won't really work
too well here because there's a lot of other factors
that go into play when you're playing "Mario." Also, if you're doing a
simulation, more than likely, you're going to end
up hitting an enemy and dying or falling
into a gap and dying. So a lot of these simulations
might all return zero. And that is, you can't really
distinguish between them. So this is why I say,
this version of MCTS introduces what I would
consider to be domain knowledge. Basically, they're
assigning scores to potential things that
could happened along the way. And this is basically telling
the AI that collecting a flower is a little bit better
than collecting a mushroom. It's telling it that
getting hurt is bad. Right off the bat, all
these things in the score are giving the AI some domain
knowledge about "Super Mario Bros," that it's helping
it calculate the simulation results. As it says here, it's just doing
a multi-objective weighted sum of all these things. Throughout the simulation it's
just adding up your score. And then that's the score that
is going to be propagated. Are there questions
about the score? AUDIENCE: You said that
it adds up all these guys and it propagates it over. Is it possible to just propagate
the multi-part sum [INAUDIBLE] as opposed to propagating
one value that you create? Are you essentially
propagating all-- what's this?-- 15 values
upwards at every node, or are you propagating one value-- PROFESSOR 3: Well,
it's one value. It's the collective-- AUDIENCE: Then you make
them add it together and you got each one
of them a sub factor. PROFESSOR 3: Then also,
just one thing to note here, is distance, you get 0.1. And these are all parameters
that have been tuned. In the initial version,
distance was, I think, a reward of five,
but probably realized that that made Mario skip past
a lot of coins and things. And so he tweaked
the score for that. And also, time left is two. So there's some weight there. You want to get to the
very end of the game. AUDIENCE: If you're
pushing up this score, it's no longer a
win over losses. So it's not w over n. What is it affecting? PROFESSOR 3: You can
just use the score. AUDIENCE: The score is the-- PROFESSOR 3: Yeah. In MCTS you have
this idea of when you're propagating your q value,
you could have that to be zero, one. AUDIENCE: It's like the sum of
all the scores and the nodes below over the number
of games you win. PROFESSOR 3: So
basically, what you would be getting when you divide
by the number of simulations is your average
score at that node. AUDIENCE: OK. AUDIENCE: When you have
killsByFire and [INAUDIBLE] like that, if you
have a positive value, then isn't it good
to be killed by fire, or something like that? PROFESSOR 3: This is
killing an enemy by fire. Like Mario could collect a
certain flower or mushroom? I think flower, then you
have a fire breath and you [INAUDIBLE]. AUDIENCE: So that's Mario's
status if Mario never dies? PROFESSOR 3: No. Mario's status is-- I believe, Mario's
status is the fact that you could upgrade Mario by
collecting [INAUDIBLE] mushroom from a fire Mario. So that gives you
a lot of points. Because if you
become fire Mario, then you're more likely to not
die by running into enemies because you have fire-spewing-- AUDIENCE: You said they
spent a lot of time tuning these parameters. Isn't it generally, though,
just an optimization framework if that's
some formula? So they tuned the
parameters just to make behave the way
that we think is nice. But if you change
the values, they'll do the right thing
for that equation. PROFESSOR 3: Yeah. AUDIENCE: OK. PROFESSOR 3: Yes. But they were tuning this to
make it play how they wanted. AUDIENCE: [INAUDIBLE] can't just
be a reflection of [INAUDIBLE] PROFESSOR 3: That's a strategy. If you choose that,
I don't see why not. That might affect
certain things. Obviously, you can change
these to whatever you want. It'll slightly tweak
which simulations as to working better, in terms
of changing which nodes you end up choosing [INAUDIBLE]. So we move on. So we know about
scoring simulations. Now we're going to look at
exactly the simulation type that's used to play
this MCTS controller. So the regular version
that Yo talked about is just choosing a
random node at each level in your simulation. But there are some
other strategies. And someone brought one up. The first is, look at best of n. So in this one, you choose three
random nodes at each level, except that you stick with
the best of those three. Choose three random nodes,
stick with this one. Go to the next one. You would choose n random
three, take the best one, and then go to the next level. You are able to do that
in this game because of the way the
scoring works, you don't have to get to the end
of the game for your score. You actually could collect
a coin along the way. If this is jump,
and then it gets to be a coin versus
moving left and right. That doesn't give
you any points. Then this is the node that would
give you the highest scores, so I would choose
that one, et cetera. And then the final one,
which is the one that is actually used
for this controller, is multi-simulation. This was brought up by him. I don't know your name. Sorry. But basically, you run
multiple random simulations from your node. And then you propagate up
whichever of those simulations give you the highest value. And the reason to do
multiple simulations is to attempt to increase the
accuracy of your simulations. If you just do
one simulation you might just get really lucky. But if you do three then you
can take the highest value use that as your value. Since the whole point of this is
to try make moves that get you the highest values,
then that will make your random simulation
value more accurate. Are there questions
about multi-simulation? AUDIENCE: So what do you
think about the simulation [INAUDIBLE] how many [INAUDIBLE] PROFESSOR 3: So there's
a trade off here. The more simulations you
do the more accurate-- the more representative
your simulation will be at the end of the game. You could run two to
the whatever simulations to try to get every
single possible action and then take the max of that. And that would give
you the maximum value. That would be ideal. But obviously, that
takes more time. So there's a trade off
between computation time and the number of
simulations you run. And that's just something
that they probably just played around with. AUDIENCE: Do you
use [INAUDIBLE] have to finish the decision losing a
couple of minutes or 10 minutes or they're going to take
your [INAUDIBLE] away. PROFESSOR 3: In this competition
there is different computation time budgets that you get. And I believe the reason for
the different computation time budgets is the frame
per second of the game. I told you all
about the setup, we went over, the scoring, the
nodes, what the advantages are, what the simulation
strategy is used. So you probably want
to see it in action. So this is always a risky move
trying to get video to play. AUDIENCE: It's actually
in the back up. Hit Escape. PROFESSOR 3: OK. Got it. AUDIENCE: And now, I guess, we-- PROFESSOR 3: And
drag it over again? AUDIENCE: Yeah. PROFESSOR 3: Running
this full screen. AUDIENCE: Hit the [INAUDIBLE] PROFESSOR 3:
[INAUDIBLE] All right. Here's this MCTS-based
"Mario" playing controller. You can see he's
actually wrecking, so doing some serious damage here. But those lines that you
see, the reason they're different colors it's not
showing different players, or anything like that. It's just using
different colors so you can see the different
layers of this tree search. You can see he actually
went backwards there. And that's because
in a simulation, when one of the backward
ones landed on an enemy-- and in fact gets you points
from our scoring system versus if you had just gone forward you
would have gotten some distance points but not-- also, he is just [INAUDIBLE] The simulation is quickly being
able to figure out that he can jump on all his enemies. So he's just wrecking
all these guys. Getting lots of points here,
collecting the coin, et cetera. You get the idea. It's pretty awesome to watch. There's that flower
we were talking about. So now he's actually a
fire-spewing Mario demon. He's doing some serious
damage with that. Stepping on missiles. I didn't even know you
could step on the missiles. All right. You could watch
this for a while. But we'll exit now. It looks super
promising in this video. I don't know how
close max stuff. AUDIENCE: Just click
on back [INAUDIBLE] PROFESSOR 3: There it is. OK. The demo looks really cool,
looks really promising. Let's take a look at the
charts here because we all want some quantitative stuff. This is the chart. The score is on the y-axis. The bottom is computation
budget, which is something that you were talking about. I just want to highlight
to make this a little more visually appealing here. All of these things
that I highlighted, it's labelled as UCT. That's Upper
Confidence Bound Tree. Remember, Yo talked about
upper confidence bounds. That's essentially
what's used in that TTS for guiding your tree search. So these are all the methods. But then UCT multi, which is
this purple square, that's saying it's using MCTS but it's
doing the multiple simulations. And you can see this multi
plus care is also in the top. Both these use the
multi-simulation technique. And then the plus car is
they added an extra scoring mechanism for carries. I believe that's probably
like carrying a shell. That made it do better. Then these ones that
aren't highlighted are using plain Astar, and then
a refined version of Astar. With increasing time,
the do increase scores, but they're even worse
than just your UCT with just random simulation,
no multi-simulations. We're running low on
time, which is not ideal. But another thing that I want to
point out is down at the bottom here, these are the
multi-simulations. They have the lowest
maximal search depth, which at first would seem like, what? I have the lowest search depth
but my score is the most? But that comes
into play when you were saying about the trade
off between the simulations and the amount time it takes. So because I'm doing
multiple simulations, I'm taking more
time at each node. But that's giving me a more
accurate value assessment. So that let's me choose
my actions more carefully, or with more information. And so that's what's able to
give me this better scores. That's all "Mario." So we're going to
moving onto AlphaGo. Are there any questions about
"Mario" before I go to AlphaGo? Yeah. AUDIENCE: What's the table
[INAUDIBLE] inference? PROFESSOR 3: That's
a good question. I have a feeling it's because
if you're doing best of n, that's really heavily relying
on your scoring metrics. Let's say at one step
if I jump and collect a coin versus if I go
left or right and play, I'll get more points
if I get that coin. But maybe, a missile is
going to hit me in the face if I do that. It gets rid of
some of the-- it's forcing you to do certain moves. AUDIENCE: Is the A*
heuristically using the same value, the same value
that you're getting by your simulation? PROFESSOR 3: Yeah. I'm not exactly sure what
the Astar heuristic is. The whole reason that A* is
difficult is because coming up with heuristics for
these types of games are. But this is not his
version of Astar. I believe this is the
Astar that was used by-- I forget the name of the guy--
but he won the AI competition a couple of years ago. I'm going to try to
move onto AlphaGo. Does someone have how
many minutes I have left? AUDIENCE: Four. PROFESSOR 3: OK. We're going to power through. Here's AlphaGo. Hopefully, you all
know the rules. Just in case, I'll just
go through a quick-- 19 by 19. You alternate black
stones and white stones. You collect enemy stones by
completely surrounding them. You can surround a single
stone. groups of stones. And your score is your
territory plus the number of captive pieces. So your territory is just the
area that you're surrounding, and then you just add the
stones you've collected. The rules aren't
super important. The main emphasis is there's
very few rules so you would think it's really simple. But the complexity of the
game is quite extreme. At each turn you have about
250 options that you can play. Each Go game lasts
about 150 turns. So that gives you a total
of 10 to the 761 games, approximately. And to put that in
comparison, here's chess. You can read those numbers. Chess is also pretty complex. But there's 35
options for turns. Deep Blue. I think you were talking about
building out the whole tree. So Deep Blue would build
out the tree for six levels. And then use this
hard core chess master inputted heuristic
evaluation that it used to find the best move. Except with Go, you
have 250 options, which already is adding
a lot more complexity. So that strategy won't
work quite as nicely. What do we do? We use a modified
version of MCTS. Well, it's not what we do. That's what Google's
DeepMind team did with Go. They combined neural
networks with MCTS. Coincidentally, we learned about
neural networks last class. Probably not a coincidence. PROFESSOR 3: It's
not a coincidence. PROFESSOR 3: The we
ordered two policy networks in the AlphaGo, and
one value network. And another big
coincidence here, the two policy
networks are actually CNN's, which we learned
specifically about last class, convolutional neural nets. And the reason for
that is the input to the policy neural networks
is an image of the game. And remember, convolutional
neural nets work really well with images. What it outputs, though, is
a probability distribution over the legal moves. And the idea is, that if a
move has a higher probability it will be a more promising
move for you to take. But another key point is
that it's not deterministic. It's not telling you
to take this move. It's just assigning a higher
probability to this move. And this network was generated
by doing supervised learning on 30 million positions
from human expert games. Apparently, there's a giant
database of Go expert games. So that came in handy. And there were two
different networks trained. One of them was a slow policy,
the other was a fast policy. The slow was able to predict an
expert move with 57% accuracy, which to me was mind blowing. Using this neural
network, 57% of the time it could pin where the
expert would place his move. That took 3,000 microseconds. Versus the fast policy, which
suffered a bit in the accuracy, but it's 1,500 times faster. And we'll see where
they used each of these different
policies later on. But it could predict the
expert move with 57% accuracy. The other Go team was,
that's not our goal. We don't want to
predict an expert move. We want to predict
a winning move. And so to do that, they
took their policy network, and then they would use
reinforcement learning. That's where you play the
network against iterations of itself in order to hone
in a better policy that's geared towards winning moves. Then they tested this
against Pachi, which uses-- for the camera, I
have no idea if that's how you pronounce Pachi. It might be Patchey. I'm not sure. But there's 100,000 MCTS
simulations at each turn. So this is purely MCTS. If it were playing just
the AlphaGo policy network, the policy network
won 85% of the game. So without any sort of trained
search or anything involved, it won 85%, which
is pretty great. And that suggests that
maybe intuition wins over long reflections in Go. And interestingly, if you
talk to expert Go players and you ask them why they did a
certain move, they'll just say, It felt good, or I
had a hunch in this. That's indicative there. Hopefully, I'm not
going overtime. Sorry. Those are the two
policy networks. There's also a value network. What the value network does
is it takes in a board, and they'll give you a value,
like how good is this board? They'll give you a win
probability number. So 77%, it would say,
77% of the time you should win from the board. That's similar to the evaluation
that comes from Deep Blue. But rather than a Go master
coming in and telling you, well, if these are
connected in this way, and down here we have
this certain thing then here's the score
we should expect, in chess, they
had chess masters, like if the knight is here
and the queen is here, all these specific things. This was actually learned from
the reinforcement learning that was happening when the
policy networks were playing each other. The value network was
learning about those positions during that time. And the predictions get
better towards the end of the game, which I think
Yo mentioned in his talk. So how do you combine
all these into MCTS? The slow policy network,
if you remember, is slower but should
give us stronger moves. It is used to guide our
tree search in order to help us decide which
nodes to expand next. When we expand that
node to get the value, the value of the state is
the simulation, like before, like normal MCTS,
except it's not a completely random simulation. We use our fast policy network
to give us a more educated simulation here. But we're using a
fast one, obviously, to save some computation time. It's giving us probably a more
indicative random simulation of what's going to
actually happen. And then we also combine that
with our value network output. So we run our value network
on this node, as well. And we add that to
our simulation value and we propagate it. Interestingly, the
AlphaGo team tested out just using the fast
policy simulation value and scrapping the value network. And they also just
used the value network and scrapped the
simulation value. And those both performed
worse than if it had these. And another added
interesting point here, is that these two
factors in our value have about the same weight. They were both about
equally important. I think I'll get
into that later. But first-- AUDIENCE: Can I just
ask a quick question? PROFESSOR 3: Yeah. AUDIENCE: So when you said
the policy network is used, is that used when you're
navigating to the tree to get to a leaf,
or is policy network being used to do the
simulation once you're at the leaf, or both? PROFESSOR 3: The slow policy
is done for this part. Then the fast policy is
used for the simulation. Because the slow policy does
take 1,500 faster than-- or the slow takes 1,500 times
longer than the fast policy. You don't want to use
that in your simulations. That would just
take way too long. It's basically just
a way of making it so our simulation isn't
completely random. It has some educated moves. Why use policy and
value network synergy? Why can't we just use
the policy network? Why can't we just use
the value network? If we have the
value network alone, we'll actually--
here's a side point. Remember, the value
network learned from the policy network. And then also, later
on, the policy network is improved by our values. They work hand-in-hand. But if we had the
value network alone, when we're deciding
on it the next move, we're going to have to evaluate
every single move, which would take forever. And so, what the
policy network does is project the best move
with a probably distribution. And it narrows our search space. And then, if we had the
policy network alone, we'd be unable to compare nodes
in different parts of our tree. The policy network
is able to tell us a distribution over
which move we should take from a certain node. But then, if I ask it if
I'm in a better position here than in some other
place, it won't know. That's where the value
network comes in. It will give us an estimated
number of the value assigned and open an evaluation
of that node. And then these
values are later used to direct our tree
searches based on updating the policy
once it realizes, oh, I thought this would be
a good path but the value is this, so update all that. Then why do we combine
neural networks with MCTS? Remember, the
policy network alone played against Pachi,
which was purely MCTS, and it did pretty well. So how does MCTS improve
our policy network? Remember, MCTS did win
15% of those games. So already, that makes you
think there's something there that maybe the policy
network is missing. Also, the policy network
is just a prediction. So by using this
tree structure, we're able to use these Monte Carlo
rollouts to adjust our policy to move towards nodes that are
actually evaluated to be good. And then, how do neural
networks improve MCTS? The point should
probably be clear by now. We're able to more intelligently
lead our tree exploration. Our simulations are more
reflective of actual games. And the value network
and our simulation value are complementary, which
I've mentioned before. And just to highlight that,
basically, the value network is going to give us a
value that is reflective as if we've played the
slow policy the whole time. And the simulation is if
we used a faster policy. So they are complementary. And I know I'm over time. So I just wanted to skim
through the stats real quick. Distributed AlphaGo
won 77% of the games against regular AlphaGo. So it's the only thing
that beat regular AlphaGo. And then distributed AlphaGo
won 100% of the games against all these. In a rematch against Pachi,
now that we've added MCTS to our policy network and
we have our value network, we slaughtered Pachi 100%. Then we decided to see how
we fare against humans. And by we, I mean not
me, I mean Google. And they won 4 to 1. And Lee Sedol rating was 3,520. Now AlphaGo's rating is
estimated to be about 3,586. So you're like, whoo,
we beat the best dude. Except we didn't because
there's another dude who has an even higher
score, apparently, 3,621. This should be the last part. Here's this timeline. Basically, tic-tac-toe,
checkers were conquered in '50. About 40 years later, we
conquered checkers, chess. Then we scroll down
to 2015, is when AlphaGo was able to
beat Fan Hui, who was a two-dan player, which
is considered lower down in the tier of professional Go. But then, Lee Sedol
was a nine-dan player. And he was able to beat
him literally last month. PROFESSOR WILLIAMS: So good job. PROFESSOR 3: We're done. [APPLAUSE]

CS

&gt;&gt; [MUSIC PLAYING] &gt;&gt; ROB BODEN: All right. So, first thing first, of video
from a familiar face. &gt;&gt; [VIDEO PLAYBACK] &gt;&gt; -All right. This is CS50, and this is
the start of week three. I'm sorry I couldn't be there with you
today, but allow me to introduce CS50's own Rob Boden. &gt;&gt; [END VIDEO PLAYBACK] &gt;&gt; [APPLAUSE AND CHEERS] &gt;&gt; ROB BODEN: The filmography in
that video is fantastic. All right. So first, there's another lunch. It's tomorrow at 1:15. There's no lunch this Friday. It is with Quora. And Tommy's not here yet, but one of
the people there is former head CF, Tommy McWilliam. So he's a fun guy. You should come. &gt;&gt; All right. So last week, we started breaking apart
about what a string really is. We've known since the beginning that
it's a sequence of characters. But last week, we delved into the fact
that what is really a sequence of characters, well, we now have
arrays of characters. And we know that a string, it's an array
of characters, at the very end, we have this special null byte, this
backslash 0, that indicates the end of the string. &gt;&gt; And so a string is an array of
characters, but we can have more than just an array of characters,
we can have an array of any type of thing we want. So, if you recall from last week, the
Ages program that David introduced really quickly. So first thing we're going to do is
ask the user for an integer, the number of people in the room. Once we have that integer,
we're declaring an array. Notice this bracket syntax. You're going to get used to that. &gt;&gt; So we're declaring an array of integers
called ages, and there are n integers in this array. So this pattern right here, this 4 int
i equals 0, i is less than n, i plus plus, that is also going to be a pattern
that you get very used to. Because that's pretty much how you're
always going to iterate over arrays. So remember that n is the
length of our array. And so here, we are repeatedly asking
for the age of person i in the room. &gt;&gt; After this, we go down, and for whatever
arbitrary reason, we then print out how old they're going
to be a year from now. And running that program, let's
make ages, dot slash ages. So number of people in the room,
let's say there are three. And say, the first person is 13,
next is 26, and the last is 30. So then it'll iterate over those three
people, print out 14, 27, and 31. &gt;&gt; So remember that when we declare an
array of size n, the indices in that array, the array has values and
indices 0, 1, 2, all the way up to n minus 1. So when we said there were three people
in the room, and we put in here the first iteration through this
loop, i is going to be 0. So in index 0. We are assigning the first
age the user enters. Then in the next one, we're entering the
second n the user enters, and in next to two, the last n. &gt;&gt; So notice that an array of size
three does not have anything in the index three. This is not valid. All right. So, going back here. So now that we've dealt with arrays,
we have some familiarity. Now we're going to move on to command
line arguments, which are going to be pretty relevant to this problem set. &gt;&gt; So up until now, whenever you've
declared your main function, we've said int main void. So void just means that
we aren't passing any arguments to this function. Now we're going to see that main
can take some arguments. Here we call them int argc
and string argv brackets. The brackets, once again, indicating
that we're dealing with arrays. So here, string argv brackets, we're
dealing with an array of strings. So argc, that's going to indicate
how many arguments we've passed to this program. And to see what that means,
let's close this. &gt;&gt; OK. So up until now, we've run every
program like dot slash ages. We can also, at the command line, past
pass arguments, thus the term, command line arguments. So the first argument, hello world. So here, argc would be three. It's the count of the arguments
at the command line. Argc is always at least 1, since dot
slash ages, itself, counts as one of the command line arguments. &gt;&gt; Then hello is the first. If dot slash ages is the zeroth, then
hello is the first, and world is the second command line argument. So the string argv, we're going to see,
contains the strings, dot slash ages, hello, and world. And, by David's request, we're going
to play a video introducing that. &gt;&gt; [VIDEO PLAYBACK] &gt;&gt; -Up until now in programs we've
written, we've declare main as int main void. And all this time, that void has
simply been specifying that the program does not take any
command line arguments. In other words, when a user runs a
program, he or she can provide command line arguments by writing additional
words or phrases after the program's name at the prompt. &gt;&gt; Well, if you do you want your program to
take command line arguments, one or more such words, we need to replace
void with a couple of arguments. So let's do that. Include CS50.h. Include standard io.h. Int main. And now, instead of void, I'm going to
specify an int called argc, and an array of strings called argv. Now, argc and argv are
simply conventions. &gt;&gt; We could have called these arguments
most anything we want. But what is important is that argc is
an int because, by definition, it is going to contain the argument count, the
number of words in total that the user has typed at his or her prompt. argv, meanwhile, argument vector, is
going to actually be an array storing all of the words that the user has
typed at his or her prompt. &gt;&gt; Let's proceed to do something now
with one or more of these command line arguments. In particular, let's go ahead and print
whatever word the user types after the program's name
at the prompt. Open bracket. Close bracket. Printf percent s backslash and comma. And now I need to tell printf what value
to plug into that placeholder. I want the first word that the user has
typed after the program's name, and so I'm going to specify
argv bracket 1, close parenthesis, semicolon. &gt;&gt; Now, why bracket 1 and not bracket 0? Well, it turns out, automatically stored
in argv 0 is going to be the program's actual name. So the first word that the user types
after the program's name is, by convention, going to be
stored in argv 1. Let's now compile and
run this program. &gt;&gt; Make argv 0, dot slash argv 0. And now a word like hello. Enter. And there we have it, hello. &gt;&gt; [END VIDEO PLAYBACK] &gt;&gt; ROB BODEN: All right. Close that. So taking a look at that program that
we just introduced to us, well, just to show, if we print argv 0, make, now
what is it, argv 0, dot slash argv 0. So, as expected, it's printing out the
name of the program, since argv 0 is always going to be the
name of the program. But let's do something a
bit more interesting. &gt;&gt; So in the problem set, you'll be
introduced to this function, atoi. So what do we use atoi for? That's going to convert a
string to an integer. So if I pass the string, one two three,
to atoi, that'll convert that to the integer, one two three. So we're going to convert the first
command line argument to an integer, and then just print that integer. &gt;&gt; So basically, we're kind of
reimplementing getint, just the integer is entered at the command
line instead of in the program interactively. So then, making argv 0, let's do
it in here, and close that. So running argv 0, and let's enter the
integer, one two three four one two. So it'll print the integer, one
two three four one two. There are some subtleties to atoi that
it'll stop caring about anything beyond a valid numeric character,
but that doesn't matter. &gt;&gt; So what do you think happens
if I do this? Segmentation fault. So why is that? If you look back at our program, we're
converting argv 1, the first argument after the program name, to an integer. But there is no argument passed
after the program name. So here, we see that this is a buggy
program, since, if we try to run it without any arguments,
it will just crash. &gt;&gt; So another common pattern you'll see
is something like, if argc is less than two, indicating that there was not
at least the program name and a first argument, then we'll do something
like printf, not enough command line arguments. That's probably not a good one to print,
it's probably something, like you should enter an integer
at the command line. I'll just end it there. And then return 1. So remember that at the end of our
program, if we return 0, that sort of indicates success. And main also automatically
returns 0 if you don't. &gt;&gt; So here, we're retuning 1 to indicate
that that's not success. And you can return whatever you want,
just, 0 indicates success, and anything else indicates failure. So let's run this version of things. So now, if we don't enter a command line
argument, it'll correctly tell us, not enough command line. Didn't finish the sentence. Else, if we actually pass it one,
it can complete the program. So this is how you would use argc in
order to validate the number of command line arguments that
are actually passed. &gt;&gt; So let's make this program a bit more
complicated, and look at the second iteration of things. So now, we're not just printing the
first command line argument. Here, we're iterating from int i equals
0, i is less than argc, i plus plus, and printing argv, index i. So this pattern, again, this is the same
pattern as before, except instead of calling the variable
n, we're using argc. &gt;&gt; So this is iterating over each index
in the array, and printing each element in that array. And so, when we run this program, well,
I didn't enter any command line arguments, so it just prints
the program name. If I enter a bunch of things, it'll
print one, each on its own line. &gt;&gt; OK. So let's take this one step further. And instead of printing each argument
on its own line, let's print each character of each argument
on its own line. So remember that argv is
an array of strings. So what is a string, but
an array of characters? So that means that argv is really an
array of an array of characters. So taking advantage of that,
let's ignore this for now. Let's just consider the string argv 0. &gt;&gt; So if we want to bring each character of
argv 0 on its own line, then I want to do the pattern we're used to, i is
less than the length of the array, which here, is strlen of, that's
not what I want to do, string s equals argv 0. So i is less than the length of our
array, which in this case is an array of characters, i plus plus. And so, as we saw last week, it's ideal
if we move that strlen outside of the condition, since n will be adding
the strlen of s each time we go through the loop, and it's
not going to be changing. So we'll set it equal to n over here. &gt;&gt; OK. So now, we're iterating over
each index in the array. And so, if we want to print each
character in that array, percent c is the flag we want to use
for characters. And now a bracket i is going to be the
string, index character i, so if the string were hello. then s 0 is going to be h, s bracket
1 will be e, and so on. &gt;&gt; So now we want to combine
these two things. We want to print each character
of each command line argument. So we're going to have
a nested for loop. And conventionally, the first counter
is i, the next is going to be j, n will be the strlen of argv i, i
is less than n, i plus plus. And now instead of printing argv i, so
argv bracket i is going to index-- that's going to be the i-th command line
argument, argv i, j is going to be the jth character of
the i-th argument. I'll get rid of this up here now
since we put it into that loop. So is equivalent to string s equals
argv i, and then s bracket j. &gt;&gt; Well, we don't need to declare
this variable s. Instead, we'll just combine these
two into what we had, argv i, j. &gt;&gt; SPEAKER 1: [INAUDIBLE]. &gt;&gt; ROB BODEN: Good call. So this is broken. If I actually ran it, we would
have realized this. So the counter I care about
in this particular for loop is j, the iterator. So you would have run into issues,
probably an infinite loop, if we hadn't fixed that. That's why we're also talking
about debugging today. &gt;&gt; OK. So let's run this program. And let's actually add a separate printf
right here that will just print another line, since this means when we
run the program, there'll be a blank line in between each character of
each command line argument. Well, we'll see what that means. Oop. Got some bug. Error implicitly declaring
library function strlen. &gt;&gt; So going back into our program, I
forgot to hash include string.h. So string.h is going to be the
header file that declares the function strlen. OK, it compiles. Now, let's run it. So just that. It's going to print out our
program name, hello world. It's going to print each thing, each
character, on its own line. OK. &gt;&gt; So let's actually take this
one step further. And instead of using string.h, let's
think about how we'd implement our own strlen function. So I'll immediately give
a function signature. So let's call in my_strlen, and it's
going to take a string as an argument, and we expect to return the
length of that string. So, where's that guy? Yes. OK. So remember from the earlier slide that
was also from last week, that an array of characters, well, a string,
so let's say this is our string s. So if s is the string, hello, then,
H-E-L-L-O, in memory, that's going to be, and then this backslash
0 character. &gt;&gt; So how do we get the length of s? Well, the trick is looking for this
backlash 0 character, this null terminator. So the algorithm Is going
to be something like few enough characters that-- let's have this hand represent some
counter, let's call this int length. So, starting from over here, we're
going to iterate over our string. &gt;&gt; So the first character, it's H,
and it's not back slash 0, so the length is 1. Iterate to the next character,
E, and it's not backslash 0. Length is 2. L, 3. L, 4. O, 5. And finally, we reach backslash
0, and so that means, well, this string is over. So let's return 5. &gt;&gt; So actually implementing that, first,
my n length equals 0, my right hand. And we're going to iterate-- &gt;&gt; SPEAKER 1: [INAUDIBLE] &gt;&gt; ROB BODEN: Oh, shoot. Good call. Boom. So n length equals 0. So now, while s length does not
equal and then, backslash 0. So remember, this backslash 0, it is an
actual character, and it indicates the end of the string. Just like, also, backslash
n is an actual character. Backslash 0 is going to indicate
the end of our string. I don't want to put that there. And while s indexed by length is not
equal to the null terminator, then we're just going to increment length. So then, at the end of our program,
length is eventually going to be 5 in this case. And we'll just return length. &gt;&gt; OK. So now down here, I don't
do my_strlen. Let's compile it to make sure
everything runs smoothly. Was I doing in 2? Or was that 1? That should do. All right. So this is argv 2. Works as anticipated, although
was that the one I did it in? Yes. OK. This version of things did not have
the printf new line after, but it doesn't make any difference. OK. So worked as expected. &gt;&gt; Now we can even combine this one step
further, where notice here, well, first, we're grabbing the strlen of argv
i, and then we're iterating over each character in that string. So instead of doing that, what if we
just combine this logic of waiting until we hit backslash 0 right
into this for loop? So iterate while argv i, j does
not equal backslash 0. So let's run it first. &gt;&gt; All right. So here, this condition is saying-- let's clear that. So now, let this be our argv. So when I just ran that program before,
argv is an array of strings. And so, if I run it with dot slash argv
2, hello world, then the argv itself is length 3, for argv
zero, hello, and world. &gt;&gt; And inside of each of these indices is,
itself an array, where this'll be dot, this will be slash, I don't know
if that was the right direction, I don't think it was. A-R-V dash, need more space. Let's cut into this array. A-R-V dash 0, and then backslash 0. And then in disarray will be hello. Let's say, H-E backslash 0. And finally, W-O backslash 0. &gt;&gt; So the algorithm that we just wrote,
the nested for loops, what they're doing is, we first have the
counter i and then j. This would be easier with code on the
screen, Let's go back to this. OK. So notice that i is the iterator that's
iterating over each command line argument. And j is the iterator iterating
over each character in that command line argument. So what this innermost printf is doing
is, we have printf argv 0 0, printf argv 0 1, printf argv 0 2, 0 3, 0 4, 0
5, 0 6, but now, argv 0 7 is going to equal backslash 0. &gt;&gt; So then we exit that for loop,
and now i iterates to 1. And now we're going to print
argv 1 0, argv 1 1-- well, now, since I cut hello short,
argv 1 2 is again going to be backslash 0. And so, increment i and continue, and
so on, until we print out all of world, and those are three command line
arguments, and we'll exit out of the outermost loop, and
finish our program. OK. &gt;&gt; So let's come back here. So you'll gain some familiarity with
command line arguments on this particular problem set. &gt;&gt; Now, debugging. So you probably have already had to do
some debugging with your previous problem set. And one very easy way of debugging,
first, let's look at a buggy program. Well, walking through this program,
we're going to ask the user for an integer, grab that integer, and then,
arbitrarily, we have a while loop that is just going to decrement
i until it's equal to 10. Let's just assume I'm entering
an integer greater than 10. So decrement i until it's equal to 10. &gt;&gt; And then we have another while loop
that, while i does not equal 0, we're going to decrement i by 3. So if you see the intent of the bug
here, it's that this'll decrement i to be 10, and then this while loop will
decrement i from 10, to 7, to 4, to 1, to negative 2, to negative 5, and so on,
to negative infinity, since i will never actually equal 0. And then at the end of this program,
we have the foo function which is going on print out that i. &gt;&gt; So this is a short and trivial program,
and the bug is obvious, especially after I just
said what the bug was. But the intent here is, well, this might
actually look like some of your solutions from greedy from the last
problem set, and maybe you do have some infinite loop in your program,
and you have no idea what's causing it. So a very useful debugging technique
is to just add printfs all over your code. &gt;&gt; So here I want a printf outside
first while loop. And here I want a printf,
and I'll just print i. I'll even do first while loop, i. Outside, second while loop. Once again, print inside
of here, the value i. And let's run this. &gt;&gt; So dot slash debug. Enter an integer. Let's do 13. And boom. We see that we are infinite looping
inside of the second while loop. So now we know what the bug is. But printf debugging is perfectly great,
but once your programs get longer and more complicated, there are
more sophisticated solutions to getting things working. So let's remove all these printfs. And let's make sure I didn't
break anything. OK. &gt;&gt; So the program we're going
to introduce is called GDB, for GNU Debugger. Well, actually, let's remove debug for
a second, and make debug again. Well, actually first, a good lesson
in command line arguments. Notice that this Clang command that is
compiling everything is being passed at the command line, these
command line arguments. So exactly how you are going to be using
command line arguments, as we did before, and as you will in PSET
2, that's how Clang is using them. &gt;&gt; So notice that this first flag, dash
ggdb3, what that's saying is, Clang, you should compile this file with the
intent that we will eventually need to debug it. So as long as you have that flag,
then we can GDB debug. And it'll open up the GNU Debugger. &gt;&gt; So there are a lot of commands
that you need to get used to. First one that you'll probably
immediately need is Run. So what is Run going to do? It's going to start our program. So run, starting program, the program
asks us for an integer, 13. And then it's infinite looping as
expected, except I removed the printfs, so we don't even see that. Exited normally. Oh. It's possible that it wrapped all the
way around, back to-- ignoring that. Assume it did not exit normally. There's a complicated answer to that. &gt;&gt; So now, that's not very useful. So just running our program inside of
this debugger doesn't help us in any way, since we could have just done
dot slash debug from outside GDB. So the one command that
you'll probably-- and I'll quit this. Control-d or quit, both work. So let's open it up again. &gt;&gt; Another command that you'll probably
immediately want to get used to is Break. So we'll break on main for now,
and then I'll explain that. Well, here we see we set a breakpoint
at this line in debug.c. So what break means is that when I
type run, the program is going to continue running until
I hit a breakpoint. So when I hit run, the program starts,
and then it breaks as soon as it enters the main function. Break main is going to be something
you pretty commonly do. &gt;&gt; And now, to introduce you
to some more commands. Notice here, that it's saying we
broke at line 11, which is printf, enter an integer. So the command Next is going to be how
we go to the next line of code. This is going to allow us to step
through our program line by line. So next. &gt;&gt; Now line 12, we're going
to get the integer. Next. And if you just hit Enter again, it'll
redo the last thing you did. So I don't need to type
next each time. So enter an integer, 13. So now, line 14, while i is greater
than 10, and I'll do next. And we see we're going to decrement i. So we're going to decrement i again. &gt;&gt; So now, another useful
command is Print. So Print is going to print out
the value of the variable. Let's bring out the value
of variable i. Let's print i. It's going to say i is 11. Now we Next again while
i is greater than 10. So i's still greater than
10, since it's 11. i minus minus. Let's print i again. As expected, it's 10. &gt;&gt; So now, next. It's going back to the condition, i is
greater than 10, but i is now 10, so it's not greater than 10, so we expect
it to fall out of the while loop. And now we're below that line of code. And another command, List, is just going
to display the previous and next couple of lines of code, in
case you lost yourself. So we just exited this while loop,
and now we have entered this while loop, line 18. So while i does not equal 0. And, next, i equals i minus 3, and we'll
notice, this'll just keep going. And we can print i. &gt;&gt; Each command sort of has shortcuts. So p is short for Print. So we can p i. Just keep holding n,
or keep doing Next. Print i again. You see now it's negative 167. So this will go on forever, but not
really forever, since you just saw, it will actually end at some point. &gt;&gt; So that is Beginning GDB. But let's do one more thing in GDB. Uh, debug. So, in this particular case, the
infinite loop happened to be inside of the main function. And for now, just accept that that I'm
going to move the infinite loop into the foo function. Just remember that, at the end of this
program, well, this was originally calling foo, which was just
going to print i. But now we're calling foo, which is
going to decrement i until it's 0, and then print that variable. OK. Save that. Make debug. And now, gdb debug. OK. &gt;&gt; So if I just Run then I'm not going to
be able to actually step through my program line-by-line. So let's break at main,
and then type run. So go through this, printf, enter
an integer, get the integer, 13. So we're going to keep decrementing
until i is greater than 10. Then we're going to fall through the
while loop, and get to the line-- let's open it up in a separate window. So we decremented until i was no longer
greater than 10, and then we called the function, foo. &gt;&gt; So what happened as soon as I hit
function foo, well, I called foo, and then I no longer had control over GDB. So as soon as I hit Next at this line,
things continued until this happened, where the program exited when-- assume it didn't exist eventually. You saw it pause for a bit though. So why did I lose control over
the program at that point? Well, when I type next, that goes to
the literal next line of code that will execute. So after line 21, the next line of code
that will execute is line 22, which is, exiting from main. So I don't want to just go
to the next line of code. I want to go into the function, foo,
and then also step through those lines of code. &gt;&gt; So for that, we have an alternative. Let's quit that again. Break main. Uh, 1, next, next, 13, next,
next, next, carefully, before we hit line foo. OK. &gt;&gt; So now, we're at line 21,
where we call foo. We don't want to type next, since that
will just call the function foo, and go to the next line of code. What we want to use is Step. So there's a difference between Step
and Next, where Step steps into the function, and Next goes
over the function. It just executes the entirety of
the function and keeps going. &gt;&gt; So Step is going to bring us
into the function, foo. And we see here, now, we're back at
this while loop that's, in theory, going to continue forever. And if you hit Step, when it isn't even
a function to call, then it's identical to Next. So it's only when you're at a line that
is calling a function that Step is going to differ from Next. So Step will bring us here. Step, step, step, step, step, step, and
we'll just infinite loop forever. &gt;&gt; So you might get used to that as your
way of identifying infinite loops, is just holding this Enter key to
see where you get stuck. There are better ways to do that, but
for now, that is perfectly sufficient. And stylistically, to conform to Style
50, I should have done this. OK. &gt;&gt; So one last command to introduce. Well, let's gdb debug in. So instead of breaking at main, if I
know the foo function is also the problem, then I could have just
said, break at foo, instead. Let's say I break at
both main and foo. So you can set as many breakpoints
as you want. When I type run, it's going
to stop at the-- ooh, let's recompile, since
I changed things. You'll see this line, Warning, source
file is more recent than executable. So that means that I just went in here
and changed these to conform to Style 50, but I did not recompile
the program. So GDB makes me aware of that. I'll quit, make debug again,
hit gdb debug. OK. &gt;&gt; So now, back to what I was doing. Break main, break foo. Now if I run the program, so it's
going to continue until hits a breakpoint. That breakpoint happens to
be the first one at main. Now, instead of doing next, next, next,
next, next, until I hit foo, I can type continue, which will continue
until you hit the next breakpoint. I have to enter the integer first. Continue will continue until I hit the
next breakpoint, which is that function of foo. &gt;&gt; So Run will run until you hit a
breakpoint, but you only type run when you're starting the program, and then,
from then on, it's continue. If I just did break main and
then ran, it'll break at main, and then continue. Since I don't have a break point at foo,
enter the integer, then now I'm not going to break at foo. It's just going to infinite
loop until that. OK. &gt;&gt; So that's Intro to GDB. You should start using it
in your problem sets. It can be very helpful
to identify bugs. If you actually just, line-by-line, go
through your code, and compare what is actually happening with what you expect
to happen, then it's pretty difficult to miss your bugs. OK. &gt;&gt; So last week David brought up this
secret-key cryptography stuff for the first time, where we don't want
passwords just be stored on our computer in some plain text file, where
someone can come over and just open it up and read them. Ideally, they would be encrypted
in some way. And in Problem Set 2, you'll be dealing
with one method of encryption, or, well, two methods, but
they aren't so great. If you do the hacker edition, you're
also going to be dealing with decrypting some things. &gt;&gt; So the issue now is, well, even if
we have the strongest encryption algorithm in the world, if you choose a
particularly poor password, then it won't help you very much, since people
will still be able to figure it out. Even if seeing the encrypted string and
it looks like a mess of garbage that means nothing to them, if they
still just need to try a few passwords to figure it out, then you
aren't very secure. So watching a video that
makes that point. &gt;&gt; [VIDEO PLAYBACK] &gt;&gt; -Helmet, you fiend. What's going on? What are you doing to my daughter? &gt;&gt; -Permit me to introduce the brilliant
young plastic surgeon, Dr. Phillip Schlotkin, the greatest nose
job man in the entire universe, and Beverly Hills. &gt;&gt; -Your Highness. &gt;&gt; -Nose job? I don't understand. She's already had a nose job. It was a sweet sixteen present. &gt;&gt; -No. It's not what you think. It's much, much worse. If you do not give me the combination to
the air shield, Dr. Schlotkin will give your daughter back her old nose. &gt;&gt; -No. Where did you get that? &gt;&gt; -All right. I'll tell. I'll tell. No, daddy. No, you mustn't. &gt;&gt; -You're right, my dear. I'll miss your new nose. But I will not tell him the combination,
no matter what. &gt;&gt; -Very well. Dr. Schlotkin, do your worst. &gt;&gt; -My pleasure. &gt;&gt; [TOOLS BEING SHARPENED] &gt;&gt; -No. Wait. Wait. I'll tell. I'll tell. &gt;&gt; -I knew it would work. All right. Give it to me. &gt;&gt; -The combination is one. &gt;&gt; -One. &gt;&gt; -One. &gt;&gt; -Two. &gt;&gt; -Two. &gt;&gt; -Two. &gt;&gt; -Three. &gt;&gt; -Three. &gt;&gt; -Three. &gt;&gt; -Four. &gt;&gt; -Four. &gt;&gt; -Four. &gt;&gt; -Five. &gt;&gt; -Five. &gt;&gt; -Five. &gt;&gt; -So the combination is one,
two, three, four, five. That's the stupidest combination
I ever heard in my life. That's the kind of thing an idiot
would have on his luggage. &gt;&gt; -Thank you, your Highness. &gt;&gt; -What did you do? &gt;&gt; -I turned off the wall. &gt;&gt; -No you didn't. You turned off the whole movie. &gt;&gt; -I must have pressed the wrong button. &gt;&gt; -Well, put it back on. Put the movie back on. &gt;&gt; -Yes, sir. Yes, sir. &gt;&gt; -Let's go, Arnold. Come, Gretchen. Of course, you know I'll still
have to bill you for this. &gt;&gt; [END VIDEO PLAYBACK] &gt;&gt; ROB BODEN: All right. So now that we're already talking about
security in some ways, nice little movie poster, so in recent
days, these issues with the NSA monitoring everything. It can be difficult to feel like you
have some sort of privacy in the online world, although I couldn't tell
you most of the details of PRISM. So moving beyond PRISM, we're not going
to be talking about that, now think about your laptop. So up here, I want to switch
to my actual account, with my little penguin. So I have a password set, and that
password is whatever I want it to be. &gt;&gt; But remember that what I'm logging
in with, so this login prompt, is some program. It's some program that was
written by some person. And so, that person, if they are
particularly malicious, they could have said, all right, so if the password
that I enter is equal to my actual password, or it's equal
to some special password-- David is awesome or something-- then let them in. So a malicious programmer could have
access to all of your Macs, or Windows, or anything. &gt;&gt; So that isn't much of a concern, since,
I mean, this is login program that's shipped with OS X, hundreds
or thousands of people have reviewed this code. And so, if, in your code somewhere, you
say if this string equals equals David is awesome, login, then someone's
going to be, like, wait. This is not right. This shouldn't be here. So that's one way we get things
to be kind of secure. &gt;&gt; But think about even programs
that you write. Let's say you wrote the login program. So this login program that you wrote,
so obviously, you are a good programmer. You're not going to put any malicious
if x equals equals David is awesome into your code. But this program, what do you
use to compile this program? Something like Clang. So what if the person who happened to
write Clang special cased in Clang something like, if I am compiling the
login program, then enter this code into the login program that says, if
x equals equals David is awesome? So not quite yet, but we have the same
issue here, where Clang, well, thousands, if not tens of thousands of
people, have looked at Clang, have looked at its lines of code and said,
all right, there's nothing bad here. Obviously, no one is doing
anything this malicious. &gt;&gt; But what is Clang itself, like,
what if I compile Clang? What if I have some compiler that
compiles Clang that inserts into Clang this special hack that says, all right,
when I compile Clang, then the executable I get should specially look
inside of the login program and insert this password, equals equals
Dave is awesome? So remember that your compiler itself
needs to be compiled at some point. So if what you choose to compile Clang
with, itself is malicious, then you could be screwed the whole
way down the line. &gt;&gt; So here, we have Ken Thompson
and Dennis Ritchie. So this is an iconic photo. Dennis Ritchie is on the right. He is a major-- pretty much wrote C. So you can
thank him for this class. Ken Thomson's on the left. The two of them basically wrote UNIX. Well, they were major contributors
in UNIX. There were some others. So Ken Thompson, at some point,
he wins the Turing Award. And the Turing award, I've always heard
it referenced this way, it's the Nobel Prize of computer science. &gt;&gt; So at the Turing Award, he has to
give his acceptance speech. And he gives this very famous speech
now, called Reflections on Trusting Trust, which we have linked
to on the course website. And in this speech, he says, all right,
so I wrote UNIX, and now all of you people are using UNIX. Now, remember today that Linux is
a direct descendant of UNIX. OS X directly uses UNIX. Windows doesn't so much, but a lot
of ideas were taken from UNIX. &gt;&gt; So he goes up to the stage and says,
all right, I wrote UNIX. And just so you guys know, I'm
able to log into every single one of your computers. Since I put one of these special if x
equals equals Ken Thomson is awesome, then I'm allowed to login. So people are like, well,
how'd you do that? We looked at the login program
and nothing's there. He's like, well, I modified the compiler
to log in the login program so that the login program now will have
that x equals equals Ken Thompson is awesome. &gt;&gt; And they say, well, that's not true. We're looking at the compiler, and the
compiler doesn't have any lines of code like that. He's like, OK, but what are you
compiling the compiler with? And they think, and he's, like, well,
I'm the one who gave you the compiler you're using to compile the compiler, so
you are compiling a compiler, that itself is malicious, and will
break the login program. So basically, at that point, there's
no way you could look at the source code of the login program
to see what is wrong. You couldn't even look in the
source code of the compiler to see what is wrong. &gt;&gt; You would need to look at the machine
code, the actual binary of the compiled compiler to see, wait, these
lines of code should not be here. But Ken Thompson took it one step
further and said, well, there are these special programs that actually
help you read the binary of programs, and so if someone used that program to
read the binary, they would see these lines of code. He modified those programs to say, all
right, if you're looking at the compiler, don't show this particular
set of binary. &gt;&gt; So then you need to take that a step
further and basically, that could have taken multiple levels of indirection,
and at some point, no one's actually going to be checking. So the moral of the story is, you're
not going to be writing Clang in this class. You're going to be using climbing
Clang a lot in this class. For all you know, Clang is a malicious
program that is sabotaging every single program you've ever compiled. And to leave you on that very ominous
note, see you on Wednesday. &gt;&gt; [APPLAUSE] &gt;&gt; SPEAKER 2: At the next CS50. &gt;&gt; SPEAKER 3: Don't you dare say that. You can do this. You've done this before, you can do this
today, you can do this tomorrow. You've been doing this for years. Just go up there and do this. You can do this. &gt;&gt; [MUSIC PLAYING]

Statistics

The following content is
provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer
high quality educational resources for free.
To make a donation or to view additional materials from
hundreds of MIT courses, visit MIT OpenCourseWare at
ocw.mit.edu. OK.
Today we have a new topic, and we are going to start to
learn about vector fields and line integrals.
Last week we had been doing double integrals.
For today we just forget all of that, but don't actually forget
it. Put it away in a corner of your
mind. It is going to come back next
week, but what we do today will include line integrals.
And these are completely different things,
so it helps, actually, if you don't think of
double integrals at all while doing line integrals.
Anyway, let's start with vector fields.
What is a vector field? Well, a vector field is
something that is of a form, while it is a vector,
but while M and N, the components,
actually depend on x and y,on the point where you are.
So, they are functions of x and y.
What that means, concretely, is that every point
in the plane you have a vector. In a corn field,
every where you have corn. In a vector field,
everywhere you have a vector. That is how it works.
A good example of a vector field, I don't know if you have
seen these maps that show the wind, but here are some cool
images done by NASA. Actually, that is a picture of
wind patterns off the coast of California with Santa Ana winds,
in case you are wondering what has been going on recently.
You have all of these vectors that show you the velocity of
the air basically at every point.
I mean, of course you don't draw it every point,
because if you drew a vector at absolutely all the points of a
plane then you would just fill up everything and you wouldn't
see anything. So, choose points and draw the
vectors at those points. Here is another cool image,
which is upside down. That is a hurricane off the
coast of Mexico with the winds spiraling around the hurricane.
Anyway, it is kind of hard to see.
You don't really see all the vectors, actually,
because the autofocus is having trouble with it.
It cannot really do it, so I guess I will go back to
the previous one. Anyway, a vector field is
something where at each point -- -- in the plane we have vector F
that depends on x and y. This occurs in real life when
you look at velocity fields in a fluid.
For example, the wind. That is what these pictures
show. At every point you have a
velocity of a fluid that is moving.
Another example is force fields. Now, force fields are not
something out of Star Wars. If you look at gravitational
attraction, you know that if you have a
mass somewhere, well, it will be attracted to
fall down because of the gravity field of the earth,
which means that at every point you have a vector that is
pointing down. And, the same thing in space,
you have the gravitational field of planets,
stars and so on. That is also an example of a
vector field because, wherever you go,
you would have that vector. And what it is depends on where
you are. The examples from the real
world are things like velocity in a fluid or force field where
you have a force that depends on the point where you are.
We are going to try to study vector fields mathematically.
We won't really care what they are most of the time,
but, as we will explore with them defined quantities and so
on, we will very often use these
motivations to justify why we would care about certain
quantities. The first thing we have to
figure out is how do we draw a vector field,
you know, how do you generate a plot like that?
Let's practice drawing a few vector fields.
Well, let's say our very first vector field will be just 2i j.
It is kind of a silly vector field because it doesn't
actually depend on x and y. That means it is the same
vector everywhere. I take a plane and take vector
. I guess it points in that
direction. It is two units to the right
and one up. And I just put that vector
everywhere. You just put it at a few points
all over the place. And when you think you have
enough so that you understand what is going on then you stop.
Here probably we don't need that many.
I mean here I think we get the picture.
Everywhere we have a vector .
Now, let's try to look at slightly more interesting
examples. Let's say I give you a vector
field x times i hat. There is no j component.
How would you draw that? Well, first of all,
we know that this guy is only in the i direction so it is
always horizontal. It doesn't have a j component.
Everywhere it would be a horizontal vector.
Now, the question is how long is it?
Well, how long it is depends on x.
For example, if x is zero then this will
actually be the zero vector. x is zero here on the y-axis.
I will take a different color. If I am on the y-axis,
I actually have the zero vector.
Now, if x becomes positive small then I will have actually
a small positive multiple of i so I will be going a little bit
to the right. And then, if I increase x,
this guy becomes larger so I get a longer vector to the
right. If x is negative then my vector
field points to the left instead.
It looks something like that. Any questions about that
picture? No.
OK. Usually, we are not going to
try to have very accurate, you know, we won't actually
take time to plot a vector field very carefully.
I mean, if we need to, computers can do it for us.
It is useful to have an idea of what a vector field does
roughly. Whether it is getting larger
and larger, in what direction it is pointing, what are the
general features? Just to do a couple of more,
actually, you will see very quickly that the examples I use
in lecture are pretty much always the same ones.
We will be playing a lot with these particular vector fields
just because they are good examples.
Let's say I give you xi yj. That one has an interesting
geometric significance. If I take a point (x,
y), there I want to take a vector x, y.
How do I do that? Well, it is the same as a
vector from the origin to this point.
I take this vector and I copy it so that it starts at one
point. It looks like that.
And the same thing at every point.
It is a vector field that is pointing radially away from the
origin, and its magnitude increases with distance from the
origin. You don't have to draw as many
as me, but the idea is this vector field everywhere points
away from the origin. And its magnitude is equal to
the distance from the origin. If these were,
for example, velocity fields,
well, you would see visually what is happening to your fluid.
Like here maybe you have a source at the origin that is
pouring fluid out and it is flowing all the way away from
that. Let's do just a last one.
Let's say I give you minus y, x. What does that look like?
That is an interesting one, actually.
Let's say that I have a point (x, y) here.
This vector here is &lt;x, y&gt;.
But the vector I want is &lt;- y, x&gt;.
What does that look like? It is perpendicular to the
position to this vector. If I rotate this vector,
let me maybe draw a picture on the side, and take vector x,
y. A vector with components
negative y and x is going to be like this.
It is the vector that I get by rotating by 90 degrees
counterclockwise. And, of course,
I do not want to put that vector at the origin.
I want to put it at the point x, y.
In fact, what I will draw is something like this.
And similarly here like that, like that, etc.
And if I am closer to the origin then it looks a bit the
same, but it is shorter. And at the origin it is zero.
And when I am further away it becomes even larger.
See, this vector field, if it was the motion of a
fluid, it would correspond to a fluid
that is just going around the origin in circles rotating at
uniform speed. This is actually the velocity
field for uniform rotation. And, if you figure out how long
it takes for a particle of fluid to go all the way around,
that would be actually 2(pi) because the length of a circle
is 2(pi) times the radius. That is actually at unit
angular velocity, one radiant per second or per
unit time. That is why this guy comes up
quite a lot in real life. And you can imagine lots of
variations on these. Of course, you can also imagine
vector fields given by much more complicated formulas,
and then you would have a hard time drawing them.
Maybe you will use a computer or maybe you will just give up
and just do whatever calculation you have to do without trying to
visualize the vector field. But if you have a nice simple
one then it is worth doing it because sometimes it will give
you insight about what you are going to compute next.
Any questions first about these pictures?
No. OK.
Oh, yes? You are asking if it should be
y, negative x. I think it would be the other
way around. See, for example,
if I am at this point then y is positive and x is zero.
If I take y, negative x, I get a positive
first component and zero for the second one.
So, y, negative x would be a rotation at unit speed in the
opposite direction. And there are a lot of tweaks
you can do to it. If you flip the sides you will
get rotation in the other direction.
Yes? How do know that it is at unit
angular velocity? Well, that is because if my
angular velocity is one then that means the actually speed is
equal to the distance from the origin.
Because the arch length on a circle of a certain radius is
equal to the radius times the angle.
If the angle varies at rate one then I travel at speed equal to
the radius. That is what I do here.
The length of this vector is equal to the distance of the
origin. I mean, it is not obvious on
the picture. But, really,
the vector that I put here is the same as this vector rotated
so it has the same length. That is why the angular
velocity is one. It doesn't really matter much
anyway. What are we going to do with
vector fields? Well, we are going to do a lot
of things but let's start somewhere.
One thing you might want to do with vector fields is I am going
to think of now the situation where we have a force.
If you have a force exerted on a particle and that particles
moves on some trajectory then probably you have seen in
physics that the work done by the force corresponds to the
force dot product with the displacement vector,
how much you have moved your particle.
And, of course, if you do just a straight line
trajectory or if the force is constant that works well.
But if you are moving on a complicated trajectory and the
force keeps changing then, actually, you want to integrate
that over time. The first thing we will do is
learn how to compute the work done by a vector field,
and mathematically that is called a line integral.
Physically, remember the work done by a force is the force
times the distance. And, more precisely,
it is actually the dot product between the force as a vector
and the displacement vector for a small motion.
Say that your point is moving from here to here,
you have the displacement delta r.
It is just the change in the position vector.
It is the vector from the old position to the new position.
And then you have your force that is being exerted.
And you do the dot product between them.
That will give you the work of a force during this motion.
And the physical significance of this, well,
the work tells you basically how much energy you have to
provide to actually perform this motion.
Just in case you haven't seen this in 8.01 yet.
I am hoping all of you have heard about work somewhere,
but in case it is completely mysterious that is the amount of
energy provided by the force. If a force goes along the
motion, it actually pushes the particle.
It provides an energy to do it to do that motion.
And, conversely, if you are trying to go against
the force then you have to provide energy to the particle
to be able to do that. In particular,
if this is the only force that is taking place then the work
would be the variation in kinetic energy of a particle
along the motion. That is a good description for
a small motion. But let's say that my particle
is not just doing that but it's doing something complicated and
my force keeps changing. Somehow maybe I have a
different force at every point. Then I want to find the total
work done along the motion. Well, what I have to do is cut
my trajectory into these little pieces.
And, for each of them, I have a vector along the
trajectory. I have a force,
I do the dot product and I sum them together.
And, of course, to get the actual answer,
I should actually cut into smaller and smaller pieces and
sum all of the small contributions to work.
So, in fact, it is going to be an integral.
Along some trajectory, let's call C the trajectory for
curve. It is some curve.
The work adds up to an integral. We write this using the
notation integral along C of F dot dr.
We have to decode this notation.
One way to decode this is to say it is a limit as we cut into
smaller and smaller pieces of the sum over each piece of a
trajectory of the force of a given point dot product with
that small vector along the trajectory.
Well, that is not how we will compute it.
To compute it, we do things differently.
How can we actually compute it? Well, what we can do is say
that actually we are cutting things into small time
intervals. The way that we split the
trajectory is we just take a picture every,
say, millisecond. Every millisecond we have a new
position. And the motion,
the amount by which you have moved during each small time
interval is basically the velocity vector times the amount
of time. In fact, let me just rewrite
this. You do the dot product between
the force and how much you have moved,
well, if I just rewrite it this way,
nothing has happened, but what this thing is,
actually, is the velocity vector dr over
dt. What I am trying to say is that
I can actually compute by integral by integrating F dot
product with dr / dt over time. Whatever the initial time to
whatever the final time is, I integrate F dot product
velocity dt. And, of course,
here this F, I mean F at the point on the
trajectory at time t. This guy depends on x and y
before it depends on t. I see a lot of confused faces,
so let's do an example. Yes?
Yes. Here I need to put a limit as
delta t to zero. I cut my trajectory into
smaller and smaller time intervals.
For each time interval, I have a small motion which is,
essentially, velocity times delta t,
and then I dot that with a force and I sum them.
Let's do an example. Let's say that we want to find
the work of this force. I guess that was the first
example we had. It is a force field that tries
to make everything rotate somehow.
Your first points along these circles.
And let's say that our trajectory, our particle is
moving along the parametric curve.
x = t, y = t^2 for t going from zero to one.
What that looks like -- Well, maybe I should draw you a
picture. Our vector field.
Our trajectory. If you try to plot this,
when you see y is actually x squared, so it a piece of
parabola that goes from the origin to (1,1).
That is what our curve looks like.
We are trying to get the work done by our force along this
trajectory. I should point out;
I mean if you are asking me how did I get this?
That is actually the wrong question.
This is all part of the data. I have a force and I have a
trajectory, and I want to find what the work done is along that
trajectory. These two guys I can choose
completely independently of each other.
The integral along C of F dot dr will be -- Well,
it is the integral from time zero to time one of F dot the
velocity vector dr over dt times dt.
That would be the integral from zero to one.
Let's try to figure it out. What is F?
F, at a point (x, y), is &lt;- y,
x&gt;. But if I take the point where I
am at time t then x is t and y is t squared.
Here I plug x equals t, y equals t squared,
and that will give me negative t squared, t.
Here I will put negative t squared, t dot product.
What is the velocity vector? Well, dx over dt is just one,
dy over dt is 2t. So, the velocity vector is 1,2t
dt. Now we have to continue the
calculation. We get integral from zero to
one of, what is this dot product?
Well, it is negative t squared plus 2t squared.
I get t squared. Well, maybe I will write it.
Negative t squared plus 2t squared dt.
That ends up being integral from zero to one of t squared
dt, which you all know how to integrate and get one-third.
That is the work done by the force along this curve.
Yes? Well, I got it by just taking
the dot product between the force and the velocity.
That is in case you are wondering, things go like this.
Any questions on how we did this calculation?
No. Yes?
Why can't you just do F dot dr? Well, soon we will be able to.
We don't know yet what dr means or how to use it as a symbol
because we haven't said yet, I mean, see,
this is a d vector r. That is kind of strange thing
to have. And certainly r is not a usual
variable. We have to be careful about
what are the rules, what does this symbol mean?
We are going to see that right now.
And then we can do it, actually, in a slightly more
efficient way. I mean r is not a scalar
quantity. R is a position vector.
You cannot integrate F with respect to r.
We don't know how to do that. OK.
Yes? The question is if I took a
different trajectory from the origin to that point (1,1),
what will happen? Well, the answer is I would get
something different. For example,
let me try to convince you of that.
For example, say I chose to instead go like
this and then around like that, first I wouldn't do any work
because here the force is perpendicular to my motion.
And then I would be going against the force all the way
around. I should get something that is
negative. Even if you don't see that,
just accept it at face value that I say now.
The value of a line integral, in general, depends on how we
got from point a to point b. That is why we have to compute
it by using the parametric equation for the curve.
It really depends on what curve you choose. Any other questions.
Yes? What happens when the force
inflects the trajectory? Well, then, actually,
you would have to solve a differential equation telling
you how a particle moves to find what the trajectory is.
That is something that would be a very useful topic.
And that is probably more like what you will do in 18.03,
or maybe you actually know how to do it in this case.
What we are trying to develop here is a method to figure out
if we know what the trajectory is what the work will be.
It doesn't tell us what the trajectory will be.
But, of course, we could also find that.
But here, see, I am not assuming,
for example, that the particle is moving
just based on that force. Maybe, actually,
I am here to hold it in my hand and force it to go where it is
going, or maybe there is some rail
that is taking it in that trajectory or whatever.
I can really do it along any trajectory.
And, if I wanted to, if I knew that was the case,
I could try to find the trajectory based on what the
force is. But that is not what we are
doing here. Let's try to make sense of what
you asked just a few minutes ago, what can we do directly
with dr? dr becomes somehow a vector.
I mean, when I replace it by dr over dt times dt,
it becomes something that is a vector with a dt next to it.
In fact -- Well, it is not really new.
Let's see. Another way to do it,
let's say that our force has components M and N.
I claim that we can write symbolically vector dr stands
for its vector whose components are dx, dy.
It is a strange kind of vector. I mean it is not a real vector,
of course, but as a notion, it is a pretty good notation
because it tells us that F of dr is M dx plus N dy.
In fact, we will very often write, instead of F dot dr line
integral along c will be line integral along c of M dx plus N
dy. And so, in this language,
of course, what we are integrating now,
rather than a vector field, becomes a differential.
But you should think of it, too, as being pretty much the
same thing. It is like when you compare the
gradient of a function and its differential,
they are different notations but have the same content.
Now, there still remains the question of how do we compute
this kind of integral? Because it is more subtle than
the notation suggests. Because M and N both depend on
x and y. And, if you just integrate it
with respect to x, you would be left with y's in
there. And you don't want to be left
with y's. You want a number at the end.
See, the catch is along the curve x and y are actually
related to each other. Whenever we write this,
we have two variables x and y, but, in fact,
along the curve C we have only one parameter.
It could be x. It could be y.
It could be time. Whatever you want.
But we have to express everything in terms of that one
parameter. And then we get a usual single
variable integral. How do we evaluate things in
this language? Well, we do it by substituting
the parameter into everything. The method to evaluate is to
express x and y in terms of a single variable.
And then substitute that variable.
Let's, for example, redo the one we had up there
just using these new notations. You will see that it is the
same calculation but with different notations.
In that example that we had, our vector field F was negative
. What we are integrating is
negative y dx plus x dy. And, see, if we have just this,
we don't know how to integrate that.
I mean, well, you could try to come up with
negative x, y or something like that.
But that actually doesn't make sense.
It doesn't work. What we will do is we will
actually have to express everything in terms of a same
variable, because it is a single integral
and we should only have on variable.
And what that variable will be, well, if we just do it the same
way that would just be t. How do we express everything in
terms of t? Well, we use the parametric
equation. We know that x is t and y is t
squared. We know what to do with these
two guys. What about dx and dy?
Well, it is easy. We just differentiate.
dx becomes dt, dy becomes 2t dt. I am just saying,
in a different language, what I said over here with dx
over dt equals one, dy over dt equals 2t.
It is the same thing but written slightly differently.
Now, I am going to do it again. I am going to switch from one
board to the next one. My integral becomes the
integral over C of negative y is minus t squared dt plus x is t
times dy is 2t dt. And now that I have only t
left, it is fine to say I have a usual single variable integral
over a variable t that goes from zero to one.
Now I can say, yes, this is the integral from
zero to one of that stuff. I can simply it a bit and it
becomes t squared dt, and I can compute it,
equals one-third. I have negative t squared and
then I have plus 2t squared, so end up with positive t
squared. It is the same as up there.
Any questions? Yes?
dy is the differential of y, y is t squared,
so I get 2t dt. I plug dt for dx,
I plug 2t dt for dy and so on. And that is the general method.
If you are given a curve then you first have to figure out how
do you express x and y in terms of the same thing?
And you get to choose, in general, what parameter we
use. You choose to parameterize your
curve in whatever way you want. The note that I want to make is
that this line integral depends on the trajectory C but not on
the parameterization. You can choose whichever
variable you want. For example,
what you could do is when you know that you have that
trajectory, you could also choose to
parameterize it as x equals, I don't know, sine theta,
y equals sine square theta, because y is x squared where
theta goes from zero to pi over two.
And then you could get dx and dy in terms of d theta.
And you would be able to do it with a lot of trig and you would
get the same answer. That would be a harder way to
get the same thing. What you should do in practice
is use the most reasonable way to parameterize your curve.
If you know that you have a piece of parabola y equals x
squared, there is no way you would put sine and sine squared.
You could set x equals, y equals t squared,
which is very reasonable. You could even take a small
shortcut and say that your variable will be just x.
That means x you just keep as it is.
And then, when you have y, you set y equals x squared,
dy equals 2x dx, and then you will have an
integral over x. That works.
So, this one is not practical. But you get to choose. Now let me tell you a bit more
about the geometry. We have said here is how we
compute it in general, and that is the general method
for computing a line integral for work.
You can always do this, try to find a parameter,
the simplest one, express everything in terms of
its variable and then you have an integral to compute.
But sometimes you can actually save a lot of work by just
thinking geometrically about what this all does.
Let me tell you about the geometric approach.
One thing I want to remind you of first is what is this vector
dr? Well, what is vector delta r?
If I take a very small piece of the trajectory then my vector
delta r will be tangent to the trajectory.
It will be going in the same direction as the unit tangent
vector t. And what is its length?
Well, its length is the arc length along the trajectory,
which we called delta s. Remember, s was the distance
along the trajectory. We can write vector dr equals
dx, dy, but that is also T times ds.
It is a vector whose direction is tangent to the curve and
whose length element is actually the arc length element.
I mean, if you don't like this notation, think about dividing
everything by dt. Then what we are saying is dr
over dt, which is the velocity vector.
Well, in coordinates, the velocity vector is dx over
dt, dy over dt. But, more geometrically,
the direction of a velocity vector is tangent to the
trajectory and its magnitude is speed ds over dt.
So, that is really the same thing.
If I say this, that means that my line
integral F to dr, well, I say I can write it as
integral of M dx plus N dy. That is what I will do if I
want to compute it by computing the integral.
But, if instead I want to think about it geometrically,
I could rewrite it as F dot T ds.
Now you can think of this, F dot T is a scalar quantity.
It is the tangent component of my force.
I take my force and project it to the tangent direction to a
trajectory and the I integrate that along the curve.
They are the same thing. And sometimes it is easier to
do it this way. Here is an example.
This is bound to be easier only when the field and the curve are
relatively simple and have a geometric relation to each
other. If I give you an evil formula
with x cubed plus y to the fifth or whatever there is very little
chance that you will be able to simplify it that way.
But let's say that my trajectory is just a circle of
radius a centered at the origin. Let's say I am doing that
counterclockwise and let's say that my vector field is xi yj.
What does that look like? Well, my trajectory is just
this circle. My vector field,
remember, xi plus yj, that is the one that is
pointing radially from the origin.
Hopefully, if you have good physics intuition here,
you will already know what the work is going to be.
It is going to be zero because the force is perpendicular to
the motion. Now we can say it directly by
saying if you have any point of a circle then the tangent vector
to the circle will be, well, it's tangent to the
circle, so that means it is
perpendicular to the radial direction,
while the force is pointing in the radial direction so you have
a right angle between them. F is perpendicular to T.
F dot T is zero. The line integral of F dot T ds
is just zero. That is much easier than
writing this is integral of x over dx plus y over dy.
What do we do? Well, we set x equals a cosine
theta, y equals a sine theta. We get a bunch of trig things.
It cancels out to zero. It is not much harder but we
saved time by not even thinking about how to parameterize
things. Let's just do a last one.
That was the first one. Let's say now that I take the
same curve C, but now my vector field is the
one that rotates negative yi plus xj.
That means along my circle the tangent vector goes like this
and my vector field is also going around.
So, in fact, at this point the vector field
will always be going in the same direction.
Now F is actually parallel to the tangent direction.
That means that the dot product of F dot T, remember,
if it is the component of F in this direction that will be the
same of the length of F. But what is the length of F on
this circle if this length is a? It is just going to be a.
That is what we said earlier about this vector field.
At every point, this dot product is a.
Now we know how to integrate that quite quickly. Because it becomes the integral
of a ds, but a is a constant so we can take it out.
And now what do we get when we integrate ds along C?
Well, we should get the total length of the curve if we sum
all the little pieces of arc length.
But we know that the length of a circle of radius a is 2pi a,
so we get 2(pi)a squared. If we were to compute that by
hand, well, what would we do? We would be computing integral
of minus y dx plus x dy. Since we are on a circle,
we will probably set x equals a times cosine theta,
y equals a times sine theta for theta between zero and 2pi.
Then we would get dx and dy out of these.
So, y is a sine theta, dx is negative a sine theta d
theta, if you differentiate a cosine, plus a cosine theta
times a cosine theta d theta. Well, you will just end up with
integral from zero to 2pi of a squared time sine squared theta
plus cosine square theta times d theta.
That becomes just one. And you get the same answer.
It took about the same amount of time because I did this one
rushing very quickly, but normally it takes about at
least twice the amount of time to do it with a calculation.
That tells you sometimes it is worth thinking geometrically.

Statistics

INTRODUCTION: The
following content is provided by MIT
OpenCourseWare under a Creative Commons license. Additional information
about our license and MIT OpenCourseWare
in general is available at ocw.mit.edu. PROFESSOR: Specific
problem, and it's a pure linear least
squares problem, but it's got two terms. So we're used to minimizing
A*u minus b square. That gives us the least
squares solution u hat to a linear system. And usually the reason we have
to go to the least square thing is that there's
no exact solution. Probably A has more
equations than unknowns. A is long and thin, and
there's no exact solution, so we look for the best
solution, and we call it u hat. OK. But there are a lot
of problems in which a second square appears. There's also a B*u equal d
hiding in the background. And so we really have like
two sets of equations. And we multiply that second
square by some factor alpha and that wise
choice of alpha is usually a big
part of the problem. And I want to speak about
some of the applications of this area. So from the point of view
of the normal equations, the system that
you actually solve, you could say no problem. If we knew how to do
this, then we certainly can do both of them together,
because instead of A transpose A showing up, we'll now have
A transpose A plus alpha B transpose A. That'll be the
positive definite coefficient matrix on the left side. And then on the
right side, instead of just the usual
A transpose b, this term is also going to give
us an alpha B transpose d. All I'm saying is we don't need
any new mathematics to reach this normal equation with the
-- sort of the two-term normal equation. And another way to think of
exactly the same thing is we're looking at the least squares
problem, where the two matrices A and B both are multiplying u. And we have two bits
of data, b and d, and all were doing
is the usual thing but with a weight in here. And the weight is the identity
matrix for the A part, and it's alpha times the
identity matrix for the B part. So this is our C right. This is our C, just
to say that, really, the notation that we created,
the formulation we have, allows us to take this step, so
C appears here and A transpose C*b, C appears over here
too, just as always. OK. But there are
important questions. And of course, always, the
first important question in applied math is what
problem are you solving? Why have we produced
this class of problems? And I have two answers. Let me just mention
first, so we are sure what the shape of these matrices is. A, as always, has more
rows than columns. Of course, u is n by 1. It's just a column vector. But A has too many
equations, too many rows, for us to get an exact
solution; B, on the other hand, has few rows. It might even only have one row. It's very common to add on
one constraint or one term in regularizing the situation. Anyway, p is relatively small. So the total matrix A, B has
m plus p rows, and the same n columns, and we're ready to go. But the two parts are
different somehow. They come for different reasons. And now, I wrote down here
two places they come from. And these are big
applications of applied math. And one of them produces
small coefficients alpha. And what's the purpose of the
B*u minus d term in that case, with just a small alpha? The problem is that
the A transpose A part is nearly
singular or is singular. So that the usual normal
equation, without the B, would be in trouble, and this
of course happens pretty often. So the idea of regularization
is get some control of the solution by putting
in another term that keeps some control over u, and
stops it from just taking off, as what happened where the
original normal equations would have a very large u hat. So we're just, like, adding
a little steady part that keeps it a bit under control. And so the A transpose A is
nearly singular in ill-posed problems, so we make them
-- it's like giving aspirin to an ill-posed problem, right? You don't fix it,
but it can operate. OK. And where do ill-posed
problems come from? And I just wanted to say that I
think the fundamental ill-posed problems in science
is: given positions -- suppose we know that
a mass, let's say, is in certain positions
at certain times -- find the velocity. So we often, in
applications have some way to know position, and
want to know velocity. And maybe you realize
that that problem is not well posed, because velocity
takes the derivative. And if you take the
derivative, that's not a good operator to invert. Taking the derivative
makes things very rough. All sorts of cases, we're
looking for the velocity, and we only have positions. One that I think
about is from GPS. So GPS uses space-based
satellites, as you all know, to give you very
accurate positions. And somehow, out
of those positions, you get pretty accurate
but not, of course, as accurate as the positions,
but you get decent velocities. And how? And there is an example where
you want to know the motion -- of course, to ask for the
acceleration would be asking for yet another derivative. You see why the derivative
is an ill-posed thing? Let me just say
ahead of time, I'm going to make today's lecture
about direction number two, not the ill-posed problems. So I'm just, like,
throwing in some comments about the ill-posed
problem, and then I'll have a weekend
to think about those, and then next week,
I'll come back to this ill posed problems. And specifically, they often
come from inverse problems, is a big source of
ill posed problems that need regularization. It's just a very large
class of equations. I mean, I was just going to say
about the derivative example. Why is that so unstable? Well, from the point
of finite differences, if we have positions, how
do you estimate velocities? You take a difference
quotient, right? You take the position at this
time, the position at a close by time, and you
divide by delta t. That's a reasonable start. But dividing by delta
t, that small number, is producing big numbers. Any errors in the position
are multiplied by that 1 over delta t and blown up. And similarly, in
frequency space, where the functions that we
think about are the functions like e to the i*k*t, the
derivatives brings down the factor k. So high oscillations,
that's the point. Oscillatory functions
can be pretty small, but their derivative
can be enormous. So it's that oscillation
which is often associated with noise in the measurements. You know, noisy
measurements are jumpy, and when we go to take their
derivative or their finite difference, we get big answers. Anyway, for me that's the
model ill-posed problem, to find velocities. And how to do it? I mean a lot of thought
has gone into that. Let me leave it there,
and come back to it. But I say all this just to
emphasize its importance. Not that we'll completely
solve it, actually, for GPS or for any
other thing, it's just all we can do is medicate. OK. Now this is the one that
we can really solve. So this is a different
application entirely. In this application, this
second term, B*u equal d, is something important,
something that we want to enforce. It's a constraint,
you could say. And one way to enforce it
which fits this pattern is to take alpha
very large, right. When we take alpha large, we're
putting a really heavy weight on that B*u minus d square,
and when we minimize, that weight will force B*u
to be pretty close to d. But of course, B*u equal
d doesn't determine u. Everybody's got
that picture clear? From B*u equal d
has many solutions. And so the real problem that
we're trying to solve is enforce B*u equal d, but
among those solutions, pick the one that
minimizes the first square, A*u minus b squared. So you see the difference? You're trying to
enforce something that the physics or the
geometry, or whatever source says has to be true. And you can do it. And you're left with
lots of options. And then the combined problem
attempts to pick the right u. OK. So that's the
application number two that I want to
speak about today. And actually, I want to
give several ways to do it. It's a very important problem. And one way will be to
actually solve B*u equal d. Find those solutions. And you may say,
well, that's what we learned in linear
algebra, that's the very foundation
of linear algebra, is there a particular
solution, right? Every solution is of this form
particular plus null space. Maybe I'll just point to
the start of that approach. So want to solve B*u equal d. And I'll come back
to this method after dealing with the
least squares approach. But here's really
the direct approach. That if I solve B*u equal
d, then there's a particular solution that solves it. And then you can always add on
the general solution which is, sorry add on the null
space solution -- the solution of B*u equals 0. And B*u equals 0 has
lots of solutions. So we would have to find them. OK. I mean that's what 18.06
would naturally do, but actually never,
I'm ashamed to say, but I didn't do it in 18.06. I never actually said how I
would scientifically compute, in a stable way, the solutions. OK. So I think that
will be important. But that's not the
only way to do it. That's called the
null space method. And sometimes it's the
right choice, sometimes not. This would be called the
heavy weight method, right. Put on a very heavy weight
and solve a standard problem. OK. So let me follow that one up. And then they'll
be a third method. And maybe there's going to be
space on the middle blackboard for it. And what would the
third method be? That will be use a
Lagrange multiplier. This thing is a constraint. I'll enforce it by a
Lagrange multiplier. OK. That's coming next. The way I'm enforcing it right
now is by a heavy weight. OK. One reason for the
popularity of this method is you don't have to
do any new thinking. You just create these
equations and solve them. Where the other methods maybe
ask us to think separately about the constraint. Here we don't have
to things separately, we just create this normal
equation, we solve it, and we get an answer u hat. Maybe I should call
it u hat alpha, because it depends
on the weight alpha, certainly, which we hope
is near the exact solution. The exact solution being the one
that exactly solves B*u equal d. Because u hat alpha will not
exactly solve B*u equal d. But we can find solutions
that do, and then among those, we can minimize
A*u minus b square. OK. So just a word about
this heavy weight method. OK. Well, first an
interesting point. A point that I think
is sort of interesting. I want to let alpha go to
infinity and see what happens, right. Everybody figures that as
alpha goes to infinity, I'm going to get
the right answer. Because as alpha
goes to infinity, it's going to more and more
enforce the constraint B*u equal d. And then, with that constraint
enforced, the other part of it will find the best
u and that's great. But let alpha go to
infinity in this equation, and what happens? So this is just like a side
comment just to say alpha, you know, taking a limit you got
to think about doing it right. Well, let's see, if I let
alpha go to infinity as it is, that'll be infinite
that'll be infinite, and I won't know
what's going on. Let me divide by alpha before
I let alpha go to infinity. So if I just divide
everything by alpha -- can I do that with
an eraser here? I'll divide by alpha. So there's a 1 over alpha here. I divide this by alpha. And this has a 1
over alpha there. And now, if I let
alpha go to infinity, I get something sensible. This goes to 0, right,
alpha going to infinity, getting bigger and bigger. This goes to 0. So what do I get in the limit? I get that this equals
this in the limit. So shall I put that up here? Well, I'll put it here, because
I don't like it, frankly. So I'll just squeeze
it in this little spot. That if I let alpha go to
into infinity, so 1 over alpha goes to 0. I get B transpose B, u hat
infinity, shall I call it? Equals B transpose d. And I guess what I
want to say is, from that I don't learn a whole lot. because B transpose B
is a singular matrix, B transpose B is a
matrix of only rank p, it's very singular, right? B had this crazy
shape, long and thin. B transpose B will
be tall, B transpose B will be a large matrix,
but its rank will only be p. It's an n by n matrix
of rank p, and it's singular and who knows
what's going on there. That little side
issue was simply to say that you
can't just let alpha go to infinity in the
central equation there, and expect to see
what's happening. OK. So somehow there's
more to it than that. So let me put alpha back where
it belongs, and think again. OK. And I guess by thinking
again, I might as well think in terms of this
way of writing it. Because I recognize this, right? This is exactly the framework
that we've developed. So this is the least
squares problem. I just want to write
down the saddle point matrix that goes with this
least squares problem. What is the saddle point matrix? Do you remember? The saddle point matrix S
is -- it has, up here -- so now I've got
an A and a B here. So it's going to be
a little bit larger. Then I have my usual zero block. And I have my usual A
transpose B transpose block. And what block goes there? That's the C inverse, right. It's our usual C inverse,
A, A transpose, 0 that we're totally
accustomed to. But now A has grown
into A, B; 0 is still 0; the transpose is
still the transpose; and up here is C inverse,
and since C was this, C inverse will be the identity,
and the identity over alpha. OK. So that's my S_alpha,
you could say. And now I'm prepared to let --
so my equation is S_alpha -- written as a block equation,
what are the pieces of it? u is the guy that I'm
looking for, the u hat alpha. And there was a Lagrange
multiplier that came in. You remember, that's how
we got to a block form from a scalar form. And I guess I usually call it
w, so I'll stay with w for here. OK. So that's what multiplies w, u. And it's what
gives -- let's see. I think it gives a B, and
it gives a d from this A*u and B*u, and I think
here if gives a 0, because we didn't have any. OK. What am I doing here? I'm just writing
the problem in a way where I can let alpha go
to 0, and see the limit. So let alpha go to infinity,
this is heavy weight part. So this will go to 0. So this approaches the
S_infinity, w_infinity, we could call it, u
hat infinity is now -- well you see what the limit
is, that's 0 in that block. This is A, this is B, this is A
transpose, this is B transpose, this is our usual zero
block, multiplying our same w_infinity,
u hat infinity, equaling our same b, d, and 0. This is the limiting equation. And it's great. This is the equation that
determines the limit as alpha goes to infinity, that
determines the best u. This is the problem that
we really want to solve. Maybe that's what I should say. Do you see the constraint
B*u equal d in here from this middle block row? That says 0, 0, B u hat is d. So we've introduced
the constraint. The first part is w_infinity
with an A*u_infinity, that's the usual error term,
the thing that we probably can't make 0. And then this is the
usual Lagrange multiplier term from there. So I've spoken pretty quickly
here, and let me just conclude. This is the limit equation,
is the correct limit equation. This is the limit
equation that we want to solve one way or another. And taking alpha large is one
way to get near the answer, but we'll look at
other ways now. So this is really the
correct equations to solve. Going the saddle point
Lagrange multiplier route. OK. So let me summarize
what I've done so far. My problem is when B*u equal
d is a constraint that I would like to satisfy, and one way
to do it is to take alpha, you know, pretty near the
largest number that the machine will hold, say 10 to the 15. Put a really heavy
weight on this. But of course, when you
let alpha be 10 to the 15, you can see that there's like
some possible problems here. When you let alpha have
an enormous weight, you're really
tilting this matrix so strongly, you know, you
couldn't let it be 10 to the 20 in single precision or you'd
wipe out A transpose A. So it's a balance here. So I guess probably a lot
of a numerical analysts would say wrong way to
do it, the right way is solve this equation, or
else do it this other way. But a lot of people with
codes say OK, you know, you're going to be
a nervous Nellie, I'm just going to use my code. And that's quite normal,
quite human response. OK. And this will
frequently succeed. OK. So that's one method to
do it, not the method that the professionals in numerical
analysis -- maybe I'm thinking, for example, the book
by Golub/van Loan, if you know that book, that
would discuss this problem. And it would actually
discuss this third method, this null space
method of solving it. OK. Maybe I'll go to that
null space method. So this was one way. Another way is
solve B*u equal d. And remember again, we
only have p equations, we have n unknowns,
so there's going to be freedom in the solution. So we have to identify
a particular solution, there's a lot of freedom in
that particular solution, and then we can add to it --
this null space is going to be n minus p dimensions, n minus p
degrees of freedom in the null space. That's the dimension of
the null space, n minus p. I'm assuming that
B has full rank p, but p is a small
number compared to n. OK. So how do you find a
particular solution? How do you find the
null space solution? As I said, that's what I
should be explaining in 18.06. And of course, we do it in
18.06, but we do it with a 3 by 3 matrix, and we practically,
you know, we do it by hand, where here we're talking about
matrices of order thousands or millions, we don't
do those by hand. And we better not do
it in an unstable way. So the question is what's
a good way to do it? And really, the heart of
modern numerical analysis is orthogonalize stuff,
get orthogonal vectors. Because if you have
orthogonal vectors, they don't get out of scale. The numbers involved
don't become unstable. And the standard
orthogonalization process is Gram-Schmidt,
that's right, those are the words we all think of. If I have a bunch of vectors,
I have to make them orthogonal, I want to make them
orthogonal, then I use -- well, Gram-Schmidt is
what we think of. But actually, MATLAB
doesn't use Gram-Schmidt, doesn't use the
usual Gram-Schmidt, as Gram and Schmidt
thought of it. MATLAB goes a different
route to the same conclusion. So let me just remind you
what Gram-Schmidt produced. And let me put in the name
of the numerical analyst long after Gram and
Schmidt, it's Householder, you know, the guy from
Tennessee with good ideas. So he had another way
to the same answer, which is this factorization. So we take our matrix,
often it's A in 18.06, and we factor it, we want to
orthogonalize its columns. So the columns of A
get orthogonalized into the columns of Q. So this
has the orthogonal columns. And then, of course,
there's some connection between the original columns
and the orthogonal columns, and that connection is
by triangular matrix R, upper triangular. I don't know if you
remember that from 18.06. What I typically do is I explain
Gram-Schmidt as they knew it, and then at the last
minute I pull Q and R out as a way to express
the result. OK. So it's the result
we want, and not the particular Gram-Schmidt
way to get there, and Householder produces
a better way to get there. OK. But the main point is that if a
matrix has independent columns, or even if it hasn't, but if
it has independent columns we know everything about it,
that we can orthogonalize those columns. In fact, we can -- here's
what I'm leading to, this B transpose I'm remembering has
this shape because B had that shape, so it's B transpose that
I'm going to do Gram-Schmidt, Householder, use -- The command
in MATLAB is [Q, R] equals, with Gram-Schmidt we could
have used the letters G and S, but since we don't use
their actual method anymore, we could use the letter HH
for Householder or something. But it's qr of, in this case,
B transpose is what we want. OK. So what that very frequently
used command in MATLAB produces is Q and R. And it
produces a square matrix Q, where these columns, the
columns of the first part, Q_1 transpose,
are orthogonalized versions of these columns. And the R just tells us the
connection between them. Then it also produces, and
this is handy as you'll see, the algorithm also produces
n minus p more columns, that are orthogonal to these guys. So it produces a complete
orthonormal basis, a complete set of n
columns altogether. Q_1 transpose has the
column that really are associated with these problems. And these are going to be
associated with the null space. So out of this, I see
that actually B transpose is Q_1 transpose
R. So you can say this is the reduced
factorization with only p columns, and this
is the full picture with the other n minus p
columns that are orthogonal. And the reason that's
handy is they tell us about the null space. So now I want to identify out
of this a particular solution and the general
null space solution. OK. So what are those? So particular solution is
going to use this part. So, let's see, I want
a particular solution. So B, transposing that
is R transpose Q_1. OK. So now I'm prepared to solve
-- step one is the particular solution. I want to get be
B*u_particular equal d. OK. But now I have B
is nicely factored. So this is R transpose
Q_1 u particular equal d. So now comes the computation
the code has to do. It has to invert that to
get Q_1 times u particular equals R inverse transpose d. So it had to solve
a triangular system, but of course, a triangular
system is quick to solve. That's the good part here. And then this final step
to get u particular, I have to put the inverse
of that guy over there, but because this has
orthogonal column, that's just Q_1 transpose. So there we go. That's the inverse
of R. So that's what I should've done
in 18.06 and never did, and you get to see it. What's a convenient
particular solution? Everybody knows, we
got a whole collection of particular solutions. We want to choose one
that's nice and stable. And the reason it's
stable is that it works with orthogonal
columns, orthonormal even, and triangular matrix for
which linear systems are highly active. OK. So that's the
particular solution. Now what's the null
space solution? What are the general
solutions to null space part? What are the solution to those? Well, I can just go
down the same steps. This is R transpose Q_1
u null space equals 0. I multiply both sides --
this is a nice square, invertible matrix -- I multiply
by its inverse, kills that. So now I have Q_1 times the --
Q_1 is really the heart of B. So what vectors are
perpendicular to Q_1? I hope I've got this right. It's easy to mix up a
transpose in the process. So let me just pause to be
sure I'm doing it correctly. OK. I hope. Did I check that I get it right? Yes. OK. I could have written
what B is here, since I have B transpose as a product. B is R_0, Q_1, Q_2. OK. And I want to multiply
by u_null and get 0. OK. So what should u_null be? u_null should be -- this
part is giving us a 0, so this is like gone. So you see the two are the same. So what vectors are
perpendicular to those? The answer is the u_null is
a combination of the columns of Q_2 transpose. It's the Q_2 part that's
telling us about the null space. It was the Q_1 part that
gave the particular solution, it's the Q_2 part that
gives the general solution. In other words, u_null
is Q_2 transpose times any vector, let me call
it z, this is any z. OK. Now this has my n minus
p degrees of freedom. Sorry, I'm trying to
do quite a bit here. I'm trying to say
how you actually solve rectangular systems
when they're not determinate. There are many solutions. This is a good particular
solution to find, and this is a good way to
find the general solution, the null space solution. This is a combination
of the other columns. OK. All right, now
we're done really, because I now know
what u looks like; u looks like this part,
which I've computed, and this part, which
has the freedom. Let me put those
two parts together. So now I want to minimize --
so I'm near the end here -- A*u minus B, but
A*u is u_particular, and I have u_particular here. Q_1 transpose R
minus transpose d, that's u_particular, plus
u_null, and that's this. This u_null was also here;
u_null was any Q_2 transpose z, right. All that is u, A*u minus b. OK. Up to possibly screwing
up on some transposes, this is the right method. So this is a fixed solution. I just want to write
that as a different way. Minimize A Q transpose z. Now, we're minimizing
over the z's. So u had n components, but
somehow p degrees of freedom were used up by the
constraint B*u equal d. And we have the n minus
p true degrees of freedom are in the z. So there's this minus the b. This is all known stuff. A Q_1 transpose R minus
transpose d, square. OK. I'm there. So this is a standard
minimization problem. Minimize, shall I
call this A tilde z? And I'll call all
this stuff b tilde. And the solution is found
from the normal equations A tilde transpose, A
tilde times the best z, I'll put a hat
on it to emphasize that it's the great one, is
A tilde transpose b tilde. OK. Finished that process
without leaving myself a lot of time for
the other method. Conclusion here, that after
you've done the QR step, the qr command, and then after
you solved a linear system with the R transpose, and you've
multiplied by Q's, and you've ended up with this problem
with a new matrix A tilde and B tilde, then you just
do the normal equation. The web will have the code
that takes those steps, reaches this conclusion,
and solves it. OK. So that's the null space method. And it would be our
method of choice when -- so z has n minus p components. If p is near n, then they're
not many z's and this is highly efficient. OK. So the null space
method is one way to go. Can I just in the
remaining minutes go back to the Lagrange
multiplier idea? So what's the Lagrange
multiplier idea? So let me write
the problem again as Lagrange would like it. Minimize A*u minus b squared
subject to B*u equal d. That's the problem
we're solving. I should have
written it earlier. Let me put a star here,
because this is our problem. OK. So one way to tackle it
was take that constraint, give it a heavy weight. That was method one. Method two was solve this
constraint in full detail, get the z's that remain
as degrees of freedom, plug in u_particular plus
u null space into here, and then you have
a problem in the z. That's method two. Now, so method
three is Lagrange. So method three would say
OK, what does Lagrange do? L, we call it the Lagrangian, he
takes this A*u minus b square, and adds to it some
Lagrange multiplier, and I'll use maybe the standard
lambda, times B*u minus d, right? That's Lagrange's idea. You recognize Lagrange's idea. Takes the constraint,
multiply it by a multiplier. In fact this is p
constraints, so p lambdas. Lambda's a vector
of p multipliers. Not just a single one, because
we've got the p constraints. And now what does Lagrange do? He sets the derivative
dL / d lambda -- well, so let me do the dL/du first. He sets dL/du to 0,
and dL / d lambda to 0. I could've started
out with this method, because it's going to lead
us to the equations faster. What equations do we
get from dL/du equals 0? What's the gradient
with respect to u? That gives us A transpose A*u. Oh, probably we want a 1/2
here, so that the numbers come out right. We get A transpose A*u, and
another u part will be the B lambda. Taking the derivative of u will
produce a B transpose lambda out of that. Yeah, a B transpose
lambda out of that. And then, in here will
be a linear term in u that we might as well put
on the right-hand side as A transpose b. Familiar. OK. And what about dL / d lambda? Well that's just our
constraint, B*u equals d right. Having built in the
constraint, when I take the derivative
with respect to lambda, the constraint just
comes back again. So this is now method three. Solve that system. And I guess what I want say
in the remaining 30 seconds is that solving this system
is the same as this one. Those two are exactly the same. So that's a system with three
parts, but I can, as always -- maybe I can even get there. Can you see that
if I take this part and I subtract A transpose
times the top row from the bottom row,
what will that give me? Let me just hope that it
works, well I won't actually. Time is up, it's asking
too much to do even this one piece of linear algebra
that can be in the notes. So this system that we got
as the correct limit equation is exactly the same
one that Lagrange gets. So that's one way. This is a system with
n plus p unknowns. That's the price you pay
for going Lagrange's route. You add p unknowns. This was a system with
n minus p unknowns. That's because you're
using the constraints to reduce the problem. And the original method one
was a method with n unknowns, the unknowns in u. So you have the choice n, n
plus p that Lagrange would like, and n minus p that
Golub/van Loan would prefer. And usually it's method two or
method three is recommended, but method one often used. OK. So that's the lecture
on this point. That's today. And then next week comes
the whole class of problems like finding velocities
from displacements, where alpha is a
small parameter. And then after that come
discussions of the completed project ones, and the upcoming
extensions into project two. OK. See you next week, thanks. Good.

Probability

  Hi. Today, we're going to do a
really fun problem called geniuses and chocolates. And what this problem is
exercising is your knowledge of properties of probability
laws. So let me just clarify
what I mean by that. Hopefully, by this point, you
have already learned what the axioms of probability are. And properties of probability
laws are essentially any rules that you can derive
from those axioms. So take for example the fact
that the probability of A union B is equal to the
probability of A plus the probability of B minus the
probability of the intersection. That's an example of a property
of a probability law. So enough with the preamble. Let's see what the problem
is asking us. In this problem, we have
a class of students. And we're told that 60% of the
students are geniuses. 70% of the students
love chocolate. So I would be in
that category. And 40% fall into
both categories. And our job is to determine
the probability that a randomly selected student is
neither a genius nor a chocolate lover. So first I just want to write
down the information that we're given in the problem
statement. So if you let G denote the event
that a randomly selected student is a genius then the
problem statement tells us that the probability of
G is equal to 0.6. Similarly, if we let C denote
the event that a randomly selected student is a chocolate
lover, then we have that the probability of
C is equal to 0.7. Lastly, we are told that the
probability a randomly selected student falls into
both categories is 0.4. And the way we can express
that using the notation already on the board is
probability of G intersect C is equal to 0.4. OK, now one way of approaching
this problem is to essentially use this information and sort of
massage it using properties of probability laws to
get to our answer. Instead, I'm going to take a
different approach, which I think will be helpful. So namely, we're going
to use something called a Venn diagram. Now a Venn diagram is just a
tool that's really useful for telling you how different sets
relate to each other and how their corresponding
probabilities relate to each other. So the way you usually draw this
is you draw a rectangle, which denotes your sample space,
which of course, we call omega. And then you draw two
intersecting circles. So one to represent our geniuses
and one to represent our chocolate lovers. And the reason why I drew them
intersecting is because we know that there are 40% of the
students in our class are both geniuses and chocolate lovers. OK, and the way you sort of
interpret this diagram is the space outside these two circles
correspond to students who are neither geniuses
nor chocolate lovers. And so just keep in mind that
the probability corresponding to these students on the
outside, that's actually what we're looking for. Similarly, students in this
little shape, this tear drop in the middle, those would
correspond to geniuses and chocolate lovers. You probably get the idea. So this is our Venn diagram. Now I'm going to give you guys
a second trick if you will. And that is to work
with partitions. So I believe you've seen
partitions in lecture by now. And a partition is essentially a
way of cutting up the sample space into pieces. But you need two properties
to be true. So the pieces that you cut up
your sample space into, they need to be disjoint, so
they can't overlap. So for instance, G and C are
not disjoint because they overlap in this tear
drop region. Now the second thing that a
partition has to satisfy is that if you put all the pieces
together, they have to comprise the entire
sample space. So I'm just going to put these
labels down on my graph. X, Y, Z, and W. So X is
everything outside the two circles but inside
the rectangle. And just note, again, that what
we're actually trying to solve in this problem is the
probability of X, the probability that you're neither
genius, because you're not in this circle, and you're
not a chocolate lover, because you're not in this circle. So Y I'm using to refer
to this sort of crescent moon shape. Z, I'm using to refer
to this tear drop. And W, I'm using to refer
to this shape. So, hopefully, you agree that
X, Y, Z, and W form a partition because they
don't overlap. So they are disjoint. And together they form omega. So now we're ready to
do some computation. The first step is to sort of
get the information we have written down here in terms
of these new labels. So hopefully, you guys buy that
G is just the union of Y and Z. And because Y and Z are
disjoint, we get that the probability of the union is the
sum of the probabilities. And, of course, we have from
before that this is 0.6. Similarly, we have that the
probability of C is equal to the probability of Z union W.
And, again, using the fact that these two guys are
disjoint, you get this expression. And that is equal to 0.7. OK, and the last piece of
information, G intersects C corresponds to Z, or our tear
drop, and so we have that the probability of Z is
equal to 0.4. And now, if you notice,
probability of Z shows up in these two equations. So we can just plug it in. So plug in 0.4 into
this equation. We get P of Y plus 0.4 is 0.6. So that implies that
P of Y is 0.2. That's just algebra. And similarly we have point. 0.4 plus P of W is
equal to 0.7. So that implies that
P of W is 0.3. Again, that's just algebra. So now we're doing really well
because we have a lot of information. We know the probability of Y,
the probability of Z, the probability of W. But remember
we're going for, we're trying to find the probability of X. So
the way we finally put all this information together to
solve for X is we use the axiom that tells us that 1 is
equal to the probability of the sample space. And then, again, we're going
to use sort of this really helpful fact that X, Y, Z, and W
form a partition of omega to go ahead and write this as
probability of X plus probability of Y plus
probability, oops, I made a mistake. Hopefully, you guys
caught that. It's really, oh, no. I'm right. Never mind. Probability of X plus
probability of Y plus probability of Z plus
probability of W. And now we can go ahead and plug-in the
values that we solved for previously. So we get probability of X plus
0.2 plus 0.4 plus 0.3. These guys sum to 0.9. So, again, just simple
arithmetic, we get that the probability of X is
equal to 0.1. So we're done because we've
successfully found that the probability that a randomly
selected student is neither a genius nor a chocolate
lover is 0.1. So this was a fairly
straightforward problem. But there are some important
takeaways. The first one is that
Venn diagrams are a really nice tool. Whenever the problem is asking
you how different sets relate to each other or how different
probabilities relate to each other, you should probably draw
Venn diagram because it will help you. And the second takeaway is that
it's frequently useful to divide your sample space into
a partition mainly because sort of the pieces
that compose a partition are disjoint. So we will be back soon to
solve more problems.  

Statistics

OPERATOR: The following content
is provided under a Creative Commons license. Your support will help MIT
OpenCourseWare continue to offer high quality educational
resources for free, To make a donation or view additional
materials from hundreds of MIT courses, visit MIT
OpenCourseWare at ocw.mit.edu. PROFESSOR: Last time, Professor
Guttag introduced the idea of objects and classes
and this wonderful phrase called object-oriented
programming. And it's a topic I want to pick
up on today, we're going to do for the next few lectures,
and it's a topic I want to spend some time on
because this idea of capturing data and methods, the term we're
going to use for it, but data and functions that belong
to that data, things that can be used to manipulate them,
is a really powerful one. What we're really getting at is
the idea of saying I want to have a way of grouping
together information into units that make sense. So I can go back to one of those
topics we had at the beginning, which is the idea
of abstraction, that I can create one of those units as a
simple entity, bury away the details and write really
modular code. And so we're going to
talk about that a lot as we go along. What we're really doing, or
I shouldn't say what we're really doing, a basic piece of
what we're doing, when we talk about classes or objects, is
we're doing something that Professor Guttag mentioned,
we're defining an abstract data type. Now what in the world
does that mean? Well basically what we're
doing is we're giving ourselves the ability to create
data types the same way that we have some built-ins,
so we have things like int, float, string, these are
built-in data types. And if you think about it,
associated with each one of those data types is a set
of functions it's intended to apply to. Sometimes the functions --
sometimes a function can be used on multiple data types,
plus, for example, we saw could add strings, or could
add ints, but each one of those data types has associated
with it a set of functions that are geared
to handling them. We want to do the same thing,
but with our data types. We want to create data types and
functions, or we're going to call them methods, that
are specifically aimed at manipulating those
kinds of objects. And our goal is then to
basically see how we can build systems that take advantage
of that modularity. Right, so the fundamental idea
then, is, I want to glue together information, I want
to take pieces of data that naturally belong together, glue
them together, and attach some methods to it. And that's, you know, saying
a lot of words, let's do an example because it's probably
easiest to see this by looking at a specific example. So here's the example I'm
going to start with. Suppose I want to do little
piece of code that's going to do planar geometry, points
in the plane. All right, so I want to have
some way of gluing those things together. Well you know what a point is,
it's got an x- and a y- coordinate, it's natural to
think about those two things as belonging as a
single entity. So an easy way to do this would
be to say, let's just represent them as a list.
Just as a 2-list, or a list of 2 elements. It's easy to think of a point as
just a list of an x- and a y- coordinate. OK, for example, I might say
point p1 is that list, x is 1, y is 2. in fact, if I draw
a little simple -- it's basically pointing to that point
in the plane, right, x is 1, y is 2. OK, fine, there's another way
to represent points on the plane now, and that's in
polar form, so this, if you like, is Cartesian. Another way to represent a point
in a plane is I've got a radius and I've got an angle
from the x-axis, right, and that's a standard thing
you might do. So I might define, for example,
in polar form p 2, and let me see, which example
did I do here, we'll make this the point of radius 2 and at
angle pi by 2, I'm going to make it easy because pi by 2
is up along this axis, and that's basically that point. Ok, just fine, it's
no big deal. But here now becomes
the problem. I've glued things together but
just using a list. Suppose I hand you one of these lists. How do you know which
kind it is? How do you know whether
it's in Cartesian form or in polar form? You have nothing that identifies
that there, you have no way of saying what this
grouping actually means. Right, and just to get a sense
of this, let's look at a simple little example, so on
your hand-out, you'll see I've got a little piece of code that
says assuming I've got one of these points, I want to
do things with it, for example I might want to add
them together. So this first little piece of
code right here says, ok you give me 2 points, I'll create
another 1 of these lists and I'll simply take the x, sorry
I shouldn't say x, I'm going to assume it's the x, the
x-values are the two points, add them together, just right
there, the y-values, add them together and return that list.
And if I actually run this, which I'm going to do -- excuse
me, do it again -- OK, you can see that I've
added together and I've printed out the value of r, and
I'll just show you that in fact that's what I've got. This looks fine, right, I'm
doing the right thing. Another way of saying it is,
I've actually said, what did I use there, (1,2) and (3,1), It's
basically saying there is the first point, there's the
second point, add them together and I get that point. OK, that sounds fine. Now, suppose in fact these
weren't x and y glued together, these were radius
and angle glued together. In that case point p 1 doesn't
correspond to this point, it actually corresponds to the
point of radius 2 and angle 1, which is about here. I think I wrote this down
carefully so I would make sure I did it right. Sorry, said that wrong, radius
1 and angle 2, 2 radians is a little bit more than pi half. And the second point is of
radius 3 and angle 1, which is up about there. So what point, sorry, bad
pun, what point am I trying to make here? Different understandings of what
that piece means gives you different values, and that's
a bit of a problem. The second problem is, suppose
actually I had p 1 and p 2 were in polar form, and I
ran add points on them. This little piece of code
here that I did. Does that even make any sense? Course not, right? You know when you add 2 polar
forms, you add the radii together, you don't add the
angles together, you need to do it in Cartesian form. So what I'm leading up to here
is that we've got a problem. And the problem is, that we want
to build this abstract data type, but we'd like to
basically know what kind of object is it, and what functions
actually belong to it, how do we use them? And so I'm going to go back to
this idea of a class, and let's build the first of these,
and that is shown right here on this piece
of your handout. I'm going to define a class,
and in particular, what I'm going to do, is walk through
what that says. So I'm going to now build
an object, it's going to represent a point. So what does that thing
say up there? It's got this funky looking
form, right, it says, I've got something that I'm going to
call a class, got that key word class right here. And I'm going to give it a name,
and right now I'm just building a simple piece of it --
but first of all, what does a class do? Think of this as, this is
a template for creating instances of an object. At the moment, it's a really
dumb template. I'm going to add to it in
a second, but I want to build up to this. Right now it's got that second
key word there called pass, which is just Python's way
of saying there's an empty body in here. Right,, we're going to add to
it in a second, but the idea is class is going
to be a template for creating instances. How do I use it? Well, I call class just like
a function, and you can see that below. Having created this thing called
Cartesian point, I'm going to create two instances
of it. c p 1 and c p 2. Notice the form of it, it's
just the name of the class followed by open paren,
close paren, treating it like a function. What that does, is that it
creates, c p 1 and c p 2 are both instances of this
type, specific versions of this type. For now the way to think about
this is, when I call that class definition, it goes off
and allocates a specific spot in memory that corresponds
to that instance. Right now it's empty, actually
it's not quite empty, it has a pointer back to the class. And I can give a name to that,
so c p 1 and c p 2 are both going to point to that. Once I've got that, I can now
start giving some variable names, sorry not, rephrase
that, I can give some attributes, I can give some characteristics to these classes. So each instance has some
internal, or will have some internal attributes. Notice how I did
that up there. Having created c p 1 and
c p 2, I had this weird looking form here. Not so weird, you've actually
seen it before. In which I said c p 1
dot x equals 1.0. What's this doing? c p 1 points
to an instance, it points to a particular version
of this class. And I have now given an internal
variable name x and a value associated with that. So I've just given
it an x variable. All right, c p 1 dot y, I've
said assign that to the value 2, 2,0. So now c p 1 has inside of
it an x and y value. Did the same thing with
c p 2, give it a different x and y value. Again, remind you, c p 2
is a different instance of this data type. All right, when I call the class
definition it goes off and finds another spot in
memory, says that the spot I'm going to give you a pointer back
to that, give it the name c p 2, and then by running these
2 little assignments statements here, I've given it
an x and a y value for c p 2. So you see why I say it's
a template, right? Right now it's a simple
template, but it's a template for creating what a class looks
like, and I now have an x- and y- value associated with
each instance of this. OK, and if I wanted to look at
it, we can come back over here, and we can see what does
c p 1 look like, interesting. It says some funky stuff,
and says it's a kind of Cartesian point. And that's going to be valuable
to me when I want to get back to using these
things, right? You see that little thing says
dot Cartesian point in there. If I want to get out right now
the versions of these things, I can ask what's the value
of c p 1 x, and it returns it back out. I could say c p 2 dot x, that
was a bad one to use because they use the same valuable
in both places, didn't I? So let's do c p 1 dot
y, c p 2 dot y. OK, so I've just created local
versions of variables with each one of these objects. I can get at them just like I
would before, I can assign them in as I might
have done before. OK, now that I've got that, we
could think about what would I want to do with these points? Well one thing I might want to
do is say, is this the same point or not? So the next little piece of code
I've written here, just move down to it slightly. I've got a little piece of
code called same point. And you can look at it. What does it say to do? It says, if you give me two of
these data objects, I'm going to call them p 1 and p 2. I'm going to say, gee, is the
x value the same in both of them, and if it is, and the y
value's the same, then this is the same point, I'm going
to return true. Notice the form. This is saying, that's a class,
or sorry, an instance of a class, and I'm going
to get the x value associated with it. I going to come back in a second
to how it actually does that, but it basically says, get
me x value for p 1, get me the x value for p 2,
compare them, just as you would normally. I've got another little thing
here that I'm going to use a little later on that just prints
out values of things. OK, let's see what happens
if I do this. Let me show you simple
little example. I'm going to go over here, and
let me define a couple of these things. I'm going to say p 1, try it
again, p 1 is a Cartesian, it would help if I could type,
Cartesian point, and I'm going to say p 1 of x is 3, p 1 of y
is 4, and I'm going to make p 2 another Cartesian point. And I'll give it an x value
of 3 and a y value of 4. OK, now I want to say,
are these the same? Thing I've got a little
procedure that could do that, but you know the simplest thing
I could do is to say well, gee, wait a minute, why
don't I just check to see if these are the same thing? So I can say is p 1 the same
as p 2, using Scheme's built-in is comparator. Say -- sorry? PROFESSOR 2: Part of Python? PROFESSOR: Part of Scheme, whoa,
there's a Freudian slip, thank you, John. I'm showing my age, and my
history here, is p 1 and p 2 the same thing? Hey, there's a bad English
sentence even worse, now I'm really thrown off. I'm using Python's is comparator
to say is it the same thing? It says no. But if I say, are p 1 and p 2
the same point, it says yes. And this is a point I
want to stress here. So what's going on in this case
is, I want to distinguish between shallow equality
and deep equality. The first thing is testing
shallow equality. What it is doing, that's another
bad English sentence, but what it is doing? Is is essentially saying, given
2 things, do they point to exactly the same referent? Or another way of thinking about
it, is remember I said when I call that class
definition it creates an instance, that's a pointer to
some spot in memory that's got some local information
around it. Is is saying, do these things
point to exactly the same spot in memory, the same instance. Deep equality, we get to define,
that's what I did by writing same point. OK, as I said, I want equality
in the case of points to be, are the x- and y- coordinates
the same? And I'm actually going to change
it. just to show you this point. If I do the following, and
I say, I'm going to assign p 1 to be p 2. What's that doing? It's taking the name p 1 and
it's changing its value to point to exactly what
p 2 points to. And then I say, are they
the same thing? Answer's yes, because now they
are pointing to exactly the same spot in memory. The same instance. OK, the reason I'm saying this
is, we have one class definition, is a cookie cutter,
it's a template that's going to let us build versions
of these things. Every time I use it, I'm
creating a new instance, that's a different thing
inside of memory. And I want to have that because
I want to have lots of versions of points. OK, now, let's go back
to where I was. I said one of the things I want
to do is, I want to have different versions of points. So I've got now things that
are Cartesian points. I could do the same thing, I
could build polar point. I wanted to show
it to you here. I've got a class called polar
point, which is right there, and same kind of thing, I can
create instances of it, and then assign to them things like
a radius and an angle, make instances of those. OK, John? PROFESSOR 2: I just want to
maybe mention that in some of the reading, you'll see terms
like object equality and value equality, instead of shallow
equality and deep equality. PROFESSOR: Right, so, this
object, this is, right, value quality. Right. And you will see both
terms used. Some people like to use shallow
and deep, object and value, but they're talking about
the same thing, which is it the same object or is it the
same, in this case, set of values, depending on what you
want to define as you use it. OK, so as I said, now I can go
off and I could create a different class. I've got Cartesian points, I
could create a polar points. And I'm going to run it in a
sec, but you can see, the same kind of idea. I define a class call polar
point, I create a couple of them, and I give them a
radius and an angle. And then I could do things like
again, say, okay having done, that let me just run it
here, run that, so I've now got polar point 1, and
polar point 2. I can say is polar point 1 the
same as polar point 2, and the answer should be no. And then I could say well, gee,
are they the same point? Oops. What happened? Well it bombed out. Because, what was I
expecting to do? I was expecting to compare
x- and y- values, not radius and angle. And so this doesn't know how
to do it, it doesn't have a method to deal with it,
so it complains. So what's my problem here, and
this is what I want to now lead up to. I could imagine writing another
function for same point, and I have to give it a
name like same point polar, and same point Cartesian. A different function to compare
polar versions of these points. But that's starting to
get to be a nuisance. What I'd really like to do is to
have 1 representation for a point that supports different
ways of getting information out, but has gathered within it,
a method or a function for dealing with things like how
do I know if it's the same point or not. So I want to take this idea
classes now, and I want to generalize it. Right, and that is going to
lead us then to this funky looking thing. Right there, and I'd
like you to look at that in your handout. OK, I'm going to go back
and rebuild the class. Ok, and again, I'm going
to remind you, the class is this template. But now I'm going to change
it, so what is that new version of class say. I'm going to call it
c point just to make it a little shorter. You can see inside of it, it's
got a set of definitions for things like functions. And that first one is this kind
of interesting thing, it's two underbars, init,
and two underbars. Underscores, I guess
is the right way to say it, not underbars. Right that's a specific name,
and what it basically says is, when I call the class
instance. That's a bad mistake. When I call the class
definition, that is I call c point, I'm going to
call it with a specific set of arguments. And what is it going to
happen is that init is going to then apply. It's going to apply to
those arguments. So let me in fact show
you an example. I've got a definition of
Cartesian point, I've got a definition of polar point. Let me just run these to
get them in there. Now let's do the following. Let's let p be Cartesian point,
and we'll give it a couple of values. OK? So what happened? Notice in the class definition
here, is there, this is the first thing that's got called,
and I just called with the value for x and the value for
y, and it went off and did something for me. Does that look right? This is where you all hate it, I
get no eye contact anywhere. Anything look odd about that? I said. When I call this class
definition, it calls init, and I give it an x and a y value. How many arguments
does init take? Three. How many arguments
did I give it? Two. What in the world's going on? Well, this is a piece of
object-oriented coding that we get to talk about
a little bit. There's this weird extra
variable in there called self. So what is self? And I have to admit, I did the
standard thing you do every time you run across something
you don't know about, you go to Wikipedia. So I went and looked up self
in Wikipedia, and I have to read it out. Wikipedia informs us that the
self is the idea of a unified being, which is the source
of an idiosyncratic consciousness. Moreover, this self is the
agent responsible for the thoughts and actions
of an individual to which they are ascribed. It is a substance which
therefore endures through time, thus thoughts and actions
at different moments of time may pertain
to the same self. OK, how do we code that up? Sounds like an AI problem,
I guess right? But there's actually hidden in
there an important element, and that is, when I create an
instance, I have to be able to get access to the things that
characterize that instance. I won't say that they're
thoughts and emotions or things, but what characterizes
an instance here, it's the internal parameters that specify
what is going on. So in fact what happens inside
of an object-oriented system, and particularly in Python's
object-oriented system, is the following. When we call init, it's going
to create the instance, all right, just as we said before. But in particular, it's
going to use self to refer to that instance. Right, so let me say this
a little differently. I have a class definition. It's actually an object
somewhere. It has inside of it all those
internal definitions. When I call that class
definition, it calls init. Init creates a pointer
to the instance. And then it needs to have access
to that, so it calls it, passing in self as the
pointer to the instance. That is, it says it has access
to that piece in memory, and now inside of that piece of
memory, I can do things like, as you see here, define self
dot x to be the value passed in for x. What's that doing? It's saying where's
self pointing to? Inside of that structure, create
a variable name x, and a value associated with it. Notice what I also do here, I
create self dot y, give it a value, and then, oh cool, I
can also set up what's the radius and angle for this point,
by just doing a little bit of work. OK, in fact if you look at what
it does there, just put the pointer over here, it says,
get the value of x that I just stored away, square it,
add it to the value of y squared that I just stored away,
and then take square root, pass it back out. So I just computed the radius
of that particular thing. Right? Compute the angle the same
way, just using the appropriate things. So the idea is that self
will always point to the particular instance. Now you might say, why? Why do it this way? Well, basically because it was
a design choice when the creators of Python decided to
create the language, they basically said, we're always
going to have an explicit pointer to the instance. Some other object-oriented
programming languages do not provide that pointer. This is kind of nice in my view,
I don't know if John, you'd agree, but this
is explicit. It actually lets you see how to
get access to that pointer so you know what you're
referring to. But it's simply design choice. So another way saying it again
is, when I call the class definition, by default I'm going
to look to see is there an init method there, and if
there is, I'm going to use it. First argument by convention is
always self, because it has to point to the instance, and
then I pass, in this case, another couple of
arguments in. OK, now, if I actually do this,
and I'm going to show you the example, I just, what
did I type over there, I got p was a c point. If I want to get values back
out, I could in fact simply send to that instance a message,
in this case I could say p dot x. In fact let's do it. If I do that over here -- aha --
it gets me back the value. Now let me spend just a second
to say, what was this actually doing? p is an instance. It knows, or has stored away,
and in fact let's look at it, if we look at what p does,
p says -- it says reading through a little bit of this
stuff here, it says -- it's a kind of Cartesian point, it's an
instance, there's actually the memory location that it's
at, that's why I say this idea of it's an instant at
a specific spot. It knows that it came from
this class, c point. So when I type, I'm sorry, I
shouldn't say type, when I write, although I would have
typed it, p dot x, here's what basically happens. p is an
instance, it's being sent a message, in this case the
message x, it says I want the x-value back out. p knows that
it is a kind of Cartesian point, it actually goes and
gets, if you like, the class definition up here. And is able to then say,
inside of that class definition, find
the value of x. All right, now, that's one of
the ways we could get things out, but in fact it's really
not a good way. A better way to do this would
be the following. If I could type. What did I just do there? One of the things that I
defined inside my class definition here was an
internal method. That method has a name,
obviously, and what does it do? It's going to go off and get the
values of x and y attached to this thing and return
them to me. And that's one of the
things I want. I would like my classes
to have methods. So you can access the values
of the specific instance. Now, this is still a nuance, why
would I like to do this? Well this is leading up to why
I want to gather things together in classes
to start with. It's perfectly legal in Python
to type that in and get the value back out. As I said, I would prefer to
do something that uses an accessor that I just wrote. So p dot Cartesian is a kind
of accessor, it's getting access to the data. And here's why I'd
like to have it. Right now, I still have the
problem that those classes, those instances of classes,
are exposed. What do I mean by that? Here's something I could do. Let's do it in fact. OK. What point in the plane
does p now point to? X-axis is foobar y-axis
ought to be foobass something else, right? I know it looks like a simple
and silly little example, but at the moment, I still have
the ability to go in and change the values of the
parameters by that little definition. And this makes no sense. And this is because I don't have
something I would really like to have, which
is data hiding. So you'll see lots of
definitions of this. I think of data hiding as
basically saying, one can only access instance values, or,
we'll call them that, instance values through defined
methods. And that's a wonderful thing to
have because it gives you that modularity, that
encapsulation that basically says, when I create a point, the
only way I can get at the values, is by using one of the
defined methods, in this case it could be Cartesian, and get
all the pieces of that. Unfortunately, Python
doesn't do this. Which is really a shame. Or another way of saying it
is, please don't do that. Don't go in and change the
values of things by using the direct access. Have the computational hygiene,
if you like, to only go through accessors, only go
through methods that are actually provided to
you as you do this. I actually don't remember,
John, C++ does have data hiding, I think, right? PROFESSOR 2: And not only
shouldn't you change it, you shouldn't even read it. PROFESSOR: Exactly. What you're going to see in a
second I violated in some of my code, which Professor Guttag
is going to yell at me shortly because I should have
done it through accessors, but, he's exactly right. A good, hygienic way of doing
this is, not only do I not go in and change things except
through a pre-defined method, I shouldn't read it other than
through a pre-defined method. I should use Cartesian
or polar to pull out those pieces of it. Once I've got that, you notice
I can now define a polar point, same way. Notice I've now solved one of my
problems, which is, in each one of these cases here, I'm
creating both x y and radius angle values inside of there. If it's in polar form I passed
in a radius and angle and I'll compute what the x-
and y- value is. If its in Cartesian form I'll
pass in an x and y and compute what a radius and angle is. But it now says that in any, in
no matter what kind of form I made it from, I can get out
that kind of information. So for example I defined p,
remember back over here, as a Cartesian point, but
I can actually ask for its polar form. It's there accessible to me. OK, this is great. Just to drive home one more
reason why I don't want to have changes to the
values other than through pre-defined things. Notice what happens if
I do the following. I could say I want to
change the radius of this particular thing. OK, perfectly reasonable
thing to do. And if I go look at the polar
form of this, OK, good, looks right, right? It's now got a different radius,
same angle, so I just changed the radius of it. Oh, but what happened to
the Cartesian form. I should have done this
earlier by typing the Cartesian form earlier, so let
me go back to where I was, sorry for that, let me go
make this a 1 again. If I look at the Cartesian, oh,
I did have the Cartesian form, don't mind me while I
mutter to myself here quietly. Yeah, that's right, I did
screw that up badly. All right, we try one more time,
here we go, let's try one more time. We'll make p a new point, ok? There's the Cartesian
representation of it, which is right, (1,2). Here's the polar representation
of it, some random set of numbers
which makes sense. If I now say, I'm going to go
ahead and change the radius of this, something, my polar form
did it right, but what happened to the Cartesian
form? Ah yes, didn't change. Which makes sense if you
think of my code. I didn't have anything in there
that says, if you change one of these values, other
values depend on it, and I want to make that
change to it. So this is one more example of
stressing why I only want to come access to the instances
through defined methods. Because I could've built that
in, it says if you change the value of this thing, by the
way you need to change recompute those other values in
order to make this hold up. OK, so what else do I have
then in my little class definitions here? So, I've got an init
in both cases. I don't have to put an init in,
but it's again, usually a good idea to put that
in originally. I've got and init that says,
when you create an instance, here's what you do. Notice that that typically also
defines for me what the internal variables are, what the
internal characteristics of the class are going to be. Again, I could have some other
functions to compute things, but this is typically
the place where I'm going to put them in. So this is giving me now that
template, better way of saying it, all right, a template
now, for a point is x, y, radius, angle. And I can see that in
those pieces there. And then I've got some things
that get me back out information about them. But I got a couple of other of
these strange looking things in there with underbars
to them. So let's look at what some of
the traditional methods for classes are in Python. I have init. This is what's actually going
to create the instance, instantiate it, create what
the set of variable values are for it. OK, I have another
one in there, underbar, underbar, str. Anybody have a sense of
what that's doing? What's s -- sorry, I heard
something, sorry go ahead. STUDENT: Display what I have. PROFESSOR: Displaying what
I have. Thank you. Yeah, I was going to say, think
about what does str do, in general? It converts things into
a string type. How do we typically
print things, we convert them to strings. So str is basically telling
us how we want to have it printed out. OK, in fact if we look at this,
if I say, print of p, it prints it out in that form. Now this is actually a poor
way to do it, because you might say, well, it's just the
list. But remember, it wasn't a list. What does it do? It says, if I want to print
out something I built in Cartesian form up here, says,
again, I'm going to pass it in a pointer to the instance, that
self thing, and then I'm going to return a string that
I combine together with an open and close paren, a comma in
the middle, and getting the x-value and the y-value and
converting them into strings before I put the whole
thing together. So it gives me basically my
printed representation. OK. What else do I have in here? Well, I have cmp. My handout's wrong, which I
discovered this morning after I printed them all out. So the version I'd like you to
have uses, that, greater than rather than equals that
I had in my handout. What's cmp doing as a method? Yeah? STUDENT: Comparing values? PROFESSOR: Yeah, comparing
values, right? And again, it's similar
to what cmp would do generically in Python. It's a way of doing
comparisons. So this is doing comparisons. Now, I put a version up there,
I have no idea if this is the right way to do comparisons
or not. I said both the x- and y-
coordinates are bigger, then I'm going to return
something to it. And I think in the polar one I
said, if, what did I do there, I said, yeah, again if the x
and y are greater than the other one, I'm going to
return them to it. The version in the handout, what
was that actually doing? You could look at the handout. Well I think it was comparing,
are they the same? So that would actually be
another method I could put in. Underbar underbar eq,
underbar underbar. Would be a default or generic
way of doing, are these things the same? OK, in each case, what these
things are doing, is they're doing, what sometimes gets
referred to as operator overloading. I know you don't remember that
far back, but in about the second lecture I made a joke of
Professor Guttag which, you know, you didn't laugh at, he
didn't laugh at, that's okay. In which I said, you know, I
didn't like the fact that things like plus are overloaded,
because you can use plus to add strings, you can
use plus to add numbers, you can use plus
to add floats. And he quite correctly, because
he's more senior than I am, more experienced
than I am, said it's actually a good thing. And he's right. Most of the time. The reason I say that is, by
having operator overloading I can use 1 generic interface
to all of the objects that I want to use. So it makes sense to be able to
say, look for many methods I do want to have a way of doing
comparison, and I don't have to remember, at top level,
what the name of the comparison method was. I can simply use the built-in
Sc -- about to say Scheme again -- the built-in Python
comparison operation. Say, are these 2 things
the same? Same thing with cmp, that's just
saying greater than, and greater than now can apply to
strings, it can apply to floats, it could apply to
points, it could add other pieces into it. So there are some downsides, in
my view, to doing operator overloading, but there's
some real pluses. And the main one is, I get to
just decide, how do I want to use this, and call it. Yes, ma'am? STUDENT: [INAUDIBLE] PROFESSOR: Right, cmp other,
so how would I call this? A good question. Here's the way I
would call it. Let me give you, I'm going to
create, a polar point, I'm going to call it q, and we'll
give it some random values. OK, and now I want to know,
is p greater than q? Now happens to return true here,
but the question is, where's the other come from? P is a particular object type. When I try and evaluate that
expression of greater than, is going to go into the class
to say greater than is a comp method. So let me say it very
carefully here. When I evaluate, yeah, when
I evaluate this, p is an instance of a point, in this
case it was actually a Cartesian point, it sends a
message to the instance, which sends a message to the
class, to get the cmp method from the class. And that then gets applied to
itself, just p, and one other argument, which is the second
piece there, so other points to the second argument
that was present. OK. John? PROFESSOR 2: -- other,
it could have said who or zort or -- PROFESSOR: Yeah, sorry, that
was part of the question, I could have a picked foobar could
put anything in here. It's simply, notice the form of
it here is, it's going to take two arguments, and
you're right, self is the original instance. This says, I need a second
argument to it, and that second argument better be
a point so I can do the comparison. Yes ma'am? STUDENT: [INAUDIBLE] PROFESSOR: What do you
think happens? Sorry, the question was, what
happens if I said p is less than q? Got it, yes? Seems pretty obvious, right? Next time I bring the
right glasses. It's still calling cmp, but it's
knowing that cmp is just reversing the order
of the arguments. Ok, which makes sense. If greater than takes, expects,
arguments in order x y, less than simply takes
greater than, but with the arguments reversed. OK, so I don't have to, it's a
great question, I don't have to create a second
one for cmp. Cmp is just saying, is this
bigger than, and if I want to reverse it, it goes
the other way. Question? STUDENT: [INAUDIBLE] PROFESSOR: Or equal equal? Let's try equal equal because
I didn't define it here. It says they're not the same,
and boy, I need help on this one, John, it's not, there's
no pre-defined eq in there. PROFESSOR 2: So, what cmp does,
and maybe this isn't exactly the right way to write
is, is cmp actually returns 1 of 3 values. A 0, minus a positive value,
zero or a negative value, depending upon whether
it's less than, equal, or greater than. PROFESSOR: Right. PROFESSOR2: So it's not really
a Boolean-valued function. It has 3 possible values
it could return. PROFESSOR: And so in this case,
it's using the same piece, but it's returning that
middle value that says they're actually the same. Right, one the things you can
see now is, we start building up classes, we get
these methods. So you can actually say, how
do I know which methods are associated with the class? For that, we can call dir. And what it does, is it gives
me back a listing of all the things, all the methods, that
are associated with it. Some of which I built:
cmp, init, str. And there, notice, are the
internal definitions and there are the internal variables. And in fact I should've
said, we often call those things fields. So inside of an instance,
associated with an instance, we have both methods
and fields. These are both altogether
called attributes of the instance. And then there were a couple of
other ones in there that I hadn't actually dealt with. The reason I want to point this
out to you is, if we go back up to the kinds of data
objects we started with, floats, ints, strings, they
actually behave the same way. They are instances of a class,
and associated with that class is a set of methods. So for example, I can say,
what are all the methods associated with the number,
or the integer 1? And you probably recognize some
of them in there, right, absolute value, add, comp, cors,
well we didn't do cors, we did a bunch of
other things. It could also say, what are the
methods associated with the string, 1. I'm sure you can quickly
graph it, but notice they aren't the same. That makes sense. We have some set of things we
want to do with strings, and different set of things we
want to do with numbers. But underlying Python
is the same idea. These are instances of a class,
and associated with that class are a set
of methods, things that I can deal with. So this is a handy way of being
able to see, what are in fact the methods that are
available if I don't happen to remember them, and want
to go back to them. OK, I want to spend the last few
minutes just showing you a couple of other things that
we can do in here. Let me see where I want
to go with this. So let's add one more
piece to this. OK, now that I've got points,
I might want to do something with points. So an easy thing to do in planar
geometry is I want to make a line segment. It's got a start point,
it's got an end point. Right, if you want to think
of it back over here. There's a line segment, it's
got a starting point and ending point. Well, I can do the same thing. And the reason I want to use
this as an example is, here's my little definition
of segment. Again, it's got an initializer,
or an instance creator, right there. Takes a start and an end point,
just going to bind local variable names start
and end to those pieces. But notice now, those aren't
just simple things like numbers, those are
actually points. And that's where the modularity
comes in. Now I have the ability to say,
I've got a new class, I can create instances of a line
segment, and it's elements are themselves instances of points. OK? And then what might I want
to do with the segment? I might want to get the
length of the segment. And I know it's kind of, you can
see it on your handout, it has the rest of the
pieces over here. Ok, what's the geometry say? The length of a line segment? Well, it's Pythagoras, right? I take the difference in the
x-values, squared, the difference in the y-values,
squared, add them up, take the square root of that. Notice what this says to do. It says if I want to get the
length of a segment, going to pass in that instance, it says
from that instance, get the start point, that's the
thing I just found. And then from that start
point, get the x-value. Same thing, from that instance,
get the endpoint, from that end point get
the x-value, square. Add the same thing to the
y-values, squared, take the square root. Yes, ma'am? STUDENT: So are you entering a
tuple in for start and end? PROFESSOR: No. I'm entering -- well,
let's look at the example right down here. In fact, let me uncomment
it so we can look at it. All right. I'm going to uncomment that. So notice what I'm
going to do. I'm going to build, this case,
a Cartesian point, I'm going to build a second Cartesian
point, and my segment passes in those class instances. All right, they're not tuples,
they're simply an instance with some structuring. And in fact if I go off and
run this, OK, what I was printing here was s 1 dot
length, and that's -- What is it doing? S 1 is a segment. It has inside of it
pointers to 2 points which are instances. And when I call length on this,
it takes that starting point, sends it the message
saying give me your x-coordinate, takes the
endpoint, says give me your x-coordinate, and add
them together. Now, I prefaced this a few
minutes ago about saying Professor Guttag wasn't
going to like me. He doesn't like me generally,
but that's between he and I. He beats me regularly at
tennis, which is why I don't like him. Sorry, John. This is being taped, which
is really good, isn't it? So why am I saying that? I said that if I was really
hygienic, and you can now wonder about how often
do I shower? If I was really hygienic. I would only ever access the
values through a method. And I'm cheating here, right,
because what am I doing? I'm taking advantage of the fact
that start is going to be a point, and I'm just directly
saying, give me your x-value. So I don't know don't, John, I
would argue if I'd written this better, I would have had a
method that returned the x- and the y- value, and it
would be cleaner to go after it that way. This is nice shorthand, all
right, but it's something that in fact I probably would
want to do differently. Why would I want to
do it differently? Imagine that I've written
code like this, written a bunch of code. And I originally decided I was
going to have as points, it's going to have internal values
of an x and a y. And then somewhere along the
line, I decide to store things in a different representation. If I had had a clean interface,
that I had a specific method to get those
values out, I wouldn't have to change anything. Other than that interface. But here, if I decide I'm going
to store things not in x and y, but with some other set
of names, for example, I've gotta go back into these pieces
of code that use the points, and change them. So I've lost modularity. I'd really like to have that
modularity that says, I'm only going to get access to the
values, not by calling their names, but by calling some
specific method to get access to their names. You could argue, well, x is
in some sense inherently a method, but it's not nearly as
clean as what I would like. And the last piece I want you to
see here, and then I'll let you go is, notice now how that
encapsulation, that binding things together has
really helped me. Given the abstraction, the
notion of a point as an instance with some
values, I can now start building segments. And I could now extend that. I could have, you know,
polygonal figures, that are a sequence of segments. And I would be able to simply
bury away the details of how those other instances are
created from how I want to use them by simply calling methods
on the classes. We'll come back to
this next time.

Linear Algebra

I'd like to talk.
Thank you. One of the things I'd like to
give a little insight into today is the mathematical basis for
hearing. For example,
if a musical tone, a pure musical tone would
consist of a pure oscillation in terms of the vibration of the
air. It would be a pure oscillation.
So, [SINGS], and if you superimpose upon
that, suppose you sing a triad, [SINGS], those are three tones.
Each has its own period of oscillation, and then another
one, which is the top one, which is even faster.
The higher it is, the faster the thing.
Anyway, what you hear, then, is the sum of those
things. So, C plus E plus G,
let's say, what you hear is the wave form.
It's periodics, still, but it's a mess.
I don't know, I can't draw it.
So, this is periodic, but a mess, some sort of mess.
Now, of course, if you hear the three tones
together, most people, if they are not tone deaf,
anyway, can hear the three tones that make up that.
So, in other words, if this is the function which
is the sum of those three, some sort of messy function,
f of t, you're able to do Fourier
analysis on it, and break it up.
You're able to take that f of t, and somehow mentally express
it as the sum of three pure oscillations.
That's Fourier analysis. We've been doing it with an
infinite series, but it's okay.
It's still Fourier analysis if you do it with just three.
So, in other words, the f of t is going to
be the sum of, let's say, sine,
I don't know, it's going to be the sign of
one frequency plus the sine of another frequency plus the sine
of a third, maybe with coefficients here.
So, somehow, since you were born,
you have been able to take the f of t,
and express it as the sum of the three signs.
And, here, therefore, the three tones that make up
the triad. Now, the question is,
how did you do that Fourier analysis?
In other words, does your brain have a little
integrator in it, which calculates the
coefficients of that series? Of course, the answer is no.
It has to do something else. So, one of the things I'd like
to aim at in this lecture is just briefly explaining what,
in fact, actually happens to do that.
Now, to do that, we'll have to make some little
detours, as always. So, first I'm going to,
throughout the lecture, in fact, I gave you last time a
couple of shortcuts for calculating Fourier series based
on evenness and oddness, and also some expansion of the
idea of Fourier series where we use the different,
but things didn't have to be periodic or period two pi,
but it can have an arbitrary period, 2L, and we could still
get a Fourier expansion for it. Let me, therefore,
begin just as a problem, another type of shortcut
exercise, to do a Fourier calculation, which we are going
to be later in the period to explain the music problem.
So, let's suppose we're starting with the function,
f of t, which is a real square wave,
and I'll make its period different from the one,
not two pi. So, suppose we had a function
like this. So, this is one,
and this is one. So, the height is one,
and this point is one as well. And then, it's periodic ever
after that. I'll tell you what,
let's do like the electrical engineers do and put these
vertical lines there even though they don't exist.
Okay, so the height is one and it goes over.
The half period is one. This really is a square wave.
I mean, it's really a square, not what they usually call a
square wave. So, my question is,
what's its Fourier series? Well, it's neither even nor
odd. That's a little dismaying.
It sounds like we're going to have to calculate an's and bn's.
So, the shortcuts I gave you last time don't seem to be
applicable. Now, of course,
nor is the period two pi, but that shouldn't be too bad.
In fact, you ought to look for an expansion in terms of things
that look like sine of n, well, what should it be? Since L is equal to one,
the half period is equal to one.
Remember, the period is 2L, not L.
It's n pi over L, but if L is one,
we should be looking for an expansion in terms of functions
that look like this. Now, since we've already done
the work for the official square way, which looks something like
this, what you always try to do is reduce these things to
problems that you've already solved.
This is a legitimate one, since I solved it in lecture
for you. So, we can consider it as
something we know. So, I observed that since I am
very lazy, that if I lower this function by one half,
it will become an odd function. Now it's an odd function.
Okay, I just cut the work in half.
So, let's call this function, let's call this,
I don't know, S of t.
The green one is the one we wanted to start with.
So, f of t is a green function.
But, I can improve things even more because the function that
we calculated in the lecture is a lot like this salmon function.
That's why I called it S. But, the difference is that the
function we calculated with this one.
In the first place, it went down further.
It went not to negative one half, which is where that one
goes. But, it went down to negative
one, and then went up here to plus one.
And, it went over to pi. So, it came down again,
but not, but at the point, pi.
And here, negative pi went up again.
Okay, let me remind you what this one was.
Suppose we call it, O doesn't look good,
I don't know, how about g of u?
Let's, for a secret reason, call the variable u this time,
okay? So, the previous knowledge that
I'm relying on was that I derived the Fourier series for
you by an orthodox calculation. And, it's not too hard to do
because this is an odd function. And therefore,
you only have to calculate the bn's.
And, half of them turn out to be zero, although you don't know
that in advance. But anyway, the answer was four
over pi times the sum of just the odd ones,
the sine of n u, and that you had to 
divide by n. So, this is the expansion of g,
this function, g of u, the Fourier expansion
of this function. Since it's an odd function,
it only involves the signs. There's no funny stuff here
because the period is now two pi.
And, this came from the first lecture on Fourier series,
or from the book, wherever you want it,
or solutions to the notes. There are lots of sources for
that. The solution's in the notes.
Okay, now, that looks so much like the salmon function,
--- -- I ought to be able to
convert one into the other. Now, I will do that by
shrinking the axis. But, since this can get rather
confusing, what I'll do is overlay this.
What I prefer to do is I think u, okay, I'm changing,
I'm keeping the thing the same. But, I'm going to change the
name of the variable, the t, in such a way that on
the t-axis, this becomes the point, one.
If I do that, then this function will turn
exactly into that one, except it will go not from
minus a half to a half, but it will go from negative
one to one, since I haven't done anything to the vertical axis.
So, how I do that? What's the relation between u
and t? Well, u is equal to pi times t,
or the other way around. You know that it's going to be
approximately this. Try one, and then check that it
works. When t is equal to one,
u is pi, which is what it's supposed to be.
So, this is the relation between the two.
And therefore, without further ado,
I can say that, let's write the relation
between them. f of t is what I want.
Well, what's f of t if I subtract one half of that?
So, that's going to be equal to the salmon function plus one
half, right, or the salmon function is f of t lowered by
one half. One thing is the same as the
other. And, what's the relation
between this salmon function and the orange function?
Well, the salmon function is, so, let's convert,
so, S of t -- it's more convenient, as I wrote the
formula g of u. Let's start it from that end.
If I start from g of u, what do I have to do to convert
it into S of-- into the salmon function?
Well, take one half of it. So, if I put them all together,
the conclusion is that f of t is equal to one half
plus S of t, which is one half of g of u, but u is pi t.
So, it's four pi, four over pi times the sum of
the sine of n. And, for u, I will write pi t divided by n.
And, sorry, I forgot to say that sum is only over the odd
values of n, not all values of n.
So, the sum over n odd of that, and, of course,
the two will cancel that. So, here we have,
in other words, just by this business of
shrinking or just stretching or shrinking the axis,
lowering it and squishing it that way a little bit.
We get from this Fourier series, we get that one just by
this geometric procedure. I'd like you to be able to do
that because it saves a lot of time.
Okay, so let's put this answer up in, I'm going to need it in a
minute, but I don't really want to recopy it.
So, let me handle it by erasing.
So, let's call that plus two over pi,
and there is our formula for that green function that we
wrote before. So, I'll put that in green.
So, we'll have a color-coded lecture again.
Now, what we're going to be doing ultimately,
to getting at the music problem that I posed at the beginning of
the lecture, is we want to solve, and this is what a study
of Fourier series has been aiming at, to solve second-order
linear equations with constant coefficients were the right-hand
side was a more general function than the kind we've been
handling. So, now, in order to simplify,
and we don't have a lot of time in the course,
I'd have to take another day to make more complicated
calculations, which I don't want to do since
you will learn a lot from them, anyway.
I think you will find you've had enough calculation by the
time Friday morning rolls around.
So, let's look at the undamped case, which is simpler,
or undamped spring, or undamped anything because it
doesn't have that extra term, which requires extra
calculations. So, I'll follow the book now
and some of the notes and the visuals, and called the
independent variable-- the dependent variable I'm going to
call x now. And, the independent variable
is, as usual, time.
So, this is going to be, in general, f of t,
and I'm going to use it by calculating example,
this is the actual f of t I'm going to be using.
But, the general problem for a general f of t is to solve this,
or at least to find a particular solution.
That's what most of the work is, because we already know how
from that to get the general solution by adding the solution
to the reduced equation, the associated homogeneous
equation. So, all our work has been,
this past couple of weeks, in how you find a particular
solution. Now, the case in which we know
what to do is, so we can find our particular
solution. Let's call that x sub p. We could find x sub p if the
right hand side is cosine omega, well, in general,
an exponential, but since we are not going to
use complex exponentials today, all these things are real.
And I'd like to keep them real. If it's either cosine omega t
or sine omega t, or some multiple of that by linearity, it's just as good.
We already know how to find the thing, and to find a particular
solution. So, the procedure is use
complex exponentials, and that magic formula I gave
you. But, right now,
just to save a little time, since I already did that on the
lecture on resonance, I solved it explicitly for
that, and you've had adequate practice I think in the problem
sets. Let's simply write down the
answer that comes out of that. The answer for the particular
solution is cosine omega t or sine omega t. That's the top.
And, it's over a constant. And, the constant is omega
naught squared. That's the natural frequency
which comes from the system, minus the imposed frequency,
the driving frequency that the system, the spring or whatever
it is, undamped spring, is being driven with.
Okay, understand the notation. Cosine this over that,
or sine, depending on whether you started driving it with
cosine or sine. So, this is from the lecture,
if you like, from the lecture on resonance,
but again it's, I hope by now,
a familiar fact. Let me remind you what this had
to do with resonance. Then, the observation was that
if omega, the driving frequency is very close to the natural
frequency, then this is close to that.
The denominator is almost zero, and that makes the amplitude of
the response very, very large.
And, that was the phenomenon of resonance.
Okay, now what I'd like to do is apply those formulas to
finding out what happens for a general f(t),
or in particular this one. So, in general,
I'll keep using the notation, f of t,
even though I've sorted used it for that.
But in general, what's the situation?
If f of t is a sine series, cosine series,
all right, let's do everything. Suppose it's,
in other words, the procedure is,
take your f of t, expand it in a Fourier series.
Well, doesn't that assume it's periodic?
Yes, sort of. So, suppose it's a Fourier
series. I'll make a very general
Fourier series, write it this way:
cosine (omega)n t, and then the sine terms, sine (omega)n t
from one to infinity where the omegas are,
omega n is short for that. Well, it's going to have the n
in it, of course, but I want, now,
to make the general period to be 2L.
So, it would be n pi over L. Of course, if L is equal to one, then it's n pi.
Or, if L equals pi, those are the two most popular
cases, by far. Then, it's simply n itself,
the driving frequency. But, this would be the general
case, n pi over L if the period is the period of f
of t is 2L. So, that's what the Fourier
series looks like. Okay, then the particular
solution will be what? Well, I got these formulas.
In other words, what I'm using is superposition
principle. If it's just this,
then I know what the answer is for the particular solution,
the response. So, if you make a sum of these
things, a sum of these inputs, you are going to get a sum of
the responses by superposition. So, let's write out the ones we
are absolutely certain of. What's the response to here?
Well, it's (a)n cosine omega n t. The only thing is, now it's divided by omega
naught squared. This constant has changed,
and the same thing here. Of course, by linearity,
if this is multiplied by a, then the answer is multiplied
by, the response is also multiplied by a.
So, the same thing happens here.
Here, it's (b)n and over, again, omega naught squared
minus omega times the sine of omega t. So, in other words,
as soon as you have the Fourier expansion, the Fourier series
for the input, you automatically get this by
just writing it down the Fourier series for the response.
That's the fundamental idea of Fourier series,
at least applied in this context.
They have many other contexts, approximations,
so on and so forth. But, that's the idea here.
All right, what about that constant term?
Well, this formula still works if omega equals zero.
If omega equals zero, then this is the constant,
one. The formula is still correct.
Omega is zero here. The only thing you have to
remember is that the original thing is written in this form.
So, the response will be, what will it be?
Well, it's one divided by omega naught squared,
if I'm in the case omega zero is equal to zero.
So, it's a zero divided by two omega naught squared.
And, as you will see, it looks just like the others.
You're just taking omega, and making it equal to zero for
that particular case. Sorry, this should be omega n's
all the way through here. All right, well,
let's apply this to the green function.
So, what have we got? We have its Fourier series.
So, if the green function is, if the input in other words is
this square wave, the green square wave,
so in your notes, this guy, this particular f of
t is the input. And, the equation is x double
prime plus omega naught squared x equals f of t. Then, the response is,
well, I can't draw you a picture of the response because
I don't know what the Fourier series actually looks like.
But, let's at least write down what the Fourier series is.
The Fourier series will be, well, what is it?
It's one half. The constant out front is one
half, except it's one over two omega naught squared. So, this is my function,
f of t. That's the general formula for
how the input is related to the response.
And, I'm applying it to this particular function,
f of t. And, the answer is plus.
Well, my Fourier series involves only odd sums,
only the summation over odd, and only of the sign.
So, it is going to be two over pi,
sorry, so it's going to be two over pi out front.
That constant will carry along by linearity.
And, I'm going to sum over odd, n odd values only.
The basic thing in the upstairs is going to be the sine of omega
n t. But, what is (omega)n?
Well, (omega)n is n pi. So, it's n pi t.
And, how about the bottom? The bottom is going to be omega
naught squared minus omega n squared. And, this is my (omega)n,
minus n pi squared. What's that? Well, I don't know.
All I could do would be to calculate it.
You could put it on MATLAB and ask MATLAB to calculate and plot
for you the first few terms, and get some vague idea of what
it looks like. That's nice,
but it's not what's interesting to do.
What's interesting to do is to look at the size of the
coefficients. And, again, rather than do it
in the abstract, let's take a specific value.
Let's suppose that the natural frequency of the system,
in other words, the frequency at which that
little spring wants to go vibrate back and forth,
whatever you got vibrating. Let's suppose the natural
frequency that's omega naught is ten for the sake of
definiteness, as they say.
Okay, if that's ten, all I want to do is calculate
in the crudest possible way what a few of these terms are.
So, the response is, so let's see,
we've got to give that a name. The response is (x)p of t. What's (x)p of t?
I'm just going to calculate it very approximately.
This means, you know, throwing caution to the winds
because I don't have a calculator with me.
And, I want you to look at this thing without a calculator.
The first term is one over 200. Okay, that's the only term I can get exactly right.
[LAUGHTER] Or, I could if I could calculate.
I suppose it's 0.005, right?
That's the constant term. Okay, so the next term,
let's see, two over pi is two thirds.
I'll keep that in mind, right?
Plus two thirds, 0.6, let's say,
that's an indication of the accuracy with which these things
are going to be performed. I think in Texas for a long
while, the legislature declared pi to be three,
anyways. One of those states did it to
save calculation time. I'm not kidding,
by the way. All right, so what's the first
term? If n equals one,
I have the sine of pi t. That's the n equals one term. What's the denominator like?
That's about 100 minus 9 squared. Let's say it's 91, sine t over 91.
What's the next term? Sine of three pi t, remember, I am omitting,
I'm only using the odd values of n because those are the only
ones that enter into the Fourier expansion for this function,
which is at the bottom of everything.
All right, what's the sine three pi t?
Well, now, I've got 100 minus three pi, --
-- that's 9 squared is 81. So, no, what am I doing?
So, we have 100 minus three times pi is 9,
squared. Well, let's say a little more.
Let's say 85. So, that's 15.
How bout the next one? Well, it's sine 5 pi t. I think I'll stop here as soon
as we do this one because at this point it's clear what's
happening. This is 100 squared minus,
that's 15 squared is 225, so that's about 125 with a
negative sign. So, minus this divided by 125.
And, after this they are going to get really quite small
because the next one will be seven pi squared.
That's 400, and this is becoming negligible.
So, what's happening? So, it's approximately,
in other words, 0.005 plus the next coefficient
is, let's see, 6/10, let's say 100,
sine pi t. And, what comes next?
Well, it's now 1/20th. It's about a 20th.
Let's call that 0.005 sine three pi t,
and now so small, minus 0.01, let's say times
this last one, sine 5 pi t.
What you find, in other words,
is that the frequencies which make up the response do not
occur with the same amplitude. What happens is that this
amplitude is roughly five times larger than any of the
neighboring ones. And after that,
it's a lot larger than the ones that come later.
In other words, the main frequency which occurs
in the response is the frequency three pi.
What's happened is, in other words,
near resonance has occurred. So, if omega is ten,
very near resonance, that is, it's not too close,
but it's not too far away either, occurs for the frequency
three pi in the input. Now, where's the frequency
three pi in the input? It isn't there.
It's just that green thing. Where in that is the frequency
three pi? I can't answer that for you,
but that's the function of Fourier series,
to say that you can decompose that green function into a sum
of frequencies, as it were, and the Fourier
coefficients tell you how much frequency goes into each of
those f of t's. Now, so, f of t is decomposed
into the sum of frequencies by the Fourier analysis.
But, the system isn't going to respond equally to all those
frequencies. It's going to pick out and
favor the one which is closest to its natural frequency.
So, what's happened, these frequencies,
the frequencies and their relative importance in f of t
are hidden, as it were.
They're hidden because we can't see them unless you do the
Fourier analysis, and look at the size of the
coefficients. But, the system can pick out.
The system picks out and favors, picks out for resonance,
or resonates with, resonates with the frequencies
closest to its natural frequency.
Well, suppose the system had natural frequency,
not ten. This is a put up job.
Suppose it had natural frequency five.
Well, in that case, none of them are close to the
hidden frequencies in f of t, and there would be no
resonance. But, because of the particular
value I gave here, I gave the value ten,
it's able to pick out n equals three as the most important,
the corresponding three pi as the most important frequency in
the input, and respond to that. Okay, so this is the way we
hear, give or take a few thousand pages.
So, what does the ear do? How does the ear,
so, it's got that thing, messy curve,
which I erased, which has a secret,
which just has three hidden frequencies.
Okay, from now on I hand wave, right, like they do in other
subjects. So, we got our frequency.
So, it's got a [SINGS]. That's one frequency.
[SINGS] And, what goes in there is the sum
of those three, and the ear has to do something
to say out of all the frequencies in the world,
I'm going to respond to that one, that one,
and that one, and send a signal to the brain,
which the brain, then, will interpret as a
beautiful triad. Okay, so what happens is that
the ear, I don't talk physiology, and I never will
again. I know nothing about it,
but anyway, the ear, when you get far enough in
there, there are little three bones, bang, bang,
bang; this is the eardrum, and then there's the part which
has wax. Then, there's the eardrum which
vibrates, at least if there is not too much wax in your ear.
And then, the vibrations go through three little bones which
send the vibrations to the inner ear, which nobody ever sees.
And, the inner ear, then, is filled with thick
fluid and a membrane, and the last bone hits up
against the membrane, and the membrane vibrates.
And, that makes the fluid vibrate.
Okay, good. So, it's vibrating according to
the function f of t. Well, what then?
Well, that's the marvelous part.
It's almost impossible to believe, but there is this,
sort of like a snail thing inside.
I've forgotten the name. It's cochlea.
And, it has these hairs. They are not hairs really.
I don't know what else to call them. They're not hairs. But, there are things so long,
you know, they stick up. And, there are 20,000 of them.
And, they are of different lengths.
And, each one is tuned to a certain frequency.
Each one has a certain natural frequency, and they are all
different, and they are all graded, just like a bunch of
organ pipes. And, when that complicated wave
hits, the complicated wave hits, each one resonates to a hidden
frequency in the wave, which is closest to its natural
frequency. Now, most of them won't be
resonating at all. Only the ones close to the
frequency [SINGS], they'll resonate,
and the nearby guys will resonate, too,
because they will be nearby, almost have the same natural
frequency. And, over here,
there will be a few which resonate to [SINGS],
and finally over here a few which go [SINGS],
and each of those little hairs, little groups of hairs will
signal, send that signal to the auditory nerve somehow or other,
which will then carry these three inputs to the brain,
and the brain, then, will interpret that as
you are hearing [SINGS]. So, the Fourier analysis is
done by resonance. You here resonance because each
of these things has a certain natural frequency which is able,
then, to pick out a resonant frequency in the input.
I'd like to finish our work on Fourier series.
So, for homework I'm asking you to do something similar.
Taken an input. I gave you a frequency here,
a different omega naught, a different input,
as you by means of this Fourier analysis to find out which it
will resonate, which of the hidden frequencies
in the input the system will resonate to, just so you can
work it out yourself and do it. Now, I'd like to first try to
match up what I just did by this formula with what's in your
book, since your book handles the identical problem but a
little differently, and it's essentially the same.
But I think I'd better say something about it.
So, the book's method, and to the extent which any of
these problems are worked out in the notes, the notes do this,
too. Use substitution.
Base uses differentiation of Fourier series term by term.
The work is almost exactly the same as here.
And, it has a slight advantage, that it allows you,
the book's method has a slight advantage that it allows you to
forget this formula. You don't have to know this
formula. It will come out in the wash.
Now, for some of you, that may be of colossal
importance, in which case, by all means,
use the book's method, term by term.
So, it requires no knowledge of this formula because after all,
I base this solution, I simply wrote down the
solution and I based it on the fact that I was able to write
down immediately the solution to this and put as being that
response. And for that,
I had to remember it, or be willing to use complex
exponentials quickly to remind myself.
There's very, very little difference between
the two. Even if you have to re-derive
that formula, the two take almost about the
same length of time. But anyway, the idea is simply
this. With the book,
you assume. In other words,
you take your function, f of t.
You expand it in a Fourier series.
Of course, which signs and cosines you use will depend upon
what the period is. So, you assume the solution of
the form-- Well, if I, for example,
carried out in this particular case, I don't know if I will do
all the work, but it would be natural to
assume a solution of the form, since the input looks like the
green guy. Assume a solution which looks
the same. In other words,
it will have a constant term because the input does.
But all the rest of the terms will be sines.
So, it will be something like (c)n times the sine of n pi t. The only question is,
what are the (c)n's? Well, I found one method up
there. But, the general method is just
plug-in. Substitute into the ODE.
Substitute into the ODE. You differentiate this twice to
do it. So, I'll do the double
differentiation and I won't stop the lecture there,
but I will stop the calculation there because it has nothing new
to offer. And, this is the way all the
calculations in the books and the solutions and the notes are
carried out. So, I don't think you'll have
any trouble. Well, this term vanishes.
This term becomes what? If I differentiate this twice,
I get summation, so, this is one to infinity
because I don't know which of these are actually going to
appear. Summation one to infinity,
(c)n times, well, if you differentiate the sine
twice, you get negative sine, right?
Do it once: you get cosine. Second time:
you get negative sine. But, each time you will get
this extra factor n pi from the chain rule.
And so, the answer will be negative (c)n times n pi squared
times the sine of n pi t. And so, the procedure is, very simply,
you substitute (x)p double prime into the differential
equation. In other words,
if you do it, we will multiply this by omega
naught squared. And, you add them.
And then, on the left-hand side, you are going to get a sum
of terms, sine n pi t times coefficients
involving the (c)n's. And, on the right,
so, you're going to get a sum involving the (c)n's,
and the sines n pi t, and on the right,
you're going to get the Fourier series for f of t,
which is exactly the same kind of expression.
The only difference is, now the sines have come with
definite coefficients. And then, you simply click the
coefficients on the left and the coefficients on the right,
and figure out what the (c)n's are.
So, by equating coefficients, you get the (c)n's.
Would you like me to carry it out?
Yeah, okay, I was going to do something else,
but I wouldn't have time to do it anyway.
So, why don't I take two minutes to complete the
calculation just so you can see you get the same answer?
All right, what do we get? If you add them up,
you get c naught, out front, plus (c)n is
multiplied by what? Well, from the top it's
multiplied by omega naught squared.
On the bottom, it's multiplied by n 
pi squared. Ah-ha, where have I seen that
combination? The sum is equal to,
sorry, one half plus what is it, sum over n odd of sine n pi
t over n. So, the conclusion is that--
I'm sorry, it should be c naught times omega naught squared. So, what's the conclusion?
If c zero is one over two omega naught squared, and that (c)n,
only for n odd, the others will be even.
The others will be zero. The (c)n is going to be equal
to two over pi here. So, it's going to be two pi, two over pi times one over n
times one over omega naught squared minus n over
pi squared. This is terrible, which is the same answer we got
before, I hope. Did I cover it up?
Same answer. So, that answer at the
left-hand end of the board is the same one.
I've calculated, in other words,
what the c zeros are. And, I got the same answer as
before.

CS

In this lesson, we're going to solve a
simple problem on binary tree which is also a famous programming
interview question, and the problem is given a binary tree we need to check if the binary tree is a
binary search tree are not. As we know a binary tree is a
tree in which each node can have atmost two
children. All these trees that I have drawn here are
binary trees, but not all of them are binary search
trees. Binary search tree, as we know is a binary tree in which for
each node value of all the nodes in left subtree
is lesser and if you want to allow duplicates we
can say lesser or equal and value of all the nodes in
right subtree is greater. We can define binary search
tree as a recursive structure like this. Elements in
left subtree must be lesser or equal and elements in right subtree must be
greater and this should be true for all nodes and not just a root node, so left and right subtrees should
themselves also be binary search trees. Of these binary trees that I'm showing
here, A and C are binary search trees but B and D are not. In B for the root node
with value 10, we have 11 in its left subtree which
is greater than 10 and in a binary tree for any node all
values in its left subtree must be lesser. In D we are good
for the root node. The value in root node is 5 and we
have 1 in left subtree which is lesser and
we have 8, 9 and 12 in right subtree which are greater. So we are
good for the root node but for this node with value 8, we have
9 in its left. So this tree is not a
binary search tree. So how should we go about solving this
problem. Basically, I want to write a function that should
take pointer or reference to root node of a binary tree as argument and
function should return true if the binary tree is BST, false otherwise. This is how my method signature look
like in C++. In C, we do not have boolean types so return type here can be int. We can return 1 for true and 0 of false. I'll also write the definition of node here. For a binary tree node would be
structure with 3 fields, 1 to store data and 2 to store addresses of left and right
children. In my definition of node here, data type is integer and we have 2 pointers to node to store
addresses of left and right children. Okay coming back
to the problem there are multiple approaches and we're
going to talk about all of them. The first approach that I'm
going to talk about is easy to think of but it's not so
efficient but let's discuss it anyway. We are saying
that for a binary tree to be called binary search tree, it should have recursive  structure like
this. For the root node all the elements in left subtree must be
lesser or equal and all the elements in right subtree
must be greater, and left and right subtrees should themselves also be binary search
trees. So let's just check for all of this. I'm going to write a function named
IsSubtreeLesser that will take address of root node
of a binary tree or subtree and and integer value as argument and this function will return
true if all the elements in the subtree are lesser
so than this value and similarly I'll write another function
named IsSubtreeGreater that will return true if all the
elements in a subtree are greater than the given value. I had just declared this functions. I'll write body of
these functions later. Let's come back to this function 
IsBinarySearchTree. In this function, I am going to say that if
all elements in left subtree are lesser and I'll verify this by making
a call to IsSubtreeLesser function passing it address of left child of my current
root. Left child would be the root of current
subtree and the data in root. This function will return true
if all the elements in left subtree would be lesser than the data in root.
Now the next thing that I want to check for is if elements in right subtree are greater
than the data in root or not. These two conditions are not sufficient.
We also need to check if left and right subtrees are binary search
trees are not. So I'll add two more conditions here
have made a recursive call to IsBinarySearchTree function passing it address
of left child and I have made another call passing
address of right child and if all these four function call
IsSubtreeLesser, IsSubtreeGreater and IsBinarySearchTree for left and
right subtrees return true if all these four checks
pass then our tree is a binary search tree. We can 
return true else we need to return false. There is
only one thing that the a missing in this function now. We are missing the base case. If the route is null
that is if the tree or subtree is empty, we can
return true. This is the base case for our recursion
where we should stop. With this much of code IsBinarySearchTree 
function is complete but let's also write IsSubtreeLesser and
ItsSubtreeGreater functions because they are also part
of our logic. This function has to be a generic function
that should check if all the elements in a given tree are
lesser than a given value or not. We will have
to traverse the complete tree or subtree and see value in all
nodes and compare these values against this given integer.
I'll first handle the base case in this function. If the tree is empty, we can return true
else we need to check if the data in root is less than or equal to the given value
and we also need to recursively check if left and right subtrees of the current
root have lesser value or not. So I'm adding two
more conditions here. I'm making two recursive calls one for the
left subtree and another for the right subtree. If all these
three conditions are true then we are good else we can return
false. IsSubtreeGreater function will be very similar. Instead of writing these two functions
IsSubtreeLesser and IsSubtreeGreater, we could
also do something like this. We could find the maximum left subtree and compared it with the data in root,
if maximum of a subtree is lesser then all the elements a lesser and
similarly if the minimum of a subtree is greater all the elements had
greater. For the right subtree, we could find a
minimum. So instead of writing these two functions IsSubtreeLesser and IsSubtreeGreater, we could write
something like find max and find min and this would also fit. So this is a solution using one of the
approaches. Let's quickly run this code on an example
binary tree and see how it will execute. I have drawn a very simple binary tree
here which actually is a binary search tree. let's assume some addresses for these
nodes the tree. Let's say the root node is that address
200 and I'll assume some random addresses for
other nodes as well. To check if this binary tree is a binary
search tree or not, we will make a call to 
IsBinarySearchTree function. I'm writing IBST here as Shortcut for
IsBinarySearchTree because I'm short of space here. So I'll
make a call to this function maybe from the main function passing
addressed 200, address of the root node. For
this function call address in this local variable address
collected in this local variable root will be 200. Root is not null. Null is only a macro for address 0. For this call root is not null, so we
will not return true at this line. We will go to the next if. Now here, we
will make a call to IsSubtreeLesser function. Arguments
passed will be address of left child which is 150 and 7 the data in node at 200. Execution of the calling function will pause
and will resume only after the called
function returns. Now in this call to IsSubtreeLesser,
root is not null so we will not return true at first line.
We will go to the next if. Now here the first condition is if data in root this time is 150 because on this call is for this
left subtree and for this left subtree address of
root is 150. Data in root is 4 which is lesser than 7, so the first condition is true and we
can go to the second condition which is a recursive call. This call will pause and we will go
to the next call. Here once again the data in node at 180, 1 is lesser than 7 so first condition is true and we will
make recursive call. Left subtree for node at
180 is null. There is no left child so we will return at first line. Root is null
this time. This particular call will simply to return
true. Now in this previous call when root is 180, second condition for if is also true.
So we will make another call for right
subtree. Once again address passed will be 0 and we will simply return
true and now for this call IsSubtreeLesser 187, all three conditions
are true. So this guy can also return true and now
ISL 150,7 will resume. Now this guy will make a recursive
call for the right subtree and this guy after everything will also
return true. Now for this call because all 3
conditions in the if statement are true, this guy will also return true and now
IsBinarySearchTree function will resume. For this call we have evaluated the
first condition we have got true now this guy will make
another call to IsSubtreeGreater, passing address of
right child and value 7. This guy after everything will return
true and now we will have 2 recursive calls, to check if left and right subtree are
binary search trees on not. We will first have a call for the left
subtree. The execution will go on like this but I
want you to see something. Each call to binary search tree
function, we are comparing the data in root with
all the elements in left subtree and then all the elements in right subtree. This example tree could be really large
then in that case in the first call IsBinarySearchTree
for this complete tree, we would recursively traverse this
whole left subtree to see whether all the values in this
subtree are less than 7 or not and then we
will traverse all nodes in this right subtree to see if values have greater than 7
or not and then in next call IsBinarySearchTree,
when we would be validating with this particular subtree is BST or not. We would recursively traverse this
subtree if values are lesser than 4 or not and this subtree to see if value so greater
than 4 or not. So all in all during this whole process
there will be a lot of traversal. Data in nodes will be read and
compared multiple times. If you can see all nodes in this
particular subtree will be traversed once in call to
IsBinarySearchTree for 200. When we will compare value in these
nodes with 7 and then these nodes will once again be
traversed in call to IsBinarySearchTree for
150 when they will be compared with 4. They will
be traversed in call to IsSubtreeLesser. All in all these two functions
IsSubtreeLesser and IsSubtreeGreater very expensive.
For each node, we are looking at all nodes in its subtrees. There is an efficient solution in which
we do not need to compare data in a node with data in all nodes 
in its subtrees and let's see what the solution
is. What we can do is we can define a
permissible range for each node and data in that node
must be in that range we can start at the root node with a
range -infinity to infinity, because for
the root node there is no upper and lower limit and
now as we are traversing we can set a range for other nodes. When we are going left, we need to reset
the upper bound so for this node at 150, data has to
be between -infinity and seven. Data in left
child cannot be greater than data in root. If we're going right, we need to set the
lower bound for this node at 300 range would be 7 to
infinity. 7 is not included in the range. Data has
to be strictly greater than 7. For this node at 180, the range will
be -infinity to 4. For this node with value 6 lower bond will
be 4 and upperbound would be 7. Now my code will go like this. My
function IsBinarySearchTree will take two more arguements, an integer to mark lower bound or
min value and another integer to mark the upper
bound or max value and now instead of checking whether all
the elements in left subtree are lesser than the data in root and all the elements in right subtree
are greater than the date in root or not. We will simply check whetheer data in root
is in this range or not. So I'll get rid of
these two function call  IsSubtreeGreater and IsSubtreeGreater which are really  
expensive and I'll add these two conditions. Data in root
must be greater than min value and data in root must be less than max
value. These two checks will take constant time. IsSubtreeLesser and IsSubtreeGreater
functions were not taking constant time. Running time for them was proportional
to number of nodes in the subtree. Okay now these two recursive calls
should also have two more arguements. For the left child lower bound will not
change, upper bound will be to data in current
node and for the right child, upper bound will not change and lower bond will be data in current
node. This recursion looks good to me. We already have to base case written. The only thing is that the Caller of this
IsBinarySearchTree function may only want to pass the address of root
node so what we can do is instead of naming this function IsBinarySearchTree.
We can name this function as a utility function like 
IsBstUtil and we can have another function name
IsBinarySearchTree in which we can take only to address of
root node and this function can call IsBstUtil to function passing address
of root. Minimum possible value in integer
variable for -infinity and maximum possible value in integer
valuable for +infinity INT_MIN and INT_MAX
here are macros for a minimum and maximum possible
values in Int. So this is a solution using second
approach which is quite efficient. In this recursion will go to each node
once and at each node we will take constant time to see whether the data at node is in a
defined range or not and time complexity would be O(N)
where N is number of nodes in the binary tree. For the previous algorithm, time
complexity was O(N^2). One more thing, in this code I have not
handled the case that Binary search tree can
have duplicates. I am saying that elements in left subtree
must be strictly lesser and elements in right subtree must be strictly greater. I leave it for
you to see how you will allow duplicates. There is another
solution to this problem. You can perform in order traversal of
binary tree and if the tree is binary search tree
you would read the data in sorted order. In-order traversal of a binary search tree gives a sorted list. You can
do some hack while performing in order traversal and
check if you're getting the elements in sorted order or not. During the whole traversal you only need
to keep track of previously read node and at any time
data in a node that you're reading must be greater than the data in previously read
node. Try implementing this solution, it will be
interesting. Okay I'll stop here now. In cominng lessons, We will discuss some
more problems on Binary tree.  Thanks for watching.

Diff. Eq.

I had a couple requests on reviewing
certain things, so I'm gonna do a few. I just picked a few examples that I'll
talk about, but if in the meantime you think of anything else you want to ask,
then ask at any point, okay? So first, here's kind of a famous example
that I wanted to go through anyway, which is good review of geometric
distribution and expectations. Then we will talk more about
the universality of the uniform, since that's the number one request, okay. But first, let's do a problem
called the coupon collector. I think a couple of the sections
did this problem or something similar, but
not all of them definitely. This is a really good example. It happens to be very
useful in a lot of cases. I think of it as the toy collector
problem cuz I don't collect coupons. Well, I don't really collect toys either. But I'd rather collect toys than coupons. The problem is that you have a certain
number of different types of toys and you want to collect a full set. And it's one of these
Happy Meal type of things, where you buy a Happy Meal at McDonald's
or whatever, and you get a random toy, and you want the full set, right? That's the problem. The problem is, on average, how long will
it take you to collect the full set, okay? So we have a certain number of toys,
let's say, n toy types. I say type just because, to distinguish
between the individual toys, you might get seven of the same toy,
right, but I mean, seven of the same type, but
seven physical objects. So there are n types and
we'll assume that they're equally likely. Now, in practice they probably will make
one really hard to find so that then people go berserk, going on eBay and stuff
trying to track down the one hard one, but we'll assume right now
they're equally likely. Problem gets very,
very messy if they're not equally likely. Like, really, really tedious,
long calculations, you'd go on for pages. So it's certainly not something
you would need to worry about for an exam in this course because
the problems that you'll have to deal with should have very short,
nice solutions. And it gets really nasty if
the probabilities are unequal. But in this case it
works out really nicely. So the question is find the expected
time to collect a full set, where time is just measured discretely in terms
of how many toys you need to buy, right? Expected time, i.e., number of toys, Until you have a complete set. And if you think of breaking up
this random variable into smaller random variables, then it's easy
to think about how to do this. So if we call this T, for time and
T for toys, we want to know the, T is the number of toys that we
need to collect to get a full set. Let's think of just breaking
down T into components. So it's T1+T2+ blah blah blah, +Tn where T1, Equals time until first new toy. By new,
I mean a toy that we didn't already have. Well, that's actually equal to 1,
cuz the first one you get, that's your first toy, and you didn't
already have it, obviously, okay? So T1 is actually just a constant,
always equal to 1. Now T2,
after you've collected your first toy, if you're unlucky the second toy
is the same as the first toy. But most of the time you get something
new on the second try, okay? So T2 is the additional time
until your second new toy. So, new just means one you didn't already
collect earlier in the process, so additional time until second new toy. And then T3 is same thing
until the third and so on. So additional time after the second. That is, collect, collect, collect till
you get your second toy, collect, collect, collect till you get the third, and keep going like that until
you have all of them. So all the way up to Tn, okay? Then notice that T1 is just equal to 1, but T2, let's think about T2. So there are n different toy types and
we have 1 out of the n, so there's an n minus 1 over n chance that the next
one is gonna be something new, right? That, so we'd consider that a success. So the success probability
is n minus 1 over n. On the other hand, with probability 1
over n, we get a failure in the sense of collecting a toy we already had and
then it's the same problem again. So therefore T2- 1 is geometric with probability of success n -1 over n. The minus 1 is just because of
the convention about the geometric, that we're taking the convention
that the geometric starts at 0. But of course, you need at least one more
new toy, so I'm subtracting off that 1. And in general, Tj-1 is gonna be geometric. So Tj, we're trying to get the jth toy,
right? That means that we already have
collected j minus 1 toys, so it's going to be n minus the number we've
already collected, which is j minus 1. So they're all geometrics. Then we can immediately
write down the answer. In this case,
these Ts are independent, but linearity would hold even
if these were dependent. Here they're independent because, right, it's just like how long does it
take to get your second new toy, but has nothing to do with telling
you the additional time, right. But even if they were not independent,
it would still be true that E(T) equals the sum
E(T1) + E (T2) + E of Tn. And just to simplify that,
write out what that is. So that's E(T) equals, so E(T1) is just one. E(T2), well we talked about
the fact that if you have a geometric starting at 1
then the mean is 1 over P, cuz that's Q over P plus 1,
because we're adding back the 1. So the next one is just gonna be
the reciprocal of this which is n over n minus 1. And then the next one is
gonna be n over n minus 2, and so on. And then for the last term, that means
that we have n- 1 out of the n types. Then our probability of success
is only 1 over n, right? So for the last one, it's n. Let's just write this as n over 1. Then we can factor out n. So this is really just n times, and I'll reverse the order of terms just to
make it look nicer, it's n times one-half. 1 plus one-half plus one-third,
plus blah, blah, blah, plus 1 over n. So that's called the n harmonic sum. That looks like the harmonic series. This is approximately n log n for large n. That's just a useful fact about the
harmonic series that we can approximate it as logarithmic. But, in general for
a midterm and homework and stuff, you should give exact answers
unless you're asked for an approximation. So this would be the exact answer, but this would be a handy approximation for
large N. So we just used linearity and
geometrics here. Sounds like it could have been a hard
problem, but you break it up into pieces like that, use linearity, and
then it becomes an easy problem. Okay, so let's talk a little more about the universality stuff, and
there's always questions about. So I just wanna draw a little picture and do a quick example to show more
about what's going on with that. So first here's a picture. So we're talking in the continuous case. So we have an x and F of x. Just imagine drawing a CDF and
remember CDFs, you know, they have to be increasing and
right continuous but for universality, we're assuming that it's strictly
increasing and it's continuous. So let's draw some kind of CDF,
maybe it looks like that. And either it hits one and then it would stay one forever, or
it just asymptotically approaches one. Either way is fine. But just to have like a picture in mind
of what's universality really saying. What it says is, for
example, what if we chose a random, this is lower case x,
this is just the x-axis. But what if we chose a point
on the x-axis randomly. So the question is basically,
let X be distributed according to F. So X is the random variable. F is this curve, this CDF. And the claim is that when you plug X,
this is one-half of the universality. Remember, we talked about two
parts which are equivalent. So just talked about
one of them right now. We said that if we plug X into
its own CDF, we'll get a uniform. So I just wanna explain using
this picture, why is that true? I think the easiest way to think of it is
just to make it a little bit more concrete and pick a certain number here. Let's say let's pick a number
on the vertical, so here's 1. And maybe it hits 1, and
then it would stay at 1 forever. Now let's pick some other
number on the vertical axis. Let's say, here is one-third,
I just picked a y value of one-third. And suppose that I ask the question,
let's call this point here x sub zero. So, x0 is the point such that
F of x0 equals one-third. That's the, you know,
just inverse of a function. So, in other words, I'm just saying,
I'm just making up, for some reason it looks really abstract
at first, but I found that if I make up a number like one-third
it's easier to see what's going on. So you could have chosen any number
between 0 and 1, but I picked one-third. So I assume that F of x0 equals 1/3. Now I want to know what's the probability that F of X, that's a random variable. Cuz we're plugging this random
variable into its own CDF. We wanna know what's the probability that
this is less than or equal to 1/3, where. So the interpretation of this F of X is, we first pick a random value on the x-axis
according to this distribution. This distribution is telling us how
do we select the random x value, and then we compute the corresponding
y value which is F of x. So that's how we compute F of X. But notice that that's true exactly, so I'm picking a random point here,
somewhere on this x axis I'm saying, when is it true that the y value is
less than or equal to one-third? Well, that's the same thing as
saying that the x value has to be between here and here. Because if x were to the right and
I go up, here it's bigger than a third. And I said less than or
equal to one-third. That's the exact same thing as saying
X is less than or equal to this x0, but by definition of the CDF,
that's F of x0 equals 1/3. And there was nothing special about
one-third here, you could do this for whatever number you want between 0 and 1. That's the uniform distribution. The uniform distribution is saying that
probability is proportional to length. And for the uniform 0,1 probability is
length within that interval 0 to 1. So that's what we just verified. That one-third became one-third. So all this is saying is that the claim is that F of X is uniform between 0 and 1. And if you do the same calculation
with any number here instead of 1/3 any number between 0 and
1, same thing holds. So that's what it's saying geometrically. We are just relating random
points on the x axis to figuring a random value vertically between 0 and 1. And as a quick example of
how you might use that. Suppose you wanted to simulate
from a logistics distribution. We haven't defined
logistics distribution yet. But I'll just tell you
what it is right now. The logistic distribution, which is another important
distribution in statistics. That it's used in what's called logistic
regression for example which is very, very widely used method in economics and
statistics and elsewhere. It's based on the logistic distribution
which has the following CDF, F of x equals e to the x
over 1 plus e to the x. This is for all real x. For practice with CDFs you can
verify that this is a valid CDF. There's three properties it's continuous,
increasing, and so on. You should check for yourself that this function I wrote
down does have the properties of a CDF. So that's a valid CDF but
that doesn't tell us how we could ever simulate random variables
that have that CDF. And if we want to do that,
an easy way to do it would be, let U be uniform zero one,
which is easy to simulate, and then consider just doing f inverse of u. So if you set this thing equal to u and then this is u in terms of x but
solve for x in terms of u just by doing the inverse just a little
bit of algebra to get the inverse. What you'll get is this function
log of u over 1 minus u. So that's just the inverse evaluated at u. Just do the algebra, compute the inverse
of this and that's what you'll get. So that will be interpreted as the log of
the odds basically, if you think of this. If u were just a probability between 0 and
1 that would be the log of the odds. But in this case,
u is a random variable, so we plugged it into its inverse CDF,
and then that would be logistic. So if you just computed this thing, then you would have a random draw
from the logistic distribution. Now if you wanted, you can check that
that's true just by taking this thing, and just compute directly its CDF. Use the definition of CDF and
you'll get this, but it's easier to recognize that
that's really why it's working. Okay, so are there more questions
that you've thought of since before? I have a few more examples that I
thought of, and things to talk about. But anyway, let me know if you
think of any other questions. I wanna do something more
with symmetry and linearity, so here's just a fun little
symmetry problem I made up. So let X, Y, Z, this is just
good practice with linearity and symmetry, and what does IID actually mean,
things like that. So let's Let X, Y and Z be IID, positive random variables. And the problem is,
find the expected value of (X/X+Y+Z). Okay, now, you can't do linearity
on a quotient, you can't just say that this is E of the top over E of
the bottom, or anything like that. That's not linearity, you can't do
things like that, linearity is for sums. Okay, so at first it looks like kind
of a mysterious problem, maybe. We haven't really dealt much with
quotients of random variables. And I didn't give you any
explicit formula for the CDF or the PDF, I didn't say if they
were discrete or continuous. I just said they are IID positive random
variables, I only said they're positive just so that we don't have to
worry about dividing by 0. No other assumptions, so
this is very general, okay. To solve this, we just need to
see the symmetry of the problem. The symmetry is that we have
three IID random variables, okay. And we took one of them divided
by the sum of all three of them. But by symmetry, that has to be the same
as E of (Y over (X + Y + Z)), right? Because I could have listed
these in a different order, and it would have been the same problem,
right? It's completely symmetrical
because they're IID, so maybe I should have written Y+X+Z,
but that's the same thing. Right, it doesn't matter what
order you add them in, so that's the exact same thing by symmetry. Similarly, that's the exact same
thing as E(Z over (X + Y + Z)). So whatever this quantity is, it doesn't matter which way we write it,
because they're IID. I'm not saying X = Y, I'm just saying this
problem, if I'd asked you this problem. That is the same structure
as this problem, it has the same structure of this problem. It must be the same thing by symmetry, so these two steps are by symmetry. Okay, well now,
I have these three different expectations, and that makes me think of adding them and using linearity, so
what if we added these three things? So if we added them, they would be E(X over (X + Y + Z)) + E(Y over (X + Y + Z)) + E(Z over (X + Y + Z)). Oops, X over, sorry, this is Z over
X + Y + Z, and then use linearity. By linearity, that's just E(((X
+ Y + Z) over (X + Y + Z)), okay, now that's a very easy problem,
expected value of 1 is 1. But on the other hand,
this is the same thing added three times. So 3 times this thing is 1, so that tells
us that the thing we want is one-third. It's basically immediate by symmetry and
linearity. And I think intuitively, this seems
like a very kind of general thing, like how did that always work? But if you think about this,
that's a pretty intuitive answer, that would have been a good guess. If you had done this and
gotten like 4 as your answer, you should immediately know
that that would be wrong. Because the denominator is
a number between 0 and 1. The denominator is bigger than
the numerator, I said they're positive, so it has to be something between 0 and 1. One/third makes sense because if I asked
you just intuitively, we have three things, and how much would you guess
that one thing contributes to the total? One out of three things,
you would guess one-third, so it's just a pretty intuitive answer. But that's not a proof,
and this is a proof, so that's just an example of symmetry,
linearity, that stuff. Another thing we should talk
a little more about is LOTUS, cuz LOTUS is something that looks very,
very simple. But students may make
mistakes a lot with it, so here's a simple LOTUS
example that I made up. So here's the problem, The problem is, let U be uniform 0,1, and let X equal U squared, and let Y equal e to the X. Is that what I want, yep, e to the X,
and the problem is to find the expected value of Y, As an integral. In general, on the midterm,
I'll be very clear about, you might have a hard integral
that you wouldn't have to do. I would say leave it as
an integral in that case. Or if it's stated that you should
actually compute the thing. So basically I'll say whether it should
be left as a sum or an integral, or if it doesn't say,
leave it as an integral. Then you should get an actual number,
and fully simplify everything. Okay so in this case, suppose I said
find this as an integral, well, it's clearly a LOTUS problem, and
there's two approaches you could take. But a lot of students in previous
years have kind of mixed up the different approaches. And it's kinda for an interesting
reason that I wanted to tell you about. So one approach would be to say E(Y),
so we want expected value of Y. Well, that just looks like
an easy LOTUS problem, right, so E(Y) equals,
LOTUS says we integrate. We'll just change capital
X to lowercase x, so we go e to the x,
times the PDF of x, f(x) dx. And we need the limits of integrations for
x, so u is between 0 and 1 so x is also 0 to 1, cuz you're
squaring a number between 0 and 1. And so you get that, so
I mean this is correct, where this is the PDF of x. I mean this is true but
this would not, you know, this is not the full answer because
you haven't said what's f of x. I haven't said what's f of x. I mean, I said it's the PDF of x,
but what's the PDF of x? So if I say write it as an integral, it should be like an actual integral where
you're saying what you're integrating. Here we just said that's the PDF,
what's the PDF? Okay, so at this point you could find the CDF of x and
then take the derivative and get the PDF. That's similar to the problem that
you just had on the homework. Slightly easier here. You had the homework problem about
uniform between minus one and one, okay? This is a little bit easier, cuz you
don't have to deal with negative numbers. So you could find the CDF and
then find the PDF and plug that in. And that would be perfectly valid but that'll be more work versus
doing it the better way. The better way is to think of it. Y equals e to the x is a function of x, but x in turn is a function of u. So I could also write this as e to the u
squared and do the LOTUS this way. So if I did it this way, then I would just say it's the integral
zero to one e to the u squared du. Because we know that the PDF of
u is just one on this integral. So if I did it this way, I can write down
the answer in like one minute, right. This way, then you have to
do a separate calculation. Now, I mean, I'm saying better just for
writing down the integral immediately. Not necessarily better for
solving the integral. If we wanted to get a number then
that kind of looks like the Gaussian integral again,
except without a minus there, okay. But the problem just says
leave it as an integral, then this I could just
write down instantly. And here we have to first find the PDF. Both of these are correct though, so you get full credit either way as
long as if you did either of these as long as you actually worked out explicitly
what the PDF is, that's perfectly fine. Where you'd run into trouble though,
a lot of students had trouble and this kind of thing. But somehow like mixing and matching
with these two things, somehow I think there's a mindset that just because we
define LOTUS and we called our random variable x and we looked at functions
of x that some how it depends on x. And then if we add another LOTUS problem
where I don't even mention X and then there's gonna be a lot
of answers where x's appear. Well, what's x, right? So the important thing is the pattern
not what you call the variables. So, okay, well anyway,
that's just some comments about lotus. Other questions on anything? Yeah. &gt;&gt; [INAUDIBLE]
&gt;&gt; Yeah. &gt;&gt; [INAUDIBLE] &gt;&gt; How do you find the PDF? Yeah, so let's do that as quick practice,
not only with PDFs but with CDFs. Okay, so, first find the CDF. So, the CDF is the problem that u squared
is less than or equal to some number x. That's x, but
I'm trying to reduce it back to you. So the CDF would be this. And as I said, it's kinda similar
to your homework, but easier, because here we don't have to
worry about negative numbers. So just take the square
root of both sides. That's just the square root
of x if x is between 0 and 1. That's the CDF,
the derivative is the PDF is just gonna be one-half x to the minus one-half,
again x is between 0 and 1. So we get the CDF take the derivative and
get the PDF. To get the CDF, all we have to really
do is understand what a CDF is and then reduce it back to the uniform
which is something we understand. Okay, other questions? Okay, so I wanted to do a quick
example of a story type of proof, just to review that a little bit. [SOUND] This is a very simple one but,
But I think it's interesting, Just as quick practice,
right? So quick story practice,
if we let x be binomial (n,p) and the problem is find
the distribution Of n- x. Well, one way to do it this is
quick practice with PMFs would be, n- x is discrete let's find its PMF. Well, it's PMF is the probability
that n- x is some number k, but let's just rewrite that as
the probability that x is n minus k. Those are the same events now
we've reduced it back to x. Probability that x = n- k,
that's just from the binomial PMF and choose n minus k. P to the n- k. q, q = 1- p, again, q to the k, right? So that was easy, but
a nicer way to write that would be n choose k,
q to the k, p to the n- k. So that would be the PMF method. Remember that n choose n- k, is n choose
k which you can prove easily by using factorials or just by the story, picking
k is the same a picking n- k, okay. But a simpler way,
not that this is difficult but an even easier way to see
this is using the story, we can immediately say that
n- x is binomial ( n,q). Just by interchanging successes and
failures. Okay, so the story is that x is the number
of successes in n, IID Bernoulli P trials. So x is the number of successes then
obviously n- x is the number of failures. Okay, but we said repeatedly that you're
gonna find success and failure however you want as long as each trial is either
success or failure, but not both. So if we redefine success
to be failure and redefine failure to be success,
then that's immediate. So, I'll just call that
swapping success and failure. Because you define it however you want,
right? So we could redefine it the other way
around and then that's immediate. So, no calculation whatsoever,
just one line. Just swap success and
failure and that's it. So swap success and failure and I hope that none of you have
to do that on the exam. But I think it's good practice. Now, [COUGH] okay. So we haven't done any Poissons yet
today, so there's a Poisson example that I think
that is also a good practice question. [INAUDIBLE]
&gt;&gt; Yeah, the question is whether on the midterm
there may be questions where you can just write things down in one line,
and yeah. There's a good chance that you'd
have to write something, right. But there may be several things
where you just write one line or one sentence of explanation
because you see the story or the pattern or the structure like
one sentence of what's going on. You know, like this. I mean this would be, I mean just writing this immediately
then it doesn't say how he got there. By saying we're interswapping success and
failure, that's a sentence and there's a good chance
of something like that. Which would save you some time,
that that's part of the point. &gt;&gt; [INAUDIBLE]
&gt;&gt; Do you get extra credit for writing, it's immediately obvious? Don't waste time writing it
as is immediately obvious. You've ought to conserve time,
time is precious. Yeah, so let's do a Poisson problem. So here's the problem. Suppose that the number of, this is just an example that I like,
suppose that the number of emails that I get, in a time interval Of length t, let's just say time t, is distributed as Poisson of lambda t or
a lambda of some constant. So we're thinking of lambda as a rate,
rate times time, right, so if I get say 20 emails
per hour times the number of hours then that would be
the expected number of emails. Remember that this is lambda
t is both the mean and the variance of this distribution, okay. So it's a random variable because
in different intervals of length t, I mean I may, I'm not always gonna get
exactly the same number of emails so there's some distribution to it, okay. Now the problem is find
the PDF of T which we define as the time of
the first email like, let's say right now it's time 0, starting now what time
will my first email come? So that's in continuous, right? That's a continuous time problem, right? Because the emails could arrive
at any time in continuous time where as this is discrete because I'm
counting the number of emails, okay. So this is kind of neat because
it just connecting discrete and continuous and well,
let's see how to do it. So find the PDF or the CDF, once we have
the CDF just take the derivative, so I would normally say
find the distribution or find the PDF or the CDF,
any of those would be fine. So the CDF would be the probability
that T is less than or equal to t. That's a little bit, remember, this is
general advice when finding probabilities, you should think about is it
easier to find that probability or find the compliment. In this case,
if you think about it a little bit, you'll see that it's easier
to find the compliment. Let's find the probability
that T is greater than t, little t, so that's just 1 minus the CDF
so if we have this, we know the CDF. This is a little bit easier to think about
because if we imagine a time line here, where here's time t. And if we draw an x every
time we receive an email, T &gt; t means that the first email,
let's say it's there, so that's time capital T,
is to the right of here, right? That says from times 0 to t,
you have no email, right? That's exactly what this says. It says that in times 0 to t,
you have no new mail, right? It's exactly equivalent. Therefore, that's the exact same thing as saying that the P(N). (T&gt;t) says that Nt = 0 where Nt = # of
emails in that time interval from 0 to 2. So I'm defining Nt to be the number of emails in this time interval, 0 to t, but T greater than t says there are none,
right? So it's the exact same thing. And we assume that this is
Poisson lambda t, so this thing, just using the Poisson PMF
is e to the minus lambda t, lambda t to the 0, over 0 factorial. Well, that's just e to the minus lambda t,
right, so the CDF is 1 minus that,
1- e to the minus lambda t for t greater than 0, and if we want
the PDF just take the derivative. So you got lambda e to the minus lambda t,
okay, so that's connecting the Poisson, counting number of emails to this is
called the exponential distribution, again related to our homework problem, and again we'll talk about
this distribution later. You don't need to know
the exponential distribution yet, but it will be reasonable to be able to
do a problem like this where we're just using the Poisson which is discrete,
to get a PDF. Well, lastly I just want
to remind you again, be very careful of the difference between,
we sort of have three classes of objects. We have our distributions,
the distribution is the blueprint for creating a random variable,
that was our random house. And then don't confuse the random
variable with a constant, a constant would be a specific house,
the random variable is the random house, the distribution,
the CFD is the blueprint. And so mixing up those things causes a lot
of trouble, so be very careful about that. All right, so good luck.

Algorithms

&gt;&gt; [MUSIC PLAYING] &gt;&gt; DAVID J MALAN: All right,
welcome back to CS50. This is the start of week two. A word from one of our
friends on campus-- if you are interested, possibly, either
now or in some future term even, once more comfortable, teaching
middle school students a little something about computer science,
do head to that URL. They are in particular need right now of
teachers, particularly if you have had some exposure to computer science. &gt;&gt; So recall that last time, we introduced
a few data types in C, and you may have started to get your
hands dirty with these thus far in problem set one. And we had a char. So in somewhat technical terms, what
is a char as you know it today? &gt;&gt; So it's a character, but let's
be more precise now. What do we mean by character
or individual char? A non-numerical character-- so not necessarily. It turns out that even numbers, even
punctuation and letters are represented with this data
type known as a char. So it's not necessarily alphabetical. Yeah? &gt;&gt; So it's an ASCII character. So if you think back to week zero, when
we had our byte of volunteers come up and either hold their hands up
or not all, they represented bits. But collectively as a group of eight,
they represented a byte. And we introduced the notion of ASCII
at that lecture, which simply is a mapping between numbers and letters. And ASCII uses, as those humans
implied, eight bits to represent a character. &gt;&gt; So accordingly, if eight bits can
each take on one of two values-- zero or one-- that means there were two possibilities
for this person-- zero or one-- two for this person, two for this
person, two for this one. So a total of two times two times
two times two times two-- so two the eighth in total. So there's a total number of characters
256 possible that you can represent with eight bits. &gt;&gt; Now, those of you who speak Asian
languages might know that there's more characters in the world than just
As and Bs and Cs and Ds. And indeed, ASCII does not suffice for
a lot of languages of the world. But more on that another time. For now, know that in C if you want
to represent a letter, a piece of punctuation, or just something character
in nature, we use a char. And it's one byte or eight bits. &gt;&gt; How about an int? Well, an int is an integer. How many bits, if you recall,
was an integer typically? Anyone recall? So it's typically 32. It actually depends on the computer
that you're using. But in the appliance, and in a lot of
computers, it's 32 bits or four bytes-- eight times four. And ints are just used for storing
numbers, either negative, positive, or zero. &gt;&gt; And if you've got 32 bits and you only
care about positive numbers, can anyone ballpark how many possible
integers a computer can represent from zero on up? So it would be two to the 32, which
is roughly four billion. So these powers of two are going to be
recurring themes in computer science. As we'll see, they're quite convenient
to work with even if it's not quite easy to do the math in one's head. &gt;&gt; So we'll say roughly four billion. Now, a long long-- you can kind of guess. It's longer than an int. How many bits? So 64 bits or eight bytes. This just means you can represent even
bigger numbers, bigger positive or bigger negative numbers. &gt;&gt; And how about float? That's a floating point
value of 32 bits. This is just a real number, something
with a decimal point. But if you instead need more places
after the decimal point or you want to represent a bigger number with some
fraction after it, you can use a double, which is 64 bits. &gt;&gt; But there's an interesting
takeaway here. So if ints are limited by 32 bits and
even long longs are limited by 64 bits, that sort of begs the question,
what if you actually want to count higher than 4 billion for an int? Well, you just use a long long. But what if you want to count higher
than two to the 64th, give or take? &gt;&gt; Now, that's a huge number. But eventually, you might actually
care about these kinds of values, especially if you are using a database
and starting to collect lots and lots and lots of data and assigning unique
numbers to each piece of that data. So we kind of have a problem. And similarly, with floating point
values-- floats or doubles-- if you've only got a finite number of
bits, how many total numbers could you possibly represent? &gt;&gt; Well, it's less clear when you
involve a decimal point. But it's surely finite. If you have a finite number of bits,
a finite number of humans, a finite number of light bulbs, surely you can
only represent a finite number of floating point values. But how many real numbers
are their in the world? There's an infinite. So that's kind of a problem because we
don't have an infinite amount of memory or RAM inside of our computers. So some challenging things can happen. &gt;&gt; So let's go ahead and try
to express this here. Let me go ahead and open up gedit. I'm going to go ahead and save a file
called "floats0.c" just to be consistent with an example that is
available online, if you would like. And I'm going to go ahead and
define it as follows-- I'm going to go ahead and say, int
main void, as we often do. &gt;&gt; And then in this program, I'm going to
declare myself a float, so a 32-bit variable called f, arbitrarily. And then I'm going to store in it
I don't know, one tenth, so 0.1. So I'm going to express that as one
divided by 10, which is perfectly legitimate in C. &gt;&gt; And then on the second line, I simply
want to print out that value. So recall that we can use
the familiar printf. We don't want to use %i for an int. We want to use %f for a float. And then I'm going to do backslash n,
close quote, comma, f, semicolon. &gt;&gt; So here's my program. There's already one bug. Does someone for whom this clicked
already want to point at least one bug I've made? Yeah? Yeah. I forgot "#include " at the
top, they symptom of which if I try to compile this is going to be that the
compiler is going to yell at me, saying undefined symbol or
something to that effect. It doesn't understand something
like printf. &gt;&gt; So I'm going to do "#include
", save the file. And now it's in better shape. But I'm also going to point
out one new detail today. In addition to specifying place
holders like %f %i %s, you can sometimes influence the behavior
of that placeholder. For instance, in the case of a floating
point value, if I only want to display one decimal place after the
period, I can actually do 0.1f. So in other words, I separate the f and
the percent sign with 0.1, just telling printf, you might have a whole
bunch of numbers after the decimal point for me. But I only want to see one of them. &gt;&gt; So I'm going to go ahead now and save
this program, go into my terminal window, and I'm going to go ahead
and type make float 0, enter. I see that somewhat cryptic line that
will begin to make more sense as we tease it apart this week and next. Now I'm going to go ahead
and run float zero. And, damn. &gt;&gt; So there's another bug
here for some reason. I'm pretty sure that one tenth, or
one divided by 10, is not 0.0. Maybe I'm just not looking
at enough digits. So why don't I say two .2 to see two
decimal places instead of just one. Let me go back to my terminal window
here and hit up a couple of times to see my history. Do make float zero again,
and then up again. And now enter. &gt;&gt; And now I'm pretty sure this is wrong. And I could do three and four, and I'm
probably going to keep seeing zeros. So where is the bug? One divided by 10 should be 0.1. Someone want to take a stab at what
the fundamental issue is? Yeah? They're both integers. So what? So with one divided by 10, that's
what I do in arithmetic. And I get 0.1. &gt;&gt; Yeah. And so it is indeed that issue. When you take an integer in a computer
and you divide it by another integer, the computer by default is going to
assume that you want an integer. The problem though, of course, is
that 0.1 is not an integer. It's a real number. And so what the computer does by
default is it just throws away everything after the decimal point. It doesn't round down or up per se. It just throws away everything
after the decimal point. And now that makes sense. Because now we're clearly
left with zero. &gt;&gt; But wait a minute. I'm not seeing an int zero. I'm actually seeing 0.00. So how do I reconcile this now? If one divided by 10 is zero, but I'm
seeing 0.00, where is it getting converted back to a real number? Yeah. Exactly. &gt;&gt; So up here in line five, when I actually
store that 0.1, which is then truncated to zero, inside of a float,
that's effectively equivalent to storing it not as an int but,
indeed, as a float. Moreover, I'm then using printf to
explicitly print that number to two decimal places even though there
might not actually be any. &gt;&gt; So this kind of sucks, right? Apparently you can't do math,
at least at this level of precision, in a computer. But surely there's a solution. What's the simplest fix we could maybe
do, even just intuitively here to solve this? Yeah? Turn the integers into-- yeah. Even if I'm not quite sure what's
really going on here, if it fundamentally has to do with these both
being ints, well, why don't I make that 10.0, making this
1.0, resave the file. Let me go back down to the
bottom and recompile. Let me now rerun. And there-- now, I've got my one tenth
represented as 0.10. &gt;&gt; All right. So that's not bad. And let me point out one other way
we could have solved this. Let me actually roll back in time
to when we had this as one tenth a moment ago. And let me go ahead and resave this file
as a different file name, just to have a little checkpoint. So that was version one. And now let me go ahead and
do one more version. We'll call this version
two zero indexed. &gt;&gt; And I'm going to instead do
this-- you know what? Adding dot zero works in this case. But suppose one were a variable. Supposed 10 were a variable. In other words, suppose that I couldn't
just hard-code .0 at the end of this arithmetic expression. Well, I can actually do something
in parentheses called casting. I can cast that integer 10 to a float,
and I can cast that integer one to a float, as well. Then the math that's going to be done
is effectively 1.0 divided by 10.0, the result of which goes
in f as before. So if I recompile this as make floats
2, and now floats 2, I get the same answer, as well. &gt;&gt; So this is a fairly contrived example,
to solve this problem by introducing casting. But in general, casting's going to be
a powerful thing, particularly for problem set two in a week's time, when
you want to convert one data type to another that at the end of the day
are represented in the same way. At the end of the day, every single
thing we've talked about thus far is just ints underneath the hood. Or if that's too low-level for
you, they're just numbers underneath the hood. Even characters, again, recall
from week zero, are numbers underneath the hood. &gt;&gt; Which is to say, we can convert between
different types of numbers if they're just bits. We can convert between numbers
and letters if they're just bits, and vice versa. And casting in this way is a mechanism
in programming that lets you forcibly change one data type to another. Unfortunately, this isn't as
straightforward as I might have liked. &gt;&gt; I'm going to go back into floats
1, which was the simpler, more straightforward one with
.0 added on to each. And just as a quick refresher,
let me go ahead and recompile this, make floats 2-- sorry, this is make floats 1. And now let's run floats 1. And in the bottom, notice
that I indeed get 0.1. So, problem solved. &gt;&gt; But not yet. I'm now going to get a little curious,
and I'm going to go back into my printf statement and
say, you know what? I'd like to confirm that this
is really one tenth. And I'm going to want to see this
to, say, five decimal places. It's not a problem. I change the two to a five,
I recompile with make. I rerun it as floats 1. Looking pretty good. My sanity checks might end there, but
I'm getting a little more adventurous. I'm going to change 0.5 to 0.10. I want to see 10 digits after
the decimal place. &gt;&gt; And I'm going to go ahead and recompile
this and rerun floats 1. I kind of regret having tested this
further because my math is not so correct anymore, it seems. But wait a minute, maybe
that's just a fluke. Maybe the computer is acting
a little bit strange. Let me go ahead and do 20 decimal points
and reassure myself that I know how to do math. I know how to program. Make floats 1, recompile, and damn it. That is really, really getting
far from the mark. &gt;&gt; So what's going on here? Intuitively, based on our assumptions
earlier about the size of data types, what must be happening here
underneath the hood? Yeah? Exactly. If you want this much precision, and
that's a heck of a lot of precision-- 20 numbers after the decimal point. You can't possibly represent an
arbitrary number unless you have an arbitrary number of bits. But we don't. For a float, we only have 32 bits. &gt;&gt; So if 32 bits can only be permuted in a
way-- just like our humans on, stage hands up or down-- in a finite number of
ways, there's only a finite number of real numbers you can represent
with those bits. And so the computer eventually
is going to have to start cutting corners. The computer can hide those details
from us for a little bit of time. But if we start poking at the numbers
and looking farther and farther at the trailing numbers in the whole number,
then we start to see that it's actually approximating the
idea of one tenth. &gt;&gt; And so it turns out, tragically, there's
an infinite number of numbers we cannot represent precisely in a
computer, at least with a finite number of bits, a finite
amount of RAM. Now unfortunately, this sometimes
has real-world consequences. If people don't quite appreciate this
or sort of take for granted the fact that their computer will just do what
they tell it to do and don't understand these underlying
representation details-- which, frankly, in some languages are
hidden from the user, unlike in C-- some bad things can happen. &gt;&gt; And what I thought we'd do
is take a step back. And this is about an
eight-minute video. It aired a few years ago, and it gives
insights into actually what can go wrong when you under-appreciate these
kinds of details in the very all-too real world. If we could dim the lights
for a few minutes. &gt;&gt; SPEAKER 1: We now return to engineering disasters on Modern Marvels. &gt;&gt; Computers-- we've all come to accept the
often frustrating problems that go with them. Bugs, viruses, and software glitches
are small prices to pay for the convenience. But in high-tech and high-speed
military and space program applications, the smallest problem
can be magnified into disaster. &gt;&gt; On June 4, 1996, scientists prepared to
launch an unmanned Ariane 5 rocket. It was carrying scientific satellites
designed to establish precisely how the Earth's magnetic field interacts
with solar winds. The rocket was built for the European
Space Agency and lifted off from its facility on the coast
of French Guiana. &gt;&gt; JACK GANSSLE: At about 37 seconds into
the flight, they first noticed something was going wrong. The nozzles were swiveling in
a way they really shouldn't. Around 40 seconds into the flight,
clearly the vehicle was in trouble. And that's when they made a
decision to destroy it. The range safety officer, with
tremendous guts, pressed the button, blew up the rocket before it could
become a hazard to public safety. &gt;&gt; SPEAKER 1: This was the maiden voyage
of the Ariane 5, and its destruction took place because of a flaw embedded
in the rocket's software. &gt;&gt; JACK GANSSLE: The problem on the Ariane
was that there was a number that required 64 bits to express. And they wanted to convert
to a 16-bit number. They assumed that the number was never
going to be very big, that most of those digits in the 64-bit
number were zeros. They were wrong. &gt;&gt; SPEAKER 1: The inability of one software
program to accept the kind of number generated by another was
at the root of the failure. Software development had become a very
costly part of new technology. The Ariane 4 rocket had been very
successful, so much of the software created for it was also
used in the Ariane 5. &gt;&gt; PHILIP COYLE: The basic problem was
that the Ariane 5 was faster, accelerated faster. And the software hadn't
accounted for that. &gt;&gt; SPEAKER 1: The destruction of the rocket
was a huge financial disaster, all due to a minute software error. But this wasn't the first time data
conversion problems had plagued modern rocket technology. &gt;&gt; JACK GANSSLE: In 1991, with the start
of the first Gulf War, the Patriot missile experienced a similar kind
of a number conversion problem. As a result, 28 American soldiers were
killed and about 100 others wounded when the Patriot, which was supposed
to protect against incoming Scuds, failed to fire a missile. &gt;&gt; SPEAKER 1: When Iraq invaded Kuwait and
America launched Desert Storm in early 1991, Patriot missile batteries
were deployed to protect Saudi Arabia and Israel from Iraqi Scud
missile attacks. The Patriot is a US medium-range
surface-to-air system manufactured by the Raytheon company. &gt;&gt; THEODORE POSTOL: The size of the Patriot
interceptor itself is roughly 20-feet long. And it weighs about 2000 pounds. And it carries a warhead of about-- I think it's roughly 150 pounds. And the warhead itself is a
high explosive which has fragments around it. The casing of the warhead is designed
to act like buckshot. &gt;&gt; SPEAKER 1: The missiles are carried four
per container and are transported by a semi trailer. &gt;&gt; PHILIP COYLE: The Patriot anti-missile
system goes back at least 20 years now. It was originally designed as
an air defense missile to shoot down enemy airplanes. In the first Gulf War, when that war
came along, the Army wanted to use it to shoot down Scuds, not airplanes. The Iraqi air force was not
so much of a problem. But the Army was worried about Scuds. And so they tried to upgrade
the Patriot. &gt;&gt; SPEAKER 1: Intercepting an enemy missile
traveling at mach five was going to be challenging enough. But when the Patriot was rushed into
service, the Army was not aware of an Iraqi modification that made their
Scuds nearly impossible to hit. &gt;&gt; THEODORE POSTOL: What happened
is the Scuds that were coming in were unstable. They were wobbling. The reason for this was the Iraqis, in
order to get 600 kilometers out of a 300-kilometer-range missile, took
weight out of the front warhead. They made the warhead lighter. So now the Patriot's trying
to come at the Scud. And most of the time, the overwhelming
majority of the time, it would just fly by the Scud. &gt;&gt; SPEAKER 1: Once the Patriot system
operators realized the Patriot missed its target, they detonated the Patriots
warhead to avoid possible casualties if it was allowed
to fall to the ground. &gt;&gt; THEODORE POSTOL: That was what most
people saw as big fireballs in the sky and misunderstood as intercepts
of Scud warheads. &gt;&gt; SPEAKER 1: Although in the night skies
Patriots appeared to be successfully destroying Scuds, at Dhahran there
could be no mistake about its performance. There, the Patriot's radar system lost
track of an incoming Scud and never launched due to a software flaw. &gt;&gt; It was the Israelis who first discovered
that the longer the system was on, the greater the time discrepancy
became due to a clock embedded in the system's computer. &gt;&gt; JACK GANSSLE: About two weeks before the
tragedy in Dhahran, the Israelis reported to the Defense Department
that the system was losing time. After about eight hours of running,
they noticed that the system is becoming noticeably less accurate. The Defense Department responded by
telling all of the Patriot batteries to not leave the systems
on for a long time. They never said what a long time was. Eight hours? 10 hours? 1,000 hours? Nobody knew. &gt;&gt; SPEAKER 1: The Patriot battery stationed
at the barracks at Dhahran and its flawed internal clock had been
on over 100 hours on the night of February 25th. &gt;&gt; JACK GANSSLE: It tracked time to an
accuracy of about a tenth of a second. Now, a tenth of a second is an
interesting number because it can't be expressed in binary exactly, which means
it can't be expressed exactly in any modern digital computer. It's hard to believe, but
use this as an example. Let's take the number one third. One third cannot be expressed
in decimal exactly. One third is 0.333 going
on for infinity. There's no way to do that with
absolute accuracy in decimal. That's exactly the same kind of problem
that happened in the Patriot. The longer the system ran, the
worst the time error became. &gt;&gt; SPEAKER 1: After 100 hours of operation,
the error in time was only about one third of a second. But in terms of targeting a missile
traveling at mach five, it resulted in a tracking error of over 600 meters. It would be a fatal error for
the soldiers at Dhahran. &gt;&gt; THEODORE POSTOL: What happened is a
Scud launch was detected by early warning satellites. And they knew that the Scud was coming
in their general direction. They didn't know where it was coming. &gt;&gt; SPEAKER 1: It was now up to the radar
component of the Patriot system defending Dhahran to locate and keep
track of the incoming enemy missile. &gt;&gt; JACK GANSSLE: The radar
was very smart. It would actually track the position of
the Scud and then predict where it probably would be the next time
the radar sent a pulse out. That was called the range gate. &gt;&gt; THEODORE POSTOL: Then once the Patriot
decides enough time has passed to go back and check the next location for
this detected object, it goes back. So when it went back to the wrong
place, it then sees no object. And it decides that there was no object,
it was a false detection, and drops the track. &gt;&gt; SPEAKER 1: The incoming Scud disappeared
from the radar screen, and seconds later it slammed
into the barracks. The Scud killed 28 and was the last one
fired during the first Gulf War. Tragically, the updated software arrived
at Dhahran the following day. The software flaw had been fixed,
closing one chapter in the troubled history of the Patriot missile. &gt;&gt; Patriot is actually an acronym
for Phased Array TRacking Intercept Of Target. &gt;&gt; DAVID J MALAN: All right, so a
sobering example, to be sure. And fortunately, these lower level
bugs are not something that we'll typically have to appreciate, certainly
not with some of our earliest of programs. Rather, most of the bugs you'll
encounter will be logical in nature, syntactic in nature whereby the
code just doesn't work right. And you know it pretty fast. &gt;&gt; But particularly when we get to the
end of the semester, it's going to become more and more of a possibility to
really think hard about the design of your programs and the underlying
representation there, too, of the data. For instance, we'll introduce MySQL,
which is a popular database engine that you can use with websites to
store data on the back end. And you'll have to start to decide at
the end of the semester not only what types of data along these lines to use
but exactly how many bits to use, whether or not you want to store dates
as dates and times as times, and also things like how big do you want the
unique IDs to be for, say, the users in your database. &gt;&gt; In fact, if some of you have had
Facebook accounts for quite some time, and you know how to get access
to your User ID-- which sometimes shows up in your
profile's URL unless you've chosen a nickname for the URL, or if you've
used Facebook's Graph API, the publicly available API by which you
can ask Facebook for raw data-- you can see what your numeric ID is. And some years ago, Facebook essentially
had to change from using the equivalent of ints to using long
long because over time as users come and go and create lots of accounts and
fake accounts, even they very easily were able to exhaust something like a 4
billion possible value like an int. &gt;&gt; So more on those kinds of issues
down the road, as well. All right, so that was casting. That was imprecision. A couple of quick announcements. So sections formally begin this coming
Sunday, Monday, Tuesday. You'll hear via email later this week
as to your section assignment. And you'll also here at that point how
to change your section if your schedule has now changed or your
comfort level has now changed. Meanwhile P-set one and hacker one are
due this Thursday with the option to extend that deadline per the
specifications to Friday in a typical way. &gt;&gt; Realize that included with the problem
set specifications are instructions on how to use the CS50 appliance, make,
as well as some CS50 specific tools like style 50, which can provide you
with feedback dynamically on the quality of your code style and also
check 50, which can provide you with dynamic feedback as to your
code's correctness. Forgive that we're still ironing
out a few kinks with check 50. A few of your classmates who did start
around four AM on Friday night when the spec went up have noticed since then
a few bugs that we are working through, and apologies for anyone who
has experienced undue frustrations. The fault is mine. But we'll follow up on the CS50
discuss when that is resolved. &gt;&gt; So a word on scores themselves. So it'll be a week or two before you
start to get feedback on problem sets because you don't yet have
a teaching fellow. And even then, we will start to evaluate
the C problem sets before we go back and evaluate scratch so
that you get more relevant feedback more quickly. But in general per the syllabus, CS50
problem sets are evaluated along the following four axes-- scope, correctness, design, and style. &gt;&gt; Scope is going to be a number typically
between zero and five that captures how much of the
piece that you bit off. Typically, you want this to be five. You at least tried everything. And notice it's a multiplicative factor
so that doing only part of the problem set is not the best strategy. &gt;&gt; Meanwhile, more obvious is the
importance of correctness-- just is your program correct with
respect to the specification? This is weighted deliberately more
heavily than the other two axes by a factor of three because we recognize
that typically you're going to spend a lot more time chasing down some bugs,
getting your code to work, then you are indenting it and choosing
appropriate variable names and the like, which is on the other end
of the spectrum of style. &gt;&gt; That's not to say style is not
important, and we'll preach it over time both in lectures and in sections. Style refers to the aesthetics
of your code. Have you chosen well-named variables
that are short but somewhat descriptive? Is your code indented as you've seen in
lecture and in a manner consistent with style 50? &gt;&gt; Lastly is design right
there in the middle. Design is the harder one to put a
finger on because it's much more subjective. But it's perhaps the most important of
the three axes in terms of pedagogical value over time and that this will be
the teaching fellow's opportunity to provide you with qualitative feedback. Indeed, in CS50 even though we do have
these formulas and scores, at the end of the day these are very deliberately
very small buckets-- point values between zero and three
and zero and five. We don't try to draw very coarse lines
between problem sets or between students but rather focus as much as
we can on qualitative, longhand feedback, either typed or verbal from
your particular teaching fellow, you'll get to know quite well. &gt;&gt; But in general, those are the weights
that the various axes will have. Meanwhile, too, it's worth keeping in
mind that you should not assume that a three out of five is a 60% and
therefore roughly failing. Three is deliberately meant to be
sort of middle of the road good. If you're getting threes at the
beginning of the semester, that's indeed meant to be a good
place to begin. If you're getting twos, fairs, there's
definitely some work to pay a little more attention, to take advantage
of sections and office hours. &gt;&gt; If you're getting fours
and fives, great. But really, we hope to see trajectories
among students-- very individualized per student, but starting
the semester here in sort of the two to the three range but ending
up here in the four to five range. That's what we're really looking for. And we do keep in mind the delta that
you exhibit between week zero and week 12 when I'm doing grades. It doesn't matter to us absolutely how
you fair at the beginning if your trajectory is indeed
upward and strong. &gt;&gt; Academic honesty-- so let me put on my
more serious voice for just a moment. So this course has the distinction of
sending more students than any other in history to the ad board, I believe. We have sort of lost count at this
point of how often this happens. And that's not because students in 50
are any more dishonest than their classmates elsewhere. But realize, too, that we are very good
at detecting this sort of thing. &gt;&gt; And that is the advantage that a
computer science class has in that we can and we do compare all students
problem sets pair-wise against every other, not only this year
but all prior years. We have the ability, like students in
the class, to Google and to find code on sites like github and
discussion forums. There are absolutely solutions to CS50's
p-sets floating around there. But if you can find them,
we can find them. And all of this is very much automated
and easy and sad for us to find. &gt;&gt; But I want to emphasize, too, that the
course's academic honesty policy is very much meant to be very much
the opposite of that spirit. Indeed, this year we've rephrased things
in the syllabus to be this, dot dot dot, with more detail
in the syllabus. But the overarching theme in the course
really is to be reasonable. We recognize that there is a significant
amount of pedagogical value in collaborating, to some extent,
with classmates, whereby you two or you three or you more are
standing at a white board whiteboarding, so to
speak, your ideas-- writing out pseudocode in pictures,
diagramming what should Mario be if you were to write it first
in pseudocode. What should the greedy algorithm-- how should it behave per
problem sets one? &gt;&gt; And so realize that behavior
that we encourage is very much along those lines. And in the syllabus, you'll see a
whole bunch of bullets under a reasonable category and a not reasonable
category that helps us help you wrap your mind around where
we do draw that line. And in general, a decent rule of thumb
is that if you are struggling to solve some bug and your friend or classmate
is sitting next to you, it is reasonable for you to show him or her
your code and say, hey, can you help me figure out what's going wrong here? &gt;&gt; We don't typically embrace
the opposite side. It is not a correct response for your
friend or classmate here to say, oh, just look at mine and figure
it out from that. That is sort of unreasonable. But having someone else, another brain,
another pair of eyes look at your screen or look at your code
and say, are you sure you want to have a loop here? Or are you sure you want
that semicolon here? Or oh, that error message means this. Those are very reasonable and
encouraged behaviors. &gt;&gt; The cases to which I was alluding to
earlier boil down to when students are late at night making poor judgment
decisions and emailing their code to someone else or just saying,
here, it's in Dropbox or Googling late at night. And so I would encourage and beg of you,
if you do have those inevitable moments of stress, you're bumping up
against the deadline, you have no late day since it's already Friday at that
point, email the course's heads or myself directly. Say, listen, I'm at my
breaking point here. Let's have a conversation
and figure it out. Resorting to the web or some other not
reasonable behavior is never the solution, and too many of your
classmates are no longer here on campus because of that poor judgment. But it's very easy to skirt that line. &gt;&gt; And here is a little picture to cheer
you up from Reddit so that now everything will be OK. &gt;&gt; So a quick recap, then,
of where we left off. So last week, recall that we introduce
conditions, not in Scratch but in C this time. And there was some new syntax but
really no new ideas per se. We had Boolean expressions that we could
or together with two vertical bars or and together with two
ampersands, saying that both the left and the right must be true
for this to execute. Then we had switches, which we looked
at briefly, but I propose are really just different syntax for achieving the
same kind of goal if you know in advance what your cases
are going to be. &gt;&gt; We looked at loops. A for loop is maybe the most common,
or at least the one that people typically reach for instinctively. Even though it looks a little cryptic,
you'll see many, many examples of this before long, as you have
already late last week. While loops can similarly
achieve the same thing. But if you want to do any incrementation
or updating of variable's values, you have to
do it more manually than the for loop before allows. And then there's the do-while loop,
which allows us to do something at least once while something
else is true. And this is particularly good for
programs or for games where you want to prompt the user for something
at least once. And then if he or she doesn't cooperate,
you might want to do it again and again. &gt;&gt; With variables, meanwhile, we had lines
of code like this, which could be two lines. You could declare an int called
counter, semicolon. Or you can just declare and
define it, so to speak. Give it a value at the same time. &gt;&gt; And then lastly, we talked
about functions. And this was a nice example in
the sense that it illustrates two types of functions. One is GetString(), which, again,
gets a string from the user. But GetString() is kind of interesting,
so far as we've used it, because we've always used it with
something on the left-hand side of an equal sign. That is to say that GetString()
returns a value. It returns, of course, a string. And then on the left-hand side, we're
simply saving that string inside of a variable called name. &gt;&gt; This is different, in a sense, from
printf because printf, at least in our usage here, does not return anything. As an aside, it does return something. We just don't care what it is. But it does have what's
called a side effect. And what is that side effect in every
case we've seen thus far? What does printf do? It prints something to the screen,
displays text or numbers or something on the screen. And that's just considered a side effect
because it's not really handing it back to me. It's not an answer inside of
a black box that I can then reach into and grab. It's just doing it on its own, much
like Colton was plugged into this black box last week, and he somehow
magically was drawing on the board without me actually involved. That would be a side effect. But if I actually had to reach back in
here and say, oh, here is the string from the user, that would
be a return value. &gt;&gt; And thus far we've only used functions
that other people have written. But we can actually do these
kinds of things ourselves. So I'm going to go into the
CS50 appliance again. Let me close the tab that we
had open a moment ago. And let me go ahead and
create a new file. And I'm going to go ahead and
call this one positive.c. So I want to do something with
positive numbers here. So I'm going to go ahead and do int-- sorry-- #include . Let's not make that same
mistake as before. Int main (void), open curly
brace, closed curly brace. &gt;&gt; And now I want to do the following. I want to write a program that
insists that the user gives me a positive integer. So there is no GetPositiveInt function
in the CS50 library. There's only GetInt(). But that's OK because I have the
constructs with which I can impose a little more constraint on that value. I could do something like this. &gt;&gt; So int n-- and if you're typing along, just realize
I'm going to go back and change some things in a moment-- so int n equals GetInt(). And that's going to put
an int inside of n. And let me be a more descriptive. Let me say something like I demand that
you give me a positive integer. &gt;&gt; All right. So just a little bit of instructions. And now what can I do? Well, I already know from my simple
conditions or branches, just like I had in Scratch, I could say something
like if n is less than or equal to zero, then I want to do something
like, that is not positive. And then I could do-- OK, but I really want to get that int. So I could go up here and I could kind
of copy this and indent this. And then, OK. So if n is less than or
equal to zero do this. &gt;&gt; Now, what if the user
doesn't cooperate? Well, then I'm going to
borrow this here. And then I go in here
and here and here. So this is clearly not
the solution, right? Because there's no end in sight. If I want to demand that the user gives
me a positive integer, I can actually get the int. I can then check for that int. But then I want to check it again and
check it again and check it again. So obviously, what's the better
construct to be using here? All right, so some kind of loop. &gt;&gt; So I'm going to get rid
of almost all of this. And I want to get this
int at least once. So I'm going to say do-- and I'll come back to the
while in just a moment-- now, do what? I'm going to do int n gets GetInt(). OK. So that's pretty good. And now how often do
I want to do this? &gt;&gt; Let me put the printf inside of the loop
so I can demand again and again, if need be. And what do I want this
while condition to do? I want to keep doing this
while what is the case? Yeah. N is less than or equal to zero. So already, we've significantly
cleaned this code up. We've borrowed a very simple construct--
the do-while loop. I've stolen just the important lines
of code that I started copying and pasting, which was not wise. And so now I'm going to actually paste
it in here and just do it once. &gt;&gt; And now what do I want to do at
the very end of this program? I'll just say something simple
like, thanks for the-- and I'll do %i for int-- backslash n, comma, and then
plug in n, semicolon. &gt;&gt; All right. So let's see what happens now
when I run this program. I'm going to go ahead and
do make positive. Damn. A few errors. So let me scroll back up to the first. Don't work through them backwards. Work through them from top down
lest they cascade and only one thing be wrong. Implicit declaration of
function GetInt(). Yeah. So it wasn't enough. I kind of made the same mistake but
a little different this time. I need to not only include stdio.h but
also cs50.h, which includes the so-called declarations of get int, which
teach the appliance, or teaches C what GetInt() is. &gt;&gt; So let me resave. I'm going to ignore the other errors
because I'm going to hope that they're somehow related to the error
I already fixed. So let me go ahead and recompile
with make positive, Enter. Damn. Three errors, still. Let me scroll up to the first. Unused variable n. We've not seen this before. And this, too, is a little cryptic. This is the output of the compiler. And what that highlighted line
there-- positive.c:9:13-- is saying, it's saying on line nine of
positive.c, at the 13th character, 13th column, you made this mistake. &gt;&gt; And in particular, it's telling
me unused variable n. So let's see-- line nine. I'm using n in the sense that
I'm giving it a value. But what the compiler doesn't like is
that I'm not seemingly using it. But wait a minute, I am using it. In line 11, I'm using it here. But if I scroll down further
at positive.c:11-- so at line 11, character 12, the
compiler's telling me, use of undeclared identifier n. &gt;&gt; So undeclared means I have
not specified it as a variable with a data type. But wait a minute. I did exactly that in line nine. So someone is really confused here. It's either me or the compiler because
in line nine, again, I'm declaring an int n, and I'm assigning it the
return value of GetInt(). Then I'm using that variable n in line
11 and checking if its value is less than or equal to zero. But this apparently is
bad and broken why? Say it again? &gt;&gt; Ah, I have to declare n before
entering the loop. But why? I mean, we just proposed a bit ago that
it's fine to declare variables all on one line and then
assign them some value. A global variable-- let's come back
to that idea in just a moment. Why do you want me to put
it outside of the loop? It is. Exactly. &gt;&gt; So, albeit, somewhat counterintuitive,
let me summarize. When you declare n inside
of the do block there-- specifically inside of
those curly braces-- that variable n has what's
called a scope-- unrelated to our scoring system in the
course-- but has a scope that's limited to those curly braces. In other words, typically if you declare
a variable inside a set of curly braces, that variable only exists
inside of those curly braces. So by that logic alone, even though
I've declared n in line nine, it essentially disappears from scope,
disappears from memory, so to speak, by the time I hit line 11. Because line 11, unfortunately, is
outside of those curly braces. &gt;&gt; So I unfortunately can't fix this by
going back to what I did it before. You might at first do this. But what are you now not
doing cyclically? You're obviously not getting
the int cyclically. So we can leave the GetInt(), and we
should leave the GetInt() inside the loop because that's what we want to
pester the user for again and again. But it does suffice to go
up to line, say, six. Int n, semicolon. Don't give it a value yet because
you don't need to just yet. &gt;&gt; But now down here, notice-- this
would be a very easy mistake. I don't want to shadow my previous
declaration of n. I want to use the n that
actually exists. And so now in line 10,
I assign n a value. But in line six, I declare n. And so can I or can I not
use it in line 12 now? I can because between which curly
braces is n declared now? The one up here on line five. To one here on line 14. So if I now zoom out, save this file, go
back into and run make positive, it compiled this time. So that's already progress. Slash. ./positive, Enter. &gt;&gt; I demand that you give me
a positive integer. Negative 1. Negative 2. Negative 3. Zero. One. And thanks for the one is
what's now printed. &gt;&gt; Let me try something else,
out of curiosity. I'm being told to input an integer. But what if I instead type in lamb? So you now see a different prompt-- retry. But nowhere in my code
did I write retry. So where, presumably, is this retry
prompt coming from, would you say? Yeah, from GetInt() itself. So one of the things CS50's staff does
for you, at least in these first few weeks, is we have written some amount
of error checking to ensure that if you call GetInt(), you will at least
get back an int from the user. You won't get a string. You won't get a char. You won't get something
else altogether. You'll get an int. &gt;&gt; Now, it might not be positive. It might not be negative. We make no guarantees around that. But we will pester the user to retry,
retry, retry until he or she actually cooperates. Similarly, if I do 1.23,
that is not an int. But if I do type in, say, 50, that
gives me a value that I wanted. &gt;&gt; All right. So not bad. Any questions on what we've just done? The key takeaway being, to be clear, not
so much the loop, which we've seen before even though we haven't really
used it, but the issue of scope, where variables can only be can only be used
within some specified scope. &gt;&gt; All right, let me address the suggestion
you made earlier, that of a global variable. As an aside, it turns out that another
solution to this problem, but typically an incorrect solution or
a poorly designed solution, is to declare your variable as what's
called a global variable. Now I'm kind of violating my definition
of scope because there are no curly braces at the very top
and the very bottom of a file. But the implication of that
is that now in line four, n is a global variable. And as the name implies, it's
just accessible everywhere. &gt;&gt; Scratch actually has these. If you used a variable, you might recall
you had to choose if it's for this sprite or for all sprites. Well, all sprites is just the clearer
way of saying global. Yeah? Ah, really good question. &gt;&gt; So recall that in the very first version
of my code, when I incorrectly declared and defined n in line nine-- I declared it as a variable
and I gave it a value with the assignment operator-- this gave me two errors. One, the fact that n wasn't used,
and two, that in line 11 it just wasn't declared. So the first one I didn't
address at the time. It is not strictly an error to declare
a variable but not use it. But one of the things we've done in
the CS50 appliance, deliberately, pedagogically, is we've cranked up the
expectations of the compiler to make sure that you're doing things not just
correctly but really correctly. &gt;&gt; Because if you're declaring a variable
like n and never using it, or using it correctly, then what
is it doing there? It truly serves no purpose. And it's very easy over time, if you
don't configure your own computer in this way, to just have code that has
little remnants here, remnants there. And then months later you look back and
you're like, why is this line of code there? And if there's no good reason, it
doesn't benefit you or your colleagues down the road to have to
stumble over it then. &gt;&gt; As an aside, where is
that coming from? Well, recall that every time we compile
program, all of this stuff is being printed. So we'll come back to this. But again, make is a utility that
automates the process of compiling by running the actual compiler
called clang. This thing, we'll eventually see, has
to do with debugging with a special program called the debugger. This has to do with optimizing the
code-- more on that in future. Std=c99-- this just means use the 1999 version of
C. C's been around even longer than that, but they made some nice
changes 10 plus years ago. &gt;&gt; And here's the relevant ones. We are saying make anything that
previously would have been a warning an error preventing the student
from compiling. And wall means do that for a
whole bunch of things, not just related to variables. And then let me scroll to
the end of this line. And this, too, we'll eventually
come back to. This is obviously the name of
the file I'm compiling. This recalls the name of the file
I'm outputting as the name of my runnable program. This -lcs50 just means use the CS50
library, and any zeros and ones that the staff wrote and compiled earlier
this year, integrate them into my program. &gt;&gt; And anyone know what -lm is? It's the math library, which is
just there even if you're not doing any math. It's just automatically provided
to us by make. &gt;&gt; Well, let me do one other example
here by opening up a new file. And let me save this one as string.c. It turns out that as we talk about data
types today, there's even more going on underneath the hood
than we've seen thus far. So let me quickly do a quick program. Include stdio.h. And I'll save that. And you know, let me not make the
same mistake again and again. Include cs50.h. And let me go ahead now
and do int main(void). &gt;&gt; And now I simply want to do a program
that does this-- declare a string called s and get a string
from the user. And let me do a little
instructions here-- please give me a string-- so
the user knows what to do. And then down here below this,
I want to do the following-- for int i gets zero. Again, computer scientists typically
start counting at zero, but we could make that one if we really wanted. Now I'm going to do i is less
than the string length of s. So strlen-- S-T-R-L-E-N-- again, it's concise because it's easier
to type, even though it's a little cryptic. &gt;&gt; That is a function we've not used
before but literally does that-- return to me a number that represents
the length of the string that the user typed. If they typed in hello, it would return
five because there's five letters in hello. Then, on each iteration of
this loop, i plus plus. So again, a standard construct even if
you're not quite too comfortable or familiar with it yet. &gt;&gt; But now on each iteration of this loop,
notice what I'm going to do. I want to go ahead and print
out a single character-- so %c backslash n on a new line. And then, you know what I want to do? Whatever the word is that the user types
in, like hello, I want to print H-E-L-L-O, one character per line. In other words, I want to get at the
individual characters in a string, whereby up until now a string has just
been a sequence of characters. &gt;&gt; And it turns out I can do s, bracket,
i, close bracket, close parenthesis, semicolon. And I do have to do one more thing. It's in a file called string.h
that strlen is declared. So if I want to use that function,
I need to tell the compiler, expect to use it. Now let me go ahead and make
the program called string. Dot, slash, string. &gt;&gt; Please give me a string. I'll go ahead and type it. Hello, in all caps, Enter. And now notice I've printed this
one character after the other. So the new detail here is that a string,
at the end of the day, can be accessed by way of its individual
characters by introducing the square bracket notation. And that's because a string underneath
the hood is indeed a sequence of characters. But what's neat about them is
in your computer's RAM-- Mac, PC, whatever it is-- they're
literally back to back to back-- H-E-L-L-O-- at individual, adjacent
bytes in memory. &gt;&gt; So if you want to get at the eighth such
byte, which in this loop would be bracket zero, bracket one, bracket two,
bracket three, bracket four-- that's zero indexed up until five-- that will print out H-E-L-L-O
on its own line. &gt;&gt; Now, as a teaser, let me show you the
sorts of things you'll eventually be able to understand, at least
with some close looking. For one, what we included in today's
examples, if you'd like, is actually one of the very first jailbreaks
for the iPhone. Jailbreaking means cracking the phone
so you can actually use it on a different carrier or install
your own software. And you'll notice this looks completely
cryptic, most likely. But look at this. The iPhone was apparently cracked with
a for loop, an if condition, an else condition, a bunch of functions
we've not seen. &gt;&gt; And again, you won't at
first glance probably understand how this is working. But everything that we sort of take
for granted in our modern lives actually tends to reduce even to some
of these fundamentals we've been looking at. Let me go ahead and open one
other program, holloway.c. So this, too, is something you
shouldn't really know. Even none of the staff or I could
probably figure this out by looking at it because this was someone's code
that was submitted to what's historically known as an obfuscated C
contest, where you write a program that compiles and runs but is so damn
cryptic no human can understand what it's going to do until
they actually run it. &gt;&gt; So indeed, if you look at this
code, I see a switch. I see main. I see these square brackets implying
some kind of an array. Does anyone want to guess what
this program actually does if I run Holloway? Yes. OK. Well done. So only the staff and I cannot figure
out what these things do. &gt;&gt; And now lastly, let me go ahead
and open up one other program. This one-- again, we'll make the source code
available online-- this one's just kind of pretty to look at. All they did is hit the
space bar quite a bit. But this is real code. So if you think that's pretty, if we
actually run this at the prompt, eventually you'll see how we
might do things like this. &gt;&gt; So we'll leave you on that note
and see you on Wednesday. &gt;&gt; [MUSIC PLAYING] &gt;&gt; SPEAKER 2: At the next CS50,
the TFs stage a mutiny. &gt;&gt; SPEAKER 3: There he is. Get him! &gt;&gt; [MUSIC PLAYING]

Calculus

The following content is
provided under a Creative Commons license. Your support will help
MIT OpenCourseWare continue to offer high quality
educational resources for free. To make a donation or to
view additional materials from hundreds of
MIT courses, visit MITOpenCourseWare@OCW.MIT.edu. PHILIPPE RIGOLLET: It's
because if I was not, this would be basically the
last topic we would ever see. And this is arguably, probably
the most important topic in statistics, or at
least that's probably the reason why most of
you are taking this class. Because regression
implies prediction, and prediction is what people
are after to now, right? You don't need to
understand what the model for the
financial market is if you actually
have a formula to predict what the stock
prices are going to be tomorrow. And regression, in a way,
allows us to do that. And we'll start with a very
simple version of regression, which is linear regression,
which is the most standard one. And then we'll move on to
slightly more advanced notions such as nonparametric
regression. At least, we're going to see
the principles behind it. And I'll touch upon a little bit
of high dimensional regression, which is what people
are doing today. So the goal of
regression is to try to predict one variable
based on another variable. All right, so here the
notation is very important. It's extremely standard. It goes everywhere essentially,
and essentially you're trying to explain why
as a function of x, which is the usual y
equals f of x question-- except that, you know, if
you look at a calculus class, people tell you y equals
f of x, and they give you a specific form for f,
and then you do something. Here, we're just going
to try to estimate what this length function is. And this is why we often
call y the explained variable and x the explanatory variable. All right, so we're
statisticians, so we start with data. All right, then what
does our data look like? Well, it looks like a
bunch of input, output to this relationship. All right, so we have
a bunch of xi, yi. Those are pairs, and I can do
a scatterplot of those guys. So each point here has a
x-coordinate, which is xi, and a y-coordinate,
which is yi, and here, I have a bunch of endpoints. And I just draw them like that. Now, the functions we're
going to be interested in are often function of the form
y equals a plus b times x, OK. And that means that this
function looks like this. So if I do x and
y, this function looks exactly like a line,
and clearly those points are not on the line. And it will basically
never happen that those points are on a line. There's a famous
T-shirt from, I think, U.C. Berkeley's
staff department, that shows this picture
and put a line between them like we're going to see it. And it says, oh,
statisticians, so many points, and you still managed
to miss all of them. And so essentially, we don't
believe that this relationship y is equal to a plus bx is true,
but maybe up to some noise. And that's where the statistics
is going to come into play. There's going to be some random
noise that's going to play out, and hopefully the noise is
going to be spread out evenly, so that we can average it
if we have enough points. Average it out, OK. And so this epsilon here is not
necessarily due to randomness. But again, just like we did
modeling in the first place, it essentially
accounts for everything we don't understand
about this relationship. All right, so for example-- so here, I'm not going to be-- give me one second, so we'll
see an example in a second. But the idea here is
that if you have data, and if you believe
that it's of the form, a plus b x plus
some noise, you're trying to find the line
that will explain your data the best, right? In the terminology
we've been using before, this would be the most likely
line that explains the data. So we can see that
it's slightly-- we've just added
another dimension to our statistical problem. We don't have just x's,
but we have y's, and we're trying to find the most likely
explanation of the relationship between y and x. All right, and so
in practice, the way it's going to look like is that
we're going to have basically two parameters to
find the slope b and the intercept
a, and given data, the goal is going to be to try
to find the best possible line. All right? So what we're going
to find is not exactly a and b, the ones that
actually generate the data, but some estimators of those
parameters, a hat and b hat constructed from the data. All right, so we'll see
that more generally, but we're not going to go too
much in the details of this. There's actually
quite a bit that you can understand if
you do what's called univariate regression
when x is actually a real valued random variable. So when this happens, this is
called univariate regression. And when x is in rp for p
larger than or equal to 2, this is called
multivariate regression. OK, and so here we're
just trying to explain y is a plus bx plus epsilon. And here we're going to have
something more complicated. We're going to have y, which is
equal to a plus b1, x1 plus b2, x2 plus bp, xp plus epsilon-- where x is equal to-- the coordinates of x are
given by x1, 2xp, rp. OK, so it's still linear. Right, they still add
all the coordinates of x with a coefficient
in front of them, but it's a bit more complicated
than just one coefficient for one coordinate of x, OK? So we'll come back to
multivariate regression. Of course, you can write
this as x transpose b, right? So this entire thing here,
this linear combination is of the form x
transpose b, where b is the vector that has
coordinates b1 to bp. OK? Sorry, here, it's in [? rd, ?]
p is the natural notation. All right, so our goal
here, in the univariate one, is to try to write
the model, make sense of this little twiddle here-- essentially, from a
statistical modeling question, the question is going to be,
what distributional assumptions do you want to put on epsilon? Are you going to say
they're Gaussian? Are you going to say
they're binomial? OK, are you going to
say they're binomial? Are you going to say
they're Bernoulli? So that's going to be what we
we're going to make sense of, and then we're going
to try to find a method to estimate a and b. And then maybe we're
going to try to do some inference about a and b-- maybe test if a and b take
certain values, if they're less than something,
maybe find some confidence regions for a and b, all right? So why would you
want to do this? Well, I'm sure all of you have
an application, if I give you some x, you're trying
to predict what y is. Machine learning is all
about doing this, right? Without maybe trying
to even understand the physics behind
this, they're saying, well, you give me
a bag of words, I want to understand whether
it's going to be a spam or not. You give me a bunch of
economic indicators, I want you to tell me how much
I should be selling my car for. You give me a bunch of
measurements on some patient, I want you to predict
how this person is going to respond to my
drug-- and things like this. All right, and often we actually
don't have much modeling intuition about what the
relationship between x and y is, and this linear thing is
basically the simplest function we can think of. Arguably, linear functions
are the simplest functions that are not trivial. Otherwise, we would just say,
well, let's just predict x of y to be a constant, meaning
it does not depend on x. But if you want it
to depend on x, then your functions are basically
as simple as it gets. It turns out, amazingly, this
does the trick quite often. So for example, if
you look at economics, you might want to assume
that the demand is a linear function of the price. So if your price
is zero, there's going to be a certain demand. And as the price increases,
the demand is going to move. Do you think b is going to
be positive or negative here? What? Typically, it's
negative unless we're talking about
maybe luxury goods, where you know,
the more expensive, the more people
actually want it. I mean, if we're talking
about actual economic demand, that's probably
definitely negative. It doesn't have to be,
you know, clearly linear, so that you can actually
make it linear, transform it into something linear. So for example, you have
this like multiplicative relationship, PV equals nRT,
which is the Ideal gas law. If you want to actually
write this relationship, if you want to predict
what the pressure is going to be as a function of
the volume and the temperature-- and well, let's assume that
n is the Avogadro constant, and let's assume that the
radius is actually fixed. Then you take the log on each
side, so you get PV equals nRT. So what that means is that
log PV is equal to log nRT. So that means log P plus log V
is equal to the log nR plus log T. So we said that R is
constant, so this is actually your constant. I'm going to call it a. And then that
means that log P is equal to minus log V. That
log P is equal to a minus log V plus log T. OK? And so in particular, if I
write b equal to negative 1 and c equal to plus 1,
this gives me the formula that I have here. Now again, it might be the case
that this is the ideal gas law. So in practice, if I
start recording pressure, and temperature, and volume, I
might make measurement errors, there might be slightly
different conditions in such a way that I'm not
going to get exactly those. And I'm just going to
put this little twiddle to account for the fact
that the points that I'm going to be recording
for log pressure, log volume, and log
temperature are not going to be exactly on one line. OK, they're going to be close. Actually, in those
physics experiments, usually, they're very close
because the conditions are controlled under
lab experiments. So it means that the
noise is very small. But for other cases,
like demand and prices, it's not a law of physics,
and so this must change. Even the linear structure is
probably not clear, right. At some points,
there's probably going to be some weird
curvature happening. All right, so this slide is
just to tell you maybe you don't have, obviously,
a linear relationship, but maybe you do
if you start taking logs exponentials, squares. You can sometimes take the
product of two variables, things like this, right. So this is variable
transformation, and it's mostly
domain-specific, so we're not going to go into
more details of this. Any questions? All right, so now I'm
going to be giving-- so if we start thinking
a little more about what these coefficients
should be, well, remember-- so
everybody's clear why I don't put the little i here? Right, I don't put the
little i because I'm just talking about a generic
x and a generic y, but the observations
are x1, y1, right. So typically, on
the blackboard I'm often going to write only xy,
but the data really is x1, y1, all the way to xn, yn. So those are those points in
this two dimensional plot. But I think of those as being
independent copies of the pair xy. They have to have-- to contain their relationship. And so when I talk
about distribution of those random variables, I
talk about the distribution of xy, and that's the same. All right, so the first
thing you might want to ask is, well, if I have an
infinite amount of data, what can I hope to
get for a and b? If my simple size
goes to infinity, then I should actually
know exactly what the distribution of xy is. And so there should
be an a and a b that captures this linear
relationship between y and x. And so in particular,
we're going to try to ask the population,
or theoretic, values of a and b, and you can see that
you can actually compute them explicitly. So let's just try to find how. So as I said, we have
a bunch of points on this line close
to a line, and I'm trying to find the best fit. All right, so this
guy is not a good fit. This guy is not a good fit. And we know that this guy
is a good fit somehow. So we need to mathematically
formulate the fact that this line here is
better than this line here or better than this line here. So what we're trying to do
is to create a function that has values that are
smaller for this curve and larger for these two curves. And the way we do it is
by measuring the fit, and the fit is essentially
the aggregate distance of all the points to the curve. And there's many
ways I can measure the distance to a curve. So if I want to find so--
let's just open a parenthesis. If I have a point
here-- so we're going to do it for
one point at a time. So if I have a point,
there's many ways I can measure its distance
to the curve, right? I can measure it like that. That is one distance
to the curve. I can measure it like that by
having a right angle here that is one distance to the curve. Or I can measure it like that. That is another distance
to the curve, right. There's many ways
I can go for it. It turns out that
one is actually going to be fairly
convenient for us, and that's the one that says,
let's look at the square of the value of x on the curve. So if this is the curve,
y is equal to a plus bx. Now, I'm going to think of
this point as a random point, capital X, capital
Y, so that means that it's going to be x1,
y1 or x2, y2, et cetera. Now, I want to
measure the distance. Can somebody tell me
which of the three-- the first one, the second
one, or the third one-- this formula, expectation of y
minus a minus bx squared is-- which of the three
is it representing? AUDIENCE: The second one. PHILIPPE RIGOLLET:
The second one where I have the right angle? OK, everybody agrees with this? Anybody wants to vote
for something else? Yeah? AUDIENCE: The third one? PHILIPPE RIGOLLET:
The third one? Everybody agrees
with the third one? So by default, everybody's
on the first one? Yeah, it is the vertical
distance actually. And the reason is if it was the
one with the straight angle, with the right angle,
it would actually be a very complicated
mathematical formula, so let's just see y, right? And by y, I mean y. OK, so this means that this
is my x, and this is my y. All right, so that means
that this point is xy. So what I'm measuring
is the difference between y minus
a plus b times x. This is the thing I'm going
to take the expectation off-- the square and then
the expectation-- so a plus b times x, if this is
this line, this is this point. So that's this value here. This value here is
a plus bx, right? So what I'm really
measuring is the difference between y and N plus bx,
which is this distance here. And since I like things
like Pythagoras theorem, I'm actually going
to put a square here before I take the expectation. So now this is a
random variable. This is this random variable. And so I want a number,
so I'm going to turn it into a deterministic number. And the way I do this is
by taking expectation. And if you think expectations
should be close to average, this is the same
thing as saying, I want that in
average, the y's are close to the a plus bx, right? So we're doing it
in expectation, but that's going to
translate into doing it in average for all the points. All right, so this is the
thing I want to measure. So that's this
vertical distance. Yeah? OK. This is my fault actually. Maybe we should
close those shades. OK, I cannot do just
one at a time, sorry. All right, so now that I do
those vertical distances, I can ask-- well, now,
I have this function, right-- to have a function that
takes two parameters a and b, maps it to the expectation
of y minus a plus bx squared. Sorry, the square is here. And I could ask, well,
this is a function that measures the fit of the
parameters a and b, right? This function should be small. The value of this
function here, function of a and b that measures
how close the point xy is to the line a plus
b times x while y is equal to a plus b
times x in expectation. OK, agreed? This is what we just said. Again, if you're not
comfortable with the reason why you get expectations, just
think about having data points and taking the average
value for this guy. So it's basically an
aggregate distance of the points to their line. OK, everybody agrees this
is a legitimate measure? If all my points were on the
line-- if my distribution-- if y was actually equal
to a plus bx for some a and b then this function
would be equal to 0 for the correct a and b, right? If they are far--
well, it's going to depend on how much
noise I'm getting, but it's still going to be
minimized for the best one. So let's minimize this thing. So here, I don't make any-- again, sorry. I don't make an assumption on
the distribution of x or y. Here, I assume, somehow,
that the variance of x is not equal to 0. Can somebody tell me why? Yeah? AUDIENCE: Not really a
question-- the slides, you have y minus a minus bx
quantity squared expectation of that, and here you've written
square of the expectation. PHILIPPE RIGOLLET:
No, here I'm actually in the expectation
of the square. If I wanted to write the
square of the expectation, I would just do this. So let's just make it clear. Right? Do you want me to put an
extra set of parenthesis? That's what you want me to do? AUDIENCE: Yeah, it's just
confusing with the [INAUDIBLE] PHILIPPE RIGOLLET: OK, that's
the one that makes sense, so the square of the expectation? AUDIENCE: Yeah. PHILIPPE RIGOLLET: Oh, the
expectation of the square, sorry. Yeah, dyslexia. All right, any question? Yeah? AUDIENCE: Does this assume
that the error is Gaussian? PHILIPPE RIGOLLET: No. AUDIENCE: I mean, in
the sense that like, if we knew that the
error was, like, even the minus followed
like-- so even the minus x to the fourth distribution,
would we want to minimise the expectation of what
the fourth power of y minus a equals bx in order to get
[? what the ?] [? best is? ?] PHILIPPE RIGOLLET: Why? So you know the answers
to your question, so I just want you to
use the words that-- right, so why would you want
to use the fourth power? AUDIENCE: Well,
because, like, we want to more strongly
penalize deviations because we'd expect very
large deviations to be very rare, or more
rare, than it would with the Gaussian
[INAUDIBLE] power. PHILIPPE RIGOLLET: Yeah so,
that would be the maximum likely estimator that you're
describing to me, right? I can actually
write the likelihood of a pair of numbers ab. And if I know this,
that's actually what's going to come
into it because I know that the density is
going to come into play when I talk about there. But here, I'm just
talking about-- this is a mechanical tool. I'm just saying, let's minimize
the distance to the curve. Another thing I could have
done is take the absolute value of this thing, for example. I just decided to take the
square root before I did it. OK, so regardless
of what I'm doing, I'm just taking the
squares because that's just going to be convenient for me
to do my computations for now. But we don't have
any statistical model at this point. I didn't say anything--
that y follows this. X follows this. I'm just doing
minimal assumptions as we go, all right? So the variance of
x is not equal to 0? Could somebody tell me why? What would my cloud point
look like if the variance of x was equal to 0? Yeah, they would all
be at the same point. So it's going to be hard for
me to start fitting in a line, right? I mean, best case
scenario, I have this x. It has variance, zero, so
this is the expectation of x. And all my points have
the same expectation, and so, yes, I could
probably fit that line. But that wouldn't help
very much for other x's. So I need a bit of variance
so that things spread out a little bit. OK, I'm going to
have to do this. I think it's just my-- All right, so I'm going to
put a little bit of variance. And the other thing is here,
I don't want to do much more, but I'm actually going to think
of x as having means zero. And the way I do
this is as follows. Let's define x tilde, which is
x minus the expectation of x. OK, so definitely the
expectation of x tilde is what? Zero, OK. And so now I want to
minimize in ab, expectation of y minus a plus b, x squared. And the way I'm going to do this
is by turning x into x tilde and stuffing the extra-- and putting the extra
expectation of x into the a. So I'm going to write this as
an expectation of y minus a plus b expectation of x-- which I'm going to a tilde-- and plus b x tilde. OK? And everybody agrees with this? So now I have two
parameters, a tilde and b, and I'm going to pretend
that now x tilde-- so now the role of x is played
by x tilde, which is now a centered random variable. OK, so I'm going to
call this guy a tilde, but for my computations
I'm going to call it a. So how do I find the
minimum of this thing? Derivative equal to zero, right? So here it's a quadratic thing. It's going to be like that. I take the derivative,
set it to zero. So I'm first going to take
the derivative with respect to a and set it equal to zero,
so that's equivalent to saying that the expectation of-- well, here, I'm going
to pick up a 2-- y minus a plus bx
tilde is equal to zero. And then I also have that the
derivative with respect to b is equal to zero, which is
equivalent to the expectation of-- well, I have a
negative sign somewhere, so let me put it here-- minus 2x tilde, y
minus a plus bx tilde. OK, see that's why I don't want
to put too many parenthesis. OK. So I just took the
derivative with respect to a, which is just
basically the square, and then I have a negative 1
that comes out from inside. And then I take the
derivative with respect to b, and since b has x tilde. In [? factor, ?] it
comes out as well. All right, so the minus 2's
really won't matter for me. And so now I have two equations. The first equation,
while it's pretty simple, it's just telling me that
the expectation of y minus a is equal to zero. So what I know is that a is
equal to the expectation of y. And really that
was a tilde, which implies that the a
I want is actually equal to the
expectation of y minus b times the expectation of x. OK? Just because a tilde is a plus
b times the expectation of x. So that's for my a. And then for my b, I
use the second one. So the second one tells me that
the expectation of x tilde of y is equal to a plus b times
the expectation of x tilde which is zero, right? OK? But this a is actually
a tilde in this problem, so it's actually a plus
b expectation of x. Now, this is the
expectation of the product of two random variables, but
x tilde is centered, right? It's x minus expectation of
x, so this thing is actually equal to the covariance
between x and y by definition of covariance. So now I have everything
I need, right. How do I just-- I'm sorry about that. So I have everything I need. Now, I now have two
equations with two unknowns, and all I have to do is
to basically plug it in. So it's essentially telling
me that the covariance of xy-- so the first equation tells
me that the covariance of xy is equal to a plus b expectation
of x, but a is expectation of y minus b expectation of x. So it's-- well, actually,
maybe I should start with b. Oh, sorry. OK, I forgot one thing. This is not true, right. I forgot this term. x tilde multiplies x
tilde here, so what I'm left with is x tilde-- it's minus b times the
expectation of x tilde squared. So that's actually minus
b times the variance of x tilde because x tilde
is already centered, which is actually
the variance of x. So now I have that this thing
is actually a plus b expectation of x minus b variance of x. And I also have that a
is equal to expectation of y minus b expectation of x. So if I sum the two, those
guys are going to cancel. Those guys are going to cancel. And so what I'm going to be
left with is covariance of xy is equal to expectation
of x, expectation of y, and then I'm left with
this term here, minus b times the variance of x. And so that tells me that b-- why do I still have
the variance there? AUDIENCE: So is the
covariance really the expectation of x tilde
times y minus expectation of y? Because y is not
centered, correct? PHILIPPE RIGOLLET: Yeah. AUDIENCE: OK, but x
is still the center. PHILIPPE RIGOLLET: But x
is still the center, right. So you just need
to have one that's centered for this to work. Right, I mean, you can check it. But basically when
you're going to have the product of the expectations,
you only need one of the two in the product to be zero. So the product is zero. OK, why do I keep my-- so I get a, a, and
then the b expectation. OK, so that's probably
earlier that I made a mistake. So I get-- so this was a tilde. Let's just be clear about the-- So that tells me that a tilde-- maybe it's not super
fair of me to-- yeah, OK, I think I know
where I made a mistake. I should not have centered. I wanted to make my life
easier, and I should not have done that. And the reason is a
tilde depends on b, so when I take the
derivative with respect to b, what I'm left with here-- since a tilde
depends on b, when I take the derivative of
this guy, I actually don't get a tilde here,
but I really get-- so again, this was not-- so that's the first one. This is actually x here-- because when I take the
derivative with respect to b. And so now, what I'm left with
is that the expectation-- so yeah, I'm basically left
with nothing that helps. So I'm sorry about. Let's start from the
beginning because this is not getting us anywhere, and a
fix is not going to help. So let's just do it again. Sorry about that. So let's not center anything
and just do brute force because we're going to-- b x squared. All right. Partial, with respect
to a, is giving equal zero is
equivalent, so my minus 2 is going to cancel, right. So I'm going to actually
forget about this. So it's actually telling
me that the expectation of y minus a plus bx
is equal to zero, which is equivalent to a plus
b expectation of x, is equal to the expectation of y. Now, if I take the
derivative with respect to b and set it equal to
zero, this is telling me that the expectation of-- well, it's the same thing
except that this time I'm going to pull out an x. This guy is equal to zero-- this guy is not here-- and so that implies that
the expectation of xy is equal to a times
the expectation of x, plus b times the
expectation of x square. OK? All right, so the first one is
actually not giving me much, so I need to actually work
with the two of those guys. So I'm going to take the first-- so let me rewrite those two
inequalities that I have. I have a plus b, e of
x is equal to e of y. And then I have e of xy. OK, and now what I do is
that I multiply this guy. So I want to cancel one
of those things, right? So what I'm going to-- so I'm going to take
this guy, and I'm going to multiply it by e of
x and take the difference. So I do times e of x, and then
I take the sum of those two, and then those two terms
are going to cancel. So then that tells
me that b times e of x squared, plus the
expectation of xy is equal to-- so this guy is the
one that cancelled. Then I get this guy
here, expectation of x times the expectation
of y, plus the guy that remains here-- which is b times the
expectation of x square. So here I have b expectation
of x, the whole thing squared. And here I have b
expectation of x square. So if I pull this guy
here, what do I get? b times the variance of x, OK? So I'm going to move here. And this guy here, when
I move this guy here, I get the expectation
of x times y, minus the expectation of x
times the expectation of y. So this is actually telling me
that the covariance of x and y is equal to b times
the variance of x. And so then that
tells me that b is equal to covariance of xy
divided by the variance of x. And that's why I actually
need the variance of x to be non-zero because
I couldn't do that otherwise. And because if it
was, it would mean that b should be
plus infinity, which is what the limit of this
guy is when the variance goes to zero or negative infinity. I can not sort them out. All right, so I'm
sorry about the mess, but that should be more clear. Then a, of course,
you can write it by plugging in the
value of b, so you know it's only a function
of your distribution, right? So what are the characteristics
of the distribution-- so distribution can
have a bunch of things. It can have movements
of order 4, of order 26. It can have heavy
tails or light tails. But when you compute
least squares, the only thing that
matters are the variance of x, the expectation
of the individual ones-- and really what captures how
y changes when you change x, is captured in the covariance. The rest is really
just normalization. It's just telling you, I want
things to cross the y-axis at the right place. I want things to cross the
x-axis at the right place. But the slope is really captured
by how much more covariance you have relative to
the variance of x. So this is essentially setting
the scale for the x-axis, and this is telling
you for a unit scale, this is the unit of y
that you're changing. OK, so we have explicit forms. And what I could do, if I
wanted to estimate those things, is just say, well again, we
have expectations, right? The expectation of xy minus the
product of the expectations, I could replace
expectations by averages and get an empirical
covariance just like we can replace the
expectations for the variance and get a sample covariance. And this is basically what
we're going to be doing. All right, this is
essentially what you want. The problem is that if
you view it that way, you sort of prevent yourself
from being able to solve the multivariate problem. Because it's only in
the univariate problem that you have closed form
solutions for your problem. But if you actually
go to multivariate, this is not where you want
to replace expectations by averages. You actually want to replace
expectation by averages here. And once you do
it here, then you can actually just solve
the minimisation problem. OK, so one thing that
arises from this guy is that this is an
interesting formula. All right, think about it. If I have that y is a
plus bx plus some noise. Things are no
longer on something. I have that y is equal to
a bx plus some noise, which is usually denoted by epsilon. So that's the
distribution, right? If I tell you the
distribution of x, and I say y is a plus b epsilon-- I tell you the
distribution of y, and if [? they mean ?] that
those two are independent, you have a distribution on y. So what happens is that I can
actually always say-- well, you know, this is
equivalent to saying that epsilon is equal to
y minus a plus bx, right? I can always write
this as just-- I mean, as tautology. But here, for those guys-- this is not for any guy, right. This is really for
the best fit, a and b, those ones that
satisfy this gradient is equal to zero thing. Then what we had is that
the expectation of epsilon was equal to expectation
of y minus a plus b expectation of x by linearity
of the expectation, which was equal to zero. So for this best
fit we have zero. Now, the covariance
between x and y-- Between, sorry, x
and epsilon, is what? Well, it's the
covariance between x-- and well, epsilon was
y minus a plus bx. Now, the covariance is
bilinear, so what I have is that the
covariance of this is the covariance of xn times y-- sorry, of x and y, minus
the variance-- well, minus a plus b,
covariance of x and x, which is the variance of x? Covariance of xy minus
a plus b variance of x. OK, I didn't write it. So here I have
covariance of xy is equal to b variance of x, right? Covariance of xy. Yeah, that's because they cannot
do that with the covariance. Yeah, I have those
averages again. No, because this
is centered, right? Sorry, this is centered,
so this is actually equal to the expectation of
x times y minus a plus bx. The covariance is
equal to the product just because this insight
is actually centered. So this is the
expectation of x times y minus the expectation of a times
the expectation of x, plus b minus b times the
expectation of x squared. Well, actually maybe I
should not really go too far. So this is actually
the one that I need. But if I stop here, this is
actually equal to zero, right. Those are the same equations. OK? Yeah? AUDIENCE: What are
we doing right now? PHILIPPE RIGOLLET:
So we're just saying that if I actually believe that
this best fit was the one that gave me the right
parameters, what would that imply on the noise
itself, on this epsilon? So here we're
actually just trying to find some necessary condition
for the noise to hold-- for the noise. And so those conditions are,
that first, the expectation is zero. That's what we've got here. And then, that the covariance
between the noise and x has to be zero as well. OK, so those are
actually conditions that the noise must satisfy. But the noise was just not
really defined as noise itself. We were just
saying, OK, if we're going to put some assumptions
on the epsilon, what do we better have? So the first one is that
it's centered, which is good, because otherwise, the noise
would shift everything. So now when you look at a
linear regression model-- typically, if you open a book,
it doesn't start by saying, let the noise be the
difference between y and what I actually
want y to be. It says let y be a
plus bx plus epsilon. So conversely, if we assume that
this is the model that we have, then we're going to have
to assume that epsilon-- we're going to assume
that epsilon is centered, and that the covariance
between x and epsilon is zero. Actually, often, we're
going to assume much more. And one way to ensure that
those two things are satisfied is to assume that x is
independent of epsilon, for example. If you assume that x is
independent of epsilon, of course the covariance
is going to be zero. Or we might assume that
the conditional expectation of epsilon, given x, is equal
to zero, then that implies that. OK, now the fact that it's
centered is one thing. So if we make this assumption,
the only thing it's telling us is that those ab's that come--
right, we started from there. y is equal to a plus bx plus
some epsilon for some a, for some b. What it turns out is that
those a's and b's are actually the ones that you would get
by solving this expectation of square thing. All right, so when you asked-- back when you were following-- so when you asked,
you know, why don't we take the square, for
example, or the power 4, or something like this-- then here, I'm saying, well, if
I have y is equal to a plus bx, I don't actually need to put
too much assumptions on epsilon. If epsilon is actually
satisfying those two things, expectation is equal to
zero and the covariance with x is equal to zero,
then the right a and b that I'm looking for are
actually the ones that come with the square-- not with power 4 or power 25. So those are actually
pretty weak assumptions. If we want to do
inference, we're going to have to
assume slightly more. If we want to use
T-distributions at some point, for example, and we
will, we're going to have to assume that epsilon
has a Gaussian distribution. So if you want to start doing
more statistics beyond just like doing this least square
thing, which is minimizing the square of criterion,
you're actually going to have to put
more assumptions. But right now, we
did not need them. We only need that epsilon
as mean zero and covariant zero with x. OK, so that was basically
probabilistic, right. If I were to do
probability and I were trying to model the
relationship between two random variables, x
and y, in the form y is a plus bx plus some noise,
this is what would come out. Everything was expectations. There was no data involved. So now let's go to the
data problem, which is now, I do not know what
those expectations are. In particular, I don't know what
the covariance of x and y is, and I don't know with
the expectation of x and the expectation of y r. So I have data to do that. So how am I going to do this? Well, I'm just
going to say, well, if I want x1, y1,
xn, yn, and I'm going to assume that
they're [? iid. ?] And I'm actually going
to assume that they have some model, right. So I'm going to assume
that I have that a-- so that Yi follows
the same model. So epsilon i
[? rad, ?] and I won't say that expectation of epsilon
i is zero and covariance of xi, epsilon i is equal to zero. So I'm going to put the
same model on all the data. So you can see that a is
not ai, and b is not bi. It's the same. So as my data
increases, I should be able to recover
the correct things-- as the size of my
data increases. OK, so this is what the
statistical problem look like. You're given the points. There is a true line
from which this point was generated, right. There was this line. There was a true ab that
I use to draw this plot, and that was the line. So first I picked an
x, say uniformly at on this intervals, 0 to 2. I said that was this one. Then I said well, I
want y to be a plus bx, so it should be
here, but then I'm going to add some noise
epsilon to go away again back from this line. And that's actually me, here, we
actually got two points correct on this line. So there's basically
two epsilons that were small enough
that the dots actually look like they're on the line. Everybody's clear
about what I'm drawing? So now of course if
you're a statistician, you don't see this. You only see this. And you have to
recover this guy, and it's going to
look like this. You're going to have an
estimated line, which is the red one. And the blue line, which is
the true one, the one that actually generated the data. And your question is,
while this line corresponds to some parameters
a hat and b hat, how could I make sure that those
two lines-- how far those two lines are? And one to address
this question is to say how far is a from a hat,
and how far is b from b hat? OK? Another question, of
course, that you may ask is, how do you find
a hat and b hat? And as you can see, it's
basically the same thing. Remember, what was a-- so b
was the covariance between x and y divided by the
variance of x, right? We check and rewrite this. The expectation of
xy minus expectation of x times the
expectation of y, divided by expectation of x squared
minus expectation of x. The whole thing's-- OK? If you look at the
expression for b hat, I basically replaced all
the expectations by bars. So I said, well,
this guy I'm going to estimate by an average. So that's the xy
bar, and is 1 over n, [? sum ?] from [? i co ?]
1, to n of Xi, times Yi. x bar, of course, is just
the one that we're used to. And same for y bar. X squared bar, the
one that's here, is the average of the squares. And x bar square is the
square of the average. OK, so you just basically
replace this guy by x bar, this guy by y bar, this
guy by x square bar, and this guy by x
bar and no square. OK, so that's basically
one way to do it. Everywhere you see
an expectation, you replace it by an average. That's the usual
statistical hammer. You can actually be slightly
more subtle about this. And as an exercise,
I invite you-- just to make sure that you know
how to do this competition, it's going to be exactly the
same kind of competitions that we've done. But as an exercise,
you can check that if you actually
look at say, well, what I wanted to minimize here,
I had an expectation, right? And I said, let's
minimize this thing. Well, let's replace this
by an average first. And now minimize. OK, so if I do
this, it turns out I'm going to actually
get the same result. The minimum of the
average is basically-- when I replace the
average by-- sorry, when I replace the
expectation by the average and then minimize,
it's the same thing as first minimizing and
then replacing expectation by averages in this case. Again, this is a much
more general principle because if you
don't have a closed form for the minimum like for
some, say, likelihood problems, well, you might not
actually have a possibility to just look at what the
formula looks like-- see where the expectations show up-- and
then just plug in the averages instead. So this is the one you
want to keep in mind. And again, as an exercise. OK, so here, and then
you do expectation replaced by averages. And then that's the same
answer, and I encourage you to solve the exercise. OK, everybody's clear that this
is actually the same expression for a hat and b hat that we had
before that we had for a and b when we replaced the
expectations by averages? Here, by the way, I minimize
the sum rather than the average. It's clear to everyone that
this is the same thing, right? Yep? AUDIENCE: [INAUDIBLE] sum
replacing it [INAUDIBLE] minimize the
expectation, I'm assuming it's switched with
the derivative on the expectation [INAUDIBLE]. PHILIPPE RIGOLLET:
So we did switch the derivative and the
expectation before you came, I think. All right, so
indeed, the picture was the one that we
said, so visually, this is what we're doing. We're looking among
all the lines. For each line, we
compute this distance. So if I give you
another line there would be another set of arrows. You look at their length. You square it. And then you sum it
all, and you find the line that has the minimum
sum of squared lengths of the arrows. All right, and those are the
arrows that we're looking at. But again, you could actually
think of other distances, and you would actually
get different-- you could actually get
different solutions, right. So there's something called,
mean absolute deviation, which rather than
minimizing this thing, is actually minimizing the
sum from i to co 1 to n of the absolute value
of y minus a plus bXi. And that's not
something for which you're going to have a closed
form, as you can imagine. You might have something
that's sort of implicit, but you can actually still
solve it numerically. And this is something
that people also like to use but way, way less
than the least squares one. AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET:
What did I just what? AUDIENCE: [INAUDIBLE] The sum of the absolute
values of Yi minus a plus bXi. So it's the same except
I don't square here. OK? So arguably, you know,
predicting a demand based on price is a
fairly naive problem. Typically, what we
have is a bunch of data that we've collected,
and we're hoping that, together, they can help
us do a better prediction. All right, so maybe I
don't have only the price, but maybe I have a bunch
of other social indicators. Maybe I know the competition,
the price of the competition. And maybe I know a
bunch of other things that are actually relevant. And so I'm trying to find a way
to combine a bunch of points, a bunch of measures. There's a nice
example that I like, which is people were
trying to measure something related to your body
mass index, so basically the volume of your-- the
density of your body. And the way you can do
this is by just, really, weighing someone and
also putting them in some cubic meter of water
and see how much overflows. And then you have
both the volume and the mass of
this person, and you can start computing density. But as you can
imagine, you know, I would not personally
like to go to a gym when the first thing
they ask me is to just go in a bucket of
water, and so people try to find ways to measure this
based on other indicators that are much easier to measure. For example, I don't know,
the length of my forearm, and the circumference of
my head, and maybe my belly would probably be
more appropriate here. And so you know, they
just try to find something that actually makes sense. And so there's
actually a nice example where you can show
that if you measure-- I think one of the
most significant was with the circumference
of your wrist. This is actually a very good
indicator of your body density. And it turns out that if you
stuff all the bunch of things together, you might actually
get a very good formula that explains things. All right, so what
we're going to do is rather than saying
we have only one x to explain y's,
let's say we have 20 x's that we're trying
to combine to explain y. And again, just like assuming
something of the form, y is a plus b times x was the
simplest thing we could do, here we're just going to
assume that we have y is a plus b1, x1 plus b2, x2, plus b3, x3. And we can write
it in a vector form by writing that Yi is
Xi transposed b, which is now a vector plus epsilon i. OK, and here, on
the board, I'm going to have a hard time
doing boldface, but all these things are
vectors except for y, which is a number. Yi is a number. It's always the
value of my y-axis. So even if my x-axis lives on-- this is x1, and this is x2, y
is really just the real valued function. And so I'm going to get
a bunch of points, x1,y1, and I'm going to see
how much they respond. So for example, my
body density is y, and then all the x's are
a bunch of other things. Agreed with that? So this is an equation that
holds on the real line, but this guy here is an r
p, and this guy's an rp. It's actually common to
talk to call b, beta, when it's a vector, and that's
the usual linear regression notation. Y is x beta plus epsilon. So x's are called
explanatory variables. y is called explained variable,
or dependent variable, or response variable. It has a bunch of names. You can use whatever you
feel more comfortable with. It should actually
be explicit, right, so that's all you care about. Now, what we typically do
is that rather-- so you notice here, that there's
actually no intercept. If I actually fold that
back down to one dimension, there's actually a is
equal to zero, right? If I go back to p
is equal to 1, that would imply that Yi is,
well, say, beta times x plus epsilon i. And that's not good, I
want to have an intercept. And the way I do this,
rather than writing a plus this, and
you know, just have like an overload of notation,
what I am actually doing is that I fold back. I fold my intercept
back into my x. And so if I measure
20 variables, I'm going to create a
21st variable, which is always equal to 1. OK, so you should need
to think of x as being 1. And then x1 xp. And sorry, xp minus 1, I guess. OK, and now this is an rp. I'm always going to assume
that the first one is 1. I can always do that. If I have a table of data-- if my data is given to me
in an Excel spreadsheet-- and here I have the density
that I measured on my data, and then maybe here
I have the height, and here I have the
wrist circumference. And I have all these things. All I have to do is to create
another column here of ones, and I just put 1-1-1-1-1. OK, that's all I have to
do to create this guy. Agreed? And now my x is going to
be just one of those rows. So that's this is
Xi, this entire row. And this entry here is Yi. So now, for my
noise coefficients, I'm still going to
ask for the same thing except that here, the
covariance is not between x-- between one random variable
and another random variable. It's between a random vector
and a random variable. OK, how do I measure the
covariance between a vector and a random variable? AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET:
Yeah, so basically-- AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: Yeah, I
mean, the covariance vector is equal to 0 is the same thing
as [INAUDIBLE] equal to zero, but yeah, this is basically
thought of entry-wise. For each coordinate of x,
I want that the covariance between epsilon and this
coordinate of x is equal to 0. So I'm just asking this
for all coordinates. Again, in most
instances, we're going to think that epsilon
is independent of x, and that's something we
can understand without thinking about coordinates. Yep? AUDIENCE: [INAUDIBLE] like
what if beta equals alpha [INAUDIBLE]? PHILIPPE RIGOLLET: I'm sorry,
can you repeat the question? I didn't hear. AUDIENCE: Is this the
parameter of beta, a parameter? PHILIPPE RIGOLLET: Yeah,
beta is the parameter we're looking for, right. Just like it was the pair ab has
become the whole vector of beta now. AUDIENCE: And
what's [INAUDIBLE]?? PHILIPPE RIGOLLET: Well, can
you think of an intercept of a function that take-- I mean, there is one actually. There's the one
for which betas-- all the betas that
don't correspond to the vector of all
ones, so the intercept is really the weight
that I put on this guy. That's the beta that's
going to come to this guy, but we don't really
talk about intercept. So if x lives in two
dimensions, the way you want to think
about this is you take a sheet of paper
like that, so now I have points that live
in three dimensions. So let's say one
direction here is x1. This direction is x2,
and this direction is y. And so what's going
to happen is that I'm going to have my points
that live in this three dimensional space. And what I'm trying
to do when I'm trying to do a linear
model for those guys-- when I assume a linear model. What I assume is that there's
a plane in those three dimensions. So think of this guy
as going everywhere, and there's a plane close to
which all my points should be. That's what's happening
in two dimensional. If you see higher dimensions
then congratulations to you, but I can't. But you know, you can definitely
formalize that fairly easily mathematically and just
talk about vectors. So now here, if I talk about the
least square error estimator, or just the least squares
estimator of beta, it's simply the same
thing as before. Just like we said-- so remember, you
should think of as beta as being both the
pair a b generalized. So we said, oh, we wanted to
minimize the expectation of y minus a plus bx squared, right? Now, so that's in--
for p is equal to 1. Now for p lower
than or equal to 2, we're just going to write it
as y minus x transpose beta squared. OK, so I'm just trying to
minimize this quantity. Of course, I don't
have access to this, so what I'm going to do
with them going to replace my expectation by an average. So here I'm using the notation
t because beta is the true one, and I don't want you to just-- so here, I have a variable
t that's just moving around. And so now I'm going to take
the square of this thing. And when I minimize this over
all t in rp, the arc min, the minimum is attained at beta
hat, which is my estimator. OK? So if I want to
actually compute-- yeah? AUDIENCE: I'm sorry,
on the last slide did we require the expectation
of [INAUDIBLE] to be zero? PHILIPPE RIGOLLET: You
mean the previous slide? AUDIENCE: Yes. [INAUDIBLE] PHILIPPE RIGOLLET: So again,
I'm just defining an estimator just like I would tell you,
just take the estimator that has coordinates for everywhere. AUDIENCE: So I'm saying like
[? in that sign, ?] we'll say the noise [? terms ?] we want to
satisfy the covariance of that [? side. ?] We also want them
to satisfy expectation of each [? noise turn ?] zero? PHILIPPE RIGOLLET: And
so the answer is yes. I was just trying to think
if this was captured. So it is not
captured in this guy because this is just telling
me that the expectation of epsilon i minus expectation
of some i is equal to zero. OK, so yes I need to have
that epsilon has mean zero-- let's assume that
expectation of epsilon is zero for this problem. And we're going
to need something about some sort of question
about the variance being not equal to zero, right, but
this is going to come up later. So let's think for one second
about doing the same approach as we did before. Take the partial
derivative with respect to the first coordinate
of t, with respect to the second coordinate
of t, with respect to the third coordinate
of t, et cetera. So that's what we did before. We had two equations,
and we reconciled them because it was fairly
easy to solve, right? But in general,
what's going to happen is we're going to have
a system of equations. We're going to have a system
of p equations, one for each of the coordinates of t. And we're going to have p
unknowns, each coordinate of t. And so we're going to
have the system to solve-- actually, i turns out it's
going to be a linear system. But it's not going
to be something that we're going to be able to
solve coordinate by coordinate. It's going to be
annoying to solve. You know, you can guess that
what's going to happen, right. Here, it involved the covariance
between x and epsilon, right. That's what it involved
to understand-- sorry, the correlation
between x and y to understand how the
solution of this problem was. In this case,
there's going to be only the covariance between
x1 and y, x2 and y, x3, et cetera, all the way to xp and y. There's also going to be all
the cross covariances between xj and xk. And so this is going
to be a nightmare to solve, like, in this system. And what we do is that we go
on to using a matrix notation, so that when we
take derivatives, we talk about
gradients, and then we can invert matrices and solve
linear systems in a somewhat formal manner by just saying
that, if I want to solve the system ax equals b-- rather than actually
solving this for each coordinate
of x individually, I just say that x is
equal to a inverse times. So that's really why we're
going to the equation one, because we have a
formalism to write that x is the solution of the system. I'm not telling you
that this is going to be easy to solve numerically,
but at least I can write it. And so here's how it goes. I have a bunch of vectors. So what are my vectors, right? So I have x1-- oh, by the way,
I didn't actually mention that when I
put the lowercase, when I put the subscript, I'm
talking about the observation. And when I put the
superscript, I'm talking about the
coordinates, right? So I have x1, which is
equal to x1, x1 [? 1, ?] x 1p, x2, which is 1. x2, 1, x2 p, all the way to
xn, which is 1, xn 1, x np. All right, so those are n
observed x's, and then I have another y1, y2, yn, that
comes paired with those guys. OK? So the first thing
is that I'm going to stack those guys
into some vector that I'm going to call y. So maybe I should put
an arrow for the purpose of the blackboard, and
it's just y1 to yn. OK, so this is a vector in rn. Now, if I want to stack
those guys together, I can either create a long
vector of size n times p, but the problem is that I lose
the role of who's a coordinate and who's an observation. And so it's actually
nicer for me to just put those guys
next to each other and create one new variable. And so the way I'm going to do
this is-- rather than actually stacking those guys like that,
I'm getting their transpose and stack them as
rows of a matrix. OK, so I'm going to
create a matrix, which here is denoted typically by-- I'm going to write x double bar. And here, I'm going to
actually just-- so since I'm taking those guys like
this, the first column is going to be only ones. And then I'm going to have-- well, x1, 1, [? 1, ?] x1, p. And here, I'm going
to have x n1, x np. OK, so here the number of rows
is n, and the number of columns is p. One row per observation,
one column per coordinate. And again, I make your life
miserable because this really should be p minus 1
because I already used the first one for this guy. I'm sorry about that. It's a bit painful. So usually we don't even
write what's in there. So we don't have
to think about it. Those are just
vectors of size p. OK? So now that I
created this thing, I can actually just basically
stack up all my models. So Yi equals Xi transpose
beta plus epsilon i for all i equal 1 to n. This transforms into-- this
is equivalent to saying that the vector y is
equal to the matrix x times beta plus a matrix,
plus a vector epsilon, where epsilon is just epsilon
1 to epsilon n, right. So I have just
this system, which I write as a matrix,
which really just consists in stacking up all these
equations next to each other. So now that I have this model--
this is the usual least squares model. And here, when I want to write
my least squares criterion in terms of matrices, right? My least squares
criterion, remember, was sum from i equal 1 to n
of Yi minus Xi transposed beta squared. Well, here it's
really just the sum of the square of the
coefficients of the vector y minus x beta. So this is actually
equal to the norm squared of y minus x beta square. That's just the square. Norm is, by definition,
the sum of the square of the coordinates. And so now I can actually
talk about minimizing a norm squared,
and here it's going to be easier for me
to take derivatives. All right, so we'll
do that next time.

CS

Picking up right where
we left off last time, we were deriving the variance
of a hypergeometric, right? So I was just wanting to
quickly recap that and make a few more comments about it. We basically did the calculation last
time, just didn't simplify the algebra. But I wanna say a few more things
about that and remind you. So we were doing the variance
of the hypergeometric, And we have parameters w, b, n. Which you think of it as w,
white balls, b, black balls, and we're taking a sample of
size n without replacement. And then we wanna study the variance of
the number of white balls in the sample. And I'll just remind you what we
did at the very end, last time. And let's actually make up a little
bit more notation to make this a little bit nicer. So let's let p equal w over w + b, that's
a natural quantity to look at, right? And that is between 0 and 1, it's just a
fraction of white balls in the population. And it's also kind of
convenient to let w + b = N. That's not a random variable,
but that's sort of a traditional statistics notation sometimes, for
the population size, is capital N. Sample size is lowercase n. Okay, and then what we did last
time was derive the variance, Of x, we decomposed x as a sum of
indicator random variables where xj is just the indicator of the jth
ball that you draw being white. And then using the stuff we
did last time for variance. The variance of the sum is the sum
of the variances, then plus, we have all these covariances. If they're independent, you don't
have to worry about the covariances. But in this case they're not independent,
so we need the covariance terms. So really this is just gonna
be a Var(X1) plus blah, blah, blah, plus Var(Xn), and
then all the covariances. And sorry, I'll put 2,
because I'm grouping covariance of X1 and X2 with covariance of X2 and
X1, group them together. So 2 times the sum over all i &lt; j Cov(Xi,
Xj). Okay, now here's where this would be,
it looks like a complicated sum. But we take advantage of symmetry,
like I was doing quickly the last time. But you should make sure that you
see the symmetry in this problem. Any of these Xj, the jth ball,
before you draw any balls, that's just equally likely
to be any of the balls. So we're not,
these are not conditional variances, these are the unconditional variances. By symmetry, they're all the same, so it's just n times the variance
of the first one. So n times Var(X1), well,
X1 is just Bernoulli p, right? So this is just n p (1-p). And then we need all these
covariance terms, and choose, by symmetry they're all the same,
again. So it's 2,
there's n choose 2 terms in this sum. So I'm just gonna write 2 (n choose 2),
then I don't have to do a sum anymore. So I'll go 2 (n choose 2), and
we want the covariance between X1 and X2. And we did this quickly last time, but it's important enough
to write it down again. Cov(X1, X2) =,
just to remind you of the definition, or the equivalent of the definition,
E(X1 X2)- (EX1)(EX2). So once we do, that's just how you
get covariance in general, I mean, that's always true. But once we have this, that will tell
us immediately what to put in here. At least immediately, once we think hard enough about indicator
random variables actually mean. Okay, so this part is just easy,
it's just (EX1)(EX2). And we already know that marginally,
these are just Bernoulli p's. They're not independent, but this is
just saying look at them separately, so those are just Bernoulli p's. So, that's very easy,
that's just gonna be p squared, that term. Now E(X1 X2), as I pointed at last time. If you multiply two
indicator random variables, that's just an indicator random
variable of the intersection. So this is just the event that
the first ball is white and the second ball is white. So first ball is w over w + b,
that's just p again, times w- 1 over w + b- 1,- p squared,
okay. So that looks messy. If you multiply everything and
then simplify and do the algebra, it actually comes out to
something surprisingly nice. So this is just algebra at this point, if you simplify it,
what you get is this factor. N- n over N- 1, times something
that looks familiar, n p (1-p). This part looks very familiar, right? That's just the variance of a binomial np. This factor in front, in statistics, is
called the finite population correction. And this answer, it's really neat
that it works out to something so simple and similar to the binomial. It looks like the binomial variance, it's we just need this extra
correction factor in front. And let's just check this in a couple
simple and extreme cases, right, I always recommend look at simple and
extreme cases. So one extreme case would
be of little n equals 1. Then this goes away, right, and
we just get the variance of a Bernoulli p. Well it had to be that way, right? Because if you're only picking one ball,
what difference does it make if it's with replacement or without replacement,
there's only one ball. So that makes sense, when n is 1. And now let's consider
another extreme case, so I'll just write that down, extreme cases. So one extreme case is N = 1. And the other extreme case is
if N is much, much, much larger, I'll just write much, much, not much,
much, much, larger than little n. Little n is say, 20, big N is 100,000. If that's the case,
this is extremely close to 1. Which says we're getting something
extremely close to the binomial variance, and that should make perfect sense. Because if the sample is so minuscule
compared to the population, it's very, very unlikely that you would sample
the same individual more than once, right? You're not doing replacement,
but what difference does it make, cuz it's unlikely to get the same
person twice anyway, in your sample. Okay, so it's gonna be close to
a binomial if this thing is close to 1, so that should make intuitive sense. All right, so
that's the variance of the hypergeometric. Okay, so I think we're ready for
a change of variables now, change of topic to change of variables. So change of variables is
synonymous with transformations. This is something we've done
before via other methods, but not as a topic in its own right. But hopefully, the method that we're gonna
write down, everything should look kind of natural, because we've already
done some similar stuff. So, and we've already been talking a lot About what happens when you have
a function of a random variable. A function of a random
variable is a random variable. And we use LOTUS a lot to get
its expected value, okay? But LOTUS is great but LOTUS only gives you the expected value
of that transformed random variable. It doesn't give you
the whole distribution. So a lot of times you don't just want the
mean or you just don't want the mean and the variance, you want the entire
distribution, well, how do you do that? So, let's state it as a theorem and
then, then do examples. So, it's more interesting
in the continuous case. So, I'm going to state this for
continuous random variables. So let X be a continuous random variable. With PDF, let's say, f sub x and let Y equals g of X. So transforming from X to Y by
multiplying some function g. We need to make some assumptions on g, if g is a really nasty function then
this may not work out very well. LOTUS will still be true, but
that doesn't give us the distribution. So let's assume to start with, let's assume that first of
all g is differentiable. So in particular it's continuous,
but it's stronger than that, we want the derivative of g to exist everywhere,
or at least everywhere of interest to us And let's assume that g
is strictly increasing, Okay, and then the question is
how do we get the PDF of Y And the answer, Is given by, fY of y equals, well,
we start with the PDF of X. And then we multiply by dx dy. And I just have to explain
the notation a little bit. Here we transformed
capital X to capital Y. So a natural thing to do is to mirror
that notation with the lowercase letters. So we're defining it to be true
that little y equals g of little x. So we're doing the same transformation. Now his looks a little
bit strange because, This is a function of y and
this is a function of x. And if I ask you for the PDF of y, I'm hoping you'll
give me a function of little y. And if you just write this down,
you have a function of little x. So the interpretation of this, it is that everything is then
written in terms of y, little y. It looks uglier if I write
it that way right now. Well, because I made these
assumptions that g is nice enough, g will have an inverse so we could
also write x equals g inverse of y. So all I'm saying to do is
plug in g inverse of y here, then it's a function of y. Dx dy is the derivative of x with
respect to y viewed as a function of y. And there are several variations on this. In particular,
you can also do dx dy is the same. This is just intro of calculus again,
but it's useful to point out. dx dy is the reciprocal of dy dx. That's just the Chain Rule. Remember from calculus
these look like fractions. They're not actually fractions but the
Chain Rule says they act like fractions. That's just the Chain Rule. So that says we have a choice in doing
this we can decide which is easier. Either we could do dx dy directly,
or we could take dy dx and flip it. And then we just have to remember to
write it as a function of y, either way. So there are a couple of choices for
how to use this, and you should think first about which
one's gonna be easier rather than just blindly jumping in
without actually thinking about it. Also, make sure to check the assumptions. Strictly increasing. So a common mistake on this kind of
problem would be to just try to blindly plug into this formula for
a function like g of X equals X squared. Now if g of X equals X squared is a very,
very nice function. I'm not saying that's not a nice function,
it's a parabola. But it's a u shape,
it goes down and then it goes up. So it's not strictly increasing so
you couldn't apply this. It doesn't mean we can't
solve the problem, it just means you have to go back
to first principles in that case. This would work, though, with g of X equals X squared if we're
dealing with positive random variables. Because then the negative
side doesn't come into play. But if you're dealing with both
negative and positive values and you're squaring it, that's not increasing. Okay, so you have to be careful
about things like that. Don't just plug into the formula all
without checking the assumptions. All right, so let's prove this. And the proof should be pretty easy, just based on kind of similar
calculations we did before. Like, in a sense,
this is easier than Universality, or things like that, that we've done before. Well, our proof is just
gonna be let's find the CDF, take the derivative to get the PDF. So it doesn't require any
great leaps of thought. We're just gonna find the CDF. We're gonna take the derivative,
that's it. So let's do that pretty quickly. The CDF of Y, Probability Y less than or equal to little y, equals,
I'm just gonna plug in the definition, g of X less than or
equal to, Little y, right? The derivative of the CDF is the PDF. So if we want we can write
this as P of X less than or equal to g inverse of y, because I am assuming this function has an inverse,
this event is equivalent to this event. That is you can get from here to here and
from here back, back to here. It's just the same event
written in a different way. But notice that this is just
the CDF of X evaluated here right. It's just the definition of CDF so
hopefully this is very, very familiar by now. That's just the CDF of x let's call
that F sub X of g inverse of y. And just to make the notation a little
bit nicer, really that's just FX of x. Cuz I defined it. I wrote over there x is g inverse of y. So that's an easier way to write it. So basically what that says is that for the CDF you don't really
have to do much of anything. But for the PDF you can't
just say this equals this, that you have this
derivative that comes up. That's from the Chain Rule. So now let's just take the derivative
of both sides, fy of y. Equals, I'm differentiating
with respect to Y. So chain rules says I can differentiate
first with respect to X and get the PDF of X. And then we have the correction
dx dy to correct for the fact that that we differentiate
both sides with respect to Y. Whereas to get to big FX to little fx
are differentiated with respect to X. So it's just the chain rule,
nothing else to it. All right, so
that's what we wanted to show. So let's do an example. So, here's a famous example
one of the most widely used distributions in practice
is called the log normal. And let's let Y =, log normal does
not mean the log of a normal. You can't take the log of
a normal random variable, because you can't take
the log of a negative number. Log normal means, so
this is log normal example. Log normal means that the log is normal,
not log of the normal. So if you take, z is standard normal here. More generally you could let
z be normal mu sigma squared. But let's just do the standard
normal case first. So if I take the log of this,
I'll get z which is normal. So that's why it's called
log normal log is normal. So we actually had a homework
problem about this before, right. Where if you did the problem,
what you did was to use the MGF of the normal to find moments
of the log normal, okay? But that's just moments,
right now we want the entire PDF, okay? We want the distribution. So this is an increasing,
this transformation, this is an increasing function. It's infinitely differentiable, right? It's a very, very nice function, so there's no problem with
applying that result. And we can just immediately therefore, write down the PDF,
fy(y) = let's do it here. Fy(y) =, so I'm just gonna write down the standard normal x
is z in this example. 1 over root 2 pi,
e to the minus z squared over 2, except that I said that we have to
express it as a function of Y, right? So z is log y, so
instead of writing z squared over 2, I'm gonna write log y squared over 2. So this would be the normal density,
except plugging in log y in for z, and then it says according to this we
also have to multiply it by dz/dy. So over here,
let's just compute the derivative, dy/dz, =, just the derivative is dy/dz equals the derivative of e to the z,
z to the z, right? But I wanna write that
in terms of y instead, e to the z in terms of y is y so
that wasn't too difficult. And then we just need to be careful
about do we multiply it by dy/dz here or dz/dy, right? But that says dx/dy. So it's the reciprocal of this,
so we're gonna just put a 1/y. And this is for y &gt; 0. So that's gonna be the PDF. By the way if you ever forget
whether this is dx/dy, or dy/dx well, I mean it shouldn't
take long to rederive it. But kind of mnemonic is kind of pretend
that the dy is over there then it looks really nice and symmetrical f(y)/dy =
f(x)/dx, is a handy way to remember it. And you may have been taught that it's
not ok to separate the dx from the dy, but when you go further in math then
people start separating them again. And as long as you're careful about what
things mean it is possible to do that if you interpret it correctly, but
I just think of that as an amonic. So, if I every write f(y)dx equals
f(x)dx just think of that as a notation, that means exactly this. All right, so
the proof was pretty short for this, and an example just use the formula. Pretty straight forward as
long as the conditions apply. But while we're on this topic,
let's do the multidimensional version, which looks uglier, but
it's conceptually the same thing. So now we're gonna have
transformations in n dimensions. So transformations again, but
now we have the multidimensional version. Okay, so now we think of Y and X as, Random vectors, so Y = g(x), where g is a mapping from Rn to Rn Random vector just think of that as a list of random variable so
this Y is really just Y1 through Yn. So it's not really a new concept. It just means we took our
n random variables and listed them together as one vector, okay. And so we have a mapping from Rn to
itself that we're doing a transformation. And then the problem is, and let's assume that the X = x1 through xn, that vector is continuous. That is, it's a continuous random vector. In other words, we just have
some joint PDF, right, cuz we've been talking about joint PDFs, so
that's a familiar concept at this point. So we have this joint PDF, and we do
this thing, and then the question is, what's the joint PDF of Y, right? So it's completely analogous,
just higher dimensional. So I'm not gonna prove
that the analog holds, because that's just basically
an exercise in multivariable calculus, which is not really that relevent for
our purposes. It's completely analogous,
so depending on how much multivariable calculus you've
done you could either prove it or just accept it as analogous,
cuz it is analogous, okay? So we want the joint PDF of Y,
In terms of the joint PDF of X, right? So I'm just gonna write down the analogous
equation to that, f sub Y(Y), I'm just using that as notation for
the join PDF, = the joint PDF of X. Times dx/dy. The only problem with this is, how do we interpret the derivative of
this vector with respect to that vector? What does that actually mean? Well this thing, and actually we wanna
put absolute value symbols around it. By the way if this function were strictly
decreasing we could do the same thing just by sticking absolute values in here. If we forgot the absolute values, we're gonna get a negative
PDF which makes no sense. All right, so I just have to
tell you what this thing means. Well this thing is called the Jacobian. And I'm sure some of you have seen it but I'm not necessarily assuming
that you've seen it before. I mean it's standard
multivariable calculus thing. And all the Jacobean,
if you haven't seen it before, all the Jacobian is it's the matrix
of all possible partial derivatives. And as I said on the first day of class,
if you know how to an ordinary derivative, you know how to do a partial derivative,
you just hold everything else constant. So dx dy equals just to write it out. It's a matrix of all possible
partial derivatives. So, what we do is we take, X is a vector. We take the first coordinate of X, we differentiate it with respect
to all the coordinates of Y. So we go dxd1 dy1,
dxd1 dy2, blah, blah, blah, dx1 dyn And then we would take x2 for
the second row, do the same thing, and we keep going till
we've done all the partial derivatives. dxn dy1 blah blah blah dxn dyn. So it's just a matrix of all
possible partial derivatives. Now it doesn't make much sense to stick
in a matrix here, so actually, these absolute value symbols actually mean take
the absolute value of the determinant. So we're taking this Jacobian matrix and
we take the determinant of it. Take the absolute value. That's the analog of this
formula we just checking up, we have this matrix somehow we need to
compress the matrix down to a number and it turns out the right way to
do that is using a determinant. And like in the other
case we could also do, we could choose to do it this way or
we could have done dy dx. This says dude you're
copying the other way around, right take all the partials
of y with respect to x. Okay, we could have done that and
then take another reciprocal, and it would be the same thing. So sometimes one of these two methods
is much easier than the other, so you wanna think first about which
direction to do the transformation in. A lot of books just write
the Jacobian as J, and I like the letter J a lot, but
I don't like that notation here because it doesn't tell you which way are you
going, from x to y or y to x. So all right.
This way then it's very obvious that just says take derivatives of the x's with
respect to the y's, that has to be this. And this one would be
the other way around. You can do either way as long
as you're careful about whether to do the reciprocal here or not. All right, and so, that's the Jacobian. One other calculus thing that we should discuss
briefly is convolution. Convolution is something
we've done already, just like we've already
done some transformations. But I just wanted to mention
it as its own topic briefly. Convolution is just the fancy word for
sums. That is we want the distribution
of a sum of random variables. Remember for the binomial we
did a convolution of binomials using story proof as long
as they have the same P. And for Poissons and
Normals, we used the MGF and all of those are pretty easy calculations. But sometimes you can't find
a story that will help you and the MGF may not exist or
you may not know how to work with the MGF. Sometimes we need a more direct method. So in the discrete case,
we've already done calculations like this. So we want, so let's let T equal X plus Y. And we want to know the distribution of T
assuming we know the distribution of X and the distribution of Y that's
called the convolution. So, in the discrete case, we can just
immediately write down a formula. It maybe a messy formula it may or may
not be something we can actually do but at least we have an expression. So in the discreet case we can immediately
just well what's the probability that T equals t? Well, that's just the sum over,
how can I get a total equal to T? Well, X has to be something and
Y has to be whatever makes that up to T. You can think of this as
just conditioning on X. But I'll write it just as you're
using the actions of probability, breaking this up into disjoint cases. So I'm just gonna sum over,
always I can make the total equal x. So I can immediately just write it down. This is P of X equals little x. We're assuming that X and
Y are independent here. It's much nastier if they're dependent. Probability Y equals t minus x. We're summing over all x
such that this is positive. So we don't need a separate proof for
this. This just says to get the total equal P,
X has to be something and Y has to be whatever makes the total P. It has to be that way. And because I assumed independence I
split it up into two probabilities. So that's true for the discreet case. Now lets write down something
analagous in the continuance case. So now we want the PDF instead. And I'm gonna write down something
that looks completely analagous. That is, instead of doing the,
this is the PMF. So cuz it's continuous,
I'm gonna replace the PMF by the PDF. Let's go from minus infinity to infinity. This is the PMF of y,
evaluated at t minus x. I'll replace that by the PDF
evaluated at t minus x dx. This is true. And the easiest way to remember this
result is by thinking by analogy, with this. However, that's not a proof. That's just an analogy. And on the new homework, you'll see an
example where if you try to reason kind of an analogous way for a product, instead
of a sum, well, you'll see what happens. This requires more justification. There are several ways to justify this. Probably the simplest way
would be to take the CDF. Let's do the CDF take the derivative and
get the PDF. So what's the CDF? Well, for the cdf let's use
the continuous probability. So we're integrating the probability
that X plus Y is less than or equal to little t given
x times the PDF of x. This is one way to do it there
are other ways to do this calculation. But I like this one. That's just the continuous
law of probability. Now we plug in X equals x. And once we've plugged in X equals x,
we can drop the condition because X and Y are independent. And so
then all we have is the integral of, notice what's left here, just in your
mind replace big X by little x and move it over to that
side of the inequality. So it says Y less than equal t minus x, that's just the CDF of Y
evaluated at t minus x. Now take the derivative of
both sides of this equation. Derivative with respect to t, and then there's a theorem that says you
can swap the derivative and the integral. Derivative of this CDF Is the PDF so it would get that way, so
it requires some justification. Usually I would like to avoid doing
convolution integrals like this but sometimes you can't avoid it. But if possible try to use a story or
an MGF or one of the other things we've done but
sometimes you need that, yeah? &gt;&gt; [INAUDIBLE]
&gt;&gt; This is capital F, this- &gt;&gt; [INAUDIBLE] &gt;&gt; Yeah, sorry, this is F sub t(t), thank you. That's the CDF of capital T, thanks. Okay, so
my favorite thing about statistics is that you can do things that
are beautiful and useful. And Jacobeans are,
it's an extremely useful technical tool, but I've never heard anyone
describe Jacobeans as beautiful. So to kind of rebalance our
beauty quotient for today, let's do something completely different
that involves no calculus at all. Something that,
you can see whether you agree with me or not but, this is something that beauty
is not really an adequate word for, this is not something I would
consider existential, okay? So, here's an idea. The idea is you can use probability
to prove existence of up, okay? So we're gonna prove existence,
what does that mean? We're gonna prove existence of
objects with desired properties. Using probability. Properties using probability. So that's a very general idea. So let me just tell you
mathematically what's the idea. The idea is, I wanna show that an object with
a certain property exists. One way to show that
would be to show that, let's say desired property A. That is A is some property, okay? I'm saying this very generally but
we'll do an example. So we want to show there's
an object with a certain property, that sounds like it has nothing whatsoever
to do with probability and statistics. That's just like if you searched
everywhere in the universe, either mathematically or whatever,
could you ever find this thing? I didn't say anything about randomness or
uncertainty, okay? Here's the strategy,
so this is a strategy. We're gonna show that P(a) is
greater than 0, for a random object. We get to choose how to define random,
that is we just have this universe of objects and we decide on some method
for randomly selecting an object. So if it is a finite set,
the most obvious thing to do is just pick one at random where they're
all equally likely, right? If I have a million objects there's
no probability anywhere but I say well just pick one at
random equally likely, okay? And then let A be the event that the
randomly chosen object has the property. Well, is it clear that if the probability
is non 0 then there must exist one? Well, of course, if it didn't exist,
the probability would be 0, so if the probability is positive,
it must exist. So if we can show this,
we've shown that it exists. And that sounds like, so this is true, I mean I don't need to write a proof for
that. But that sounds like a very,
very wishful thinking strategy, that if we can't even exhibit
existence of even one such object, how are we ever gonna
compute the probability? We can't even find one, but
we're gonna compute its probability, that's pretty weird. Notice that we don't actually
have to compute P(A) exactly, we only need a bound that shows
that it's greater than 0, okay? WE don't need to know exactly P(A),
just that it's positive. That's method 1,
let's extend this a little bit. Suppose each object has
a number associated with it, let's think of a score. So we have this universe of objects,
no probability yet, each object has a number attached to it,
so some kind of a score. We wanna show there exists
an object with a good score. To say what does good mean,
I will talk a little bit about that. We wanna show there is an object
with a good score, but suppose it's really hard to actually
find one that has a good score. show there is an object with a good score,
I had to say what good means. Well here's the strategy. You may guess this has something
to do with probability. Pick a random object again,
look at its score. So, in other words, what's
the average score of a random object? Now, here's the theorem. There is an object, Whose score is at least the average, right? Let's just call it E(X), where this
is the score of a random object. So, we're defining a random
variable by taking a random object, find its score, take the average. Well obviously, there must be at least one object
that at least is the average, right? They can't all be below average,
that would make no sense, right? So therefore, if now of course,
E of X may be pretty lousy. But if E of X is actually pretty good then we've shown that there exists a good
one without actually exhibiting it. So again that sounds like a group
of people, and at least one person has to have at least the average salary of
the people in the room, things like that. That's an extremely crude statement. Is that ever gonna be useful for anything? I think this is a neat idea,
but is it actually useful? Well, what I consider one
of the most beautiful and useful results of the 20th
century was Shannon's theorem. Claude Shannon is the father
of information theory. Also the father of this
modern communications theory. So anytime you use the cell phone,that's
all based on communication and coding theory that goes
back to Shannon's work. So you can thank Shannon for this. Let me just tell you,
this is not an information theory course. It's a really amazing idea that you
can quantify information, though. But let me tell you very briefly
what one of Shannon's theorems was. Shannon theorem, was that he showed that
if you're trying to communicate over a noisy channel, so you're trying to send
messages from one place to another, but bits get corrupted, things, there's a lot
of noise and interference, or whatever. He showed that there's something
called a capacity of the channel, and you can communicate at rates
arbitrarily close to the capacity, with arbitrarily small chance of error
That is even if you have a very, very noisy channel, you can make
the air probability very, very low. That sounds like a very difficult theorem. And no one out he proved this in 1948. No one else was even close to
thinking of that as far as I know. The way he proved it, that there
exists what he called a good code, right, a good code is gonna
be one that works well for sending messages across
this noisy channel. The way he showed that a good code
exists was to pick a random code. And that's like kinda the most
daring thing you can imagine, he probably spent months trying to
actually find one couldn't find one so he picked a random one. And to think that a random one is actually
gonna do well is kinda unbelievable, and it turns out to be true. It was only 30 or 40 years later that people actually
explicitly could write down a good code. Until then Shannon showed that
they exist because a random one has the right properties. Even though you can't actually write down
a specific one, without a lot of work. That's one of the most amazing results,
just mathematically extremely beautiful, but it underlies all of modern
communication and information theory. All right, so I'm not going try to
prove Shannon's theorem in ten, or five minutes, but
I am going to do one quick example. Along these lines so I just made up a simple example
just to illustrate this idea. So the idea is, and here is the problem. So suppose we have. 100 people, I just made up some numbers, just so that we can actually do something
reasonably concrete and simple, just to show you how this idea
would work in a small example. Okay so there are 100 people and those people form committees. Now one person can be on
more than committee, so let's assume that there are,
how many committees do I want? I made up some numbers last night
I think I wanted 15 committees. I just made up some numbers where it works
out nicely but we can try this something more general like M and N and
whatever, but I made up some numbers. 15 committees of 20,
that is each committee has 20 people. So I chose these numbers such that
15 times 20 is 300 which means that if everyone is on the same
number committees, than that means each person
is on three committees. You can generalize this to cases where
different people can be on different numbers of committees. But well, for simplicity, let's assume
each person is on three committees. No probability yet so far, right? That's just okay,
there's different way to do it. You can think of it as a counting problem, how many ways are there to do it,
there's some vast number of possibilities. Okay, now here's the problem. The problem is to show
that there exist two committees whos overlap is at least three. So I can find two committees, or there exist two committees, where a group
of three people is on both committees. So show there exists, two committees With overlap greater than or equal to 3. All right, so clearly the way to solve
this is not gonna be like write down every possible [INAUDIBLE] committees and
then search through and find over the computer all the overall
laps, all the intersections and go through everything right? That would be a nightmare. So we're going to use this idea and
we're gonna prove existence. This is an existence problem. We're gonna prove existence
just by computing the average. So the idea is find the average
intersection I said average. That involves probability. We didn't have any probability yet. We introduce our own probability
structure by just saying let's just choose two random committees. So find average overlap
of two random committees All right, so
hopefully we can do that quickly. So, I'll just write E,
you can make up some fancy notation and stuff, but we're just picking two, we're assuming that we have this
fixed assignment of who's on what. We have specific people with names. The so and so is on this committee and
so and so is on this committee, and so on that's not random. Our randomness is because we're
choosing two random committees. Okay, and we want the expected
overlap of those two committees. So how do we do that? Indicator random variables. We create an indicator random variable for
each person. There's 100 people, so I'm not gonna
write all the indicator random variables, because this should be familiar by now. We have 100 people, so we create an
indicator for each person, use linearity. So it's gonna be 100 times and
over here all we need to do by the fundamental bridge, all we need
to do is write down the probability that, let's say person number one is on both
of those random committees, right? So now we're looking, okay, person number one what's the probability
that that person is on both of those randomly chosen committees well,
you can think of that as a hypergeometric. You don't have to let's just
think about it directly. I'm assuming I chose
two random committees. So it's 100, choose two possibilities. Naive definition applies, because I'm assuming equally likely that
we chose any two with equal probabilities. Then the numerator, sorry, this is number of committee,
how many committees are there? 15 committees, choose two out of the 15
committees and then the numerator. Person number one is on three committees
so choose two out of the three committees. So this is three choose two, three choose
two is three, so that's 300 over 1500 choose two is 15 times 14 divided by 2. 300 divided by 15 is 20,
the 2 comes up so it's 40 over 14. Which we can simplify as 20 over 7. If I did the arithmetic correctly that
looks like we came a little bit short. It's like almost good enough
cuz we wanted at least 3. And we only have 20 over 7 and
if only it were 21 over 7. Then we'd be so happy. But here's the idea. According to that, there must be,
so the average is 20/7. That implies that there
exists a pair of committees. With at least. An overlap of 20/ 7. Now there's no way that two
committees can have an overlap equal to 20/7 if the overlap were only
2 that would not be good enough. So we get to round this up to the next
integer because the overlap is an integer so that means we can have
overlap of at least 3. Than means we have proven that. So we prove that it exist,
we ran out of time so have a good weekend.

CS

  In this problem, we'll be
working with a object called random walk, where we have a
person on the line-- or a tight rope, according
to the problem. Let's start from the origin, and
each time step, it would randomly either go forward
or backward with certain probability. In our case, with probability
P, the person would go forward, and 1 minus
P going backwards. Now, the walk is random in the
following sense-- that the choice going forward or backward
in each step is random, and it's completely
independent from all past history. So let's look at the problem. It has three parts. In the first part, we'd like to
know what's the probability that after two steps the person
returns to the starting point, which in this
case is 0? Now, throughout this problem,
I'm going to be using the following notation. F indicates the action of going
forward and B indicates the action of going backwards. A sequence says F and B implies
the sample that the person first goes forward,
and then backwards. If I add another F, it will
mean, forward, backward, forward again. OK?   So in order for the person to
go somewhere after two steps and return to the origin, the
following must happen. Either the person went forward
followed by backward, or backward followed by forward. And indeed, this event-- namely, the union of these
two possibilities-- defines the event of interest
in our case. And we'd like to know what's
the probability of A, which we'll break down into the
probability of forward, backward, backward, forward. Now, forward, backward and
backward, forward-- they are two completely different
outcomes. And we know that because they're
disjoint, this would just be the sum of the
two probabilities-- plus probability of
backward/forward.   Here's where the independence
will come in. When we try to compute the
probability of going forward and backward, because
the action-- each step is completely
independent from the past, we know this is the same as saying,
in the first step, we have probability P of going
forward, in the next step, probability 1 minus P
of going backwards. We can do so-- namely, writing
the probability of forward, backward as a product of going
forward times the probability of going backwards, because
these actions are independent. And similarly, for the second
one, we have going backwards first, times going forward
the second time. Adding these two up, we have 2
times P times 1 minus P. And that will be the answer to the
first part of the problem.   In the second part of the
problem, we're interested in the probability that after three
steps, the person ends up in position 1, or
one step forward compared to where he started. Now, the only possibilities here
are that among the three steps, exactly two steps are
forward, and one step is backwards, because otherwise
there's no way the person will end up in position 1. To do so, there, again, are
three possibilities in which we go forward, forward,
backward, or forward, backward, forward, or backward,
forward, forward. And that exhausts all the
possibilities that the person can end up in position
1 after three steps. And we'll define the collection
of all these outcomes as event C. The
probability of event C-- same as before-- is simply the sum of
the probability of each individual outcome. Now, based on the independence
assumption that we used before, each outcome here has
the same probability, which is equal to P squared times 1 minus
P. The P squared comes from the fact that two forward
steps are taken, and 1 minus P, the probability of that
one backwards step. And since there are three of
them, we multiply 3 in front, and that will give us
the probability. In the last part of the problem,
we're asked to compute that, conditional on
event C already took place, what is the probability that the
first step he took was a forward step? Without going into the details,
let's take a look at the C, in which we have three
elements, and only the first two elements correspond
to a forward step in the first step. So we can define event
D as simply the first two outcomes-- forward, forward, backward, and forward, backward, forward. Now, the probability we're
interested in is simply probability of D conditional on
C. We'd write it out using the law of conditional
probability-- D intersection C conditional
on C. Now, because D is a subset of C, we have probability
of D divided by the probability of C. Again, because all samples
here have the same probability, all we need to do
is to count the number of samples here, which is 2, and
divide by the number of samples here, which is 3. So we end up with 2 over 3. And that concludes
the problem. See you next time.  

Math for Eng.

The following
content is provided under a Creative
Commons license. Your support will help MIT
OpenCourseWare continue to offer high quality
educational resources for free. To make a donation, or
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: Let's start. Are there any questions? We would like to have a
perspective for this really common observation that if you
have a gas that is initially in one half of a box, and
the other half is empty, and some kind of a partition
is removed so that the gas can expand, and it can
flow, and eventually we will reach another
equilibrium state where the gas occupies more chambers. How do we describe
this observation? We can certainly characterize
it thermodynamically from the perspectives
of atoms and molecules. We said that if I
want to describe the configuration of the gas
before it starts, and also throughout the expansion,
I would basically have to look at all sets
of coordinates and momenta that make up this particle. There would be some
point in this [? six ?], and I mention our
phase space, that would correspond to where
this particle was originally. We can certainly follow
the dynamics of this point, but is that useful? Normally, I could
start with billions of different types of
boxes, or the same box in a different
instance of time, and I would have totally different
initial conditions. The initial
conditions presumably can be characterized to a
density in this phase space. You can look at some volume
and see how it changes, and how many points
you have there, and define this
phase space density row of all of the
Q's and P's, and it works as a function of time. One way of looking at how it
works as a function of time is to look at this box
and where this box will be in some other
instance of time. Essentially then, we are
following a kind of evolution that goes along this streamline. Basically, the derivative
that we are going to look at involves changes both
explicitly in the time variable, and also increasingly
to the changes of all of the
coordinates and momenta, according to the Hamiltonian
that governs the system. I have to do, essentially,
a sum over all coordinates. I would have the
change in coordinate i, Qi dot, dot, d row by dQi. Then I would have Pi, dot-- I
guess these are all vectors-- d row by dPi. There are six end coordinates
that implicitly depend on time. In principle, if I am
following along the streamline, I have to look at
all of these things. The characteristic of evolution,
according to some Hamiltonian, was that this volume of
phase space does not change. Secondly, we could
characterize, once we wrote Qi dot, as dH by dP,
and the i dot as the H by dQ. This combination of derivatives
essentially could be captured, and be written as 0 by
dt is the Poisson bracket of H and [? P. ?] One of the things,
however, that we emphasize is that as far as evolution
according to a Hamiltonian and this set of
dynamics is concerned, the situation is completely
reversible in time so that some
intermediate process, if I were to reverse all of
the momenta, then the gas would basically come back
to the initial position. That's true. There is nothing to do about it. That kind of seems to
go against the intuition that we have from
thermodynamics. We said, well, in
practical situations, I really don't care about
all the six end pieces of information that
are embedded currently in this full phase
space density. If I'm really trying
to physically describe this gas expanding,
typically the things that I'm interested in are
that at some intermediate time, whether the particles
have reached this point or that
point, and what is this streamline velocity
that I'm seeing before the thing relaxes, presumably,
eventually into zero velocity? There's a lot of
things that I would need to characterize
this relaxation process, but that is still
much, much, much less than all of the information
that is currently encoded in all of these six end
coordinates and momenta. We said that for things that
I'm really interested in, what I could, for
example, look at, is a density that involves
only one particle. What I can do is to
then integrate over all of the positions and
coordinates of particles that I'm not interested in. I'm sort of repeating this to
introduce some notation so as to not to repeat all of
these integration variables, so I will call dVi the phase
place contribution of particle i. What I may be interested in
is that this is something that, if I integrate
over P1 and Q1, it is clearly
normalized to unity because my row, by definition,
was normalized to unity. Typically we may be
interested in something else that I call F1, P1
Q1 P, which is simply n times this-- n times
the integral product out i2 to n, dVi, the full row. Why we do that is
because typically you are interested or used
to calculating things in [? terms ?] of
a number density, like how many particles are
within some small volume here, defining the density so
that when I integrate over the entire volume
of f1, I would get the total number of
particles, for example. That's the kind of normalization
that people have used for f. More generally,
we also introduced fs, which depended on
coordinates representing s sets of points, or s
particles, if you like, that was normalized to be-- We said, OK, what I'm really
interested in, in order to calculate the properties of
the gases it expands in terms of things that I'm
able to measure, is f1. Let's write down the
time evolution of f1. Actually, we said,
let's write down the time evolution
of fs, along with it. So there's the time
evolution of fs. If I were to go
along this stream, it would be the fs
by dt, and then I would have contributions
that would correspond to a the changes in
coordinates of these particles. In order to progress
along this direction, we said, let's define
the total Hamiltonian. We will have a simple form,
and certainly for the gas, it would be a good
representation. I have the kinetic energies
of all of the particles. I have the box that confines
the particles, or some other one particle potential, if you like,
but I will write in this much. Then you have the interactions
between all pairs of particles. Let's write it as sum over i,
less than j, V of Qi minus Qj. This depends on n set of
particles, coordinates, and momenta. Then we said that for purposes
of manipulations that you have to deal with, since
there are s coordinates that are appearing here
whose time derivatives I have to look at, I'm going
to simply rewrite this as the
contribution that comes from those s particles,
the contribution that comes from the remaining
n minus s particles, and some kind of [? term ?]
that covers the two sets of particles. This, actually, I didn't quite
need here until the next stage because what I write
here could, presumably, be sufficiently general,
like we have here some n running from 1 to s. Let me be consistent
with my S's. Then I have Qn, dot, dFs by
dQn, plus Pn, dot, dFs by dPn. If I just look at the
coordinates that appear here, and say, following this
as they move in time, there is the explicit
time dependence on all of the implicit
time dependence, this would be the
total derivative moving along the streamline. Qn dot I know is
simply the momentum. It is the H by dPn. The H by dPn I have from
this formula over here. It is simply Pn divided by m. It's the velocity--
momentum divided by mass. This is the velocity
of the particle. Pn dot, the rate of
change of momentum is the force that is
acting on the particle. What I need to do is to take
the derivatives of various terms here. So I have minus dU by dPn. What is this? This is essentially the
force that the particle feels from the
external potential. If you are in the
box in this room, It is zero until you
hit the edge of the box. I will call this Fn to represent
external potential that is acting on the system. What else is there? I have the force that will
come from the interaction with all other [? guys. ?] I
will write here a sum over m, dV of Qm minus Qn,
by dQn-- dU by dQm. I'm sorry. What is this? This Is basically the
sum of the forces that is exerted by the n
particle on the m particle. Define it in this fashion. If this was the entire
story, what I would have had here is a group of
s particles that are dominated by
their own dynamics. If there is no other
particle involved, they basically have to
satisfy the Liouville equation that I have written, now
appropriate to s particles. Of course, we know that
that's not the entire story because there are
all these other terms involving the interactions
with particles that I have not included. That's the whole
essence of the story. Let's say I want to think
about one or two particles. There is the interaction
between the two particles, and they would be evolving
according to some trajectories. But there are all of
these other particles in the gas in this room
that will collide with them. So those conditions
are not something that we had in the
Liouville equation, with everything considered. Here, I have to
include the effect of all of those other particles. We saw that the
way that it appears is that I have to imagine that
there's another particle whose coordinates and
momenta are captured through some volume for
the s plus 1 particle. This s plus 1
particle can interact with any of the particles
that are in the set that I have on the other side. There is an index
that runs from 1 to s. What I would have here is the
force that will come from this s plus 1 particle, acting on
particle n the same way that this force was deriving
the change of the momentum, this force will derive the
change of the momentum of-- I guess I put an m here-- The thing that I
have to put here is now a density that also
keeps track of the probability to find the s plus 1 particle
in the location in phase place that I need to
integrate with both. I have to integrate
over all positions. One particle is moving along
a straight line by itself, let's say. Then there are all of the
other particles in the system. I have to ask, what is
the possibility that there is a second particle with
some particular momentum and coordinate that I
will be interacting with. This is the general set up
of these D-B-G-K-Y hierarchy of equations. At this stage, we
really have just rewritten what we had for
the Liouville equation. We said, I'm really, really
interested only one particle [? thing, ?] row one and F1. Let's focus on that. Let's write those
equations in more detail In the first equation, I
have that the explicit time dependence, plus the time
dependence of the position coordinate, plus the time
dependence of the momentum coordinate, which is driven
by the external force, acting on this one
particle density, which is dependent on
p1, q1 at time t. On the right hand
side of the equation. I need to worry about a second
particle with momenta P2 at position Q2 that
will, therefore, be able to exert a force. Once I know the position,
I can calculate the force that particle exerts. What was my notation? The order was 2 and
1, dotted by d by dP1. I need now f2, p1, q2 at time t. We say, well, this
is unfortunate. I have to worry about
dependence on F2, but maybe I can get
away with things by estimating
order of magnitudes of the various terms. What is the left hand
side set of operations? The left hand side
set of operations describes essentially one
particle moving by itself. If that particle has to cross
a distance of this order of L, and I tell you that the typical
velocity of these particles is off the order of V, then
that time scale is going to be of the order of L
over V. The operations here will give me
a V over L, which is what we call the
inverse of Tau u. This is a reasonably
long macroscopic time. OK, that's fine. How big is the right hand side? We said that the
right hand side has something to do with collisions. I have a particle in my system. Let's say that particle has
some characteristic dimension that we call d. This particle is moving with
velocity V. Alternatively, you can think of this
particle as being stationary, and all the other
particles are coming at it with some velocity V. If I say that the density
of these particles is n, then the typical time for which,
as I shoot these particles, they will hit this
target is related to V squared and V, the
volume of particles. Over time t, I have to
consider this times V tau x. V tau xn V squared should
be of the order of one. This gave us a
formula for tau x. The inverse of tau
x that controls what's happening on this
side is n V squared V. Is the term on the right
hand side more important, or the term on the
left hand side? The term on the
right hand side has to do with the two body term. There's a particle
that is moving, and then there's
another particle with a slightly different
velocity that it is behind it. In the absence of
collisions, these particles would just go along
a straight line. They would bounce off the
walls, but the magnitude of their energy,
and hence, velocity, would not change from
these elastic collisions. But if the particles
can catch up and interact, which is governed
by V2, V on the other side, then what happens is that the
particles, when they interact, would collide and
go different ways. Quickly, their velocities,
and momenta, and everything would get mixed up. How rapidly that happens depends
on this collision distance, which is much less than
the size of the system, and, therefore, the term that
you have on the right hand side in magnitude is
much larger than what is happening on
the left hand side. There is no way in
order to describe the relaxation of the gas
that I can neglect collisions between gas particles. If I neglect collisions
between gas particles, there is no reason why
the kinetic energies of individual particles
should change. They would stay
the same forever. I have to keep this. Let's go and look at the second
equation in the hierarchy. What do you have? You have d by dT, P1 over m d
by d Q1, P2 over m, P d by d Q2. Then we have F1 d
by d Q1, plus F2, d by d Q2 coming from
the external potential. Then we have the force
that the involves the collision between
particles one and two. When I write down the
Hamiltonian for two particles, there is going to be
already for two particles and interactions between them. That's where the
F1 2 comes from. F1 2 changes d by the
momentum of particle one. I should write, it's
2 1 that changes momentum of particle two. But as 2 1 is simply minus F1
2, I can put the two of them together in this fashion. This acting on F2 is
then equal to something like integral over V3, F3 1,
d by dP1, plus F3 2, d by dP2. [INAUDIBLE] on F3 P1
and Q3 [INAUDIBLE]. Are we going to do this forever? Well, we said, let's
take another look at the magnitude of
the various terms. This term on the
right hand side still involves a collision that
involves a third particle. I have to find that
third particle, so I need to have,
essentially, a third particle within some
characteristic volume, so I have something
that is of that order. Whereas on the
left hand side now, I have a term that
from all perspectives, looks like the kinds of terms
that I had before except that it involves the collision
between two particles. What it describes is the
duration that collision. We said this is of the
order of 1 over tau c, which replaces
the n over there with some characteristic
dimension. Suddenly, this term is very big. We should be able to use that. There was a question. AUDIENCE: On the left hand
side of both of your equations, for F1 and F2, shouldn't
all the derivatives that are multiplied by your
forces be derivatives of the effects of momentum?
[INAUDIBLE] the coordinates? [INAUDIBLE] reasons? PROFESSOR: Let's go back here. I have a function that
depends on P, Q, and t. Then there's the explicit
time derivative, d by dt. Then there is the Q dot here,
which will go by d by dQ. Then there's the P dot term
that will go by d by dP. All of things have to be there. I should have derivatives
in respect to momenta, and derivatives with
respect to coordinate. Dimensions are, of
course, important. Somewhat, what I write
for this and for this should make up for that. As I have written it now,
it's obvious, of course. This has dimensions of Q
over T. The Q's cancel. I would have one over
T. D over Dps cancel. I have 1 over P. Here,
dimensionality is correct. I have to just make sure
I haven't made a mistake. Q dot is a velocity. Velocity is momentum
divided by mass. So that should
dimensionally work out. P dot is a force. Everything here is force. In a reasonable coordinate-- AUDIENCE: [INAUDIBLE] PROFESSOR: What did I do here? I made mistakes? AUDIENCE: [INAUDIBLE] PROFESSOR: Why didn't
you say that in a way that-- If I don't
understand the question, please correct me before I
spend another five minutes. Hopefully, this is now
free of these deficiencies. This there is very big. Now, compared to the
right hand side in fact, we said that the
right hand side is smaller by a factor that
measures how many particles are within an
interaction volume. And for a typical
gas, this would be a number that's of the
order of 10 to the minus 4. Using 10 to the minus
4 being this small, we are going to set the
right hand side to zero. Now, I don't have to
write the equation for F2. I'll answer a question here that
may arise, which is ultimately, we will do sufficient
manipulations so that we end up with a
particular equation, known as the Boltzmann
Equation, that we will show does not obey
the time reversibility that we wrote over here. Clearly, that is built in to the
various approximations I make. The first question
is, the approximation that I've made here,
did I destroy this time reversibility? The answer is no. You can look at this
set of equations, and do the manipulations
necessary to see what happens if P goes to minus
P. You will find that you will be able to reverse your
trajectory without any problem. Yes? AUDIENCE: Given that it
is only an interaction from our left side
that's very big, that's the reason why we can
ignore the stuff on the right. Why is it that we
are then keeping all of the other terms that
were even smaller before? PROFESSOR: I will ignore them. Sure. AUDIENCE: [LAUGHTER] PROFESSOR: There was the
question of time reversibility. This term here has to do
with three particles coming together, and how
that would modify what we have for just
two-body collisions. In principle, there
is some probability to have three particles
coming together and some combined interactions. You can imagine some
fictitious model, which in addition to these
two-body interactions, you cook up some body
interaction so that it precisely cancels what
would have happened when three particles
come together. We can write a computer
program in which we have two body conditions. But if three bodies come
close enough to each other, they essentially become ghosts
and pass through each other. That computer program
would be fully reversible. That's why sort of
dropping this there is not causing any
problems at this point. What is it that you
have included so far? What we have is a situation
where the change in F1 is governed by a
process in which I have a particle that I
describe on the left hand side with momentum one, and
it collides with some particle that I'm integrating over, but
in some particular instance of integration, has momentum P2. Presumably they come
close enough to each other so that afterwards, the
momenta have changed over so that I have some P1 prime,
and I have some P2 prime. We want to make sure that we
characterize these correctly. There was a question about
while this term is big, these kinds of terms are small. Why should I basically
bother to keep them? It is reasonable. What we are following here are
particles in my picture that were ejected by the
first box, and they collide into each
other, or they were colliding in the first box. As long as you are away from
the [? vols ?] of the container, you really don't care
about these terms. They don't really
moved very rapidly. This is the process of
collision of two particles, and it's also the same process
that is described over here. Somehow, I should be able
to simplify the collision process that is going on
here with the knowledge that the evolution
of two particles is now completely deterministic. This equation by itself
says, take two particles as if they are the only
thing in the universe, and they would follow some
completely deterministic trajectory, that if you
put lots of them together, is captured through
this density. Let's see whether we can
massage this equation to look like this equation. Well, the force term, we
have, except that here we have dP by P1 here. We have d by dP
1 minus d by dP2. So let's do this. Minus d by dP2, acting on F2. Did I do something wrong? The answer is no, because I
added the complete derivative over something that
I'm integrating over. This is perfectly
legitimate mathematics. This part now looks like this. I have to find what is the
most important term that matches this. Again, let's think
about this procedure. What I have to make
sure of is what is the extent of the
collision, and how important is the collision? If I have one
particle moving here, and another particle off there,
they will pass each other. Nothing interesting
could happen. The important thing is how
close they come together. It Is kind of
important that I keep track of the relative
coordinate, Q, which is Q2 minus Q1,
as opposed to the center of mass coordinate, which
is just Q1 plus Q2 over 2. That kind of also
indicates maybe it's a good thing for me to
look at this entire process in the center of mass frame. So this is the lab frame. If I were to look at this
same picture in the center of mass frame,
what would I have? In the center of mass frame, I
would have the initial particle coming with P1 prime, P1
minus P center of mass. The other particle that
you are interacting with comes with P2 minus
P center of mass. I actually drew these
vectors that are hopefully equal and opposite,
because you know that in the center of
mass, one of them, in fact, would be P1 minus P2 over 2. The other would be
P2 minus P1 over 2. They would, indeed,
in the center of mass be equal and opposite momenta. Along the direction
of these objects, I can look at how close
they come together. I can look at some
coordinate that I will call A, which measures
the separation between them at some instant of time. Then there's another
pair of coordinates that I could put into a vector
that tells me how head to head they are. If I think about they're
being on the center of mass, two things that are
approaching each other, they can either
approach head on-- that would correspond
to be equal to 0-- or they could be slightly
off a head-on collision. There is a so-called
impact parameter B, which is a measure
of this addition fact. Why is that going to
be relevant to us? Again, we said that there
are parts of this expression that all of the
order of this term, they're kind of
not that important. If I think about the collision,
and what the collision does, I will have forces that
are significant when I am within this
range of interactions, D. I really have to look at
what happens when the two things come close to each other. It Is only when this relative
parameter A has approached D that these particles
will start to deviate from their straight
line trajectory, and presumably go, to say in
this case, P2 prime minus P center of mass. This one occurs
[? and ?] will go, and eventually P1 prime
minus P center of mass. These deviations will
occur over a distance that is of the order of
this collision and D. The important changes that
occur in various densities, in various
potentials, et cetera, are all taking place when this
relative coordinate is small. Things become big when the
relative coordinate is small. They are big as a function
of the relative coordinate. In order to get big
things, what I need to do is to replace these
d by dQ's with the corresponding
derivatives with respect to the center of mass. One of them would come
be the minus sign. The other would come
be the plus sign. It doesn't matter
which is which. It depends on the
definition, whether I make Q2 minus Q1,
or Q1 minus Q2. We see that the big
terms are the force that changes the momenta
and the variations that you have over these
relative coordinates. What I can do now
is to replace this by equating the two big
terms that I have over here. The two big terms
are P2 minus P1 over m, dotted by d by dQ of F2. There is some other
approximation that I did. As was told to me before,
this is the biggest term, and there is the
part of this that is big and compensates for that. But there are all these
other bunches of terms. There's also this d by dt. What I have done
over here is to look at this slightly coarser
perspective on time. Increasing all the equations
that I have over there tells me everything
about particles approaching each
other and going away. I can follow through
the mechanics precisely everything
that is happening, even in the vicinity
of this collision. If I have two squishy balls,
and I run my hand through them properly, I can
see how the things get squished then released. There's a lot of
information, but again, a lot of information
that I don't really care to know as far as
the properties of this gas expansion process is concerned. What you have done is to forget
about the detailed variations in time and space that
are taking place here. We're going to shortly make
that even more explicit by noting the following. This integration over
here is an integration over phase space of
the second particle. I had written before d
cubed, P2, d cubed, Q2, but I can change
coordinates and look at the relative
coordinate, Q, over here. What I'm asking is,
I have one particle moving through the gas. What is the chance that
the second particle comes with momentum P2, and the
appropriate relative distance Q, and I integrate over both the
P and the relative distance Q? This is the quantity
that I have to integrate. Let's do one more
calculation, and then we will try to give a
physical perspective. In this picture of the center
of mass, what did I do? I do replaced the coordinate, Q,
with a part that was the impact parameter, which
had two components, and a part that was
the relative distance. What was this relative distance? The relative
distance was measured along this line that was
giving me the closest approach. What is the direction
of this line? The direction of this
line is P1 minus P2. This is P1 minus P2 over 2. It doesn't matter. The direction is P1 minus P2. What I'm doing here is I am
taking the derivative precisely along this line of
constant approach. I'm taking a derivative, and
I'm integrating along that. If I were to rewrite the
whole thing, what do I have? I have d by dt, plus P1 over m,
d by dQ1, plus F1, d by dP1-- don't make a mistake--
acting on F1, P1, Q1, t. What do I have to write
on the right hand side? I have an integral
over the momentum of this particle with which
I'm going to make a collision. I have an integral over
the impact parameter that tells me the distance
of closest approach. I have to do the
magnitude of P2 minus P1 over n, which is
really the magnitude of the relative velocity
of the two particles. I can write it as P2
minus P1, or P1 minus P2. These are, of course, vectors. and I look at the modulus. I have the integral
of the derivative. Very simply, I will
write the answer as F2 that is evaluated
at some large distance, plus infinity minus F2
evaluated at minus infinity. I have infinity. In principle, I have
to integrate over F2 from minus infinity
to plus infinity. But once I am beyond
the range of where the interaction changes,
then the two particles just move away forever. They will never see each other. Really, what I should write here
is F2 of-- after the collision, I have P1 prime, P2
prime, at some Q plus, minus F2, P1, P2, at
some position minus. What I need to do is to
do the integration when I'm far away from the
collision, or wait until I am far
after the collision. Really, I have to just
integrate slightly below, after, and before the collision occurs. In principle, if I
just go a few d's in one direction or
the other direction, this should be enough. Let's see physically
what this describes. There is a connection
between this and this thing that I
had over here, in fact. This equation on the left
hand side, if it was zero, it would describe
one particle that is just moving by itself until
it hits the wall, at which point it basically
reverses its trajectory, and otherwise goes forward. But what you have on
the right hand side says that suddenly
there could be another particle with
which I interact. Then I change my direction. I need to know the
probability, given that I'm moving
with velocity P1, that there is a second
particle with P2 that comes close enough. There is this additional factor. From what does this
additional factor come? It's the same factor
that we have over here. It is, if you have a
target of size d squared, and we have a set of
bullets with a density of n, the number of collisions that
I get depends both on density and how fast these things go. The time between collisions, if
you like, is proportional to n, and it is also related to
V. That's what this is. I need some kind of a time
between the collisions that I make. I have already
specified that I'm only interested in the
set of particles that have momentum P2 for this
particular [? point in ?] integration, and that they
have this kind of area or cross section. So I replace this
V squared and V with the relative coordinates. This is the corresponding
thing to V squared, and this is really a
two particle density. This is a subtraction. The addition is
because it is true that I'm going with velocity P1,
and practically, any collisions that are significant
will move me off kilter. So there has to be a
subtraction for the channel that was described by P1
because of this collision. This then, is the
addition, because it says that it could be that
there is no particle going in the horizontal direction. I was actually coming along
the vertical direction. Because of the
collision, I suddenly was shifted to move
along this direction. The addition comes from having
particles that would correspond to momenta that somehow,
if I were in some sense to reverse this, and
then put a minus sign, a reverse collision
would create something that was along the
direction of P1. Here I also made
several approximations. I said, what is chief among
them is that basically I ignored the details of the
process that is taking place at scale the order of
d, so I have thrown away some amount of detail
and information. It is, again, legitimate
to say, is this the stage at which you
made an approximation so that the time
reversibility was lost? The answer is still no. If you are careful enough with
making precise definitions of what these Q's are before
and after the collision, and follow what happens if you
were to reverse everything, you'll find that the
equations is fully reversible. Even at this stage, I have
not made any transition. I have made approximations,
but I haven't made something to be time irreversible. That comes at the
next stage where we make the so-called
assumption of molecular chaos. The assumption is
that what's the chance that I have a particle
here and a particle there? You would say, it's
a chance that I have one here and one there. You say that if two of
any P1, P2, Q1, Q2, t is the same thing as the product
of F1, P1, Q1, t, F1, P2, Q2, t. Of course, this assumption
is generally varied. If I were to look
at the probability that I have two particles
as a function of, let's say, the relative
separation, I certainly expect that if
they are far away, the density should be the
product of the one particle densities. But you would say that if the
two particles come to distances that are closer than
their separation d, then the probability and
the range of interaction d-- and let's say the
interaction is highly repulsive like hardcore-- then
the probability should go to 0. Clearly, you can
make this assumption, but up to some degree. Part of the reason we
went through this process was to indeed make sure that
we are integrating things at the locations where
the particles are far away from each other. I said that the range of
that integration over A would be someplace
where they are far apart after the
collision, and far apart before the collision. You have an
assumption like that, which is, in
principle, something that I can insert into that. Having to make a distinction
between the arguments that are appearing in this equation
is kind of not so pleasant. What you are going to do is
to make another assumption. Make sure that everything is
evaluated at the same point. What we will eventually now have
is the equation that d by dt, plus P1 over n, d by dQ1,
plus F1, dot, d by dP1, acting on F1, on
the left hand side, is, on the right hand side,
equal to all collisions in the particle of
momentum P2, approaching at all possible cross
sections, calculating the flux of the
incoming particle that corresponds to
that channel, which is proportional to V2 minus V1. Then here, we subtract the
collision of the two particles. We write that as F1 of
P1 at this location, Q1, t, F1 of t2 at the
same location Q1, t. Then add F1 prime, P1 prime, Q1
t, F1 prime, P2 prime, Q2, t. In order to make the
equation eventually manageable, what you
did is to evaluate all off the coordinates that
we have on the right hand side at the same location, which
is the same Q1 that you specify on the left hand side. That immediately means
that what you have done is you have changed the
resolution with which you are looking at space. You have kind of washed
out the difference between here and here. Your resolution has to
put this whole area that is of the order of d squared
or d cubed in three dimensions into one pixel. You have changed the
resolution that you have. You are not looking at things
at this [? fine ?] [? state. ?] You are losing additional
information here through this change of
the resolution in space. You have also lost
some information in making the assumption that
the two [? point ?] densities are completely within always
as the product one particle densities. Both of those things
correspond to taking something that is very precise
and deterministic, and making it kind of vague
and a little undefined. It's not surprising then,
that if you have in some sense changed the precision of
your computer-- let's say, that is running the particles
forward-- at some point, you've changed the resolution. Then you can't
really run backward. In fact, to sort of precisely
be able to run the equations forward and backward,
you would need to keep resolution
at all levels. Here, we have sort of removed
some amount of resolution. We have a very good guess
that the equation that you have over here no longer
respects time reversal inversions that you
had originally posed. Our next task is to prove
that you need this equation. It goes in one particular
direction in time, and cannot be drawn
backward, as opposed to all of the predecessors that I
had written up to this point. Are there any questions? AUDIENCE: [INAUDIBLE] PROFESSOR: Yes, Q prime
and Q1, not Q1 prime. There is no dash. AUDIENCE: Oh, I see. It is Q1. PROFESSOR: Yes, it is. Look at this equation. On the left hand side,
what are the arguments? The arguments are P1 and Q1. What is it that I have
on the other side? I still have P1 and Q1. I have introduced
P1 and b, which is simply an impact parameter. What I will do is
I will evaluate all of these things, always
at the same location, Q1. Then I have P1 and P2. That's part of my story of
the change in resolution. When I write here Q1,
and you say Q1 prime, but what is Q1 prime? Is it Q1 plus b? Is it Q1 minus b? Something like this
I'm going to ignore. It's also legitimate,
and you should ask, what is P1
prime and Q2 prime? What are they? What I have to do, is I
have to run on the computer or otherwise, the
equations for what happens if I have P1
and P2 come together at an impact parameter
that is set by me. I then integrate
the equations, and I find that deterministically,
that collision will lead to some P1
prime and P2 prime. P1 prime and P2 prime are
some complicated functions of P1, P2, and b. Given that you know two
particles are approaching each other at distance d with
momenta P1 P2, in principle, you can integrate
Newton's equations, and figure out with what
momenta they end up. This equation, in fact, hides a
very, very complicated function here, which describes
P1 prime and P2 prime as a function of P1 and P2. If you really needed all of
the details of that function, you would surely be in trouble. Fortunately, we don't. As we shall see shortly, you
can kind of get a lot of mileage without knowing that. Yes, what is your question? AUDIENCE: There
was an assumption that all the interactions
between different molecules are central potentials
[INAUDIBLE]. Does the force of the
direction between two particles lie along the [INAUDIBLE]? PROFESSOR: For the things that
I have written, yes it does. I should have been more precise. I should have put
absolute value here. AUDIENCE: You have
particles moving along one line
towards each other, and b is some arbitrary vector. You have two directions,
so you define a plane. Opposite direction particles
stay at the same plane. Have you reduced-- PROFESSOR: Particles
stay in the same plane? AUDIENCE: If the two particles
were moving towards each other, and also you have
in the integral your input parameter,
which one is [INAUDIBLE]. There's two directions. All particles align,
and all b's align. They form a plane. [? Opposite ?] direction
particles [? stand ?] in the-- PROFESSOR: Yes, they
stand in the same plane. AUDIENCE: My
question is, what is [INAUDIBLE] use the
integral on the right from a two-dimensional
integral [? in v ?] into employing central symmetry? PROFESSOR: Yes, you could. You could, in principle, write
this as b db, if you like, if that's what you want. AUDIENCE: [INAUDIBLE] PROFESSOR: Yes, you
could do that if you have simple enough potential. Let's show that this equation
leads to irreversibility. That you are going to do here. This, by the way, is called
the Boltzmann equation. There's an associated
Boltzmann H-Theorem, which restates the following--
If F of P1, Q1, and t satisfies the above
Boltzmann equation, then there is a quantity H that
always decreases in time, where H is the integral over P
and Q of F1, log of F1. The composition of
irreversibility, as we saw in thermal
dynamics, was that there was a
quantity entropy that was always increasing. If you have calculated
for this system, entropy before for the half
box, and entropy afterwards for the space both boxes
occupy, the second one would certainly be larger. This H is a quantity like
that, except that when it is defined this
way, it always decreases as a function of time. But it certainly is very
much related to entropy. You may have asked,
why did Boltzmann come across such
a function, which is F log F, except that
actually right now, you should know
why you write this. When we were dealing
with probabilities, we introduced the entropy of the
probability distribution, which was related to something
like sum over iPi, log of Pi, with a minus sign. Up to this factor
of normalization N, this F1 really is a
one-particle probability. After this normalization
N, you have a one-particle probability,
the probability that you have occupation
of one-particle free space. This occupation of
one-particle phase space is changing as a
function of time. What this statement says is
that if the one-particle density evolves in time according
to this equation, the corresponding
minus entropy decreases as a function of time. Let's see if that's the case. To prove that, let's do this. We have the formula for H, so
let's calculate the H by dt. I have an integral
over the phase space of particle one, the particle
that I just called one. I could have
labeled it anything. After integration, H is
only a function of time. I have to take the
time derivative. The time derivative
can act on F1. Then I will get the F1
by dt, times log F1. Or I will have F1 times
the derivative of log F1. The derivative of log F1
would be dF1 by dt, and then 1 over F1. Then I multiply by F1. This term is simply 1. AUDIENCE: Don't you want to
write the full derivative, F1 with respect [INAUDIBLE]? PROFESSOR: I thought we
did that with this before. If you have something that
I am summing over lots of [? points, ?] and these
[? points ?] can be positioned, then I have S at location
one, S at location two, S at location three,
discretized versions of x. If I take the time
derivative, I take the time derivative
of this, plus this, plus this, which are
partial derivatives. If I actually take the
time derivative here, I get the integral d cubed P1,
d cubed Q1, the time derivative. This would be that
partial dF1 by dt is the time derivative
of n, which is 0. The number of particles
does not change. Indeed, I realize that 1
integrated against dF1 by dt is the same thing that's here. This term gives you 0. All I need to worry
about is integrating log F against the Fydt. I have an integral over P1 and
Q1 of log F against the Fydt. We have said that F1 satisfies
the Boltzmann equation. So the F1 by dt, if I
were to rearrange it, I have the F1 by dt. I take this part to the
other side of the equation. This part is also
the Poisson bracket of a one-particle H with F1. If I take it to
the other side, it will be the Poisson
bracket of H with F1. Then there is this
whole thing that involves the collision
of two particles. So I define whatever is
on the right hand side to be some collision
operator that acts on two [? powers ?] of F1. This is plus a collision
operator, F1, F1. What I do is I
replace this dF1 by dt with the Poisson bracket of H,
or H1, if you like, with F1. The collision operator I will
shortly write explicitly. But for the time being, let
me just write it as C of F1. There is a first
term in this sum-- let's call it number one--
which I claim to be 0. Typically, when you
get these integrations with Poisson brackets,
you would get 0. Let's explicitly show that. I have an integral over
P1 and Q1 of log of F1, and this Poisson
bracket of H1 and F1, which is essentially
these terms. Alternatively, I could write
it as dH1 by dQ1, dF1 by dt1, minus the H1 by
dt1, dF1, by dQ1. I've explicitly written this
form for the one-particle in terms of the Hamiltonian. The advantage of
that is that now I can start doing
integrations by parts. I'm taking derivatives
with respect to P, but I have integrations
with respect to P here. I could take the F1 out. I will have a minus. I have an integral, P1, Q1. I took F1 out. Then this d by dP1 acts on
everything that came before it. It can act on the H1. I would get d2 H1 with
respect to dP1, dQ1. Or it could act on the
log of F1, in which case I will get set dH1 by dQ1. Then I would have d
by dP acting on log of F, which would
give me dF1 by dP1, then the derivative of the
log, which is 1 over F1. This is only the first term. I also have this term, with
which I will do the same thing. AUDIENCE: [INAUDIBLE] The
second derivative [INAUDIBLE] should be multiplied
by log of F. PROFESSOR: Yes, it should be. It is Log F1. Thank you. For the next term, I have F1. I have d2 H1, and the other
order of derivatives, dQ1, dP1. Now I'll make sure I
write down the log of F1. Then I have dH1
with respect to dQ1. Then I have a dot product with
the derivative of log F, which is the derivative of F1 with
respect to Q1 and 1 over F1. Here are the terms
that are proportional to the second derivative. The order of the
derivatives does not matter. One often is positive. One often is negative,
so they cancel out. Then I have these
additional terms. For the additional
terms, you'll note that the F1 and the
1 over F1 cancels. These are just a product
of two first derivatives. I will apply the five
parts process one more time to get rid of the derivative
that is acting on F1. The answer becomes plus
d cubed P1, d cubed Q1. Then I have F1, d2 H1, dP1,
dQ1, minus d2 H1, dQ1, dP1. These two cancel each other
out, and the answer is 0. So that first term vanishes. Now for the second term,
number two, what I have is the first term vanished. So I have the H by dt. It is the integral
over P1 and Q1. I have log of F1. F1 is a function of
P1, and Q1, and t. I will focus, and make sure I
write the argument of momentum, for reasons that will
become shortly apparent. I have to multiply with
the collision term. The collision term
involves integrations over a second particle,
over an impact parameter, a relative velocity, once I
have defined what P2 and P1 are. I have a subtraction
of F evaluated at P1, F evaluated at
P2, plus addition, F evaluated at P1 prime,
F evaluated at P2 prime. Eventually, this whole thing
is only a function of time. There are a whole bunch of
arguments appearing here, but all of those arguments
are being integrated over. In particular, I have arguments
that are indexed by P1 and P2. These are dummy
variables of integration. If I have a function
of x and y that I'm integrating over x and
y, I can call x "z." I can call y "t." I would integrate
over z and t, and I would have the same answer. I would have exactly
the same answer if I were to call all of the
dummy integration variable that is indexed 1, "2." Any dummy variable
that is indexed 2, if I rename it and call it 1,
the integral would not change. If I do that, what do I have? I have integral
over Q-- actually, let's get of the
integration number on Q. It really doesn't matter. I have the integrals
over P1 and P1. I have to integrate over
both sets of momenta. I have to integrate over
the cross section, which is relative between 1 and 2. I have V2 minus V1, rather
than V1 minus V2, rather than V2 minus V1. The absolute value
doesn't matter. If I were to replace these
indices with an absolute value, [? or do a ?] V2 minus V1
goes to minus V1 minus V2. The absolute value
does not change. Here, what do I have? I have minus F of P1. It becomes F of P2, F of
P1, plus F of P2 prime, f of P1 prime. They are a product. It doesn't really matter in
which order I write them. The only thing
that really matters is that the argument was
previously called F1 of P1 for the log, and now it
will be called F1 of P2. Just its name changed. If I take this, and the
first way of writing things, which are really two ways of
writing the same integral, and just average them, I will
get 1/2 an integral d cubed Q, d cubed P1, d cubed P2,
d2 b, and V2 minus V1. I will have F1 of P1, F1 of
P2, plus F1 of P1 prime, F1 of P2 prime. Then in one term, I
had log of F1 of P1, and I averaged it
with the other way of writing things, which was
log of F-- let's put the two logs together, multiplied by F1. So the sum of the
two logs I wrote, that's a log of the product. I just rewrote that equation. If you like, I symmetrized It
with respect to index 1 and 2. So the log of 1,
that previously had one argument through
this symmetrization, became one half
of the sum of it. The next thing one has to
think about, what I want to do, is to replace primed and
unprimed coordinates. What I would
eventually write down is d cubed P1 prime, d cubed P2
prime, d2 b, V2 prime minus V1 prime, minus F1 of P1 prime,
F1 of P2 prime, plus F1 of P1, F1 of P2. Then log of F1 of P1
prime, F1 of P2 prime. I've symmetrized originally
the indices 1 and 2 that were not quite
symmetric, and I end up with an expression that has
variables P1, P2, and functions P1 prime and P2 prime, which
are not quite symmetric again, because I have F's evaluated
for P's, but not for P primes. What does this mean? This mathematical expression
that I have written down here actually is not correct,
because what this amounts to, is to change variables
of integration. In the expression
that I have up here, P1 and P2 are variables
of integration. P1 prime and P2 prime are
some complicated functions of P1 and P2. P1 prime is some complicated
function that I don't know. P1, P2, and V, for which I
need to solve in principle, is Newton's equation. This is similarly for P2 prime. What I have done
is I have changed from my original variables
to these functions. When I write things over here,
now P1 prime and P2 prime are the integration variables. P1 and P2 are supposed
to be regarded as functions of P1
prime and P2 prime. You say, well, what
does that mean? You can't simply
take an integral dx, let's say F of some function of
x, and replace this function. You can't call it
a new variable, and do integral dx prime. You have to multiply with the
Jacobian of the transformation that takes you from the P
variables to the new variables. My claim is that this
Jacobian of the integration is, in fact, the unit. The reason is as follows. These equations that have
to be integrated to give me the correlation are
time reversible. If I give you two momenta, and
I know what the outcomes are, I can write the
equations backward, and I will have the
opposite momenta go back to minus the original momenta. Up to a factor of minus, you
can see that this equation has this character, that P1, P2
go to P1 prime, P2 prime, then minus P1 prime, minus P2
prime, go to P1, and P2. If you sort of
follow that, and say that you do the
transformation twice, you have to get back up
to where a sign actually disappears to where you want. You have to multiply
by two Jacobians, and you get the same unit. You can convince yourself that
this Jacobian has to be unit. Next time, I guess we'll
take it from there. I will explain this
stuff a little bit more, and show that this implies what
we had said about the Boltzmann equation.

Algorithms

OK, this is linear
algebra lecture nine. And this is a key lecture, this
is where we get these ideas of linear independence,
when a bunch of vectors are independent -- or dependent,
that's the opposite. The space they span. A basis for a subspace
or a basis for a vector space, that's a central idea. And then the dimension
of that subspace. So this is the day
that those words get assigned clear meanings. And emphasize that we talk
about a bunch of vectors being independent. Wouldn't talk about a
matrix being independent. A bunch of vectors
being independent. A bunch of vectors
spanning a space. A bunch of vectors
being a basis. And the dimension
is some number. OK, so what are the definitions? Can I begin with a fact,
a highly important fact, that, I didn't call directly
attention to earlier. Suppose I have a matrix and
I look at Ax equals zero. Suppose the matrix
has a lot of columns, so that n is bigger than m. So I'm looking at n equations -- I mean, sorry, m
equations, a small number of equations m,
and more unknowns. I have more unknowns
than equations. Let me write that down. More unknowns than equations. More unknown x-s than equations. Then the conclusion
is that there's something in the null
space of A, other than just the zero vector. The conclusion is there
are some non-zero x-s such that Ax is zero. There are some
special solutions. And why? We know why. I mean, it sort of like seems
like a reasonable thing, more unknowns than equations,
then it seems reasonable that we can solve them. But we have a, a clear algorithm
which starts with a system and does elimination, gets
the thing into an echelon form with some pivots
and pivot columns, and possibly some free columns
that don't have pivots. And the point is here there
will be some free columns. The reason, so the
reason is there must -- there will be free
variables, at least one. That's the reason. That we now have this -- a complete, algorithm, a
complete systematic way to say, OK, we take the
system Ax equals zero, we row reduce, we identify
the free variables, and, since there are n
variables and at most m pivots, there will be some free
variables, at least one, at least n-m in fact, left over. And those variables I can
assign non-zero values to. I don't have to
set those to zero. I can take them to be
one or whatever I like, and then I can solve
for the pivot variables. So then it gives me a
solution to Ax equals zero. And it's a solution
that isn't all zeros. So, that's an important
point that we'll use now in this lecture. So now I want to say what does
it mean for a bunch of vectors to be independent. OK. So this is like the
background that we know. Now I want to speak
about independence. OK. Let's see. I can give you the abstract
definition, and I will, but I would also like to
give you the direct meaning. So the question is, when
vectors x1, x2 up to -- Suppose I have n vectors
are independent if. Now I have to give you -- or linearly independent -- I'll often just say and
write independent for short. OK. I'll give you the
full definition. These are just vectors
in some vector space. I can take combinations of them. The question is, do any
combinations give zero? If some combination
of those vectors gives the zero vector,
other than the combination of all zeros, then
they're dependent. They're independent if no
combination gives the zero vector -- and then I have, I'll have
to put in an except the zero combination. So what do I mean by that? No combination gives
the zero vector. Any combination
c1 x1+c2 x2 plus, plus cn xn is not zero except
for the zero combination. This is when all the c-s,
all the c-s are zero. Then of course. That combination --
I know I'll get zero. But the question is, does any
other combination give zero? If not, then the
vectors are independent. If some other combination
does give zero, the vectors are dependent. OK. Let's just take examples. Suppose I'm in, say, in
two dimensional space. OK. I give you -- I'd like to first
take an example -- let me take an example where
I have a vector and twice that vector. So that's two vectors, V and 2V. Are those dependent
or independent? Those are dependent
for sure, right, because there's one
vector is twice the other. One vector is twice
as long as the other, so if the word dependent
means anything, these should be dependent. And they are. And in fact, I would
take two of the first -- so here's, here is a vector V
and the other guy is a vector 2V, that's my -- so there's a vector V1 and
my next vector V2 is 2V1. Of course those are
dependent, because two of these first vectors minus
the second vector is zero. That's a combination of these
two vectors that gives the zero vector. OK, that was clear. Suppose, suppose
I have a vector -- here's another example. It's easy example. Suppose I have a vector and the
other guy is the zero vector. Suppose I have a vector V1
and V2 is the zero vector. Then are those vectors
dependent or independent? They're dependent again. You could say, well, this
guy is zero times that one. This one is some
combination of those. But let me write
it the other way. Let me say -- what combination,
how many V1s and how many V2s shall I take to get the zero vector? If, if V1 is like the vector two
one and V2 is the zero vector, zero zero, then I
would like to show that some combination of
those gives the zero vector. What shall I take? How many V1s shall I take? Zero of them. Yeah, no, take no V1s. But how many V2s? Six. OK. Or five. Then -- in other words,
the point is if the zero vector's in there,
if the zero -- if one of these vectors
is the zero vector, independence is dead, right? If one of those vectors is the
zero vector then I could always take -- include that one and
none of the others, and I would get the zero answer,
and I would show dependence. OK. Now, let me, let me
finally draw an example where they will be independent. Suppose that's V1 and that's V2. Those are surely
independent, right? Any combination of
V1 and V2, will not be zero except, the
zero combination. So those would be independent. But now let me, let me
stick in a third vector, V3. Independent or dependent
now, those three vectors? So now n is three here. I'm in two dimensional space,
whatever, I'm in the plane. I have three vectors that
I didn't draw so carefully. I didn't even tell you
what exactly they were. But what's this answer on
dependent or independent? Dependent. How do I know those
are dependent? How do I know that some
combination of V1, V2, and V3 gives me the zero vector? I know because of that. That's the key
fact that tells me that three vectors in the
plane have to be dependent. Why's that? What's the connection between
the dependence of these three vectors and that fact? OK. So here's the connection. I take the matrix A that has
V1 in its first column, V2 in its second column,
V3 in its third column. So it's got three columns. And V1 -- I don't know, that
looks like about two one to me. V2 looks like it
might be one two. V3 looks like it might be maybe
two, maybe two and a half, minus one. OK. Those are my three vectors, and
I put them in the columns of A. Now that matrix A
is two by three. It fits this pattern, that
where we know we've got extra variables, we know we
have some free variables, we know that there's
some combination -- and let me instead of x-s, let
me call them c1, c2, and c3 -- that gives the zero vector. Sorry that my little bit
of art got in the way. Do you see the point? When I have a matrix,
I'm interested in whether its columns are
dependent or independent. The columns are
dependent if there is something in the null space. The columns are
dependent because this, this thing in the
null space says that c1 of that plus c2 of
that plus c3 of this is zero. So in other words, I can go
out some V1, out some more V2, back on V3, and end up zero. OK. So let -- here I've give the
general, abstract definition, but let me repeat
that definition -- this is like repeat -- let me call them Vs now. V1 up to Vn are the
columns of a matrix A. In other words,
this is telling me that if I'm in m
dimensional space, like two dimensional
space in the example, I can answer the
dependence-independence question directly by
putting those vectors in the columns of a matrix. They are independent if the
null space of A, of A, is what? If I have a bunch of
columns in a matrix, I'm looking at
their combinations, but that's just A times
the vector of c-s. And these columns
will be independent if the null space of
A is the zero vector. They are dependent if there's
something else in there. If there's something else in
the null space, if A times c gives the zero vector
for some non-zero vector c in the null space. Then they're dependent,
because that's telling me a combination of the
columns gives the zero column. I think you're with
be, because we've seen, like, lecture after
lecture, we're looking at the combinations
of the columns and asking, do we get zero or don't we? And now we're giving
the official name, dependent if we do,
independent if we don't. So I could express this
in other words now. I could say the rank -- what's
the rank in this independent case? The rank r of the,
of the matrix, in the case of
independent columns, is? So the columns are independent. So how many pivot
columns have I got. All n. All the columns would
be pivot columns, because free columns
are telling me that they're a combination
of earlier columns. So this would be the
case where the rank is n. This would be the case where
the rank is smaller than n. So in this case the rank is
n and the null space of A is only the zero vector. And no free variables. No free variables. And this is the case
yes free variables. If you'll allow me to stretch
the English language that far. That's the case where
we have, a combination that gives the zero column. I'm often interested in the
case when my vectors are popped into a matrix. So the, the definition
over there of independence didn't talk about any matrix. The vectors didn't have to be
vectors in N dimensional space. And I want to give
you some examples of vectors that
aren't what you think of immediately as vectors. But most of the time, this is
-- the vectors we think of are columns. And we can put them in a matrix. And then independence
or dependence comes back to the null space. OK. So that's the idea
of independence. Can I just, yeah, let
me go on to spanning a What does it mean for a bunch
of vectors to span a space? space. Well, actually, we've
seen it already. You remember, if we had
a columns in a matrix, we took all their
combinations and that gave us the column space. Those vectors that we started
with span that column space. So spanning a space means -- so let me move that
important stuff right up. OK. So vectors -- let me call
them, say, V1 up to -- call you some different
letter, say Vl -- span a space, a subspace,
or just a vector space I could say, span a
space means, means the space consists of all
combinations of those vectors. That's exactly what we
did with the column space. So now I could say in shorthand
the columns of a matrix span the column space. So you remember it's a bunch of
vectors that have this property that they span a space, and
actually if I give you a bunch of vectors and say -- OK, let S be the
space that they span, in other words let S contain
all their combinations, that space S will
be the smallest space with those
vectors in it, right? Because any space with
those vectors in it must have all the combinations
of those vectors in it. And if I stop there, then
I've got the smallest space, and that's the space
that they span. OK. So I'm just -- rather than, needing to say,
take all linear combinations and put them in a space,
I'm compressing that into the word span. Straightforward. So if I think of a, of
the column space of a OK. matrix. I've got their -- so I
start with the columns. I take all their combinations. That gives me the columns space. They span the column space. Now are they independent? Maybe yes, maybe no. It depends on the particular
columns that went into that matrix. But obviously I'm highly
interested in a set of vectors that spans a
space and is independent. That's, that means like I've
got the right number of vectors. If I didn't have all of them,
I wouldn't have my whole space. If I had more than that,
they probably wouldn't -- they wouldn't be independent. So, like, basis -- and that's
the word that's coming -- is just right. So here let me put
what that word means. A basis for a vector space is,
is a, is a sequence of vectors -- shall I call them V1, V2,
up to let me say Vd now, I'll stop with that letters
-- that has two properties. I've got enough vectors
and not too many. It's a natural idea of a basis. So a basis is a bunch
of vectors in the space and it's a so it's a sequence
of vectors with two properties, with two properties. One, they are independent. And two -- you
know what's coming? -- they span the space. OK. Let me take -- so time for examples, of course. So I'm asking you now
to put definition one, the definition of independence,
together with definition two, and let's look at examples,
because this is -- this combination
means the set I've -- of vectors I have is
just right, and the -- so that this idea of a
basis will be central. I'll always be asking
you now for a basis. Whenever I look at a
subspace, if I ask you for -- if you give me a basis
for that subspace, you've told me what it is. You've told me everything I need
to know about that subspace. Those -- I take their
combinations and I know that I need all the combinations. OK. Examples. OK, so examples of a basis. Let me start with two
dimensional space. Suppose the space
-- say example. The space is, oh,
let's make it R^3. Real three dimensional space. Give me one basis. One basis is? So I want some vectors, because
if I ask you for a basis, I'm asking you for vectors,
a little list of vectors. And it should be just right. So what would be a basis
for three dimensional space? Well, the first basis that
comes to mind, why don't we write that down. The first basis
that comes to mind is this vector, this
vector, and this vector. OK. That's one basis. Not the only basis, that's
going to be my point. But let's just see --
yes, that's a basis. Are, are those
vectors independent? So that's the like the x, y,
z axes, so if those are not independent, we're in trouble. Certainly, they are. Take a combination c1 of this
vector plus c2 of this vector plus c3 of that
vector and try to make it give the zero vector. What are the c-s? If c1 of that plus c2 of that
plus c3 of that gives me 0 0 0, then the c-s are all -- 0, right. So that's the test
for independence. In the language of matrices,
which was under that board, I could make those the
columns of a matrix. Well, it would be
the identity matrix. Then I would ask, what's the
null space of the identity matrix? And you would say it's
only the zero vector. And I would say, fine, then
the columns are independent. The only thing -- the identity
times a vector giving zero, the only vector that
does that is zero. OK. Now that's not the only basis. Far from it. Tell me another basis, a
second basis, another basis. So, give me -- well,
I'll just start it out. One one two. Two two five. Suppose I stopped there. Has that little bunch of
vectors got the properties that I'm asking for in
a basis for R^3? We're looking for
a basis for R^3. Are they independent,
those two column vectors? Yes. Do they span R^3? No. Our feeling is no. Our feeling is no. Our feeling is that there're
some vectors in R3 that are not combinations of those. OK. So suppose I add in -- I need another vector
then, because these two don't span the space. OK. Now it would be foolish for me
to put in three three seven, right, as the third vector. That would be a goof. Because that, if I put
in three three seven, those vectors would
be dependent, right? If I put in three
three seven, it would be the sum
of those two, it would lie in the
same plane as those. It wouldn't be independent. My attempt to create
a basis would be dead. But if I take -- so
what vector can I take? I can take any vector
that's not in that plane. Let me try -- I hope that 3 3 8 would do it. At least it's not the
sum of those two vectors. But I believe that's a basis. And what's the test then,
for that to be a basis? Because I just picked those
numbers, and if I had picked, 5 7 -14 how would we know do
we have a basis or don't we? You would put them in
the columns of a matrix, and you would do
elimination, row reduction -- and you would see do you
get any free variables or are all the
columns pivot columns. Well now actually
we have a square -- the matrix would
be three by three. So, what's the test
on the matrix then? The matrix -- so in this case,
when my space is R^3 and I have three vectors, my matrix is
square and what I asking about that matrix in order for
those columns to be a basis? So in this -- for R^n, if I have -- n vectors
give a basis if the n by n matrix with those columns,
with those columns, is what? What's the requirement
on that matrix? Invertible, right, right. The matrix should be invertible. For a square matrix, that's
the, that's the perfect answer. Is invertible. So that's when, that's when the
space is the whole space R^n. Let me, let me be sure
you're with me here. Let me remove that. Are those two vectors a
basis for any space at all? Is there a vector
space that those really are a basis for, those, that
pair of vectors, this guy and this 1, 1 1 2 and 2 2 5? Is there a space for
which that's a basis? Sure. They're independent, so they
satisfy the first requirement, so what space shall I take
for them to be a basis of? What spaces will
they be a basis for? The one they span. Their combinations. It's a plane, right? It'll be a plane inside R^3. So if I take this vector
1 1 2, say it goes there, and this vector 2 2
5, say it goes there, those are a basis for --
because they span a plane. And they're a basis for
the plane, because they're independent. If I stick in some
third guy, like 3 3 7, which is in the plane -- suppose
I put in, try to put in 3 3 7, then the three vectors
would still span the plane, but they wouldn't be a basis
anymore because they're not independent anymore. So, we're looking at
the question of -- again, OK. the case with
independent columns is the case where the column
vectors span the column space. They're independent, so they're
a basis for the column space. OK. So now there's one
bit of intuition. Let me go back to all of R^n. So I -- where I put 3 3 8. OK. The first message is that the
basis is not unique, right. There's zillions of bases. I take any invertible
three by three matrix, its columns are a basis for R^3. The column space is
R^3, and if those, if that matrix is invertible,
those columns are independent, I've got a basis for R^3. So there're many, many bases. But there is something in
common for all those bases. There's something that this
basis shares with that basis and every other basis for R^3. And what's that? Well, you saw it coming, because
when I stopped here and asked if that was a basis
for R^3, you said no. And I know that you said
no because you knew there weren't enough vectors there. And the great fact is that
there're many, many bases, but -- let me put in somebody
else, just for variety. There are many, many
bases, but they all have the same number of vectors. If we're talking
about the space R^3, then that number of
vectors is three. If we're talking
about the space R^n, then that number
of vectors is n. If we're talking about
some other space, the column space of some matrix,
or the null space of some matrix, or some other space
that we haven't even thought of, then that still is
true that every basis -- that there're lots of bases but
every basis has the same number of vectors. Let me write that
great fact down. Every basis --
we're given a space. Given a space. R^3 or R^n or some other column
space of a matrix or the null space of a matrix or
some other vector space. Then the great fact
is that every basis for this, for the space has
the same number of vectors. If one basis has six vectors,
then every other basis has six vectors. So that number six
is telling me like it's telling me how
big is the space. It's telling me
how many vectors do I have to have to have a basis. And of course we're
seeing it this way. That number six, if
we had seven vectors, then we've got too many. If we have five vectors
we haven't got enough. Sixes are like just right
for whatever space that is. And what do we call that number? That number is -- now I'm ready
for the last definition today. It's the dimension
of that space. So every basis for a space has
the same number of vectors in it. Not the same vectors,
all sorts of bases -- but the same number of
vectors is always the same, and that number
is the dimension. This is definitional. This number is the
dimension of the space. OK. OK. Let's do some examples. Because now we've
got definitions. Let me repeat the four
things, the four words that have now got defined. Independence, that looks
at combinations not being zero. Spanning, that looks at
all the combinations. Basis, that's the
one that combines independence and spanning. And now we've got the idea
of the dimension of a space. It's the number of
vectors in any basis, because all bases
have the same number. OK. Let's take examples. Suppose I take, my space
is -- examples now -- space is the, say, the
column space of this matrix. Let me write down a matrix. 1 1 1, 2 1 2, and I'll
-- just to make it clear, I'll take the sum there, 3 2 3,
and let me take the sum of all -- oh, let me put
in one -- yeah, I'll put in one one one again. OK. So that's four vectors. OK, do they span the column
space of that matrix? Let me repeat, do they span the
column space of that matrix? By definition, that's
what the column space -- Yes. where it comes from. Are they a basis for
the column space? Are they independent? No, they're not independent. There's something
in that null space. Maybe we can -- so let's look
at the null space of the matrix. Tell me a vector that's in
the null space of that matrix. So I'm looking for some vector
that combines those columns and produces the zero column. Or in other words, I'm
looking for solutions to A X equals zero. So tell me a vector
in the null space. Maybe -- well, this was, this
column was that one plus that one, so maybe if I have one of
those and minus one of those that would be a
vector in the null space. So, you've already told me now,
are those vectors independent, the answer is -- those column
vectors, the answer is -- no. Right? They're not independent. Because -- you knew they
weren't independent. Anyway, minus one
of this minus one of this plus one of this zero
of that is the zero vector. OK, so they're not independent. OK. They span, but they're
not independent. Tell me a basis for
that column space. What's a basis for
the column space? These are all the questions that
the homework asks, the quizzes ask, the final exam will ask. Find a basis for the column
space of this matrix. OK. Now there's many
answers, but give me the most natural answer. Columns one and two. Columns one and two. That's the natural answer. Those are the pivot
columns, because, I mean, we s- we begin systematically. We look at the first
column, it's OK. We can put that in the basis. We look at the second
column, it's OK. We can put that in the basis. The third column we
can't put in the basis. The fourth column
we can't, again. So the rank of the matrix is -- what's the rank of our matrix? Two. Two. And, and now that rank is also
-- we also have another word. We, we have a
great theorem here. The rank of A, that rank r,
is the number of pivot columns and it's also -- well, so now please
use my new word. This, it's the number
two, of course, two is the rank
of my matrix, it's the number of pivot columns,
those pivot columns form a basis, of course,
so what's two? It's the dimension. The rank of A, the
number of pivot columns, is the dimension of
the column space. Of course, you say. It had to be. Right. But just watch, look
for one moment at the, the language, the
way the English words get involved here. I take the rank of a matrix,
the rank of a matrix. It's a number of columns
and it's the dimension of -- not the dimension of the matrix,
that's what I want to say. It's the dimension of a space,
a subspace, the column space. Do you see, I don't
take the dimension of A. That's not what I want. I'm looking for the dimension
of the column space of A. If you use those words right,
it shows you've got the idea right. Similarly here. I don't talk about the
rank of a subspace. It's a matrix that has a rank. I talk about the
rank of a matrix. And the beauty is that
these definitions just merge so that the
rank of a matrix is the dimension of
its column space. And in this example it's two. And then the further
question is, what's a basis? And the first two
columns are a basis. Tell me another basis. Another basis for
the columns space. You see I just keep
hammering away. I apologize, but it's,
I have to be sure you have the idea of basis. Tell me another basis
for the column space. Well, you could take
columns one and three. That would be a basis
for the column space. Or columns two and
three would be a basis. Or columns two and four. Or tell me another basis that's
not made out of those columns at all? So -- I guess I'm giving you
infinitely many possibilities, so I can't expect a
unanimous answer here. I'll tell you -- but let's
look at another basis, though. I'll just -- because it's
only one out of zillions, I'm going to put it down
and I'm going to erase it. Another basis for the
column space would be -- let's see. I'll put in some things
that are not there. Say, oh well, just to make
it -- my life easy, 2 2 2. That's in the column space. And, that was sort of obvious. Let me take the sum
of those, say 6 4 6. Or the sum of all of the
columns, 7 5 7, why not. That's in the column space. Those are independent and
I've got the number right, I've got two. Actually, this is a key point. If you know the dimension of
the space you're working with, and we know that this column
-- we know that the dimension, DIM, the dimension of
the column space is two. If you know the
dimension, then -- and we have a couple of
vectors that are independent, they'll automatically be a basis. If we've got the number
of vectors right, two vectors in this case,
then if they're independent, they can't help
but span the space. Because if they
didn't span the space, there'd be a third guy
to help span the space, but it couldn't be independent. So, it just has
to be independent if we've got the numbers right. And they span. OK. Very good. So you got the
dimension of a space. So this was another basis
that I just invented. OK. Now, now I get to ask
about the null space. What's the dimension
of the null space? So we, we got a
great fact there, the dimension of the
column space is the rank. Now I want to ask you
about the null space. That's the other
part of the lecture, and it'll go on to
the next lecture. OK. So we know the dimension of the
column space is two, the rank. What about the null space? This is a vector
in the null space. Are there other vectors
in the null space? Yes or no? Yes. So this isn't a basis because
it's doesn't span, right? There's more in the null
space than we've got so far. I need another vector at least. So tell me another
vector in the null space. Well, the natural choice, the
choice you naturally think of is I'm going on to
the fourth column, I'm letting that free
variable be a one, and that free variable
be a zero, and I'm asking is that fourth
column a combination of my pivot columns? Yes, it is. And it's -- that will do. So what I've written
there are actually the two special solutions, right? I took the two free
variables, free and free. I gave them the values 1 0
or 0 I figured out the rest. So do you see, let me
just say it in words. This vector, these vectors in
the null space are telling me, they're telling me
the combinations of the columns that give zero. They're telling me in what way
the, the columns are dependent. That's what the
null space is doing. Have I got enough now? And what's the null space now? We have to think
about the null space. These are two vectors
in the null space. They're independent. Are they a basis
for the null space? What's the dimension
of the null space? You see that those questions
just keep coming up all the time. Are they a basis
for the null space? You can tell me the answer
even though we haven't written out a proof of that. Can you? Yes or no? Do these two special
solutions form a basis for the null space? In other words,
does the null space consist of all combinations
of those two guys? Yes or no? Yes. Yes. The null space is
two dimensional. The null space, the
dimension of the null space, is the number of free variables. So the dimension
of the null space is the number of free variables. And at the last second,
give me the formula. This is then the key
formula that we know. How many free variables are
there in terms of R, the rank, m -- the number of rows,
n, the number of columns? What do we get? We have n columns, r of
them are pivot columns, so n-r is the number of free
columns, free variables. And now it's the dimension
of the null space. OK. That's great. That's the key spaces, their
bases, and their dimensions. Thanks.

Calculus

The following content is
provided under a Creative Commons license. Your support will help
MIT OpenCourseWare continue to offer high-quality,
educational resources for free. To make a donation, or
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. ANA BELL: All right. Let's begin. As I mentioned
before, this lecture will be recorded for OCW. Again, in future
lectures, if you don't want to have the
back of your head show up, just don't sit in
this front area here. First of all, wow,
what a crowd, you guys. We're finally in 26-100. 6.0001 made it big, huh? Good afternoon and welcome to
the very first class of 6.0001, and also 600, this semester. My name is Ana Bell. First name, Ana. Last name, Bell. I'm a lecturer in
the EECS Department. And I'll be giving some
of the lectures for today, along with later on in the term,
Professor Eric Grimson, who's sitting right down there, will
be giving some of the lectures, as well. Today we're going to go over
some basic administrivia, a little bit of
course information. And then, we're going
to talk a little bit about what is computation? We'll discuss at
a very high level what computers do just
to make sure we're all on the same page. And then, we're going to dive
right into Python basics. We're going to talk a little bit
about mathematical operations you can do with Python. And then, we're going to
talk about Python variables and types. As I mentioned in my
introductory email, all the slides and code that I'll
talk about during lectures will be up before
lecture, so I highly encourage you to download
them and to have them open. We're going to go through some
in-class exercises which will be available on those slides. And it's fun to do. And it's also great if could
take notes about the code just for future reference. It's true. This is a really
fast-paced course, and we ramp up really quickly. We do want to position you
to succeed in this course. As I was writing
this, I was trying to think about when
I was first starting to program what helped
me get through my very first programming course. And this is really a good list. The first thing was I just
read the psets as soon as they came out, made sure that
the terminology just sunk in. And then, during
lectures, if the lecturer was talking about something
that suddenly I remembered, oh, I saw that word in the pset
and I didn't know what it was. Well, hey, now I
know what it is. Right? So just give it a read. You don't need to start it. If you're new to programming, I
think the key word is practice. It's like math or reading. The more you practice,
the better you get at it. You're not going to
absorb programming by watching me write programs
because I already know how to program. You guys need to practice. Download the code
before lecture. Follow along. Whatever I type,
you guys can type. And I think, also,
one of the big things is if you're new to
programming, you're kind of afraid that you're
going to break your computer. And you can't really do that
just by running Anaconda and typing in some commands. So don't be afraid to
just type some stuff in and see what it does. Worst case, you just
restart the computer. Yeah. That's probably the
big thing right there. I should have probably
highlighted it, but don't be afraid. Great. So this is pretty much a
roadmap of all of 6.0001 or 600 as I've just explained it. There's three big things we
want to get out of this course. The first thing is the
knowledge of concepts, which is pretty much true of
any class that you'll take. The class will teach you
something through lectures. Exams will test
how much you know. This is a class in programming. The other thing we want
you to get out of it is programming skills. And the last thing,
and I think this is what makes this
class really great, is we teach you how
to solve problems. And we do that
through the psets. That's really how I feel
the roadmap of this course looks like. And underlying all of
these is just practice. You have to just type some
stuff away and code a lot. And you'll succeed in
this course, I think. OK. So what are the things we're
going to learn in this class? I feel like the things we're
going learn in this class can be divided into basically
three different sections. The first one is related to
these first two items here. It's really about
learning how to program. Learning how to
program, part of it is figuring out what
objects to create. You'll learn about these later. How do you represent knowledge
with data structures? That's sort of the
broad term for that. And then, as you're
writing programs, you need to-- programs
aren't just linear. Sometimes programs jump around. They make decisions. There's some control
flow to programs. That's what the second
line is going to be about. The second big
part of this course is a little bit more
abstract, and it deals with how do you write
good code, good style, code that's readable. When you write code, you
want to write it such that-- you're in big company,
other people will read it, other people will
use it, so it has to be readable and
understandable by others. To that end, you
need to write code that's well organized,
modular, easy to understand. And not only that, not
only will your code be read by other people,
but next year, maybe, you'll take another
course, and you'll want to look back at
some of the problems that you wrote in this class. You want to be able
to reread your code. If it's a big mess, you might
not be able to understand-- or reunderstand--
what you were doing. So writing readable
code and organizing code is also a big part. And the last section is going
to deal with-- the first two are actually part of the
programming in Introduction to Programming and
Computer Science in Python. And the last one deals mostly
with the computer science part in Introduction to Programming
and Computer Science in Python. We're going to talk about,
once you have learned how to write programs
in Python, how do you compare programs in Python? How do you know that one program
is better than the other? How do you know
that one program is more efficient than the other? How do you know
that one algorithm is better than the other? That's what we're going to
talk about in the last part of the course. OK. That's all for the
administrative part of the course. Let's start by talking at a high
level what does a computer do. Fundamentally, it
does two things. One, performs calculations. It performs a lot
of calculations. Computers these days
are really, really fast, a billion calculations per
second is probably not far off. It performs these
calculations and it has to store them somewhere. Right? Stores them in computer memory. So a computer also has
to remember results. And these days, it's not
uncommon to find computers with hundreds of
gigabytes of storage. The kinds of calculations
that computers do, there are two kinds. One are calculations that
are built into the language. These are the very
low level types of calculations,
things like addition, subtraction,
multiplication, and so on. And once you have
a language that has these primitive calculation
types, you, as a programmer, can put these types
together and then define your own calculations. You can create new
types of calculations. And the computer will be able
to perform those, as well. I think, one thing
I want to stress-- and we're going to
come back to this again during this entire
lecture, actually-- is computers only know
what you tell them. Computers only do what
you tell them to do. They're not magical. They don't have a mind. They just know how to
perform calculations really, really quickly. But you have to tell them
what calculations to do. Computers don't know anything. All right. We've come to that. Let's go into the
types of knowledge. The first type of knowledge
is declarative knowledge. And those are things
like statements of fact. And this is where my
email came into play. If you read it all
the way to the bottom, you would have entered a raffle. So a statement of fact
for today's lecture is, someone will win a
prize before class ends. And the prize was
a Google Cardboard. Google state-of-the-art
virtual reality glasses. And I have them right here. Yea. I delivered on my promise. That's a statement of fact. So pretend I'm a machine. OK? I don't know anything
except what you tell me. I don't know. I know that you tell
me this statement. I'm like, OK. But how is someone going
to win a Google Cardboard before class ends, right? That's where imperative
knowledge comes in. Imperative knowledge is
the recipe, or the how-to, or the sequence of steps. Sorry. That's just my
funny for that one. So the sequence of steps
is imperative knowledge. If I'm a machine,
you need to tell me how someone will win a Google
Cardboard before class. If I follow these
steps, then technically, I should reach a conclusion. Step one, I think we've
already done that. Whoever wanted to
sign up has signed up. Now I'm going to open my IDE. I'm just basically
being a machine and following the steps
that you've told me. The IDE that we're using in
this class is called Anaconda. I'm just scrolling
down to the bottom. Hopefully, you've installed
it in problem set zero. I've opened my IDE. I'm going to follow the
next set of instructions. I'm going to choose a random
number between the first and the nth responder. Now, I'm going to actually
use Python to do this . And this is also an
example of how just a really simple
task in your life, you can use computers or
programming to do that. Because if I chose
a random number, I might be biased
because, for example, I might like the number 8. To choose a random number,
I'm going to go and say, OK, where's the list of responders? It starts at 15. Actually, it starts at
16 because that's me. We're going to choose a
random number between 16 and the end person 266. Oh, we just got-- oh. OK. OK. I'm going to cut
it off right here. 271. OK. 16 and 271. Perfect. OK. I'm going to choose
a random number. I'm going to go to my IDE. And you don't need to
know how to do this yet, but by the end of
this class, you will. I'm just going to use Python. I'm just going to get the random
number package that's going to give me a random number. I'm going to say random.randint. And I'm going to choose a random
number between 16 and 272, OK. 75. OK. Great. I chose a random number. And I'm going to find the
number in the responder's sheet. What was the number again? Sorry. 75. OK. Up we go. There we go. Lauren Z-O-V. Yeah. Nice. You're here. Awesome. All right. That's an example of me
being a machine and also, at the same time, using
Python in my everyday life, just lecturing, to
find a random number. Try to use Python
wherever you can. And that just
gives you practice. That was fun. But we're at MIT. We're MIT students. And we love numbers here at MIT. Here's a numerical
example that shows the difference between
declarative and imperative knowledge. An example of
declarative knowledge is the square root of a number
x is y such that y times y is equal to x. That's just a statement
of fact It's true. Computers don't know
what to do with that. They don't know what to
do with that statement. But computers do know
how to follow a recipe. Here's a well-known algorithm. To find the square
root of a number x, let's say x is originally
16, if a computer follows this algorithm, it's going
to start with a guess, g, let's say, 3. We're trying to find
the square root of 16. We're going to calculate
g times g is 9. And we're going to
ask is if g times g is close enough to x, then
stop and say, g is the answer. I'm not really happy with
9 being really close to 16. So I'm going to say,
I'm not stopping here. I'm going to keep going. If it's not close
enough, then I'm going to make a new guess
by averaging g and x over g. That's x over g here. And that's the
average over there. And the new average is
going to be my new guess. And that's what it says. And then, the last step
is using the new guess, repeat the process. Then we go back to the beginning
and repeat the whole process over and over again. And that's what the
rest of the rows do. And you keep doing
this until you decide that you're close enough. What we saw for the
imperative knowledge in the previous
numerical example was the recipe for how to
find the square root of x. What were the three
parts of the recipe? One was a simple
sequence of steps. There were four steps. The other was a flow of
control, so there were parts where we made decisions. Are we close enough? There were parts where
we repeated some steps. At the end, we said,
repeat steps 1, 2, 3. That's the flow of control. And the last part of the
recipe was a way to stop. You don't want a program
that keeps going and going. Or for a recipe, you don't want
to keep baking bread forever. You want to stop at some point. Like 10 breads is enough, right? So you have to have
a way of stopping. In the previous example,
the way of stopping was that we decided
we were close enough. Close enough was maybe
being within .01, .001, whatever you pick. This recipe is there
for an algorithm. In computer science speak,
it's going to be an algorithm. And that's what we're going
to learn about in this class. We're dealing with computers. And we actually want
to capture a recipe inside a computer, a computer
being a mechanical process. Historically, there were two
different types of computers. Originally, there
were these things called fixed-program computers. And I'm old enough to
have used something like this, where there's
just numbers and plus, minus, multiplication,
divide, and equal. But calculators these days
are a lot more complicated. But way back then, an example
of a fixed-program computer is this calculator. It only knows how to do
addition, multiplication, subtraction, division. If you want to plot
something, you can't. If you want to go on the
internet, send email with it, you can't. It can only do this one thing. And if you wanted to create a
machine that did another thing, then you'd have to create
another fixed-program computer that did a completely
separate test. That's not very great. That's when stored-program
computers came into play. And these were machines
that could store a sequence of instructions. And these machines could execute
the sequence of instructions. And you could change the
sequence of instructions and execute this different
sequence of instructions. You could do different
tasks in the same machine. And that's the computer
as we know it these days. The central processing unit is
where all of these decisions get made. And these are all
the peripherals. The basic machine architecture--
at the heart of every computer there's just this
basic architecture-- and it contains, I
guess, four main parts. The first is the memory. Input and output
is the other one. The ALU is where all of
the operations are done. And the operations
that the ALU can do are really primitive operations,
addition, subtraction, and so on. What the memory contains
is a bunch of data and your sequence
of instructions. Interacting with the Arithmetic
Logic Unit is the Control Unit. And the Control Unit
contains one program counter. When you load a sequence
of instructions, the program counter starts
at the first sequence. It starts at the sequence,
at the first instruction. It gets what the instruction
is, and it sends it to the ALU. The ALU asks, what are we
doing operations on here? What's happening? It might get some data. If you're adding two numbers,
it might get two numbers from memory. It might do some operations. And it might store
data back into memory. And after it's done, the
ALU is going to go back, and the program counter
is going to increase by 1, which means
that we're going to go to the next sequence
in the instruction set. And it just goes linearly,
instruction by instruction. There might be one
particular instruction that does some sort of test. It's going to say, is
this particular value greater or equal to or the same
as this other particular value? That's a test, an
example of a test. And the test is going to
either return true or false. And depending on the
result of that test, you might either go to
the next instruction, or you might set
the program counter to go all the way back to
the beginning, and so on. You're not just
linearly stepping through all the instructions. There might be some
control flow involved, where you might
skip an instruction, or start from the
beginning, or so on. And after you're done,
when you finished executing the last
instruction, then you might output something. That's really the basic
way that a computer works. Just to recap, you have
the stored program computer that contains these
sequences of instructions. The primitive operations
that it can do are addition, subtraction,
logic operations, tests-- which are something equal
to something else, something less than, and so
on-- and moving data, so storing data, moving data
around, and things like that. And the interpreter goes
through every instruction and decides whether you're going
to go to the next instruction, skip instructions, or repeat
instructions, and so on. So we've talked
about primitives. And in fact, Alan Turing, who
was a really great computer scientist, he showed that
you can compute anything using the six primitives. And the six primitives are move
left, move right, read, write, scan, and do nothing. Using those six instructions
and the piece of tape, he showed that you
can compute anything. And using those
six instructions, programming languages
came about that created a more convenient
set of primitives. You don't have to program
in only these six commands. And one interesting thing, or
one really important thing, that came about from
these six primitives is that if you can compute
something in Python, let's say-- if you write a
program that computes something in Python, then,
in theory, you can write a program that
computes the exact same thing in any other language. And that's a really
powerful statement. Think about that today when
you review your slides. Think about that again. That's really powerful. Once you have your
set of primitives for a particular language, you
can start creating expressions. And these expressions
are going to be combinations of the primitives
in the programming language. And the expressions are
going to have some value. And they're going up some
meaning in the programming language. Let's do a little bit of
a parallel with English just so you see what I mean. In English, the
primitive constructs are going to be words. There's a lot of words
in the English language. Programming languages-- in
Python, there are primitives, but there aren't
as many of them. There are floats,
Booleans, these are numbers, strings,
and simple operators, like addition,
subtraction, and so on. So we have primitive constructs. Using these
primitive constructs, we can start creating, in
English, phrases, sentences, and the same in
programming languages. In English, we can say
something like, "cat, dog, boy. That, we say, is not
syntactically valid. That's bad syntax. That's noun, noun, noun. That doesn't make sense. What does have good syntax in
English is noun, verb, noun. So, "cat, hugs boy" is
syntactically valid. Similarly, in a
programming language, something like this-- in
Python, in this case-- a word and then the number five
doesn't really make sense. It's not syntactically valid. But something like operator,
operand, operator is OK. So once you've created these
phrases, or these expressions, that are syntactically
valid, you have to think about the static
semantics of your phrase, or of your expression. For example, in English, "I
are hungry" is good syntax. But it's weird to say. We have a pronoun, a verb,
and an adjective, which doesn't really make sense. "I am hungry" is better. This does not have
good static semantics. Similarly, in
programming languages-- and you'll get the
hang of this the more you do it-- something like
this, "3.2 times 5, is OK. But what does it mean? What's the meaning to have
a word added to a number? There's no meaning behind that. Its syntax is OK,
because you have operator, operand, operator. But it doesn't really make
sense to add a number to a word, for example. Once you have created
these expressions that are syntactically correct and
static, semantically correct, in English, for example, you
think about the semantics. What's the meaning
of the phrase? In English, you can
actually have more than one meaning to an entire phrase. In this case, "flying
planes can be dangerous" can have two meanings. It's the act of flying
a plane is dangerous, or the plane that is in
the air is dangerous. And this might be
a cuter example. "This reading lamp
hasn't uttered a word since I bought it. What's going on?" So that has two meanings. It's playing on the
word "reading lamp." That's in English. In English, you
can have a sentence that has more than
one meaning, that's syntactically correct and
static, semantically correct. But in programming languages,
the program that you write, the set of instructions that
you write, only has one meaning. Remember, we're coming
back to the fact that the computer only does
what you tell it to do. It's not going to
suddenly decide to add another variable
for some reason. It's just going to execute
whatever statements you've put up. In programming languages,
there's only one meaning. But the problem that comes into
play in programming languages is it's not the meaning
that you might have intended, as the programmer. That's where things
can go wrong. And there's going
to be a lecture on debugging a little
bit later in the course. But this is here
just to tell you that if you see an error
pop up in your program, it's just some text
that says, error. For example, if we do
something like this, this is syntactically correct. Incorrect. Syntactically incorrect. See? There's some angry
text right here. What is going on? The more you program,
the more you'll get the hang of
reading these errors. But this is basically
telling me the line that I wrote is
syntactically incorrect. And it's pointing to the exact
line and says, this is wrong, so I can go back and
fix it as a programmer. Syntax errors are actually
really easily caught by Python. That was an example
of a syntax error. Static semantic
errors can also be caught by Python as long as, if
your program has some decisions to make, as long as you've
gone down the branch where the static semantic
error happens. And this is probably going to
be the most frustrating one, especially as
you're starting out. The program might do
something different than what you expected it to do. And that's not because the
program suddenly-- for example, you expected the program
to give you an output of 0 for a certain test case, and
the output that you got was 10. Well, the program
didn't suddenly decide to change
its answer to 10. It just executed the
program that you wrote. That's the case where
the program gave you a different answer
than expected. Programs might crash, which
means they stop running. That's OK. Just go back to your code and
figure out what was wrong. And another example
of a different meaning than what you intended was
maybe the program won't stop. It's also OK. There are ways to stop
it besides restarting the computer. So then Python
programs are going to be sequences of
definitions and commands. We're going to have expressions
that are going to be evaluated and commands that tell the
interpreter to do something. If you've done
problem set 0, you'll see that you can type
commands directly in the shell here, which is
the part on the right where I did some really
simple things, 2 plus 4. Or you can type commands up in
here, on the left-hand side, and then run your program. Notice that, well, we'll
talk about this-- I won't talk about this now. But these are-- on the
right-hand side, typically, you write very simple
commands just if you're testing something out. And on the left-hand
side here in the editor, you write more lines and
more complicated programs. Now we're going to start
talking about Python. And in Python, we're going
to come back to this, everything is an object. And Python programs
manipulate these data objects. All objects in Python
are going to have a type. And the type is going to tell
Python the kinds of operations that you can do
on these objects. If an object is the
number five, for example, you can add the number
to another number, subtract the number, take it
to the power of something, and so on. As a more general example,
for example, I am a human. So that's my type. And I can walk, speak
English, et cetera. Chewbacca is going
to be a type Wookie. He can walk, do that
sound that I can't do. He can do that, but I can't. I'm not even going
to try, and so on. Once you have these
Python objects, everything is an
object in Python. There are actually
two types of objects. One are scalar objects. That means these are very basic
objects in Python from which everything can be made. These are scalar objects. That can't be subdivided. The other type of object
is a non-scalar object. And these are objects that
have some internal structure. For example, the
number five is a scalar object because it
can't be subdivided. But a list of numbers,
for example, 5, 6, 7,8, is going to be
a non-scalar object because you can subdivide it. You can subdivide it into--
you can find parts to it. It's made up of a
sequence of numbers. Here's the list of all of
the scalar objects in Python. We have integers, for example,
all of the whole numbers. Floats, which are all of
the real numbers, anything with a decimal. Bools are Booleans. There's only two
values to Booleans. That's True and False. Note the capitalization,
capital T and capital F. And this other thing
called NoneType. It's special. It has only one
value called None. And it represents the
absence of a type. And it sometimes comes in
handy for some programs. If you want to find
the type of an object, you can use this special
command called type. And then in the
parentheses, you put down what you want to
find the type of. You can write into
the shell "type of 5," and the shell will tell
you, that's an integer. If you happen to want to convert
between two different types, Python allows you to do that. And to do that, you
put the type that you want to convert to
right before the object that you want to convert to. So float(3) will convert the
integer 3 to the float 3.0. And similarly, you can convert
any float into an integer. And converting to an
integer just truncates. It just takes away the
decimal and whatever's after it-- it does not round--
and keeps just the integer part. For this slide, I'm
going to talk about it. But if you'd like if
you have the slides up, go to go to this exercise. And after I'm done
talking about the slide, we'll see what people
think for that exercise. One of the most
important things that you can do in basically
any programming, in Python also, is
to print things out. Printing out is how you
interact with the user. To print things out, you
use the print command. If you're in the shell, if
you simply type "3 plus 2," you do see a value here. Five, right? But that's not actually
printing something out. And that becomes apparent
when you actually type things into the editor. If you just do "3 plus 2," and
you run the program-- that's the green button here-- you see
on the right-hand side here, it ran my program. But it didn't actually
print anything. If you type this
into the console, it does show you this
value, but that's just like peeking into the
value for you as a programmer. It's not actually
printing it out to anyone. If you want to
print something out, you have to use the print
statement like that. In this case, this is actually
going to print this number five to the console. That's basically what it says. It just tells you it's an
interaction within the shell only. It's not interacting
with anyone else. And if you don't
have any "Out," that means it got printed
out to the console. All right. We talked a little
bit about objects. Once you have objects, you can
combine objects and operators to form these expressions. And each expression is
going to have a value. So an expression
evaluates to a value. The syntax for an
expression is going to be object, operator,
object, like that. And these are some operators
you can do on ints and floats. There's the typical ones,
addition, subtraction, multiplication, and division. If, for the first
three, the answer that you get-- the type of
the answer that you get-- is going to depend on the
type of your variables. If both of the variables of
the operands are integers, then the result you're going
to get is of type integer. But if at least one of
them is a float, then the result you're going
to get is a float. Division is a little
bit special in that no matter what the
operands are, the result is always going to be a float. The other operations you can
do, and these are also useful, are the remainder,
so the percent sign. If you use the percent
sign between two operands, that's going to give you the
remainder when you divide i by j. And raising something to
the power of something else is using the star star operator. And i star stars j is going
to take i to the power of j. These operations have
the typical precedence that you might expect
in math, for example. And if you'd like
to put precedence toward some other
operations, you can use parentheses to do that. All right. So we have ways of
creating expressions. And we have operations
we can do on objects. But what's going to be useful
is to be able to save values to some name. And the name is going to
be something that you pick. And it should be a
descriptive name. And when you save
the value to a name, you're going to be able
to access that value later on in your program. And that's very useful. To save a value to a variable
name, you use the equal sign. And the equal sign
is an assignment. It assigns the
right-hand side, which is a value, to the
left-hand side, which is going to be a variable name. In this case, I assigned
the float 3.14159 to the variable pi. And in the second
line, I'm going to take this expression,
22 divided by 7, I'm going to evaluate it. It's going to come up
with some decimal number. And I'm going to save it
into the variable pi_approx. values are stored in memory. And this assignment
in Python, we say the assignment binds
the name to the value. When you use that name
later on in your program, you're going to be referring
to the value in memory. And if you ever want
to refer to the value later on in your code,
you just simply type the name of the variable
that you've assigned it to. So why do we want to give
names to expressions? Well, you want to reuse the
names instead of the values. And it makes your
code look a lot nicer. This is a piece of
code that calculates the area of a circle. And notice, I've assigned
a variable pi to 3.14159. I've assigned another variable
called radius to be 2.2. And then, later on in my
code, I have another line that says area-- this
is another variable-- is equal to-- this
is an assignment-- to this expression. And this expression is referring
to these variable names, pi and radius. And it's going look up
their values in memory. And it's going to replace
these variable names with those values. And it's going to do
the calculation for me. And in the end, this
whole expression is going to be
replaced by one number. And it's going to be the float. Here's another exercise, while
I'm talking about the slide. I do want to make a note
about programming versus math. In math, you're often
presented with a problem that says, solve for x. x plus y is equal to
something something. Solve for x, for example. That's coming back to the
fact that computers don't know what to do with that. Computers need to
be told what to do. In programming, if you
want to solve for x, you need to tell the computer
exactly how to solve for x. You need to figure
out what formula you need to give the
computer in order to be able to solve for x. That means always in programming
the right-hand side is going to be an expression. It's something that's going
to be evaluated to a value. And the left-hand side
is always a variable. It's going to be an assignment. The equal sign is
not like in math where you can have a lot
of things to the left and a lot of things to the
right of the equal sign. There's only one thing to
the left of the equal sign. And that's going
to be a variable. An equal sign stands
for an assignment. Once we've created expressions,
and we have these assignments, you can rebind variable
names using new assignment statements. Let's look at an
example for that. Let's say this is our memory. Let's type back in the example
with finding the radius. Let's say, pi is equal to 3.14. In memory, we're going to
create this value 3.14. We're going to bind it
to the variable named pi. Next line, radius
is equal to 2.2. In memory, we're
creating this value 2.2. And we're going to bind it
to the variable named radius. Then we have this
expression here. It's going to substitute
the values for pi from memory and the value
for radius from memory. It's going to calculate the
value that this expression evaluates to. It's going to pop
that into the memory. And it's going to
assign-- because we're using the equal
sign-- it's going to assign that value
to that variable area. Now, let's say we rebind
radius to be something else. Radius i is bound
to the value 2.2. But when we do this line, radius
is equal to radius plus 1, we're going to take
away the binding to 2.2. We're going to do
this calculation. The new value is 3.2. And we're going to rebind that
value to that same variable. In memory, notice
we're still going to have this value,
2.2, floating around. But we've lost
the handle for it. There's no way to get it back. It's just in memory
sitting there. At some point, it might
get collected by what we call the garbage collector. In Python, And it'll
retrieve these lost values, and it'll reuse them for new
values, and things like that. But radius now points
to the new value. We can never get back 2.2. And that's it. The value of area-- notice,
this is very important. The value of area
did not change. And it did not change because
these are all the instructions we told the computer to do. We just told it to change
radius to be radius plus 1. We never told it to
recalculate the value of area. If I copied that line down
here, then the value of area would change. But we never told it to do that. The computer only does
what we tell it to do. That's the last thing. Next lecture, we're going
to talk about adding control flow to our programs, so
how do you tell the computer to do one thing or another? All right.

Linear Algebra

First of all,
the way a nonlinear autonomous system looks,
you have had some practice with it by now.
This is nonlinear. The right-hand side are no
longer simple combinations ax plus by.
Nonlinear and autonomous, these are function just of x
and y. There is no t on the right-hand
side. Now, most of today will be
geometric. The way to get a geometric
picture of that is first by constructing the velocity field
whose components are the functions f and g.
This is a velocity field that gives a picture of the system
and has solutions. The solutions to the system,
from the point of view of functions, they would look like
pairs of functions, x of t, y of t. But, from the point of view of
geometry, when you plot them as parametric equations,
they are called trajectories of the field F, which simply means
that they are curves everywhere having the right velocity.
So a typical curve would look like --
There is a trajectory. And we know it is a trajectory
because at each point the vector on it has, of course,
the right direction, the tangent direction,
but more than that, it has the right velocity.
So here, for example, the point is traveling more
slowly. Here it is traveling more
rapidly because the velocity vector is bigger,
longer. So this is a picture of a
typical trajectory. The only other things that I
should mention are the critical points.
If you have worked the problems for this week,
the first couple of problems, you have already seen the
significance of the critical points.
Well, from Monday's lecture you know from the point of view of
solutions they are constant solutions. From the point of view of the
field they are where the field is zero.
There is no velocity vector, in other words.
The velocity vector is zero. And, therefore,
a point being there has no reason to go anywhere else.
And, spelling it out, it's where the partial
derivatives, where the values of the functions on the right-hand
side, which give the two components, the i and j
components of the field, where they are zero. That is all I will need by way
of a recall today. I don't think I will need
anything else. The topic for today is another
kind of behavior that you have not yet observed at the computer
screen, unless you have worked ahead, and that is there are
trajectories which go along to infinity or end up at a critical
point. They are the critical points
that just sit there all the time.
But there is a third type of behavior that a trajectory can
have where it neither sits for all time nor goes off for all
time. Instead, it repeats itself.
Such a thing is called a closed trajectory.
What does it look like? Well, it is a closed curve in
the plane that at every point, it is a trajectory,
i.e., the arrows at each point, let's say it is traced in the
clockwise direction. And so the arrows of the field
will go like this. Here it is going slowly,
here it is very slow and here it picks up a little speed again
and so on. Now, for such a trajectory what
is happening? Well, it goes around in finite
time and then repeats itself. It just goes round and round
forever if you land on that trajectory.
It represents a system that returns to its original state
periodically. It represents periodic behavior
of the system. Now, we have seen one example
of that, a simple example where this simple system,
x prime equals y, y prime equals negative x. We could write down the
solutions to that directly, but if you want to do
eigenvalues and eigenvectors the matrix will look like this.
The equation will be lambda squared plus zero lambda plus
one equals zero, so the eigenvalues will be plus or minus i.
In fact, from then on you could work out in the usual ways the
eigenvectors, complex eigenvectors and
separate them. But, look, you can avoid all
that just by writing down the solution.
The solutions are sines and cosines.
One basic solution will be x equals cosine t,
in which case what is y? Well, y is the derivative of
that. That will be minus sine t. Another basic solution,
we will start with x equals sine t.
In which case y will be cosine t, its derivative. Now, if you do that,
what do these things look like? Well, either of these two basic
solutions looks like a circle, not traced in the usual way but
in the opposite way. For example,
when t is equal to zero it starts at the point one,
zero. Now, if the minus sign were not
there this would be x equals cosine t,
y equals sine t, which is the usual
counterclockwise circle. But if I change y from sine t
to negative sine t it is going around the
other way. So this circle is traced that
way. And this is a family of
circles, according to the values of c1 and c2,
concentric, all of which go around clockwise.
So those are closed trajectories.
Those are the solutions. They are trajectories of the
vector field. They are closed.
They come around and they repeat in finite time.
Now, these are no good. These are the kind I am not
interested in. These are commonplace,
and we are interested in good stuff today.
And the good stuff we are interested in is limit cycles. A limit cycle is a closed
trajectory with a couple of extra hypotheses.
It is a closed trajectory, just like those guys,
but it has something they don't have, namely,
it is king of the roost. They have to be isolated,
no other guys nearby. And they also have to be
stable. See, the problem here is that
none of these stands out from any of the others.
In other words, there must be,
isolated means, no others nearby. That is just what goes wrong
here. Arbitrarily close to each of
these circles is yet another circle doing exactly the same
thing. That means that there are some
that are only of mild interest. What is much more interesting
is to find a cycle where there is nothing nearby.
Something, therefore, that looks like this. Here is our pink guy.
Let's make this one go counterclockwise.
Here is a limit cycle, it seems to be.
And now what do nearby guys do? Well, they should approach it.
Somebody here like that does this, spirals in and gets ever
and every closer to that thing. Now, it can never join it
because, if it joined it at the joining point,
I would have two solutions going through this point.
And that is illegal. All it can do is get
arbitrarily close. On the computer screen it will
look as if it joins it but, of course, it cannot.
It is just the resolution, the pixels.
Not enough pixels. The resolution isn't good
enough. And the ones that start further
away will take longer to find their way to the limit cycle and
they will always stay outside of the earlier guys,
but they will get arbitrarily close, too.
How about inside? Inside, well,
it starts somewhere and does the same thing.
It starts here and will try to join the limit cycle.
That is what I mean by stability.
Stability means that nearby guys, the guys that start
somewhere else eventually approach the limit cycle,
regardless of whether they start from the outside or start
from the inside. So that is stable.
An unstable limit cycle -- But I am not calling it a limit
cycle if it is unstable. I am just calling it a closed
trajectory, but let's draw one which is unstable.
Here is the way we will look if it is unstable.
Guys that start nearby will be repelled, driven somewhere else.
Or, if they start here, they will go away from the
thing instead of going toward it.
This is unstable. And I don't call it a limit
cycle. It is just a closed trajectory. Cycle because it cycles round
and round. Limit because it is the limit
of the nearby curves. The other case where it is
unstable is not the limit. Of course, you could have a
case also where the curves outside spiral in toward it but
the ones inside are repelled and do this.
That would be called semi-stable.
And you can make up all sorts of cases.
And I think I, at one point,
drew them in the notes, but I am not going to.
The only interesting one, of permanent importance that
people study, are the actual limit cycles.
No, it was the stable closed trajectories.
Notice, by the way, a closed trajectory is always a
simple curve. Remember what that means from
18.02? Simple means it doesn't cross
itself. Why doesn't it cross itself?
It cannot cross itself because, if it tried to,
what is wrong with that point? At that point which way does
the vector field go, that way or that way?
Why the interest of limit cycles?
Well, because there are systems in nature in which just this
type of behavior, they have a certain periodic
motion. And, if you disturb it,
gradually it returns to its original periodic state.
A simple example is breathing. Now I have made you all
self-conscious. All of you are breathing.
If you are here you are breathing.
At what rate are you breathing? Well, you are unaware of it,
of course, except now. If you are sitting here
listening. There is a certain temperature
and a certain air circulation in the room.
You are not thinking of anything, certainly not of the
lecture, and the lecture is not unduly exciting,
you will breathe at a certain steady rate which is a little
different for every person but that is your rate.
Now, you can artificially change that.
You could say now I am going to breathe faster.
And indeed you can. But, as soon as you stop being
aware of what you are doing, the levels of various hormones
and carbon dioxide in your bloodstream and so on will
return your breathing to its natural state.
In other words, that system of your breathing,
which is controlled by various chemicals and hormones in the
body, is exhibiting exactly this type of behavior.
It has a certain regular periodic motion as a system.
And, if disturbed, if artificially you set it out
somewhere else, it will gradually return to its
original state. Now, of course,
if I am running it will be different.
Sure. If you are running you breathe
faster, but that is because the parameters in the system,
the a's and the b's in the equation, the f of (x,
y) and g of (x, y), the parameters in those functions will be set at
different levels. You will have different
hormones, a different of carbon dioxide and so on.
Now, I am not saying that breathing is modeled by a limit
cycle. It is the sort of thing which
one might look for a limit cycle.
That is, of course, a question for biologists.
And, in general, any type of periodic behavior
in nature, people try to see if there is some system of
differential equations which governs it in which perhaps
there is a limit cycle, which contains a limit cycle.
Well, what are the problems? In a sense, limit cycles are
easy to lecture about because so little is known about them.
At the end of the period, if I have time,
I will show you that the simplest possible question you
could ask, the answer to it is totally known after 120 years of
steady trying. But let's first talk about what
sorts of problems people address with limit cycles.
First of all is the existence problem. If I give you a system,
you know, the right-hand side is x squared plus 2y cubed minus
3xy, and the g is something similar.
I say does this have limit cycles?
Well, you know how to find its critical points.
But how do you find out if it has limit cycles?
The answer to that is nobody has any idea.
This problem, in general, there are not much
in the way of methods. Not much. Not much is known.
There is one theorem that you will find in the notes,
a simple theorem called the Poincare-Bendixson theorem
which, for about 60 or 70 years was about the only result known
which enabled people to find limit cycles.
Nowadays the theorem is used relatively little because people
try to find limit cycles by computer.
Now, the difficulty is you have to know where to look for them.
In other words, the computer screen shows that
much and you set the axes and it doesn't show any limit cycles.
That doesn't mean there are not any.
That means they are over there, or it means there is a big one
like there. And you are looking in the
middle of it and don't see it. So, in general,
people don't look for limit cycles unless the physical
system that gave rise to the pair of differential equations
suggests that there is something repetitive going on like
breathing. And, if it tells you that,
then it often gives you approximate values of the
parameters and the variables so you know where to look.
Basically this is done by computer search guided by the
physical problem. Therefore, I cannot say much
more about it today. Instead I am going to focus my
attention on nonexistence. When can you be sure that a
system will not have any limit cycles?
And there are two theorems. One, again, due to Bendixson
who was a Swedish mathematician who lived around 1900 or so.
There is a criterion due to Bendixson.
And there is one involving critical points.
And I would like to describe both of them for you today.
First of all, Bendixson's criterion. It is very simply stated and
has a marvelous proof, which I am going to give you.
We have D as a region of the plane. And what Bendixson's criterion
tells you to do is take your vector field and calculate its
divergence. We are set back in 1802,
and this proof is going to be straight 18.02.
You will enjoy it. Calculate the divergence.
Now, I am talking about the two-dimensional divergence.
Remember that is fx, the partial of f with respect
to x, plus the partial of the g, the j component with respect to
y. And assume that that is a
continuous function. It always will be with us.
Practically all the examples I will give you f and g will be
simple polynomials. They are smooth,
continuous and nice and behave as you want.
And you calculate that and assume --
Suppose, in other words, that the divergence of f,
I need more room. The hypothesis is that the
divergence of f is not zero in that region D.
It is never zero. It is not zero at any point in
that region. The conclusion is that there
are no limit cycles in the region.
If it is not zero in D, there are no limit cycles.
In fact, there are not even any closed trajectories.
You couldn't even have those bunch of concentric circles,
so there are no closed trajectories of the original
system whose divergence you calculated.
There are no closed trajectories in D.
For example, let me give you a simple
example to put a little flesh on it.
Let's see. What do I have?
I prepared an example. x prime equals,
here is a simple nonlinear system, x cubed plus y cubed. And y prime equals 3x plus y
cubed plus 2y. Does this system have limit
cycles? Well, even to calculate its
critical points would be a little task, but we can easily
answer the question as to whether it has limit cycles or
not by Bendixson's criterion. Let's calculate the divergence.
The divergence of the vector field whose components are these
two functions is, well, 3x squared,
it's the partial of the first guy with respect to x plus the
partial of the second guy with respect to y,
which is 3y squared plus two. Now, can that be zero anywhere in the x,y-plane?
No, because it is the sum of these two squares.
This much of it could be zero only at the origin,
but that plus two eliminates even that.
This is always positive in the entire x,y-plane.
Here my domain is the whole x,y-plane and,
therefore, the conclusion is that there are no closed
trajectories in the x,y-plane, anywhere. And we have done that with just
a couple of lines of calculation and nothing further required.
No computer search. In fact, no computer search
could ever proof this. It would be impossible because,
no matter where you look, there is always some other
place to look. This is an example where a
couple lines of mathematics dispose of the matter far more
effectively than a million dollars worth of calculation.
Well, where does Bendixson's theorem come from?
Yes, Bendixson's theorem comes from 18.02.
And I am giving it to you both to recall a little bit of 18.02
to you. Because it is about the first
example in the course that we have had of an indirect
argument. And indirect arguments are
something you have to slowly get used to.
I am going to give you an indirect proof.
Remember what that is? You assume the contrary and you
show it leads to a contradiction.
What would assuming the contrary be?
Contrary would be I will assume the divergence is not zero,
but I will suppose there is a closed trajectory.
Suppose there is a closed trajectory that exists. Let's draw a picture of it. And let's say it goes around
this way. There is a closed trajectory
for our system. Let's call the curve C.
And I am going to call the inside of it R,
the way one often does in 18.02.
D is all this region out here, in which everything is taking
place. This is to exist in D.
Now, what I am going to do is calculate a line integral around
that curve. A line integral of this vector
field. Now, there are two things you
can calculate. One of the line integrals,
I will put in a few of the vectors here.
The vectors I know are pointing this way because that is the
direction in which the curve is being traversed in order to make
it a trajectory. Those are a few of the typical
vectors in the field. I am going to calculate the
line integral around that curve in the positive sense.
In other words, not in the direction of the
salmon-colored arrow, but in the normal sense in
which you calculate it using Green's theorem,
for example. The positive sense means the
one which keeps the region, the inside on your left,
as you walk around like that the region stays on your left.
That is the positive sense. That is the sense in which I am
integrating. I am going to use Green's
theorem, but the integral that I am going to calculate is not the
work integral. I am going to calculate instead
the flux integral, the integral that represents
the flux of F across C. Now, what is that integral?
Well, at each point, you station a little ant and
the ant reports the outward flow rate across that point which is
F dotted with the normal vector. I will put in a few normal
vectors just to remind you. The normal vectors look like
little unit vectors pointing perpendicularly outwards
everywhere. These are the n's.
F dotted with the unit normal vector, and that is added up
around the curve. This quantity gives me the flux
of the field across C. Now, we are going to calculate
that by Green's theorem. But, before we calculate it by
Green's theorem, we are going to psych it out.
What is it? What is the value of that
integral? Well, since I am asking you to
do it in your head there can only be one possible answer.
It is zero. Why is that integral zero?
Well, because at each point the field vector,
the velocity vector is perpendicular to the normal
vector. Why?
The normal vector points perpendicularly to the curve but
the field vector always is tangent to the curve because
this curve is a trajectory. It is always supposed to be
going in the direction given by that white field vector.
Do you follow? A trajectory means that it is
always tangent to the field vector and, therefore,
always perpendicular to the normal vector.
This is zero since F dot n is always zero.
Everywhere on the curve, F dot n has to be zero.
There is no flux of this field across the curve because the
field is always in the same direction as the curve,
never perpendicular to it. It has no components
perpendicular to it. Good.
Now let's do it the hard way. Let's use Green's theorem.
Green's theorem says that the flux across C should be equal to
the double integral over that region of the divergence of F.
It's like Gauss theorem in two dimensions, this version of it.
Divergence of F, that is a function,
I double integrate it over the region, and then that is dx /
dy, or let's say da because you might want to do it in polar
coordinates. And, on the problem set,
you certainly will want to do it in polar coordinates,
I think. All right.
How much is that? Well, we haven't yet used the
hypothesis. All we have done is set up the
problem. Now, the hypothesis was that
the divergence is never zero anywhere in D.
Therefore, the divergence is never zero anywhere in R.
What I say is the divergence is either greater than zero
everywhere in R. Or less than zero everywhere in
R. But it cannot be sometimes
positive and sometimes negative. Why not?
In other words, I say it is not possible the
divergence here is one and here is minus two.
That is not possible because, if I drew a line from this
point to that, along that line the divergence
would start positive and end up negative.
And, therefore, have to be zero some time in
between. It's because it is a continuous
function. It is a continuous function.
I am assuming that. And, therefore,
if it sometimes positive and sometimes negative it has to be
zero in between. You cannot get continuously
from plus one to minus two without passing through zero.
The reason for this is, since the divergence is never
zero in R it therefore must always stay positive or always
stay negative. Now, if it always stays
positive, the conclusion is then this double integral must be
positive. Therefore, this double integral
is either greater than zero. That is if the divergence is
always positive. Or, it is less than zero if the
divergence is always negative. But the one thing it cannot be
is not zero. Well, the left-hand side,
Green's theorem is supposed to be true.
Green's theorem is our bedrock. 18.02 would crumble without
that so it must be true. One way of calculating the
left-hand side gives us zero. If we calculate the right-hand
side it is not zero. That is called the
contradiction. Where did the contradiction
arise from? It arose from the fact that I
supposed that there was a closed trajectory in that region.
The conclusion is there cannot be any closed trajectory of that
region because it leads to a contradiction via Green's
theorem. Let me see if I can give you
some of the argument for the other, well, let's at least
state the other criterion I wanted to give you. Suppose, for example,
we use this system, x prime equals -- Does this have limit cycles? Does that have limit cycles? Let's Bendixson it.
We will calculate the divergence of a vector field.
It is 2x from the top function. The partial with respect to x
is 2x. The second function with
respect to y is negative 2y. That certainly could be zero.
In fact, this is zero along the entire line x equals y.
Its divergence is zero here along that whole line.
The best I could conclude was, I could conclude that there is
no limit cycle like this and there is no limit cycle like
this, but there is nothing so far that says a limit cycle
could not cross that because that would not violate
Bendixson's theorem. In other words,
any domain that contained part of this line,
the divergence would be zero along that line.
And, therefore, I could conclude nothing.
I could have limit cycles that cross that line,
as long as they included a piece of that line in them.
The answer is I cannot make a conclusion.
Well, that is because I am using the wrong criterion.
Let's instead use the critical point criterion. Now, I am going to say that it
makes a nice positive statement but nobody ever uses it this
way. Nonetheless,
let's first state it positively, even though that is
not the way to use it. The positive statement will be,
once again, we have our region D and we have a region of the xy
plane and we have our C, a closed trajectory in it.
A closed trajectory of what? Of our system.
And that is supposed to be in D.
The critical point criterion says something very simple.
If you have that situation it says that inside that closed
trajectory there must be a critical point somewhere. It says that inside C is a
critical point. Now, this won't help us with
the existence problem. This won't help us find a
closed trajectory. We will take our system and say
it has a critical point here and a critical point there.
Does it have a closed trajectory?
Well, all I know is the closed trajectory, if it exists,
will have to go around one or more of those critical points.
But I don't know where. It is not going to go around it
like this. It might go around it like
this. And my computer search won't
find it because it is looking at too small a part of the screen.
It doesn't work that way. It works negatively by
contraposition. Do you know what the
contrapositive is? You will at least learn that.
A implies B says the same thing as not B implies not A. They are different statements
but they are equivalent to each other.
If you prove one you prove the other.
What would be the contrapositive here?
If you have a closed trajectory inside is a critical point.
The theorem is used this way. If D has no critical points,
it has no closed trajectories and therefore has no limit
cycle. Because, if it did have a
closed trajectory, inside it would be a critical
point. But I said B had no critical
point. That enables us to dispose of
this system that Bendixson could not handle at all.
We can dispose of this system immediately.
Namely, what is it? Where are its critical points?
Well, where is that zero? x squared plus y 
squared is one, plus one is never zero.
This is positive. Or, worse, zero.
And then I add the one to it and it is not zero anymore.
This has no zeros and, therefore, it does not matter
that this one has a lot of zeros.
It makes no difference. It has no critical points.
It has none, therefore, no limit cycles. Now, I desperately wanted to
give you the proof of this. It is clearly impossible in the
time remaining. The proof requires a little
time. I haven't decided what to do
about that. It might leak over until
Friday's lecture. Instead, I will finish by
telling you a story. How is that? And along side of it was little
y prime. I am not going to continue on
with the letters of the alphabet.
I will prime the earlier one. This has a total of 12
parameters in it. But, in fact,
if you change variables you can get rid of all the linear terms.
The important part of it is only the quadratic terms in the
beginning. This sort of thing is called a
quadratic system. After you have departed from
linear systems, it is the simplest kind there
is. And the predictor-prey,
the robin-earthworm example I gave you is of a typical
quadratic system and exhibits typical nonlinear quadratic
system behavior. Now, the problem is the
following. A, b, c, d, e,
f and so on, those are just real numbers,
parameters, so I am allowed to give them any values I want.
And the problem that has bothered people since 1880 when
it was first proposed is how many limit cycles can a
quadratic system have? After 120 years this problem is
totally unsolved, and the mathematicians of the
world who are interested in it cannot even agree with each
other on what the right conjecture is.
But let me tell you a little bit of its history.
There were attempts to solve it in the 20 or 30 years after it
was first proposed, through the 1920 and `30s which
all seemed to have gaps in them. Until finally around 1950 two
Russians mathematicians, one of whom is extremely
well-known, Petrovski, a specialist in systems of
ordinary differential equations published a long and difficult,
complicated 100 page paper in which they proved that the
maximum number is three. I won't put down their names.
Petrovski-Landis. The maximum number was three.
And then not many people were able to read the paper,
and those who did there seemed to be gaps in the reasoning in
various places until finally Arnold who was the greatest
Russian, in my opinion, one of the greatest Russian
mathematician, certainly in this field of
analysis and differential equations, but in other fields,
too, he still is great, although he is somewhat older
now, criticized it. He said look,
there is a really big gap in this argument and it really
cannot be considered to be proven.
People tried working very hard to patch it up and without
success. Then about 1972 or so,
'75 maybe, a Chinese mathematician found a system
with four. Wrote down the numbers,
the number a is so much, b is so much,
and they were absurd numbers like 10^-6 and 40 billion and so
on, nothing you could plot on a computer screen,
but found a system with four. Nobody after this tried to fill
in the gap in the Petrovski-Landis paper.
I was then chairman of the math department, and one of my tasks
was protocol and so on. Anyway, we were trying very
hard to attract a Chinese mathematician to our department
to become a full professor. He was a really outstanding
analyst and specialist in various fields.
Anyway, he came in for a courtesy interview and we
chatted. At the time,
I was very much interested in limit cycles.
And I had on my desk the Math Society's translation of the
Chinese book on limit cycles. A collection of papers by
Chinese mathematicians all on limit cycles.
After a certain point he said, oh, I see you're interested in
limit cycle problems. I said yeah,
in particular, I was reading this paper of the
mathematician who found four limit cycles.
And I opened to that system and said the name is,
and I read it out loud. I said do you by any chance
know him? And he smiled and said yes,
very well. That is my mother.
[LAUGHTER] Well, bye-bye.

CS

The following content is
provided under a Creative Commons license. Your support will help
MIT OpenCourseWare continue to offer high quality
educational resources for free. To make a donation or
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. ERIK DEMAINE: All right,
let's get started. Today we're going to continue
the theme of randomization and data structures. Last time we saw skip lists. Skip lists solve the
predecessor-successor problem. You can search for an item
and if it's not there, you get the closest item
on either side in log n with high probability. But we already knew how to
do that deterministically. Today we're going to solve a
slightly different problem, the dictionary problem
with hash tables. Something you already
think you know. But we're going to show you
how much you didn't know. But after today you will know. And we're going to get
constant time and not with high probability. That's hard. But we'll do constant
expected time. So that's in some sense better. It's going to solve
a weaker problem. But we're going to get
tighter bound constant instead of logarithmic. So for starters let me remind
you what problem we're solving and the basics of hashing
which you learned in 6006. I'm going to give this problem
a name because it's important and we often forget
to distinguish between two types of things. This is kind of an old
term, but I would call this an abstract data type. This is just the
problem specification of what you're trying to do. You might call this an
interface or something. This is the problem statement
versus the data structure is how you actually solve it. The hash tables are
the data structure. The dictionary is the problem
or the abstract data type. So what we're
trying to do today, as in most data
structures, is maintain a dynamic set of items. And here I'm going to
distinguish between the items and their keys. Each item has a key. And normally you'd
think of there also being a value like in Python. But we're just
worrying about the keys and moving the items around. And we want to support
three operations. We want to be able to insert
an item, delete an item, and search for an item. But search is going to
be different from what we know from AVL trees
or skip lists or even Venom [INAUDIBLE] That was a
predecessor-successor search. Here we just want
to know-- sorry, your not searching for an item. Usually you're searching
for just a key-- here you just want to know is
there any item with that key, and return it. This is often called
an exact search because if the key
is not in there, you learn absolutely nothing. You can't find the nearest key. And for whatever reason this
is called a dictionary problem though it's unlike
a real dictionary. Usually when you search for a
word you do find its neighbors. Here we're just going to
either-- if the key's there we find that, otherwise not. And this is exactly what a
Python dictionary implements. So I guess that's why Python
dictionaries are called dicts. So today I'm going to assume
all items have distinct keys. So in the insertion I will
assume key is not already in the table. With a little bit
of work, you can allow inserting an item
with an existing key, and you just overwrite
that existing item. But I don't want to
worry about that here. So we could, of course,
solve this using an AVL tree in log n time. But our goal is to do better
because it's an easier problem. And I'm going to remind you
of the simplest way you learn to do this which was hashing
with chaining in 006. And the catch is you didn't
really analyze this in 006. So we're going make a
constant time per operation. It's going to be expected or
something and linear space. And remember the
variables we care about, there's u, n, and m. So u is the size
of the universe. This is the all possible keys. The space of all possible keys. n is the size of the set
your currently storing. So that's the number
of items or keys currently in the data structure. And then m is the
size of your table. So say it's the number
of slots in the table. So you remember the picture. You have a table of slots. Let's say 0 to m minus 1. Each of them is a
pointer to a linked list. And if you have,
let's say over here is your universe of
all possible keys, then we have a hash function
which maps each universe item into one of these slots. And then the linked
list here is storing all of the items that
hash to that slot. So we have a hash function
which maps the universe. I'm going to assume the
universe has already been mapped into integers 0 to u minus 1. And it maps to slots. And when we do
hashing with chaining, I think I mentioned this
last week, the bounds you get, we achieve
a bound of 1 plus alpha where alpha is
the load factor n/m. The average number of items
you'd expect to hash to a slot is the number of items divided
by the number of slots. OK. And you proved this
in 6006 but you assumed something called
simple uniform hashing. Simple uniform hashing
is an assumption, I think invented for CLRS. It makes the
analysis very simple, but it's also
basically cheating. So today our goal
is to not cheat. It's nice as a warm up. But we don't like cheating. So you may recall the assumption
is about the hash function. You want a good hash function. And good means this. I want the probability
of two distinct keys mapping to the same slot to
be 1/m if there are m slots. If everything was
completely random, if h was basically choosing a
random number for every key, then that's what we
would expect to happen. So this is like the
idealized scenario. Now, we can't have
a hash function could choosing a random
number for every key because it has to choose the
same value if you give it the same key. So it has to be some kind
of deterministic strategy or at least repeatable
strategy where if you plug in the same
key you get the same thing. So really what this
assumption is saying is that the key's that you
give are in some sense random. If I give you random keys
and I have not-too-crazy hash function then this will be true. But I don't like assuming
anything about the keys maybe. I want my keys to
be worst case maybe. There are lots of examples in
the real world where you apply some hash function
and it turns out your data has some very
particular structure. And if you choose a
bad hash function, then your hash table
gets really, really slow. Maybe everything hashes
to the same slot. Or say you take--
well yeah, there are lots of examples of that. We want to avoid that. After today you will know how to
achieve constant expected time no matter what your keys
are, for worst case keys. But it's going to take
some work to do that. So this assumption
requires assuming that the keys are random. And this is what we would
call an average case analysis. You might think that
average case analysis is necessary for
randomized algorithms, but that's not true. And we saw that last
week with quicksort. Quicksort, if you say I
will always choose a of 1 to be my partition
element, that's what the textbook calls
basic quicksort, then for an average input
that will do really well. If you have a uniform
random permutation of items and you sort with the method of
always choosing the first item as your partition, then that
will be n log n on average if your data is average. But we saw we could
avoid that assumption by choosing a random pivot. If you choose a
random pivot, then you don't need to assume
anything about the input. You just need to assume
that the pivots are random. So it's a big difference between
assuming your inputs are random versus assuming your
coin flips are random. It's pretty reasonable to
assume you can flip coins. If you've got enough
dexterity in your thumb then you can do it. But it's not so
reasonable to assume that your input is random. So we'd like to avoid average
case analysis whenever we can, and that's the goal of today. So what you saw in 006 was
essentially assuming the inputs are random. We're going to get rid of that
unreasonable assumption today. So that's, in some
sense, review from 006. I'm going to take a
brief pause and tell you about the etymology of the word
hash in case you're curious. Hash is an English word since
the 1650's, so it's pretty old. It means literally
cut into small pieces. It's usually used
in a culinary sense, like these days you have
corned beef hash or something. I'll put the
definition over here. It comes from French, hacher,
which means to chop up. You know it in English
from the word hatchet. So it's the same derivation. And it comes from old French--
I don't actually know whether that's "hash-ay" or "hash"
but-- which means axe. So you can see the derivation. If you look this
up in OED or pick your favorite dictionary or even
Google, that's what you find. But in fact there's a
new prevailing theory that in fact hash comes
from another language which is Vulcan, la'ash, I mean you
can see the derivation right? Actually means axe. So maybe French got it
from Vulcan or vice versa but I think that's pretty clear. Live long and prosper,
and farewell to Spock. Sad news of last week. So enough about hashing. We'll come back to
that in a little bit. But hash functions
essentially take up this idea of taking your
key, chopping up into pieces, and mixing it like
in a good dish. All right, so we're going
to cover two ways to get strong constant time bounds. Probably the most useful one
is called universal hashing. We'll spend most of
our time on that. But the theoretically cooler
one is called perfect hashing. Universal hashing,
we're going to guarantee there are very few
conflicts in expectation. Perfect hashing , we're going
to guarantee there are zero conflicts. The catch is, at least
in its obvious form, it only works for static sets. If you forbid, insert,
and delete and just want to do search, then perfect
hashing is a good method. So like if you're
actually storing a dictionary, like
the OED, English doesn't change that quickly. So you can afford to recompute
your data structure whenever you release a new edition. But let's start with
universal hashing. This is a nice
powerful technique. It works for dynamic data. Insert, delete, and
search will be constant expected time with no
assumptions about the input. So it will not be average case. It's in some sense worse
case but randomized. So the idea is we need
to do something random. If you just say, well, I
choose one hash function once and for all, and I
use that for my table, OK maybe my table
doubles in size and I change the hash function. But there's no randomness there. We need to introduce
randomness somehow into this data structure. And the way we're
going to do that is in how we choose
the hash function. We're going to choose our
hash function randomly from some set of hash functions. Call it h. This is going to be a
universal hash family. We're going to imagine
there are many possible hash functions we could choose. If we choose one of them
uniformly at random, that's a random choice. And that randomness
is going to be enough that we no longer need to
assume anything about the keys. So for that to work, we need
some assumption about h. Maybe it's just a set
of one hash function. That wouldn't add
much randomness. Two also would not
add much randomness. We need a lot of them. And so we're going to require
H to have this property. And we're going to call it
the property universality. Generally you would call
it a universal hash family. Just a set of hash functions. What we want is that-- so we're
choosing our hash function h from H. And
among those choices we want the probability that
two keys hash to the same value to be small. I'll say-- and this
is very similar looking to simple
uniform hashing. Looks almost the same here
except I switched from k1 and k2 to k and
k', but same thing. But what we're taking
the probability over, what we're assuming is
random is different. Here we're assuming k1 and
k2 a are because h was fixed. This was an assumption
about the inputs. Over here we're thinking
of k and k' as being fixed. This has to work for every
pair of distinct keys. And the probability
we're considering is the distribution of h. So we're trying all the
different h's Or we're trying little h uniformly at random. We want the probability that a
random h makes k and k' collide to be at most 1/m. The other difference is we
switch from equals to at most. I mean less would be better. And there are ways to make
it less for a couple pairs but it doesn't really matter. But of course anything
less than or equal to 1/m will be just as good. So this is an
assumption about H. We'll see how to achieve this
assumption in a little bit. Let me first prove to
you that this is enough. It's going to be basically
the same as the 006 analysis. But it's worth repeating just
so we are sure everything's OK. And so I can be more precise
about what we're assuming. The key difference between this
theorem and the 006 theorem is we get to make no
assumptions about the keys. They are arbitrary. You get to choose
them however you want. But then I choose a
random hash function. The hash function cannot
depend on these keys. But it's going to be random. And I choose the hash function
after you choose the keys. That's important. So we're going to
choose a random h and H. And we're assuming
H is universal. Then the expected number of keys
in a slot among those n keys is at most 1 plus alpha. Alpha is n/m. So this is exactly
what we had over here. Here we're talking
about time bound. But the time bound
followed because the length of each chain was expected
to be 1 plus alpha. And here the expectation
is over the choice of h. Not assuming anything
about the keys. So let's prove this theorem. It's pretty easy. But I'm going to introduce
some analysis techniques that we will use for
more interesting things. So let's give the keys a name. I'll just call
them-- I'll be lazy. Use k1 up to kn. And I just want to
compute that expectation. So I want to compute let's say
the number of keys colliding with one of those
keys, let's say ki. So this is of course the size of
the slot that ki happens to go. This is going to work for all i. And so if I can say that this
is at most 1/alpha for each i, then I have my theorem. Just another way
to talk about it. Now the number of keys
colliding with ki, here's a general trick, whenever
you want to count something in expectation, a
very helpful tool is indicator random variables. Let's name all of the different
events that we want to count. And then we're basically
summing those variables. So I'm going to say-- I'm going
to use I ij to be an indicator random variable. It's going to be 1 or 0. 1 if hash function of ki
equals the hash function of kj. So there's a collision
between ki and kj j and 0 if they hash to different slots. Now this is, it's a random
variable because it depends on h and h is a random thing. ki and kj are not random. They're given to you. And then I want to know
when does h back those two keys to the same slot. And so this number is really
just the sum of Iij over all j. This is the same thing. The number in here is the sum
for j not equal to i of Iij. Because we get a 1 every time
they collide, zero otherwise. So that counts how many collide. Once we have it
in this notation, we can use all the great
dilemmas and theorems about in this case,
E, expectation. What should I use here? STUDENT: What? ERIK DEMAINE: What's
a good-- how can I simplify this formula? STUDENT: The linearity
of expectation. ERIK DEMAINE: The
linearity of expectation. Thank you. If you don't know
all these things, read the probability
appendix in the textbook. So we want to talk
about expectation of the simplest thing possible. So linearity let's us
put the E inside the sum without losing anything. Now the expectation of an
indicator random variable is pretty simple
because the zeros don't contribute to the expectation. The 1's contribute 1. So this is the same thing
as just the probability of this being 1. So we get sum of j9 equal
to I of the probability that Iij equals 1. And the probability
that Iij equals 1, well, that's the probability
that this happens. And what's the probability
that that happens? At most 1/m our universality. So I'm going to--
I'll write it out. This is sum j not
equal to I. Probability that h maps ki and
kj to the same slot. So that's the definition of Iij. And this is at most
sum j not equal to i of 1/m by universality. So here's where we're using it. And sum of j not equal to
I, well that's basically n. But I made a mistake here. Slightly off. From here-- yeah. So this line is wrong. Sorry. Let me fix it. Because this
assumption only works when the keys are distinct. So in fact-- how did I get
j-- yeah. , Yeah, sorry. This should have been
this-- actually everything I said is true, but if you want
to count the number of keys-- I really wanted to count the
total number of keys that hash to the same place as ki. So there's one more
which is ki itself. Always hashes to
wherever ki hashes. So I did a summation
j not equal i but I should also have
a plus Iii-- captain. So there's the case when I
hashing to the same place which of course is always going to
happen so you get basically plus 1 everywhere. So that makes me
happier because then I actually get with the theorem
said which is 1 plus alpha. There is always going to be
the one guy hashing there when I assume that ki
hashed to wherever it does. So this tells you that if we
could find a universal hash family, then we're guaranteed
insert, delete, and search cost order 1 plus
alpha in expectation. And the expectation is
only over the choice of h, not over the inputs. I think I've stressed
that enough times. But the remaining question
is can we actually design a universal hash family? Are there any universal
hash families? Yes, as you might
expect there are. Otherwise this wouldn't
be very interesting. Let me give you an example of
a bad universal hash family. Sort of an oxymoron
but it's possible. Bad. Here's a hash family
that's universal. h is the set of
all hash functions. h from 0,1 to u minus 1. This is what's normally
called uniform hashing. It makes analysis
really easy because you get to assume-- I
mean this says ahead of time for every
universe item, I'm going to choose a
random slot to put it. And then I'll just
remember that. And so whenever you give me
the key, I'll just map it by h. And I get a consistent slot
and definitely it's universal. What's bad about
this hash function? Many things but-- STUDENT: [INAUDIBLE] That's
just as hard as the problem I'm solving. ERIK DEMAINE: Sort of. I'm begging the
question that it's just as hard as the
problem I'm solving. And what, algorithmically,
what goes wrong here? There are two things I guess. Yeah? STUDENT: It's not deterministic? ERIK DEMAINE: It's
not deterministic. That's OK because we're
allowing randomization in this algorithm. So I mean how I
would compute this is I would do a four loop
over all universe items. And I assume I have a way
to generate a random number between 0 and m minus 1. That's legitimate. But there's something
bad about that algorithm. STUDENT: It's not consistent. ERIK DEMAINE: Not consistent? It is consistent if I precompute
for every universe item where to map it. That's good. So all these things
are actually OK. STUDENT: It takes too
much time and space. ERIK DEMAINE: It takes
too much time and space. Yeah. That's the bad thing. It's hard to isolate in a bad
thing what is so bad about it. But we need u time to compute
all those random numbers. And we need u space to
store that hash function. In order to get to the
consistency we have to-- Oops. Good catch. In order to get
consistency, we need to keep track of all those
hash function values. And that's not good. You could try to not
store them all, you know, use a hash table. But you can't use a hash table
to store a hash function. That would be-- that would
be infinite recursion. So but at least
they're out there. So the challenge is to find
an efficient hash family that doesn't take much space
to store and doesn't take much time to compute. OK, we're allowing randomness. But we don't want
to much randomness. We can't afford u units
of time of randomness. I mean u could be huge. We're only doing n operations
probably on this hash table. u could be way bigger than n. We don't want to have to
precompute this giant table and then use it for
like five steps. It would be really, really
slow even amortized. So here's one that
I will analyze. And there's another one in the
textbook which I'll mention. This one's a little
bit simpler to analyze. We're going to need a little
bit of number theory, just prime numbers. And you've probably heard of
the idea of your hash table size being prime. Here you'll see
why that's useful, at least for this family. You don't always need
primality, but it's going to make this family work. So I'm going to assume that
my table size is prime. Now really my table
size is doubling, so that's a little awkward. But luckily there are
algorithms given a number to find a nearby prime number. We're not going to
cover that here, but that's an algorithmic
number theory thing. And in polylogarithmic
time, I guess you can find a
nearby prime number. So you want it to
be a power of 2. And you'll just look around
for nearby prime numbers. And then we have a prime that's
about the same size so that will work just as well from
a table doubling perspective. Then furthermore,
for convenience, I'm going to assume that u
is an integer power of m. I want my universe to be
a power of that prime. I mean, if it isn't, just
make u a little bigger. It's OK if u gets
bigger as long as it covers all of the same items. Now once I view my universe
as a power of the table size, a natural thing to do is
take my universe items, to take my input integers,
and think of them in base m. So that's what I'm going to do. I'm going to view
a key k in base m. Whenever I have a
key, I can think of it as a vector of subkeys,
k1 up to kr minus 1. There are digits in base m
because of this relation. And I don't even care which
is the least significant and which is the
most significant. That won't matter so
whatever, whichever order you want to think of it. And each of the
ki's here I guess is between 0 and m minus 1. So far so good. So with this perspective,
the base m perspective, I can define a dot product
hash function as follows. It's going to be
parametrized by another key, I'll call it a, which we can
think of again as a vector. I want to define h sub a of k. So this is parametrized
by a, but it's a function of a given
key k as the dot product of those two vectors mod m. So remember dot products
are just the sum from i equals 0 to r minus
1 of a1 times ki. I want to do all
of that modulo m. We'll worry about
how long this takes to compute in a moment I guess. Maybe very soon. But the hash family h is
just all of these ha's for all possible choices of a. a was a key so it comes
from the universe u. And so what that means is
to do universal hashing, I want to choose one of these
ha's uniformly at random. How do I do that? I just choose a
uniformly at random. Pretty easy. It's one random value
from one random key. So that should take constant
time and constant space to store one number. In general we're in a world
called the Word RAM model. This is actually-- I
guess m stands for model so I shouldn't write model. Random access machine
which you may have heard. The word RAM assumes
that in general we're manipulating integers. And the integers fit in a word. And the computational
assumption is that manipulating a
constant number of words and doing essentially
any operation you want on constant number of
words takes constant time. And the other part
of the word RAM model is to assume that the things
you care about fit in a word. Say individual data values,
here we're talking about keys, fit in a word. This is what you need
to assume in [INAUDIBLE] that you can compute high
of x in constant time or low of x in constant time. Here I'm going to use it to
assume that we can compute h sub a of k in constant time. In practice this would
be done by implementing this computation, this
dot product computation, in hardware. And the reason a 64-bit
edition on a modern processor or a 32-bit on most
phones takes constant time is because there's
hardware that's designed to do that really fast. And in general we're assuming
that the things we care about fit in a single word. And we're assuming random access
and that we can have a raise. That's what we need in
order to store a table. And same thing in [INAUDIBLE],
we needed to assume we had a raise. And I think this
operation is actually pretty-- exists in Intel
architectures in some form. But it's certainly not
a normal operation. If you're going to
do this explicitly, adding up and
multiplying things this would be r is the log base m of
u, so it's kind of logish time. Maybe I'll mention
another hash family that's more obviously computable. But I won't analyze here. It's analyzed in the textbook. So if you're curious you
can check it out there. Let's call this just another. It's a bit weird
because it has two mods. You take mod p and then mod m. But the main computation
is very simple. You choose a uniformly
random value a. You multiply it by your key
in usual binary multiplication instead of dot product. And then you add another
uniformly random key. This is also universal. So H is hab for all a
and b that are keys. So if you're not happy
with this assumption that you can compute
this in constant time, you should be happy
with this assumption. If you believe in addition and
multiplication and division being constant time, then
this will be constant time. So both of these
families are universal. I'm going to prove that this
one is universal because it's a little bit easier. Yeah? STUDENT: Is this p a
choice that you made? ERIK DEMAINE: OK, right. What is p? P just has to be bigger than
m, and it should be prime. It's not random. You can just choose one prime
that's bigger than your table size, and this will work. STUDENT: [INAUDIBLE] ERIK DEMAINE: I
forget whether you have to assume that m is prime. I'd have to check. I'm guessing not, but
don't quote me on that. Check the section
in the textbook. So good. Easy to compute. The analysis is simpler, but
it's a little bit easier here. Essentially this is
very much like products but there's no
carries here from one. When we do the dot product
instead of just multiplying in base m we multiply
them based on that would give the same thing
as multiplying in base 2, but we get carries from one
m-sized digit to the next one. And that's just more
annoying to think about. So here we're essentially
getting rid of carries. So it's in some sense
even easier to compute. And in both cases,
it's universal. So we want to prove
this property. That if we choose a random
a then the probability of two keys, k and k'
which are distinct mapping via h to the same value is at
most 1/m So let's prove that. So we're given two keys. We have no control
over them because this has to work for all
keys that are distinct. The only thing we know
is that they're distinct. Now if two keys are
distinct, then their vectors must be distinct. If two vectors
are distinct, that means at least one
item must be different. Should sound familiar. So this was like in the matrix
multiplication verification algorithm that
[INAUDIBLE] taught. So k and k' differ
in some digit. Let's call that digit d. So k sub d is different
from k sub d'. And I want to compute
this probability. We'll rewrite it. The probability is over a. I'm choosing a
uniformly at random. I want another
probability that that maps k and k' to the same slot. So let me just write
out the definition. It's probability over a that
the dot product of a and k is the same thing as when I do
the dot product with k' mod m. These two, that sum should
come out the same, mod m. So let me move this part over to
this side because in both cases we have the same ai. So I can group
terms and say this is the probability--
probability sum over i equals 0 to r minus 1
of ai times ki minus ki prime equals 0. Mod m. OK, no pun intended. Now we care about this digit d. d is a place where we know
that this is non-zero. So let me separate out the terms
for d and everything but d. So this is the same as ability
of, let's do the d term first, so we have ad times
kd minus kd prime. That's one term. I'm going to write
the summation of i not equal to d of ai
ki minus ki prime. These ones, some of
them might be zero. Some are not. We're not going
to worry about it. It's enough to just isolate
one term that is non-zero. So this thing we know
does not equal zero. Cool. Here's where I'm going to use
a little bit of number theory. I haven't yet used
that m is prime. I required m is prime because
when you're working modulo m, you have multiplicative
inverses. Because this is
not zero, there is something I can
multiply on both sides and get this to cancel
out and become one. For every value x
there is a value y. So x times y equals 1 modulo m. And you can even compute
it in constant time in a reasonable model. So then I can say I want the
probability that ad is minus kd minus kd prime inverse. This is the multiplicative
inverse I was talking about. And then the sum i not equal
to d whatever, I don't actually care what this is too much, I've
already done the equals part. I still need to write mod m. The point is this
is all about ad. Remember we're choosing
a uniformly at random. That's the same
thing as choosing each of the ai's independently
uniformly at random. Yeah? STUDENT: Is the second line over
there isolating d [INAUDIBLE]? Second from the top. ERIK DEMAINE: Which? This one? STUDENT: No up. ERIK DEMAINE: This? STUDENT: Down. That one. No. The one below that. ERIK DEMAINE: Yes. STUDENT: Is that line
isolating d or is that-- ERIK DEMAINE: No. I haven't isolated d yet. This is all the terms. And then going from
this line to this one, I'm just pulling out
the i equals d term. That's this term. And then separating out
the i not equal to d. STUDENT: I get it. ERIK DEMAINE: Right? This sum is just the
same as that sum. But I've done the
d term explicitly. STUDENT: Sure. I get it. ERIK DEMAINE: So I've
done all this rewriting because I know that ad is
chosen uniformly at random. Here we have this
thing, this monstrosity, but it does not depend on ad. In fact it is independent of ad. I'm going to write this
as a function of k and k' because those are
given to us and fixed. And then it's also a
function of a0 and a1. Everything except d. So ad minus 1, ad plus 1,
and so on up to ar minus 1. This is awkward to write. But everything except
ad appears here because we have
i not equal to d. And these ai's are
random variables. But we're assuming that they're
all chosen independently from each other. So I don't really care what's
going on in this function. It's something. And if I rewrite
this probability, it's the probability
over the choice of a. I can separate out the
choice of all these things from the choice of ad. And this is just
a useful formula. I'm going to write
a not equal to d. All the other-- maybe I'll
write a sub i not equal to d. All the choices of
those guys separately from the probability
of choosing ad of ad equal to this function. If you just think about the
definition of expectation, this is doing the same thing. We're thinking of first
choosing the ai's where i is not equal to d. And then we choose ad. And this computational will
come out the same as that. But this is the probability
of a uniformly random number equaling something. So we just need to
think about-- sorry. Important. That would be pretty
unlikely that would be 1/u, but this is all
working modulo m. So if I just take a
uniformly random integer and the chance of it hitting any
particular value mod m is 1/m. And that's universality. So in this case, you get exactly
1/m, no less than or equal to. Sorry, I should have written
it's the expectation of 1/m, but that's 1/m because 1/m
has no random parts in it. Yeah? STUDENT: How do
we know that the, that this expression doesn't
have any biases in the sense that it doesn't give more,
more, like if you give it the uniform
distribution of numbers, it doesn't spit out
more numbers than others and that could potentially-- ERIK DEMAINE: Oh,
so you're asking how do we know that
this hash family doesn't prefer some slots
over others, I guess. STUDENT: Of course like
after the equals sign, like in this middle
line in the middle. Middle board. ERIK DEMAINE: This one? Oh, this one. STUDENT: Middle board. ERIK DEMAINE: Middle board. Here. STUDENT: Yes. So how do we know
that if you give it-- ERIK DEMAINE: This function. STUDENT: --random variables,
it won't prefer certain numbers over others? ERIK DEMAINE: So this function
may prefer some numbers over others. But it doesn't matter. All we need is
that this function is independent of
our choice of ad. So you can think
of this function, you choose all of these
random-- actually k and k' are not random-- but you choose
all these random numbers. Then you evaluate your f. Maybe it always comes out to 5. Who knows. It could be super biased. But then you choose ad
uniformly at random. So the chance of ad
equalling 5 is the same as the chance of ad equaling 3. So in all cases, you get
the probability is 1/m. What we need is independence. We need that the ad is chosen
independently from the other ai's. But we don't need to know
anything about f other than it doesn't depend on ad. So and we made it not depend
on ad because I isolated ad by pulling it out
of that summation. So we know there's
no ad's over here. Good question. You get a bonus Frisbee
for your question. All right. That ends universal hashing. Any more questions? So at this point we
have at least one universal hash family. So we're just choosing, in this
case, a uniformly at random. In the other method, we choose
a and b uniformly at random. And then we build
our hash table. And the hash function
depends on m. So also every time we
double our table size, we're going to have to
choose a new hash function for the new value of m. And that's about it. So this will give us constant
expected time-- or in general 1 plus alpha if you're not doing
table doubling-- for insert, delete, and exact search. Just building on the
hashing with chaining. And so this is a good method. Question? STUDENT: Why do you say expected
value of the probability? Isn't it sufficient to just say
the probability of [INAUDIBLE]? ERIK DEMAINE: Uh, yeah,
I wanted to isolate-- it is the overall probability
of this happening. I rewrote it this
way because I wanted to think about first choosing
the ai's where i does not equal d and then choosing ad. So this probability
was supposed to be only over the choice of ad. And you have to do something
with the other ai's because they're random. You can't just say,
what's the probability ad equaling a random variable? That's a little sketchy. I wanted to have no
random variables over all. So I have to kind of bind
those variables with something. And I just want to see what
the-- This doesn't really affect very much, but to
make this algebraically correct I need to say
what the ai's, i not equal to d are doing. Other questions? Yeah. STUDENT: Um, I'm a bit
confused about your definition of the collision in
the lower left board. Why are you adding
i's [INAUDIBLE]? ERIK DEMAINE: Yeah, sorry. This is a funny
notion of colliding. I just mean I want to count
the number of keys that hash to the same slot as ki. STUDENT: So it's not necessarily
like a collision [INAUDIBLE]. ERIK DEMAINE: You
may not call it a collision when it
collides with itself, yeah. Whatever you want to call it. But I just mean hashing
to the same slot is ki. Yeah. Just because I want to count
the total length of the chain. I don't want to count the number
of collisions in the chain. Sorry. Probably a poor choice of word. We're hashing because
we're taking our key, we're cutting it up
into little bits, and then we're mixing them up
just like a good corned beef hash or something. All right let's move
on to perfect hashing. This is more
exciting I would say. Even cooler-- this was cool
from a probability perspective, depending on your
notion of cool. This method will be cool from
a data structures perspective and a probability perspective. But so far data structures
are what we know from 006. Now we're going to go
up a level, literally. We're going to have two levels. So here we're solving-- you
can actually make this data structure dynamic. But we're going to solve
the static dictionary problem which is when you
have no inserts and deletes. You're given the keys up front. You're given n keys. You want to build a table
that supports search. And that's it. You want search to
be constant time and perfect hashing,
also known as FKS hashing because it was invented by
Fredman, Komlos, and Szemeredi in 1984. What we will achieve is constant
time worst case for search. So that's a little
better because here we're just doing constant
expected time for search. But it's worse in that we have
to know the keys up in advance. We're going to take the linear
space in the worst case. And then the
remaining question is how long does it take you to
build this data structure? And for now I'll just
say it's polynomial time. It's actually going
to be nearly linear. And this is also
an expected bounds. Actually with high probability
could be a little more strong here. So it's going to take
us a little bit of time to build this structure,
but once you have it, you have the perfect scenario. There's going to
be in some sense no collisions in our hash
table so it would be constant times first search
and linear space. So that part's great. The only catch is it's static. But beggars can't
be choosers I guess. All right. I'm not sure who's begging
in that analogy but. The keys who want to be stored. I don't know. All right, so the big
idea for perfect hashing is to use two levels. So let me draw a picture. We have our universe, and we're
mapping that via hash function h1 into a table. Look familiar? Exactly the diagram
I drew before. It's going to have
some table size m. And we're going to set m to be
within a constant factor of n. So right now it looks
exactly like regular-- and it's going to
be a universal, h1 is chosen from a
universal hash family, so universal hashing applies. The trouble is we're going
to get some lists here. And we don't want to store
the set of colliding elements, the set of elements that hash to
that place, with a linked list because linked lists are slow. Instead we're going to store
them using a hash table. It sounds crazy. But we're going to have--
so this is position 1. This is going to be h2,1. There's going to be another hash
function h2,0 that maps to some other hash table. These hash tables are going
to be of varying sizes. Some of them will be of size 0
because nothing hashes there. But in general
each of these slots is going to map instead of to
a linked list to a hash table. So this would be h2, m minus 1. I'm going to guarantee in
the second level of hashing there are zero collisions. Let that sink in a little bit. Let me write down a little
more carefully what I'm doing. So h1 is picked from a
universal hash family. Where m is theta n. I want to put a theta-- I
mean I could m equals n, but sometimes we
require m to be a prime. So I'm going to give you some
slop in how you choose m. So it can be prime
or whatever you want. And then at the
first level we're basically doing
hashing with chaining. And now I want to look at
each slot in that hash table. So between 0 and m-1. I'm going to let lj be the
number of keys that hash, it's the length of the
list that would go there. It's going to be
the number of keys, among just the n keys, Number
of, keys hashing to slot j. So now the big question
is, if I have lj keys here, how big do I make that table? You might say, well
I make a theta lj. That's what I always do. But that's not what
I'm going to do. That wouldn't help. We get exactly, I
think, the same number of collisions if we did that,
more or less, in expectation. So we're going do
something else. We're going to pick a hash
function from a universal family, h2,j. It again maps the same universe. The key thing is the
size of the hash table I'm going to choose
which is lj squared. So if there are 3 elements that
happen to hash to this slot, this table will have size 9. So it's mostly empty. Only square root fraction--
if that's a word, if that's a phrase-- will be full. Most of it's empty. Why squared? Any ideas? I claim this will guarantee zero
collisions with decent chance. Yeah. STUDENT: With 1/2
probability you're going to end up
with no collisions. ERIK DEMAINE: With
1/2 probability I'm going to end up
with no collisions. Why? What's it called? STUDENT: Markov [INAUDIBLE] ERIK DEMAINE: Markov's
inequality would prove it. But it's more commonly
known as the, whoa, as the birthday paradox. So the whole name of the game
here is the birthday paradox. If I have, how's
it go, if I have n squared people with n
possible birthdays then-- is that the right way? No, less. If I have n people and n
squared possible birthdays, the probability of getting a
collision, a shared birthday, is 1/2. Normally we think of
that as a funny thing. You know, if I choose a
fair number of people, then I get immediately
a collision. I'm going to do it
the opposite way. I'm going to guarantee that
there's so many birthdays that no 2 of them will collide
with probability of 1/2 No, 1/2 is not great. We're going to fix that. So actually I haven't given
you the whole algorithm yet. There are two steps, 1 and 2. But there are also two
other steps 1.5 and 2.5. But this is the right
idea and this will make things work in expectation. But I'm going to
tweak it a little bit. So first let me
tell you step 1.5. It fits in between the two. I want that the space of this
data structure is linear. So I need to make sure it is. If the sum j equals 0 to
m minus 1 of lj squared is bigger than
some constant times n-- we'll figure out what the
constant is later-- then redo step 1. So after I do step 1, I know
how big all these tables are going to be. If the sum of those squares is
bigger than linear, start over. I need to prove
that this will only have to take-- this
will happen an expected constant number of times. log n times with
high probability. In fact why don't we-- yeah,
let's worry about that later. Let me first tell
you step 2.5 which is I want there to be
zero collisions in each of these tables. It's only going to happen
with probability of 1/2 So if it doesn't
happen, just try again. So 2.5 is while there's some
hash function h2,j that maps 2 keys that we're given to the
same slot at the second level, this is for some j and let's
say ki different from ki prime. But they map to the same place
by the first hash function. So if two keys map to
the same secondary table and there's a
conflict, then I'm just going to redo that construction. So I'm going to repick h2,j. h2,j was a random choice. So if I get a bad choice,
I'll just try another one. Just keep randomly
choosing the a or randomly choosing
this hash function until there are zero collisions
in that secondary table. And I'm going to do
this for each table. So we worry about how
long these will take, but I claim expected
constant number of trials. So let's do the
second one first. After we do this y loop
there are no collisions with the proper notion of
the word collisions, which is two different keys
mapping to the same value. So at this point
we have guaranteed that searches are
constant time worst case after we do all these
4 steps because we apply h1, we figure out which
slot we fit in. Say it's slot j,
then we apply h2j and if your item's
in the overall table, it should be in that
secondary table. Because there are no
collisions you can see, is that one item the
one I'm looking for? If so, return it. If not, it's not anywhere. If there are no
collisions then I don't need chains coming out
of here because it is just a single item. The big question-- so
constant worst case space because 1.5 guarantees that. Constant worst case
time first search. The big question is, how
long does it take to build? How many times do
we have to redo steps 1 and 2 before we
get a decent-- before we get a perfect hash table. So let me remind
you of the birthday paradox, why it works here. As mentioned earlier this is
going to be a union bounds. We want to know the
probability of collision at that second level. Well that's at most the sum
of all possible collisions, probabilities of collisions. So I'm going to say
the sum over all i not equal to ij of
the probability. Now this is over our choice
of the hash function h2,j. Of h2,j of ki equaling
h2,j of ki prime. So union bounds says, of course. The probability of any
of them happening-- we don't know about
interdependence or whatnot-- but certainly almost the sum of
each of these possible events. There are a lot of
possible events. If there 's li
things, that there are going to be li choose
2 possible collisions we have to worry about. We know i is not
equal to i prime. So the number of terms
here is li choose 2. And what's this probability? STUDENT: [INAUDIBLE] ERIK DEMAINE: 1/li at most
because we're assuming h2,j is a universal hash function so
the probability of choosing-- sorry? li squared. Thank you. The size of the table. 1/m but m in this case, the
size of our table is li squared. So the probability that we
choose a good hash function and that these
particular keys don't hit is at most 1/li squared. This is basically li squared/ 2. And so this is at most 1/2. It's a slightly less
than li squared/2. So this is at most 1/2. And this is basically
a birthday paradox in this particular case. That means there
is a probability of at least a half that there
is zero collisions in one of these tables. So that means I'm basically
flipping a fair coin. If I ever get a heads I'm happy. Each time I get a
tails I have to reflip. This should sound
familiar from last time. So this is 2 expected trials
or log n with high probability. We've proved log n
with high probability. That's the same as saying the
number of levels in a skip list is log n with high probability. How many times do I have to flip
a coin before I get a heads? Definitely at most log n. Now we have to do this
for each secondary table. There are m equal theta
and secondary tables. There's a slight question of how
big are the secondary tables. If one of these tables
is like linear size, then I have to spend
linear time for a trial. And then I multiply that
by the number of trials and also the number of different
things that would be like n squared log n n. But you know a secondary table
better not have linear sides. I mean a linear
number of li equal n. That would be bad because
then li squared is n squared and we guaranteed that
we had linear space. So in fact you can prove
with another Chernoff bound. Let me put this over here. That all the li's
are pretty small. Not constant but logarithmic. So li is order log n with
high probability for each i and therefore for all i. So I can just change the alpha
my minus 1 n to the alpha and get that for
all i this happens. In fact, the right answer
is log over log log, if you want to do some
really messy analysis. But we just, logarithmic
is fine for us. So what this means
is we're doing n different things
for each of them with high probability
li is of size log n. And then maybe we'll have
to do like log n trials repeating until we get a
good hash function there. And so the total build
time for steps 1 and 2.5 is going to be at most
n times log squared n. You can prove a tighter
bound but it's polynomial. That's all I wanted to go
for and it's almost linear. So I'm left with one thing
to analyze which is step 1.5. This to me is maybe the
most surprising thing that it works out. I mean here we designed--
we did this li to li squared so the birthday
paradox would happen. This is not surprising. I mean it's a cool idea,
but once you have the idea, it's not surprising
that it works. What's a little more
surprising is that squaring is OK from a space perspective. 1.5 says we're going
to have to rebuild that first table until the
sum of these squared lengths is at most linear. I can guarantee
that each of these is logarithmic so the sum of the
squares is at most like n log squared n. But I claim I can get linear. Let's do that. So for step 1.5
we're looking at what is the expectation of the
sum of the lj squareds being more than linear. Sorry. Expectation. Let's first compute
the expectation and then we'll talk
about a tail bound which is the probability
that we're much bigger than the expectation. First thing is I claim
the expectation is linear. So again whenever we're
counting something-- I mean this is basically
the total number of pairs of items that collide
at the first level with double counting. So I mean if you think of lj
and then I make a complete graph on those lj items,
that's going to have like the squared
number of edges, so, if I also multiply by 2. So this is the same
thing as counting how many pairs of items map to
the same spot, the same slot. So this is going to-- and that
I can write as an indicator random variable which
lets me use linearity of expectation
which makes me happy because then everything simple. So I'm going to write Ii,j. This is going to be 1 if each 1
of ki, I guess, equals h1 if kj and it's going to be
0 if h1 otherwise. This is the total number
of pairwise colliding items including i versus i. And so like if li equals
1, li squared is also 1. There's 1 item
colliding with itself. So this actually works exactly. All right, with the wrong
definition of colliding. If you bear with me. So now we can use
linear of expectation and put the E in here. So this is sum i equals 1
to n sum j equals 1 to n of the expectation of Ii,j. But we know the expectation
of the Ii,j is the probability of it equaling 1 because it's
an indicator random variable. The probability of this
happening over our choice of h1 is at most 1/m by universality. Here it actually is m because
we're at the first level. So this is at most
1/m which is theta n. So when i does not equal j,
so it's a little bit annoying. I do have to separate out the Ii
terms from the i and different i not equal to j terms. But there's only-- I
mean it's basically the diagonal of this matrix. There's n things that will
always collide with themselves. So we're going to get like
n plus the number of i not equal to pairs
double counted. So it's like 2 times n choose 2. But we get to divide by m. So this is like n squared /n. So we get order n. So that's not--
well, that's cool. Expected space is linear. This is what makes
everything work. Last class was about getting
with high probability bounds when we're working with logs. When you want to
get that something is log with high
probability, you have to use, with
respect to n, you have to use a turn off bound. But this is about-- now I
want to show that the space is linear with high probability. Linear is actually really easy. You can use a much weaker
bound called Markov inequality. So I want to claim that the
probability of h1 of this thing lj squareds being bigger
than some constant times n is at most the expectation
of that thing divided by cn. This is Markov's inequality. It holds for anything here. So I'm just repeating
it over here. So this is nice because we
know that this expectation is linear. So we're getting like a
linear function divided by cn. Remember we get to choose c. The step said if it's bigger
than some constant times n then we're redoing the thing. So I can choose c
to be 100, whatever. I'm going to choose it to
be twice this constant. And then this is at most half. So the probability of
my space being too big is at most a half. We're back to coin flipping. Every time I flip
a coin, if I get heads I have the right amount
of space at less than c times n space. If I get a tails I try again. So the expected number
of trials is 2 at most not trails, trials. And it's also log n trials
with high probability. How much time do I
spend for each trial? Linear time. I choose one hash function. I hash all the items. I count the number of collision
squared or the sum of lj squared. That takes linear time to do. And so the total work I'm doing
for these steps is n log n. So n log n to do
step 1 and 1 prime and log squared n to
do steps 2 and 2 prime. Overall n Polylog
or polynomial time. And we get guaranteed no
collisions for static data. Constant worst case search
and linear worst case space. This is kind of surprising
that this works out but everything's nice. Now you know hashing.

Linear Algebra

The following content is
provided under a Creative Commons license.
Your support will help MIT OpenCourseWare continue to offer
high quality educational resources for free.
To make a donation or to view additional materials from
hundreds of MIT courses, visit MIT OpenCourseWare at
ocw.mit.edu. We were looking at vector
fields last time. Last time we saw that if a
vector field happens to be a gradient field -- -- then the
line integral can be computed actually by taking the change in
value of the potential between the end point and the starting
point of the curve. If we have a curve c,
from a point p0 to a point p1 then the line integral for work
depends only on the end points and not on the actual path we
chose. We say that the line integral
is path independent. And we also said that the
vector field is conservative because of conservation of
energy which tells you if you start at a point and you come
back to the same point then you haven't gotten any work out of
that force. If we have a closed curve then
the line integral for work is just zero.
And, basically, we say that these properties
are equivalent being a gradient field or being path independent
or being conservative. And what I promised to you is
that today we would see a criterion to decide whether a
vector field is a gradient field or not and how to find the
potential function if it is a gradient field.
So, that is the topic for today. The question is testing whether
a given vector field, let's say M and N compliments,
is a gradient field. For that, well,
let's start with an observation.
Say that it is a gradient field. That means that the first
component of a field is just the partial of f with respect to
some variable x and the second component is the partial of f
with respect to y. Now we have seen an interesting
property of the second partial derivatives of the function,
which is if you take the partial derivative first with
respect to x, then with respect to y,
or first with respect to y, then with respect to x you get
the same thing. We know f sub xy equals f sub
yx, and that means M sub y equals N sub x.
If you have a gradient field then it should have this
property. You take the y component,
take the derivative with respect to x,
take the x component, differentiate with respect to
y, you should get the same answer.
And that is important to know. So, I am going to put that in a
box. It is a broken box.
The claim that I want to make is that there is a converse of
sorts. This is actually basically all
we need to check. Conversely, if,
and I am going to put here a condition, My equals Nx,
then F is a gradient field. What is the condition that I
need to put here? Well, we will see a more
precise version of that next week.
But for now let's just say if our vector field is defined and
differentiable everywhere in the plane.
We need, actually, a vector field that is
well-defined everywhere. You are not allowed to have
somehow places where it is not well-defined.
Otherwise, actually, you have a counter example on
your problem set this week. If you look at the last problem
on the problem set this week, it gives you a vector field
that satisfies this condition everywhere where it is defined.
But, actually, there is a point where it is
not defined. And that causes it,
actually, to somehow -- I mean everything that I am going to
say today breaks down for that example because of that.
I mean, we will shed more light on this a bit later with the
notion of simply connected regions and so on.
But for now let's just say if it is defined everywhere and it
satisfies this criterion then it is a gradient field.
If you ignore the technical condition, being a gradient
field means essentially the same thing as having this property.
That is what we need to check. Let's look at an example.
Well, one vector field that we have been looking at a lot was -
yi xj. Remember that was the vector
field that looked like a rotation at the unit speed.
I think last time we already decided that this guy should not
be allowed to be a gradient field and should not be
conservative because if we integrate on the unit circle
then we would get a positive answer.
But let's check that indeed it fails our test.
Well, let's call this M and let's call this guy N.
If you look at partial M, partial y, that is going to be
a negative one. If you take partial N,
partial x, that is going to be one.
These are not the same. So, indeed, this is not a
gradient field. Any questions about that?
Yes? Your question is if I have the
property M sub y equals N sub x only in a certain part of a
plane for some values of x and y,
can I conclude these things? And it is a gradient field in
that part of the plane and conservative and so on.
The answer for now is, in general, no.
And when we spend a bit more time on it, actually,
maybe I should move that up. Maybe we will talk about it
later this week instead of when I had planned.
There is a notion what it means for a region to be without
holes. Basically, if you have that
kind of property in a region that doesn't have any holes
inside it then things will work. The problem comes from a vector
field satisfying this criterion in a region but it has a hole in
it. Because what you don't know is
whether your potential is actually well-defined and takes
the same value when you move all around the hole.
It might come back to take a different value.
If you look carefully and think hard about the example in the
problem sets that is exactly what happens there.
Again, I will say more about that later.
For now we basically need our function to be,
I mean, I should still say if you have
this property for a vector field that is not quite defined
everywhere, you are more than welcome,
you know, you should probably still try
to look for a potential using methods that we will see.
But something might go wrong later.
You might end up with a potential that is not
well-defined. Let's do another example.
Let's say that I give you this vector field.
And this a here is a number. The question is for which value
of a is this going to be possibly a gradient?
If you have your flashcards then that is a good time to use
them to vote, assuming that the number is
small enough to be made with. Let's try to think about it.
We want to call this guy M. We want to call that guy N.
And we want to test M sub y versus N sub x.
I don't see anyone. I see people doing it with
their hands, and that works very well.
OK. The question is for which value
of a is this a gradient? I see various people with the
correct answer. OK.
That a strange answer. That is a good answer.
OK. The vote seems to be for a
equals eight. Let's see.
What if I take M sub y? That is going to be just ax.
And N sub x? That is 8x.
I would like a equals eight. By the way, when you set these
two equal to each other, they really have to be equal
everywhere. You don't want to somehow solve
for x or anything like that. You just want these
expressions, in terms of x and y, to be the same quantities.
I mean you cannot say if x equals z they are always equal.
Yeah, that is true. But that is not what we are
asking. Now we come to the next logical
question. Let's say that we have passed
the test. We have put a equals eight in
here. Now it should be a gradient
field. The question is how do we find
the potential? That becomes eight from now on.
The question is how do we find the function which has this as
gradient? One option is to try to guess.
Actually, quite often you will succeed that way.
But that is not a valid method on next week's test.
We are going to see two different systematic methods.
And you should be using one of these because guessing doesn't
always work. And, actually,
I can come up with examples where if you try to guess you
will surely fail. I can come up with trick ones,
but I don't want to put that on the test.
The next stage is finding the potential.
And let me just emphasize that we can only do that if step one
was successful. If we have a vector field that
cannot possibly be a gradient then we shouldn't try to look
for a potential. It is kind of obvious but is
probably worth pointing out. There are two methods.
The first method that we will see is computing line integrals.
Let's see how that works. Let's say that I take some path
that starts at the origin. Or, actually,
anywhere you want, but let's take the origin.
That is my favorite point. And let's go to a point with
coordinates (x1, y1).
And let's take my favorite curve and compute the line
integral of that field, you know, the work done along
the curve. Well, by the fundamental
theorem, that should be equal to the value of the potential at
the end point minus the value at the origin.
That means I can actually write f of (x1, y1) equals -- -- that
line integral plus the value at the origin.
And that is just a constant. We don't know what it is.
And, actually, we can choose what it is.
Because if you have a potential, say that you have
some potential function. And let's say that you add one
to it. It is still a potential
function. Adding one doesn't change the
gradient. You can even add 18 or any
number that you want. This is just going to be an
integration constant. It is the same thing as,
in one variable calculus, when you take the
anti-derivative of a function it is only defined up to adding the
constant. We have this integration
constant, but apart from that we know that we should be able to
get a potential from this. And this we can compute using
the definition of the line integral.
And we don't know what little f is, but we know what the vector
field is so we can compute that. Of course, to do the
calculation we probably don't want to use this kind of path.
I mean if that is your favorite path then that is fine,
but it is not very easy to compute the line integral along
this, especially since I didn't tell
you what the definition is. There are easier favorite paths
to have. For example,
you can go on a straight line from the origin to that point.
That would be slightly easier. But then there is one easier.
The easiest of all, probably, is to just go first
along the x-axis to (x1,0) and then go up parallel to the
y-axis. Why is that easy?
Well, that is because when we do the line integral it becomes
M dx N dy. And then, on each of these
pieces, one-half just goes away because x, y is constant.
Let's try to use that method in our example. Let's say that I want to go
along this path from the origin, first along the x-axis to
(x1,0) and then vertically to (x1, y1).
And so I want to compute for the line integral along that
curve. Let's say I want to do it for
this vector field. I want to find the potential
for this vector field. Let me copy it because I will
have to erase at some point. 4x squared plus 8xy and 3y
squared plus 4x squared. That will become the integral
of 4x squared plus 8 xy times dx plus 3y squared plus 4x squared
times dy. To evaluate on this broken
line, I will, of course, evaluate separately
on each of the two segments. I will start with this segment
that I will call c1 and then I will do this one that I will
call c2. On c1, how do I evaluate my
integral? Well, if I am on c1 then x
varies from zero to x1. Well, actually,
I don't know if x1 is positive or not so I shouldn't write
this. I really should say just x goes
from zero to x1. And what about y?
y is just 0. I will set y equal to zero and
also dy equal to zero. I get that the line integral on
c1 -- Well, a lot of stuff goes away.
The entire second term with dy goes away because dy is zero.
And, in the first term, 8xy goes away because y is zero
as well. I just have an integral of 4x
squared dx from zero to x1. By the way, now you see why I
have been using an x1 and a y1 for my point and not just x and
y. It is to avoid confusion.
I am using x and y as my integration variables and x1,
y1 as constants that are representing the end point of my
path. And so, if I integrate this,
I should get four-thirds x1 cubed.
That is the first part. Next I need to do the second
segment. If I am on c2,
y goes from zero to y1. And what about x?
x is constant equal to x1 so dx becomes just zero.
It is a constant. If I take the line integral of
c2, F dot dr then I will get the integral from zero to y1.
The entire first term with dx goes away and then I have 3y
squared plus 4x1 squared times dy.
That integrates to y cubed plus 4x1 squared y from zero to y1.
Or, if you prefer, that is y1 cubed plus 4x1
squared y1. Now that we have done both of
them we can just add them together, and that will give us
the formula for the potential. F of x1 and y1 is four-thirds
x1 cubed plus y1 cubed plus 4x1 squared y1 plus a constant.
That constant is just the integration constant that we had
from the beginning. Now you can drop the subscripts
if you prefer. You can just say f is
four-thirds x cubed plus y cubed plus 4x squared y plus constant.
And you can check. If you take the gradient of
this, you should get again this vector field over there.
Any questions about this method? Yes?
No. Well, it depends whether you
are just trying to find one potential or if you are trying
to find all the possible potentials.
If a problem just says find a potential then you don't have to
use the constant. This guy without the constant
is a valid potential. You just have others.
If your neighbor comes to you and say your answer must be
wrong because I got this plus 18, well, both answers are
correct. By the way.
Instead of going first along the x-axis vertically,
you could do it the other way around.
Of course, start along the y-axis and then horizontally.
That is the same level of difficulty.
You just exchange roles of x and y.
In some cases, it is actually even making more
sense maybe to go radially, start out from the origin to
your end point. But usually this setting is
easier just because each of these two guys were very easy to
compute. But somehow maybe if you
suspect that polar coordinates will be involved somehow in the
answer then maybe it makes sense to choose different paths.
Maybe a straight line is better. Now we have another method to
look at which is using anti-derivatives.
The goal is the same, still to find the potential
function. And you see that finding the
potential is really the multivariable analog of finding
the anti-derivative in the one variable.
Here we did it basically by hand by computing the integral.
The other thing you could try to say is, wait,
I already know how to take anti-derivatives.
Let's use that instead of computing integrals.
And it works but you have to be careful about how you do it.
Let's see how that works. Let's still do it with the same
example. We want to solve the equations.
We want a function such that f sub x is 4x squared plus 8xy and
f sub y is 3y squared plus 4x squared.
Let's just look at one of these at a time.
If we look at this one, well, we know how to solve this
because it is just telling us we have to integrate this with
respect to x. Well, let's call them one and
two because I will have to refer to them again.
Let's start with equation one and lets integrate with respect
to x. Well, it tells us that f should
be, what do I get when I integrate
this with respect to x, four-thirds x cubed plus,
when I integrate 8xy, y is just a constant,
so I will get 4x squared y. And that is not quite the end
to it because there is an integration constant.
And here, when I say there is an integration constant,
it just means the extra term does not depend on x.
That is what it means to be a constant in this setting.
But maybe my constant still depends on y so it is not
actually a true constant. A constant that depends on y is
not really a constant. It is actually a function of y.
The good news that we have is that this function normally
depends on x. We have made some progress.
We have part of the answer and we have simplified the problem.
If we have anything that looks like this, it will satisfy the
first condition. Now we need to look at the
second condition. We want f sub y to be that.
But we know what f is, so let's compute f sub y from
this. From this I get f sub y.
What do I get if I differentiate this with respect
to y? Well, I get zero plus 4x
squared plus the derivative of g.
I would like to match this with what I had.
If I match this with equation two then that will tell me what
the derivative of g should be. If we compare the two things
there, we get 4x squared plus g prime of y should be equal to 3y
squared by 4x squared. And, of course,
the 4x squares go away. That tells you g prime is 3y
squared. And that integrates to y cubed
plus constant. Now, this time the constant is
a true constant because g did not depend on anything other
than y. And the constant does not
depend on y so it is a real constant now.
Now we just plug this back into this guy.
Let's call him star. If we plug this into star,
we get f equals four-thirds x cubed plus 4x squared y plus y
cubed plus constant. I mean, of course,
again, now this constant is optional.
The advantage of this method is you don't have to write any
integrals. The small drawback is you have
to follow this procedure carefully.
By the way, one common pitfall that is tempting.
After you have done this, what is very tempting is to
just say, well, let's do the same with this
guy. Let's integrate this with
respect to y. You will get another expression
for f up to a constant that depends on x.
And then let's match them. Well, the difficulty is
matching is actually quite tricky because you don't know in
advance whether they will be the same expression.
It could be you could say let's just take the terms that are
here and missing there and combine the terms,
you know, take all the terms that appear in either one.
That is actually not a good way to do it,
because if I put sufficiently complicated trig functions in
there then you might not be able to see that two terms are the
same. Take an easy one.
Let's say that here I have one plus tangent square and here I
have a secan square then you might not actually notice that
there is a difference. But there is no difference.
Whatever. Anyway, I am saying do it this
way, don't do it any other way because there is a risk of
making a mistake otherwise. I mean, on the other hand,
you could start with integrating with respect to y
and then differentiate and match with respect to x.
But what I am saying is just take one of them,
integrate, get an answer that involves a
function of the other variable, then differentiate that answer
and compare and see what you get.
By the way, here, of course, after we simplified
there were only y's here. There were no x's.
And that is kind of good news. I mean, if you had had an x
here in this expression that would have told you that
something is going wrong. g is a function of y only.
If you get an x here, maybe you want to go back and
check whether it is really a gradient field.
Yes? Yes, this will work with
functions of more than two variables.
Both methods work with more than two variables.
We are going to see it in the case where more than two means
three. We are going to see that in two
or three weeks from now. I mean, basically starting at
the end of next week, we are going to do triple
integrals, line integrals in space and so on.
The format is first we do everything in two variables.
Then we will do three variables. And then what happens with more
than three will be left to your imagination.
Any other questions about either of these methods?
A quick poll. Who prefers the first method?
Who prefers the second method? Wow.
OK. Anyway, you will get to use
whichever one you want. And I would agree with you,
but the second method is slightly more effective in that
you are writing less stuff. You don't have to set up all
these line integrals. On the other hand,
it does require a little bit more attention.
Let's move on a bit. Let me start by actually doing
a small recap. We said we have various notions.
One is to say that the vector field is a gradient in a certain
region of a plane. And we have another notion
which is being conservative. It says that the line integral
is zero along any closed curve. Actually, let me introduce a
new piece of notation. To remind ourselves that we are
doing it along a closed curve, very often we put just a circle
for the integral to tell us this is a curve that closes on
itself. It ends where it started.
I mean it doesn't change anything concerning the
definition or how you compute it or anything.
It just reminds you that you are doing it on a closed curve.
It is actually useful for various physical applications.
And also, when you state theorems in that way,
it reminds you,oh.. I need to be on a closed curve
to do it. And so we have said these two
things are equivalent. Now we have a third thing which
is N sub x equals M sub y at every point.
Just to summarize the discussion.
We have said if we have a gradient field then we have
this. And the converse is true in
suitable regions. We have a converse if F is
defined in the entire plane. Or, as we will see soon,
in a simply connected region. I guess some of you cannot see
what I am writing here, but it doesn't matter because
you are not officially supposed to know it yet.
That will be next week. Anyway,
I said the fact that Nx equals My implies that we have a
gradient field and is only if a vector field is defined in the
entire plane or in a region that is called simply connected.
And more about that later. Now let me just introduce a
quantity that probably a lot of you have heard about in physics
that measures precisely fairly ought to be conservative.
That is called the curl of a vector field. For the definition we say that
the curl of F is the quantity N sub x - M sub y.
It is just replicating the information we had but in a way
that is a single quantity. In this new language,
the conditions that we had over there, this condition says curl
F equals zero. That is the new version of Nx
equals My. It measures failure of a vector
field to be conservative. The test for conservativeness
is that the curl of F should be zero.
I should probably tell you a little bit about what the curl
is, what it measures and what it does because that is something
that is probably useful. It is a very strange quantity
if you put it in that form. Yes?
I think it is the same as the physics one, but I haven't
checked the physics textbook. I believe it is the same.
Yes, I think it is the same as the physics one.
It is not the opposite this time.
Of course, in physics maybe you have seen curl in space.
We are going to see curl in space in two or three weeks.
Yes? Yes. Well, you can also use it.
If you fail this test then you know for sure that you are not
gradient field so you might as well do that.
If you satisfy the test but you are not defined everywhere then
there is still a bit of ambiguity and you don't know for
sure. OK.
Let's try to see a little bit what the curl measures.
Just to give you some intuition, let's first think
about a velocity field. The curl measures the rotation
component of a motion. If you want a fancy word,
it measures the vorticity of a motion.
It tells you how much twisting is taking place at a given
point. For example,
if I take a constant vector field where my fluid is just all
moving in the same direction where this is just constants
then, of course, the curl is zero.
Because if you take the partials you get zero.
And, indeed, that is not what you would call
swirling. There is no vortex in here.
Let's do another one where this is still nothing going on.
Let's say that I take the radial vector field where
everything just flows away from the origin.
That is f equals x, y. Well, if I take the curl,
I have to take partial over partial x of the second
component, which is y,
minus partial over partial y of the first component,
which is x. I will get zero.
And, indeed, if you think about what is
going on here, there is no rotation involved.
On the other hand, if you consider our favorite
rotation vector field -- -- negative y and x then this curl
is going to be N sub x minus M sub y,
one plus one equals two. That corresponds to the fact
that we are rotating. Actually, we are rotating at
unit angular speed. The curl actually measures
twice the angular speed of a rotation part of a motion at any
given point. Now, if you have an actual
motion, a more complicated field than
these then no matter where you are you can think of a motion as
a combination of translation effects,
maybe dilation effects, maybe rotation effects,
possibly other things like that. And what a curl will measure is
how intense the rotation effect is at that particular point.
I am not going to try to make a much more precise statement.
A precise statement is what a curl measures is really this
quantity up there. But the intuition you should
have is it measures how much rotation is taking place at any
given point. And, of course,
in a complicated motion you might have more rotation at some
point than at some others, which is why the curl will
depend on x and y. It is not just a constant
because how much you rotate depends on where you are.
If you are looking at actual wind velocities in weather
prediction then the regions with high curl tend to be hurricanes
or tornadoes or things like that.
They are not very pleasant things.
And the sign of a curl tells you whether you are going
clockwise or counterclockwise. Curl measures twice the angular
velocity of the rotation component of a velocity field.
Now, what about a force field? Because, after all,
how we got to this was coming from and trying to understand
forces and the work they do. So I should tell you what it
means for a force. Well, the curl of a force field
-- -- measures the torque exerted on a test object that
you put at any point. Remember, torque is the
rotational analog of the force. We had this analogy about
velocity versus angular velocity and mass versus moment of
inertia. And then, in that analogy,
force divided by the mass is what will cause acceleration,
which is the derivative of velocity.
Torque divided by moment of inertia is what will cause the
angular acceleration, namely the derivative of
angular velocity. Maybe I should write that down. Torque divided by moment of
inertia is going to be d over dt of angular velocity.
I leave it up to your physics teachers to decide what letters
to use for all these things. That is the analog of force
divided by mass equals acceleration,
which is d over dt of velocity. And so now you see if the curl
of a velocity field measure the angular velocity of its rotation
then, by this analogy,
the curl of a force field should measure the torque it
exerts on a mass per unit moment of inertia.
Concretely, if you imagine that you are putting something in
there, you know, if you are in a
velocity field the curl will tell you how fast your guy is
spinning at a given time. If you put something that
floats, for example, in your fluid,
something very light then it is going to start spinning.
And the curl of a velocity field tells you how fast it is
spinning at any given time up to a factor of two.
And the curl of a force field tells you how quickly the
angular velocity is going to increase or decrease.
OK. Well, next time we are going to
see Green's theorem which is actually going to tell us a lot
more about curl and failure of conservativeness.

Math for Eng.

The following content is
provided under a Creative Commons license. Your support will help
MIT OpenCourseWare continue to offer high quality
educational resources for free. To make a donation, or
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: Last week we talked
about the key components of a proof, propositions,
axioms, and logical deductions, and as you probably talked
about during recitation, we're not going
to worry too much about what axioms or logical
deductions that you use. Anything that is
reasonable, is fine by us. We're not going to ask you
to know what modus ponens is, or to label some law when
you make a logical deduction. Just, you know, be reasonable. Any facts you knew coming into
this course about mathematics, probably close enough
to use as an axiom. Want to make sure your axioms
are consistent, but that's OK. Now the exception to this would
be, is say we're on an exam, and we ask you to prove
some proposition, p. Well you can't say, I already
knew p, it's an axiom, check. That's not so good. We're asking you to
prove it from some more elementary facts. OK it's also don't want me
making wild leaps of faith, or saying it's obvious that,
unless it really is obvious. That kind of stuff can
get you in trouble. Much better to sort of explain
the reasoning in the proof. Now the proofs that we covered
last week in recitation, the problems set,
were all examples of what are called
direct proofs. You start with some axioms,
you have some theorems you knew before or you
proved along the way, and you make logical
deductions until you get to where you want
to go, the theorem. We're going to start today
with an indirect proof. For example, a proof
by contradiction. And this is a little
bit different. In a proof by contradiction,
you assume the opposite of what you're trying to prove. Then you just take steps
for logical deductions forward until you arrive
at a contradiction, something where you
prove false equals true. Now if you can ever
get to the point we approve something
is false and true, that means what you assumed
at the start had to be wrong. Namely, what you're trying
to prove has to be true. So let's write that
down, because it can be a little
confusing the first time. So to prove a
proposition p is true, you will assume that it's false. In other words,
that not p is true. And then you use that
hypothesis, namely the p is false, to derive a falsehood. In other words, you prove
a falsehood is true. And this is called
deriving a contradiction. And so it must be that,
in fact, p is not false, namely that it's true. Now this works because
if you can prove, if not p implies false is
true, well from last time, the only way this is a true
statement is if this is false, namely p is true. All right? So we can conclude
that p is true, if we can show that not
P implies a falsehood. Any questions about that? It's sort of a lot
of sort of notation, and until you've seen,
it can be confusing. So maybe we should
do an example. Let's prove that square
root of 2 is irrational. Is irrational. OK, everybody knows what
an irrational number is? That's something that
can't be expressed as the ratio of integers. OK, and probably
most people already know that-- how
many people have not seen a proof that square root
of 2 is a rational before? So most of you have seen
a proof of that, good. You know, if you try to do
a direct proof for this, it's pretty hard. How do you show there's
no way to represent the square root of 2 is
as integers a over b? But it's very easy, if we
do a proof by contradiction. Now when you're doing a
proof by contradiction, always start off by
saying, by contradiction. Write that down. And then what you
do next is you say, I'm going to assume, for the
purpose of contradiction, that p is false and
when not p is true. In this case, that would be
square root of 2 is rational. So in this case, here's
what we're trying to prove. That's p. I'm going to assume not p,
namely that square root of 2 is irrational number. Then I'm going to get a
contradiction or falsehood, and then I'm going to know
that p was true after all. All right, so let's see
where this leads us. Well, if square root
of 2 is rational then we can express it as
a over b, where a over b is a fraction in lowest terms. That means a and b have
no common divisors. And then I can
square both sides, and I get two is a
squared over b squared. Then I multiply by
b squared, and I get 2b squared equals a squared. And what does that
imply about a? What can you tell me
about a if it equals-- if a squared is 2b squared? Anything special about a? Could a be anything? A squared is even, all right? Because 2b squared is an even
number, so a squared is even, and what does that mean about a? A is even. B squared is even,
then a is even. So a is even. So we could write
that as two divides a. That's the divide symbol. You'll see a lot
of that next week. All right if a is even, what
do I know about a squared? I know more than
just, it's even. What is it? It's multiple of 4, yeah. So that means that
four divides a squared. A squared is 2b
squared, so that means that four divides 2b squared. Divide each side by 2
means 2 divides b squared. What does that imply about b? B is even. All right, b is even, good. Well, I've got a is
even and b is even. I got a contradiction
here, somewhere? Yeah a over b was not a
fraction in lowest terms, because both and b are even. All right, so that implies a
over b is not in lowest terms. And that is a contradiction. Now you'll see that
written lots of ways. One is, you can say
a contradiction, sometimes you'll see just
this sharp symbol written, and that means you've
got to a contradiction. Because here, we had it
being in lowest terms, and here we have it
not in lowest terms. You can't have both
at the same time, so you got a contradiction. And that means we've now proved
that this assumption was wrong. Square root of 2
is not rational, so it must be irrational. and then we put a little
box here at the end, or sometimes you'll see
a check, sometimes you'll see QED, that says
the proof's done now. It's over. Any questions about that proof? We're going to do a lot of
proofs by contradiction. Actually, there's an interesting
story behind this proof. As far as we know, it was first
discovered by the Pythagoreans, way back when in ancient Greece. And the Pythagoreans
were a religious society started by Pythagoras, of
Pythagoras theorem frame. Now it sounds weird today, but
back then, in ancient Greece, math was a religion, all right? Every once in while you'll
see somebody around MIT, and you'll think he must
think math is a religion, but back then it really was,
and it was ruled by God, because this is ancient Greece. And there were two key gods
in this religion, Apeiron and Peros. Now Apeiron was the bad god,
and he was the god of infinity, because infinity was
considered all that was bad. And I don't think we'll do a
lot with infinity this term, but if we do you'll appreciate
why that's the case. And Peros was a good god. He was the god of
the finite world, and represented everything that
was good to the ancient Greeks. Now one of their main
axioms, or beliefs, was that there were
no irrational numbers. They just didn't exist. Now the reason is, they didn't
like irrational numbers. They were bad. Because, well they're infinite. You can't write
them as a decimal without repeating forever, and
you know, just an infinite sort of decimal representation. They liked rational numbers,
you know, like one seventh? That's a good number,
because you can write it as 0.142857 repeating. So rational numbers
are finite, in that you can always find a repeating
pattern of finite length. Irrational numbers are not. They're infinite in that
sense of the ancient Greeks. So they said there were
no irrational numbers. That was an early axiom. Now they also had
an axiom that said that every length of a line
was finite, therefore rational. All right, you know
the ancient Greeks were good with a compass,
and drawing lines with straight edges and stuff. So they said any line
that you can construct has a finite length
to it, so therefore it has a rational length. That was axiom number two. Now they were good geometers,
so they knew, of course, they developed the
Pythagorean theorem. But if you took a triangle,
and you have side lengths one, the length of the hypotenuse
was square root 2. Therefore they had
a theorem that said square root 2 was rational. Because you've got the
2 axioms there, right? Now eventually, they
discovered a proof like that, that it wasn't rational. It was irrational. This caused quite a stir. First it meant that their
axioms were inconsistent. Every theorem they
proved was now suspect, once you have
inconsistent axioms. Even worse, the devil
is in their midst. Square root 2 is
infinite is the bad god, and this is the most basic
length they had, besides one. So that's a very bad thing. Sort of like, you
know, today we were to wake up and discover
that there were only nine commandments, and the 10th was
planted there by the devil. And you're not sure
which one, maybe. That would be a big
mess, sacrilege. Well, so what were they to do? This is a disaster
of major proportions. So they covered it up, and
they denied the result. So they didn't want
to publish that proof. So they kept on saying
square root 2 was rational. But then, according to legend
there was a Deep Throat. Somebody who let out the word
and the proof the square root 2 is irrational, because that
would be very destabilizing for the society, and
so they killed them. This is the legend. Now, hard to imagine
getting killed over the irrationality of 2. All right, we're
going to do a lot more of these kind of
proofs in homework and throughout the term. The next proof I
want to show you is one of my favorite
proof techniques. One of my favorite proofs. And that is a false proof. And we're going to see a lot
of these during the term, too. And if we could bring
the screens down. Somebody back there to-- yeah. Great. Bring the screens
down, I'm going to-- and then turn this on for me. So I'm going to prove to you
that 90 is bigger than 92. All right? And that hopefully,
is not really true. But, you know, watch this
proof and see if there's any problems with it. All right, the
proof, by PowerPoint. Right away you
should be suspicious. I'm going to take two
triangles with total area 90 and put them together,
and then divide them up into four triangles with
area greater than 92. And by the conservation
of area axiom, which you want to be maybe
thinking about, this will imply an area of 90
is larger than an area of 92. And therefore that
90 is bigger than 92. All right, those
are my triangles. They are right
triangles, 9 by 10. If I put them together, I
get a 9 by 10 rectangle. Of course it has
area 90, 9 times 10. Now I'm going to slide these
triangles across their diagonal so that I get, instead
of having 10 on the side, I'm going to slide it
so I get a 2 and an 8. OK I had 10 there
before, now it's 2 and 8. And then you see
I've got this-- I'm going to cut off
along the dotted line, and as you can see
that dotted line, yeah but won't compute
it exactly, but it's a little bit bigger than 2. All right? It's a little bit longer
than that 2 there, you could see that. All right, so I'll call
it 2 plus, bigger than 2. Now I slice off along
the dotted line, and now I'm going to put
those two triangles together, and I create a rectangle. Two by two-- little
more than two. Now the area of the
little rectangle is bigger than 4, cause I got 2
times something bigger than 2, and you can see here I
have the 8 left over, and I have 9 plus 2 plus,
means it's a little bit bigger than 11 by 8. I got area bigger than 88,
add them up, get area bigger than 92. So I started with 90 and
I created more than 92. All right, what's
the problem here? This would be good
if I could do it. I'd get some gold,
you know, bars of gold and cut them up, and do the
game, and I make more gold. That would be pretty
good if I could do that. Because of the
conservation of area axiom? Did I assume something
there that was too powerful that if I manipulate the
area of rectangles like this, that the area needs
to be the same? Yeah? My bigger ones aren't closed. Well, let's see, I don't know. Let's go back and see I
made those bigger ones. All right, I got my
triangles, right? I'm just chopping along the line
there, and I got 2 plus and 9. Those are rectangles. They look like rectangles. Yeah. AUDIENCE: [INAUDIBLE] PROFESSOR: Well it looks
bigger than 2, doesn't it? Yeah, 2 pluses. Well bigger than 2 is bigger
than 2, but, yeah, maybe that should have been a 2 minus. Would that change anything? Yeah, if that had
been a 2 minus, then we'd have area less than 4,
area less than 88, and then 90 would be less than 92. That would be OK. But it sure as heck looks
bigger than-- the 2 plus looks bigger than the 2 doesn't it? I don't know, do they
got distortion out there? I guarantee you, if I
measured on my screen, that 2 plus is bigger than
the length of the tube. I guarantee you. We can to take a
ruler on my screen, and the 2 plus will be longer
on the ruler than the 2. That I guarantee you, but
you're on the right track. Yeah Good, all right,
now how did you-- but it is true, if I measured
on my screen, it's bigger. Yeah. AUDIENCE: Are the triangles,
like, drawn to scale? It's like, on the SAT they
always say, not to scale, right? PROFESSOR: Yeah. Yeah, they're not
drawn to scale. In fact, if I go back to the
beginning, it says 9 by 10. But in fact, if I measure the
nine, it's bigger than the 10. All right? So this is one of the big
problems of proofs by picture. Because look at it,
you're not thinking which is bigger, 9 or 10 up there? 10's bigger even though
it's not, it's shorter. But when you get
down into the proof, well, clearly the 2 plus
was bigger than the 2, because it looks that way. It is, on the paper. All right? In fact if you go on a
computer, you're probably right, it's 1.8, not 2 plus. But that's how the error
crept in and how it was drawn, and then you're going
along with a proof, and everything
else is just fine. So the mistake is
right up front. I drew it wrong. OK. And this happens,
you're doing graphs-- we'll talk about graphs here
in a couple weeks-- over and over again, people do this. They draw it, it looks
like this, you accept that and then you're dead. Everything else,
there's no hope. Everybody clear what went
wrong in this picture? How we got off track? This is sort of a nasty one. OK. Now one of the nice
things about proofs is that when there is a
bug, if you really write it out step by step,
and there's a bug, you can go back and find it. And so this, I
didn't sort of really write it out very
well step by step, and it was harder to find. Now, all right, so we
can pull the screens up. Thanks. OK, so for the rest of today
and the rest of this week, we're going to talk about
a different kind of proof technique, which is induction. Now, induction is by far the
most powerful and commonly used proof technique in
computer science. If there's one thing
you should know by the time you're
done with this class, it's how to do a
proof by induction. In fact, if there's
one thing you will know by the time
we're done with this class, is how to do a
proof by induction. And in some sense, when we
get to grading the final, how we measure ourselves
as instructors, the first test is,
can you do the proof by induction question,
and do a good proof there? You will see it on the
midterm and the final, probably in multiple instances. Now just to make sure you
become intimately familiar with induction, we're going
to do over a dozen proofs with induction in class,
and many more in homework over the next five to six weeks. You'll probably be
dreaming about induction, if we're successful here. Soon. The good news is that
induction is very easy. Once you get your mind
around it, get some practice, it really is not a hard thing. In fact, induction is
really just an axiom. So let me state it. Let P(n) be a predicate. If P(0) is true, and for all
natural numbers n, p of n implies p of n plus 1 is true. So here I'm saying that
this is true for all n. OK. Then for all n, natural
numbers, P of n is true. I had another way of
saying this without the for alls there, is
that if P(0) is true, if P(0) implies P(1) is true,
if P(1) implies P(2) is true, and so on, forever, then p
of n is true for all that. So then, P(0), P(1),
P(2), forever, are true. OK. Now you can sort of see why
this is a reasonable axiom, because if P(0) is true,
and P(0) implies P(1), then by that modus ponens thing or
one of the logical deductions, we know P(1) is true. And if P(1) is true, and
P(1) implies P(2) is true, then we know by
the same reasoning, P(2) is true, and
so on, forever. Now the reason it becomes an
axiom is that and so on forever bit is part of it. That's why we need
it as an axiom. You could sort of view this
as a series of dominoes. You know, I got a
domino for each n, and each domino knocks over
the next in terms of truth, knocking over corresponds
to the truth here. And if I know P(0) is true,
that means I knock over P(0), P(0) implies P(1) means
P(1) one goes down. P(1) implies P(2) means P(2)
goes down, and so forth. Pretty basic. Raise your hand if you think you
don't have a lot of experience with induction. All right, yeah,
that's pretty typical. About a third to a half of you. So we're going to change that. Let's do a simple
proof using induction. So let's prove that for all
n bigger than or equal to 0, again natural numbers, that
1, plus 2, plus 3, plus n equals n times n plus 1 over 2. This is actually a
useful thing to remember. We're going to use this
all term, this identity. Now the first thing,
before we prove it, I want to make sure we
understand this dot, dot, dot, notation. Because it is the source
of a lot of errors. What it means is that you need
to fill in the pattern here, which is vague. What it means in this case,
you fill in four, five, six all the way up to n
minus 1, and then n. That's what it means. Figure out the pattern
and fill it in. Pretty risky thing. Now in this case,
because that's so vague, there's other terminology
we use for this. For example, we would
use a big sigma, capital sigma i
equals 0 to n of i. That means the sum of i,
where i is the integers from 0 to n inclusive. Actually, let me put 1 here. Another way to write this--
these are all equivalent ways to write it-- is you
could put 1 less than or equal to i, less
than or equal to n of i. So you could put something i
over the range, from one to n, or you can write it on
the bottom if you want. So these are four
different ways of writing the same thing, the sum of the
natural numbers from 1 to n. And we'll use them
all during the term. All right, now there's
some special cases that make this a little
more interesting. What if n equals 1? I've got 1 plus 2 plus
dot, dot, dot, plus n. What do you suppose it
equals if n equals 1? 1, because we're summing
the numbers from 1 to 1. That's just 1, the number 1. There is no 2. And there aren't
two copies of 1. So this notation
is very ambiguous. You'll see a 2 here in the sum. If n is 1, you'll see
a 1 here and a 1 here. So you've got to be careful. I guarantee you, you'll
make a mistake with this. In fact I'm going to show
you another false proof later where this comes into play. What about if n is less
than or equal to zero? What is it then? Any thoughts? 0. There are no integers to sum,
no 1, no 2, no n, because you never get started because--
sorry, n is less than or equal to 0-- because
you're summing from 1 to 0, 0 is below. You never-- it doesn't
include anything. So these are the conventions
to keep in mind with the edge cases here. All right, it's easy enough
to check that the theorem is true for certain values of n. For example, if n equals 4,
I've got 1 plus 2 plus 3 plus 4. That's 10. And 10 equals 4 times 5 over
2, plugging in the formula. So for any value of n you could
check this formula is true. Proving is true for all n. It takes a little more effort
unless you use induction. So let's do that. So we'll prove the theorem. Now, whenever you're using
a proof by induction, first thing you do is you
write down by induction so we know what
you're going to do. And the next thing
you need to do is figure out, what's
your predicate. What's your
inductive hypothesis? What's p? So usually p will
be the thing you're trying to prove, namely
that 1 plus 2 plus 3 up to n is n times n plus 1 over 2. And you state that. You say let p of n be the
proposition, the predicate, that the sum i equals 1
to n of i equals n times n plus 1 over 2. And once you've got
that established, now we're going to go
verify that p 0 is true and that p n implies p n plus 1. So we always have
to write this down. The next thing to do is to check
what's called the base case, p zero. So let's do that. So we write down base case. Some people call
it the basis step. And we have to check
that p 0 is true. Well, what's the sum of
i equals 1 to 0 of i? 0. There are no terms in this sum. And if I look over there,
n times n plus 1 over 2. If n is 0, it equals 0. So we're done with
the base case. We've now proved
that p0 is true. And the second part is
called the inductive step. And here we have to show, for
n greater than or equal to 0, we need to show that pn
implies pn plus 1 is true. Now how do we show an
implication is true? How do I show this is true? What am I going to do to
show that's true in general? Yeah? AUDIENCE: [INAUDIBLE] PROFESSOR: Right. Because an implication is true
in every case, except for true implies false. So if pn is false, we're done. This implication is true. The only case we have to
worry about is p of n is true. So we assume p of n is true. And now we confirm that that
means pn plus 1 is true. So we write that down. Assume pn is true. And you might also write down,
"for purposes of induction" or "purposes of it verifying
the inductive hypothesis." just to let us know why
you're assuming it. In other words,
you're not assuming pn is true for purposes
of contradiction. You're assuming it for
purposes of induction. All right, let's do that. That means, in this case, i.e. we assume 1 plus 2 plus up to n
equals n times n plus 1 over 2. And we need to show that
the n plus 1 case is true, and the 1 plus 2 plus n plus
1 equals n plus 1 times n plus 2 over 2. It's sort of weird,
and this does confuse people that aren't
familiar yet with induction. It looks like we just assumed
what we're trying to prove. We're trying to prove
this is true for all n. And we just assumed it. But we're assuming it in
the context of establishing this implication is true. And then we apply the
induction axiom to conclude pn is true for all n. All right, well, let's
rewrite this as 1 plus 2 plus n plus n plus 1. Because I've assumed
pn, I can rewrite this as an n plus 1 over 2. And now plus n plus 1 out here. That equals, well, I got
n squared plus n, here, over 2, plus 2 n plus 2. And that equals n plus
one times n plus 2 over 2, which is what we're
trying to show. So we've completed,
now, the inductive step. We have shown that for all n,
pn implies pn plus one for all n greater than or equal to 0. Any questions about that? So, the proof is done. We've done everything we
need to imply induction. We've got p0 is true. And pn implies pn plus 1 for n
bigger than or equal to zero. Now induction helped
us prove the theorem. Did it help us understand
why the theorem is true? Do you have any feel
for why the theorem is true after seeing the proof? Not really. I don't think--
sometimes induction will give you an understanding. Sometimes it won't. Here you've got no understanding
of why the theorem's true, which is sort of unfortunate. Did induction help you figure
out the answer to the sum? Namely, say you were
trying to derive this answer from this sum. Did induction give
you the answer? No. You had to know the answer,
namely this, in order to prove it was true. Now later we'll see examples
where induction actually can give you the answer,
but often it does not. Often, induction gives
you no hints, no answer, just prove that it's right once
you had the clever idea that, oh, maybe that's the answer. You'll see that with things
like the beaver flu problem. Figuring out the inductive
hypothesis or the answer, you know, the details
of it is hard. But once you do it, then
applying the induction, not so hard. It gives you a concrete proof. OK, let's do another one. In fact, we're just going
to spend the rest of today doing induction proofs. So for all natural numbers
n, 3 divides n cubed minus n. Means than n cubed minus
n is a multiple of three. For example, n is 5. 3 divides 125 minus
5, because that's 120. Let's prove that. And we're going
to use induction. What do you suppose
pn's going to be? What's my predicate or my
inductive hypothesis here? Any thoughts? Yeah? PROFESSOR: [INAUDIBLE] PROFESSOR: Yeah, that's you. Go ahead. AUDIENCE: [INAUDIBLE] PROFESSOR: Yeah. First thing you
always want to try is you assume that is
your induction hypothesis. So we say, let pn be 3
divides n cubed minus n. What's the next thing
we do in our proof? Base case. Always easy to forget,
but not a good idea. Base case is n equals 0. And sure enough, 3
divides 0 minus 0. So that's done. What's the next step we do? What is it? Next step? Inductive step. And for that we're going to
need to show for n bigger than or equal to 0, we want
to show pn implies pn plus 1 is true. So to do that we
assume pn is true. In other words, we assume that
3 divides n cubed minus n. And we're trying to show that
3 divides n plus 1 cubed minus n plus 1. So we take a look at, we examine
n plus 1 cubed minus n plus 1. And we want to show it's
a multiple of three. Well, let's expand that out. This is n cubed plus 3n
squared, plus 3n plus 1. And I subtract off n plus 1. So I get n cubed plus
3n squared plus 2n. Is this a multiple of 3? I need to show that's a multiple
of 3 and then I'd be all done. Is it a multiple of 3? Beats me. It doesn't look like a
multiple of 3, necessarily. So maybe we need to
massage it a little bit. I do know that this
is a multiple of 3. I can use that fact. And in proofs by
induction you always want to-- if you're not
making use of that fact, then you're not really
making use of induction. Sort of a warning sign. So I want to use this
fact to prove this. Well, let's get a
minus n in here. This equals n cubed minus
n plus 3n squared plus-- if I subtracted an n I
got to add an n-- plus 3n. So I've rewritten it. Now is it clear? Now it's clear. Very simple, because
3 divide this by pn. 3 divides that. And 3 divides that. So this is a multiple
of 3, because I've got 3 divides 3n squared, 3
divides 3n, and 3 divides n cubed minus n by pn. Or you could say, by the
inductive hypothesis. pn is the inductive hypothesis,
another name for it. So therefore, 3 divides that,
which means 3 divides this. So that means 3 divides n
plus 1 cubed minus n plus 1. And we are done with
the proof by induction. Any questions about that one? So the key steps in induction
are always the same. You write down
"proof by induction." You identify your predicate. You do the base case,
usually n equals 0, but it could be something else. And then you do
your inductive step. Now in general, you could
start your induction-- you don't have to start it
at 0, you could start it at some value b, some integer b. Let's take a look at that. So you could have
for the base case, you could have p of b
is true, not p of 0. And then for your
induction step you would have for all n bigger
than or equal to b, pn implies pn plus 1. And then your conclusion is
that for all n bigger than or equal to b, pn is true. So inductions don't
always have to start at 0. You can pick where you start. Just make sure that you
verify the starting point and you verify the
implication for all n at that starting
point and beyond. All right, let's now do a
false proof using induction. We're going to prove that all
horses are the same color. So let's go through the process. So the proof, we write
down "by induction." Now we need our induction
hypothesis, the predicate. So we're going to let that
be-- we can't let it be this. Why can't this be our predicate? All horses are the same color. What's wrong with making that
be the induction hypothesis? You can't plug anything into it. There's no number here. I've got to have a
number, n, to induct on. So what I'm going to say is
that in any set of n horses-- and let's make n bigger
than or equal to 1-- the horses are all
the same color. All right, now if I prove
this is true for all n, well then all horses
are the same color. Because in any set of n horses,
they're all the same color. So all horses must
be the same color. What's the next thing I do? What's the next step? Base case. Now, what am I going to
use as my base case here? One horse, OK, or n equals 1. So p of 1. That would say that any set
of one horses the horses are all the same color. That's true. I've got one horse. It's the same color as itself. So that's easy. It's true since just one horse. All right, what's the
next step of the proof? What's the next thing I do? Inductive step. So I'm going to assume that pn
is true to prove pn-- and show pn plus 1 is true. All right, so I'm
going to assume that in any set of
n horses, the horses are all the same color
that I start with. And now I look at a
set of n plus 1 horses. So we consider a set
of n plus 1 horses. And let's call those
horses h1, h2, hn plus 1. What do I know about
horses h1 to hn? They're the same color,
because pn is true. There are a set of n horses. By p1 they're all
the same color. Also, what do I know but
h2, h3 and hn plus 1? All the same color, because
they're a set of n horses. All right. Well, since the
color of h1 equals the color of those
guys, h2 to hn, I know h1 is the same
color as these guys. I also know that hn plus 1 is
the same color as those guys. That means that h1 is the
same color as hn plus 1. And all n plus 1
are the same color. And that's pn plus 1. That implies pn plus 1. And I'm done. Now a few years ago we assigned
this problem as homework. And we asked students
to figure out what went wrong with the problem. Why doesn't this work? And the responses were
a little discouraging. Half the class responded,
effectively as follows, this example just goes to show
that induction doesn't always work. A third of the
class said, I always knew that you can't
trust mathematics. This example just proves it. That really hurt. And most of the rest were
something similar to that. Not exactly what we were
looking for in the homework. So now we don't
leave it to homework. We do it in class. What's the flaw here? What was it? AUDIENCE: P of n. PROFESSOR: P of n? What's wrong with p of n? AUDIENCE: [INAUDIBLE] PROFESSOR: No, this
is a real assumption in any set of n horses--
depends on n-- the horses are all the same color. That is a proposition. Because it depends on
n, it's a predicate. It's true or it's false. Now we know it's false, because
you could get a set of a couple horses with different color. But it is a proposition. Yeah, way back there? AUDIENCE: [INAUDIBLE] for
like a certain set of horses. So even though it's the
same number of horses, so it was the same number
as a different set of horses it's not something you
can assume anymore. PROFESSOR: That's a great point. I did gloss over something
here, because in the predicate I've got "in any set." So there's really a "for
all sets" sitting out here. So I've going to
be careful that I establish that when I'm trying
to prove pn implies pn plus 1. So to establish
pn plus 1 here, I assume pn which means
in any set of n horses they're all the same color. That I'm given. That's pn. I've got to look at any
set of n plus 1 horses. So consider any set, not a set,
any set of n plus 1 horses. So you pick any set you want. Call them this, h1
through hn plus 1. Well then, the first n are a set
of n horses, so I can apply pn, therefore they have
the same color. The last n horses in the
set are a set of n horses, so I can imply pn. So these guys all have the same
color, and I'm on my merry way here. So you're right, I had to
do a little bit more work, but I can do that work
and I still get a proof. Yeah? AUDIENCE: [INAUDIBLE] PROFESSOR: Good question. Is there a problem
with the base case? So my base case was
here, n equals 1. In any set of one horse, the
horses-- let's say in the set just to be really careful--
are all the same color. No, in any set of one horses
there's only one horse, so it's the same
color as itself. Yeah? AUDIENCE: [INAUDIBLE] PROFESSOR: n plus 1 is
not the same as n, yeah. AUDIENCE: [INAUDIBLE]. So you can't have
the same assumption because it's not
a set of n horses. It's a set of n plus 1? PROFESSOR: Well, let's see. So you're arguing I made
a mistake here, right? Well, I've got horses--
horse 2, 3, up to n plus 1. How many horses are
in this set here? AUDIENCE: Oh, in that set? PROFESSOR: In this set. How many horses are there? AUDIENCE: N. PROFESSOR: N. So I can apply p
of n to this set just the same as that set, because p of
n is any set of n horses. Well, I'm picking this one
now and applying pn to it. And so therefore they're
all the same color. Yeah? AUDIENCE: H1, h2, dot, dot, dot. PROFESSOR: Yes. AUDIENCE: Because it does
work it there's two or more, but you didn't prove it with
just one-- or from one to two. PROFESSOR: Exactly. Remember I told you
that dot, dot, dot is so reasonable,
so easy to use. Everybody uses it. It was going to catch us up. The bug is in the dot, dot, dot. And in particular, it has to
do with the case n equal 2. So there's two ways
to look at the bug, dot, dot, dot, or
we didn't completely do all the inductive steps. So let's look at the case--
actually it's the case n equals 1, here, in
this inductive step. Did I prove p1 implies p2? Let's just double check
that worked for n equals 1. n equals 1. I've got a set of two horses. This becomes 2. So I've got h1, h2. Nothing in the dot, dot, dot. It's just h1 and h2. Then this becomes h1. So sure enough, h1 is
the same color as itself. This becomes-- what does this
become, this set of horses? What is it really? h2. All right, so I've
got the color of h1 equals the color
of-- oh man, this is so hard to see this bug. I wrote down h2 dot, dot,
dot, to hn because-- take out h1, what's left? What's left is h2 through hn. But this set is only h1. What's really left here? What is this set? What is this set? h2, h1? No. You see, what is the whole
set? h1, dot, dot, dot, hn. What is the whole thing? It's just h1. pull out h1 and look
at the rest of it, how many horses are here? 0 horses. This is the empty set. There are no horses
here to compare to. Even though I got an h2, I got
an hn, I got a dot, dot, dot. Because generally,
if n is big, this has n minus 1 horses are here. There's n total. I got n minus 1 right here. But n minus 1 is 0. There are no horses here. And so this bridge in the
equality totally breaks. I got color of h1 equals--
there's no information here-- equals the color of h2. There's no equality
here, because there's no horses in this set, all
because of that dot, dot, dot. Do you see where the problem
is by using the dot, dot, dot? For the case n equals-- it was
true for every other case of n. n equals 2, n equals
3, it's all true. In fact, what we proved, we
proved the base case of p1 is true. This is an argument
that p2 implies p3. That is true. p3 implies p4, that is true. And so forth. We proved for all n
bigger than or equal to 2, pn implies pn plus 1. That we proved. What is the one missing
implication we did not prove? The missing link, p1 implies p2. We did not prove that. Now is that true? No. You can find a set of two
horses are not the same color. And just because every horse
is the same color as itself, does not give you that. That was missing in this proof. And so we have to
be really careful when you're doing these
proofs that you establish the inductive step
for all n bigger than or equal to the base case. And then make sure,
if you're going to use this really convenient
dot dot dot notation, that you don't wind up here saying, oh
this is horses h2 through hn when there's no horses
there, because n is 1. Any questions about this? Yeah? AUDIENCE: [INAUDIBLE] PROFESSOR: Great question. All right, let's fix the proof. Start with the base
case of p of 2, and now I've got all this done. So therefore, that's
another proof. Yeah? Say it again. AUDIENCE: [INAUDIBLE] PROFESSOR: This
is still the same. In any set of n bigger than
or equal to two horses, all the horses in the
set are the same color. That's what he saying. And he's saying, hey look,
the proof worked here. The inductive step is just fine. p2 does imply p3. Yeah? AUDIENCE: The base
case for 2 isn't true. PROFESSOR: That's right. The base case fails. That's why you've always
got to check the base case. Yeah? AUDIENCE: What it does prove is
that if you find any two horse and they're always going
to be the same color, then all horses have
to be the same color. PROFESSOR: That's correct. That's a great point. We have given a proof that if
you look at any pair of horses and they're the same color, then
all horses are the same color. That's true. That is true. Of course, there
are pairs of horses that aren't the same color. So the base case would fail. So always check the base case. You could prove some
great stuff if you don't check the base case. All right, any other
questions about that? Yeah? AUDIENCE: Negative
number [INAUDIBLE]? PROFESSOR: Yeah. As long as it's an integer. And as long as you
prove pn implies pn plus 1 starting
there all the way out. Yeah, you can start at
negative numbers if you want. Usually there's not many
cases where that comes up you want to, but you can. Nothing wrong with that. Any other questions? Yeah, you can see why it was
a messy homework problem. So far we've seen
examples of how induction is useful in proving the
hypothesis is true, but not in solving the problem, per se. Or even figuring out what
the hypothesis should be. Now in the last
example, we're going to show you how
induction can be used to prove there is a
solution to a problem, and also how to
find the solution. So it's actually going to be a
very useful, constructive thing in this case. Now this problem arose
in the construction of the status center, the
building we're in now. This whole building was
originally supposed to cost, completely furnished,
under $100 million. That was the goal. But the first mistake they
made was the first step, was hiring the architect. They hired Frank Gehry. I think MIT is now in a
lawsuit with Frank Gehry. So he was the architect. And costs just went nuts. As you could imagine,
all these slanted walls and crazy things happening
actually are expensive. And the cost quickly got
over $300 million, literally. That that's parts true. Now I'm going to
fabricate a little bit. Now actually, fund raising
became a huge priority once they're more
than $200 million over budget they haven't even
bought the furniture yet. So some pretty radical
ideas were proposed. And one of them was to build
a large 2 to the n by 2 to the n courtyard
and put a statue of a wealthy, potential donor
in the center of the courtyard. So let's draw this. So the courtyard--
and of course, you know, it's computer science,
so it has to be a power of 2. So it's 2 to the
n by 2 to the n. And I've drawn here
the case n equals 2. And we've got to get the
statue of the wealthy guy in the middle. And I'm not supposed
to reveal his name. So we're just going
to call him Bill. So Bill's got to
go in the center. Now this would be fine,
except to that nothing was easy with Frank
Gehry, everything was some weird,
weird thing going on. And he insisted on
using l-shaped tiles for the courtyard. So the tiles that we're going
to use looked like this. So it's almost a two
by two, except you're missing that piece. So what you need to
figure out how to do is tie all this courtyard
perfectly leaving one spot for Bill
using tiles like this, these l-shaped tiles. And this is 2 by 2 here. So that's the task. So let's see if we can do
that for n equals 2 here. Let's see, we can
do a tile here. We can do a tile here. A tile here. A tile here. And a tile there. So we can. In this case we can tile
the courtyard perfectly using these L shaped
tiles, leaving one square in the Center for the statue. All right, everyone understand
what we're trying to do? The goal? Now I want to do it for n. So let's start proving
it by induction, even though we don't
know how to do it yet. Because we're going to see
how induction's going to help us show it's possible and maybe
even show us how to do it. So let's state a theorem. For all n there exists away
to tile a 2 to the n by 2 to the n region, or courtyard,
with a center square missing for Bill. And the proof will
be by induction. And our induction
hypothesis, the predicate, pn is going to be what
we're trying to prove. So this is the
induction hypothesis. Almost always when
you do the induction you want to start out with
that as your hypothesis. What's the next step? Base case. Never ever forget the
base case or you'll be thinking all horses
are the same color. p0. Well, the courtyard for n
equals 0 is just 1 square. And that's for Bill. So you're done. There's no tiles at
all to worry about. That's easy. So that's true. And then we do the
inductive step. So inductive step. For n bigger than
or equal to 0, got to remember to keep
track of that now, we assume pn to verify
the inductive hypothesis, or to prove pn plus 1. A lot of ways to
write this down, but you always want to say
what you're assuming and why. So we need to show
pn plus 1 is true. Well, so let's look at a 2 to
the n by-- 2 to the n plus 1 by 2 to the n plus 1 courtyard. So let's draw it out here. 2 to the n plus 1 by
2 to the n plus 1. What are we going to do to
use our inductive hypothesis? Yeah? AUDIENCE: [INAUDIBLE] PROFESSOR: For the-- yeah. So you're on a good
track there, for sure. But I've got to apply--
I'm not going from 1 to 2, I want to get-- I
want to use pn here. So I've got a 2 to the n plus 1
by 2 to the n plus 1 courtyard. How do I use pn? 2 to the n by 2 to
the n courtyard. Yeah? AUDIENCE: Oh, never mind. I don't think it works. I was about to say that
would be as if 2 to the n would divide that
into four blocks. PROFESSOR: Good idea, yeah. Let's divide our courtyard
into four blocks. That's a great idea. And now each of these is
2 to the n by 2 to the n. Right? And I can apply the
inductive hypothesis there. AUDIENCE: [INAUDIBLE] PROFESSOR: Yeah. Hm. Yeah, something--
yeah, it doesn't quite work because Bill
wants to be here. Got a little square
for Bill there. But I can't use my
inductive hypothesis to tile this, because there's
no square missing. In fact, even if there wasn't a
square missing, I'm in trouble. If I've got-- say this
is size 4 by 4 here. Can I tile a 4 by 4 region
with L shaped tiles? No, they're size 3. 3 doesn't divide into 16. Yeah? AUDIENCE: [INAUDIBLE]
There's one tile missing from each of those blocks at
the corner that [INAUDIBLE]. PROFESSOR: There's a great idea. All right, we're
making progress now. Take a corner out
of each of them. Put my l-shaped tile here. Now I can use the
inductive hypothesis to tile each one of these. Yeah? Yeah? AUDIENCE: Well, why can't
you put a 4 in the center and then you have a
bunch of 2s on the side? PROFESSOR: A 4 in the center-- AUDIENCE: Like a 2
by 2 in the center. PROFESSOR: I got that. I got Bill and the tile. AUDIENCE: No, but
like make it bigger. Not just like 4 single
tiles, but like-- so you have something like
that over there, right? Put that in the center. PROFESSOR: Put that
in the center, OK. AUDIENCE: And the rest of them
will have like [INAUDIBLE]. PROFESSOR: Well, the rest
of them aren't 2 by 2, because this is n. I've got to use pn here. And pn says that I can--
in a region 2 to the n by 2 to the n with a square out
of the center, I can tile it. Am I good here so far? I claimed I was sort of done. Yeah? AUDIENCE: No, because
p of n tells us-- well, assume p of n tells us that you
can set up a 2 to the n by 2 to the n courtyard
with a centerpiece of-- PROFESSOR: Yeah. AUDIENCE: [INAUDIBLE] PROFESSOR: Yeah. And what's the problem
for this 2 to the n by 2 to the n region? Bill's not in the center. He wants to be in the center. We put Bill in the corner. So you can't use the
inductive hypothesis here. So I went a little
too fast, here. This is not a proof so far. I've got a problem. Yeah? AUDIENCE: [INAUDIBLE] PROFESSOR: Great. OK, let's then change
the inductive hypothesis. Good. Let's say there exists a way
to tile a 2 to the n by 2 to the n region with a corner
square missing for Bill. All right, but now
I got to put-- oh, but I can make this work. Yeah. Yeah. We can make something--
we can prove this now. The inductive step
is going to work because I'll put Bill-- say
he's in one of the four regions. Then use the l down here. Put the l here. And now I've got a
corner out of each one. And now I'm done by induction. I have proved there's a
way to tile any 2 to the n by 2 to the n region with
a corner square missing. I think that proof works, Yeah? AUDIENCE: [INAUDIBLE]
with Bill in the middle, because to prove that you can
put Bill in the corner in a 2 by 2 case, you can just
blow that square off. [INAUDIBLE] just becomes just
another square, which is 4 by 4 and then you can put
him in the middle. PROFESSOR: That is true. So you've jumped ahead. We have successfully now
proved this is true for p of n. But Bill didn't want
to be in the corner. We're trying to
prove there's a way to do it with Bill
in the center, which is not what we proved. And you've come up with a way--
yeah, that might be doable. First prove you can
put Bill in the corner. And do it in 2 to the
n by 2 to the n's. I don't know about
this, actually. I got Bill in the corner. There's some way to do it. But it might have
involved doing this. And now I can't rotate that. I don't think we have-- I don't
think that proof necessarily works. AUDIENCE: [INAUDIBLE] prove
that he could be here. But likewise, we could
prove that if the block just rotates you can [INAUDIBLE]. PROFESSOR: Yes, I agree
with you and I liked it when I first heard it. And you might still convince me. But all we proved is this,
there's a 2 to the n by 2 to the n-- any 2 to the
n by 2 to the n region, we could tile it
with a corner, Bill in a corner, any
corner square missing. Good, so now you want to say,
I can get Bill in the middle. And what you want
to say is OK, I tile this region with Bill
here, in a 2 to the n by-- maybe you're right. So then I would apply the
theorem here, with Bill here. Oh, Bill here. No, I think I like it now. And then I would take these out. AUDIENCE: [INAUDIBLE] just
takes up up three blocks. And that big square is just
like a zoomed out version of a little square. And since you know that little
square fits in a 2 by 2-- like for example
in the 4 by 4 case. We proved the 2 by 2 case. In the 4 by 4 case
you have a big size. It's just like a-- you
have four squares, right? And each of those four
squares has [INAUDIBLE]. So you take that top right
square and you put Bill within the smaller
square exactly where you want him to be. And then you fill those
other-- all those empty spaces. PROFESSOR: I agree. You could make a proof. So in this case
you'd make a Lemma that uses induction
that says you can do it with Bill in the corner. And then as a
corollary or a theorem you'd take that and apply
it to four sub-squares. Put Bill in here. Take these out. And then now you'd have your
result of Bill in the center. So that is is a way to do it. It ends up being
more complicated. There is a simpler approach. But that is a way that works. It is a natural thing you would
do if you had this on homework. Is you'd think of a different
thing to prove by induction, then use that as a Lemma to
get you where you wanted to go. There's another way to do it
without having that first step. And that's-- yeah? AUDIENCE: That courtyard
where n equals 2. Divide each cell into a 2 by 2-- PROFESSOR: Yeah. AUDIENCE: Something. PROFESSOR: Well, yeah. You want to do it bottoms up. You want to take it and make
that your inductive step, 2 by 2's inside. I haven't thought
about that approach. AUDIENCE: [INAUDIBLE]
So if you could do that then you're all
set except for the 2 by 2 where Bill was. PROFESSOR: I'm worried
about a lot of 2 by 2's with a corner
missing if I do it that way. I'll think about that. There is-- let me get to
another approach here. We couldn't make it
work with the center. We could make it
work with a corner. But then we had to
do more work after. One general technique
to use with induction, when you're having struggling,
what's the induction hypothesis to use. If what you've got doesn't work,
you could pick a different one, but better to pick
a stronger one. Yeah? AUDIENCE: [INAUDIBLE] PROFESSOR: Yes. You could. And that is a good thing to do. Make the induction
hypothesis be much stronger. I had trouble proving
there's a way to do it with a center square missing. Turns out to be
easier to prove it where you say any
square could be missing. Seems like this should
be harder, right? We had a hard enough
time just showing this square missing was doable. But by assuming something--
by trying to prove a harder problem, assuming
something stronger in pn, it gets easier to prove. All right, now we better
check the base case, p0, but that's easy. There's only one
square Bill can be. But now let's look at
pn implies pn plus 1. So we go back to this. We've got a 2 to the n plus 1
by 2 to the n plus 1 courtyard. And now Bill can be anywhere. Put him here. There's Bill. Now I'm going to place my
first l-shaped tile here, in the other three regions. And now I apply
pn to each region. A pn says I can do it
with any square missing. So I'll pick this one out
here, this one out here, that one there, that one there. And now I'm done. That was easy. No extra steps. Now, how is it possible that it
was easier to prove something that was harder? Yeah? AUDIENCE: First need to be
in the center [INAUDIBLE], you're putting another
constraint on yourself. PROFESSOR: Yes. That is true. But by allowing
him to be anywhere, I could have started-- I have
to-- that's a possibility. See, what I'm trying to do
here, this inductive step is all about proving pn
implies pn plus 1 is true. That's what I'm trying to show. Now, how is it useful
for me if pn is stronger? Has more? AUDIENCE: You grow it. You prove that he
can be in a corner. But when you grow
it, the corner moves. But since we proved
that it can be in any, it's fine if it moves. PROFESSOR: Exactly. That's exactly right. ph got more
powerful, which means I get more to assume here. In the recursive problem,
Bill can be anywhere now. It gives me more power. I can tile any courtyard
with any square missing. This is more powerful. So this got more powerful. So did this. So what it means
is that my tool set is bigger with a stronger pn. And what I'm trying to
construct or prove got harder. And sometimes, if
I've got more tools, it becomes easier to
prove, even a harder thing. And so a general
rule with induction is if you don't first
succeed, try, try again. Well, the rule with induction is
if you don't succeed at first, try something harder. All right? And it's amazing, but it
actually works a lot of time, as it did here. If I don't assume
something strong enough, and pn is some little weak thing
like Bill just in the center, it's not enough to go anywhere. But if I could assume
a lot more, like Bill could be anywhere he pleases. Then I could prove
lots of things here. So it's all the art--
and you're going to learn this over the
next several weeks-- it's all the art of what's
your induction hypothesis. Picking a good
one, life is easy. Picking the wrong
one, very painful, as you'll see with beaver flu. OK, that's it for today.

Diff. Eq.

&gt;&gt; [MUSIC] &gt;&gt; DAVID J. MALAN: All right. So this is CS50 and this
is the end of week 10. So some of you might have seen this
already, but being circulated of late is an article that I thought I'd read
an excerpt from and then show you a three minute video that paints
the same picture. It was really a touching story, I
thought, of this intersection of the real world with genuinely compelling
uses of technology. &gt;&gt; So the article was entitled, "A boy
oversleeps on train, uses Google Maps to find family 25 years later." And the
first couple of paragraphs were, "When Saroo was five years old he went
with his older brother to scrounge for change on a passenger train
in a town about two hours from his small hometown. Saroo became tired and hopped on a
nearby train where he thought his brother was, then fell asleep. When he woke up he was in Calcutta,
nearly 900 miles away. Saroo tried to find his way
back, but he didn't know the name of his hometown. And as a tiny illiterate boy in a vast
city full of forgotten children he had virtually no chance of getting home. &gt;&gt; He was a street child for a while until
a local adoption agency hooked him up with an Australian couple
who brought him to live in Hobart, Tasmania. Saroo moved there, learned
English, and grew up. But he never stopped looking for
his family and his hometown. &gt;&gt; Decades later, he discovered Google
Earth and followed rail tracks. And giving himself a prescribed radius
based on how long he thought he was asleep and how fast he thought the train
was going, he knew he'd grown up in a warm climate, he knew he spoke
Hindi as a child, and he'd been told that he looked like he
was from East India. &gt;&gt; Finally, after years of scouring
the satellite photos, he recognized a few landmarks. And after chatting with an administrator
of a nearby town's Facebook page, he realized
he'd found home." &gt;&gt; So here then is the video telling
that tale from his perspective. &gt;&gt; [VIDEO PLAYBACK] &gt;&gt; -It was 26 years ago and I was
just about to turn five. We got to the train station and
we boarded a train together. My brother just said I'll stay
here and I'll come back. And I just thought, well, you know, I
might as well just go to sleep and then he'll just wake me up. And when I wake up the next day, the
whole carriage was empty on a runaway train, a ghost train taking
me I don't know where. &gt;&gt; I was adopted out to Australia
to a Australian family. And Mom had decorated my room with
the map of India, which she put next to my bedside. I woke up every morning seeing that map,
and hence, it sort of kept the memories alive. &gt;&gt; People would say, you're trying to
find a needle in a haystack. Saroo, you'll never find it. I'd have flashes of the places that
I used to go, the flashes of my family's faces. There was the image of my mother sitting
down with her legs crossed just watching her cry. Life is just so hard. That was my treasure. &gt;&gt; And I was looking in Google Map and
realized there's Google Earth as well. In a world where you could zoom into I
started to have all these thoughts and what possibilities that
this could do for me. I said to myself, well, you know,
you've got all the photographic memories and landmarks where you're
from and you know what the town looks like. This could be an application that you
can use to find your way back. &gt;&gt; I thought, well, I'll put a dot on
Calcutta Train Station in a radius line that you should be searching
around this area. I came across these train tracks. And I started following it and I came to
a train station which reflected the same image that was in my memories. &gt;&gt; Everything matched. I just thought, yep. I know where I'm going. I'm just going to let the map that I
have in my head to lead me and take me back to my hometown. &gt;&gt; I came to the doorstep of the house
that I was born and walked around about fifteen meters
around the corner. There was three ladies standing outside
adjacent to each other. And the middle one stepped forward. And I just thought, this
is your mother. She came forward, she hugged me, and we
were there for about five minutes. &gt;&gt; She grabbed my hand and she took me to
the house and got on the phone and she rang my sister and my brother to say
that your brother has just all of the sudden appeared like a ghost. &gt;&gt; And then the family was
reunited again. Everything's all good. I help my mother out. She doesn't have to be slaving away. She can lead the rest of
her life in peace. &gt;&gt; It was a needle in a haystack,
but the needle was there. Everything's there. Everything we have in the world
is the tap of a button. But you've got to have the will and
the determination to wanting it. &gt;&gt; [END VIDEO PLAYBACK] &gt;&gt; So a really sweet story. And it actually reminds me of quite a
topic that's been getting quite a bit of attention of late in The Crimson,
more nationally in general. Especially as MOOCs are taking
the stage of late. MOOCs being these massive and open
online courses of which CS50 is one. &gt;&gt; And people talking about how, for
instance, the humanities aren't really catching up or aren't nearly as
in vogue as they once were. And I would encourage you guys, much
like Jonathan did on Monday, to think about as you exit 50, and we know
already about 50% of you will not continue on to take another computer
science course, and that's totally fine and expected. Because one of the overarching goals
of a class like this is really to empower you guys with just an
understanding of how all of this stuff works and how this world
of technology works. &gt;&gt; So that when you are back in your own
worlds, whether it's pre-med or whether it's the humanities or the
social sciences or some other field altogether, that you guys are bringing
some technical savvy to the table and helping to make smart decisions when
it comes to the use of and introduction of technology
into your world. &gt;&gt; For instance I was reminded of late
too of two of the undergraduate classes I took two years ago, which were
such simple uses of technology but ever so compelling. First Nights with Professor Tom Kelly
if you've taken the class. It's a class on classical music on
this stage here where you learn a little something about music. It's actually First Nights that CS50
borrowed the idea of tracks for those less comfortable in between
and more comfortable. &gt;&gt; In my time they had different tracks
for kids with absolutely no music experience like me, and then kids who
had been performing since they were five years old. And that class, for instance, just had
a website like most any other, but it was a website that allowed you to
explore music on it and play back musical clips from class, from the web,
and just use technology in a very seamless way. &gt;&gt; Another class years later that I
audited, essentially, in grad school, Anthro 1010, Introduction
to Archaeology here. It was amazing. And one of the most compelling yet super
obvious, in retrospect, uses of software was that the professors in
that class used Google Earth. We were sitting across the street
in some lecture hall. And you couldn't travel, for instance,
to the Middle East to the dig that one of the professors had just come back on,
but we could do that virtually by flying around in Google Earth and
looking at a bird's eye view at the dig site he had just returned
from a week ago. &gt;&gt; So I would encourage you guys,
especially in the humanities, to go back to those departments after this
class bringing your final projects with you or ideas of your own, and see
just what you can do to infuse your own fields in humanities or beyond
with a little bit of this sort of thing that we've explored
here in CS50. &gt;&gt; So with that picture painted, thought
we'd try to tackle two things today. One, try to give you a sense of
where you can go after 50. And in particular, if you choose to
tackle a web based project as is incredibly common, how you can go
about taking off all of CS50's training wheels and going out there on
your own and not having to rely on a PDF or a specification of a pset? Not having to rely on a CS50
appliance anymore. But can really pull yourself
up by your bootstraps. &gt;&gt; With that said, C-based final
projects are welcome. Things that use the stand for
a portable library in graphics are welcome. We just know that statistically a lot of
people bite off projects in PHP and Python and Ruby and MySQL and other
environments, so we'll bias some of our remarks toward that. &gt;&gt; But a quick look back. So we took for granted in pset7 the
fact that $_SESSION existed. This was a super global, a global,
associative array. And what does this let you do? Functionally, what's the
feature this gives us? Yeah? To track the user's ID. And why is this useful? To be able to store inside of this super
global JHarvard or [? Scroobs ?] or Malan's user ID when he
or she visits a site. &gt;&gt; Exactly. So you don't have to log
in again and again. It would be a really lame world wide web
if every time you clicked a link on a site like Facebook or every time
you clicked on an email in Gmail you had to re-authenticate to prove that
it's still you and not your roommate who might have walked up to your
computer in your absence. &gt;&gt; So we use SESSION to just
remember who you are. And how is this implemented
underneath the hood? How does a website that uses , the
protocol that web browsers and servers speak, how does HTTP, which is a
stateless protocol, let's say. &gt;&gt; And by stateless I mean, once you
connect to a website, download some HTMLs, some JavaScript, some CSS, your
browser's icon stops spinning. You don't have a constant connection
to the server typically. That's it. There's no state maintained
constantly. So how is SESSION implemented in such
a way that every time you do visit a new page, the website remembers
who you are? What's the underlying implementation
detail? Shout it out. It's one word. &gt;&gt; Cookies. All right. So cookies. Well, how are cookies used? We'll recall that a cookie is generally
just a piece of information. And it's often a big random
number, but not always. And a cookie is planted on your hard
drive or in your computer's RAM so that every time you revisit that same
website, your browser reminds the server, I am user 1234567. I am user 1234567. &gt;&gt; And so long as the server has remembered
that user 1234567 is JHarvard, the website will just assume
that you are who you say you are. And recall that we present these cookies
sort of in the form of a virtual hand stand. It's sent in the HTTP headers just to
remind the server that you are who it thinks you are. &gt;&gt; Of course, there's a threat. What threat does this open us up to if
we're essentially using sort of a club or an amusement park mechanism
for remembering who we are? &gt;&gt; If you copy someone's cookie and hijack
their session, so to speak, you can pretend to be someone else and the
website most likely is just going to believe you. So we'll come back to that. Because the other theme for today beyond
empowerment is also talking about the very scary world we live in
and just how much of what you do on the web, how much of what you do even
on your cell phones today can be tracked really by anyone between
you and point B. &gt;&gt; And Ajax, recall. We looked only briefly at this,
although you've been using it indirectly in pset8 because you're using
Google Maps and because you're using Google Earth. Google Maps and Google Earth don't
download the entire world to your desktop, obviously, the
moment you load pset8. It only downloads a square of the world
or a bigger square of the earth. And then every time you sort of steer
out of range you might notice-- especially if on a slow connection-- you
might see some gray for a moment or a bit of fuzzy imagery as the
computer downloads more such tiles, more such imagery from the
world or the earth. &gt;&gt; And Ajax is generally the technique
by which websites are doing that. Once you need more of the map, your
browser is going to use Ajax, which is not itself a language or technology,
it's just a technique. It's the use of JavaScript to go get
more information from a server that allows your browser to go get what's to
the east or what's to the west of what's otherwise currently
being shown in that map. So this is a topic that many of you
will encounter either directly or indirectly via final projects if you
choose to make something that's similarly dynamic that's pulling data
from some third party website. &gt;&gt; So we've got a really exciting
next Wednesday ahead. Quiz one, the information for which
is on CS50.net already. Know that there'll be a review session
this coming Monday at 5:30. The date and time is already posted
on CS50.net in that About sheet. And do let us know you
have any questions. Pset8 meanwhile is already
in your hands. &gt;&gt; And let me just address one FAQ
to save folks some stress. For the most part a lot of the chatter
we see at office hours and a lot of the bugs we see reported on Discuss are
indeed bugs in a student's code. But when you've encountered something
like the Google Earth plug-in crashing or not even working and you are
confident it's not you, it's not a [? chamad ?] issue, it is not a
bug you introduced into the distribution code. &gt;&gt; Realize just FYI-- this is sort of plan Z-- that the last time we used this problem
set and we ran into similar issues, there's a line of code in
service.js that essentially is this, that says, turn buildings on. And they work around the last time we
did this in, again, corner cases where students just couldn't get the darn
thing to work is change true to false in that one line of code. And you'll find it if you search
through service.js. &gt;&gt; I don't recommend this because you will
create the most barren landscape of Cambridge, Massachusetts. This will literally flatten your world
so that all you see are the teaching fellows and course assistants on
the horizon and no buildings. But realize for whatever reason the
Google Earth plug-in seems still to be buggy a year later, so this
might be your fail save. So rather than resort to tears, resort
to turning buildings off if you know it's the plug-in that's not cooperating
on your Mac or PC. But, this is again last resort if
you're sure it's not a bug. &gt;&gt; So the Hackathon. A couple of teasers just
to get you excited. We had quite a few RSVPs. And just to paint a picture of what
awaits, I thought I'd give you a few seconds recall of this imagery
from last year. &gt;&gt; [MUSIC] &gt;&gt; DAVID J. MALAN: Wait, oh. We even have our literal CS50 shuttles. &gt;&gt; [MUSIC] &gt;&gt; DAVID J. MALAN: So that's what awaits
you in terms of the Hackathon. And this will be an opportunity, to
be clear, not to start your final projects but to continue working on
your final projects alongside classmates and staff and lots of food. And again, if you're awake at 5:00 AM
we'll take you down the road to IHOP. &gt;&gt; The CS50 fair, meanwhile, is the climax
for the entire class where you'll bring your laptops and friends,
maybe even family to a room on campus down the street to exhibit your projects
on laptops, on tall tables like this with lots of food and friends
and music in the background, as well as our friends from industry. Companies like Facebook and Microsoft
and Google and Amazon and bunches of others so that if interested in just
hearing about the real world or chatting with folks about real world
internship or full time opportunities, know that some of our friends
from industry will be there. And a couple of pictures we can
paint here are as follows. &gt;&gt; [MUSIC] &gt;&gt; DAVID J. MALAN: All right. So that then is the CS50 fair. So let's now proceed to tell a story
that really will empower you hopefully for things like final projects. So one of few little things to seed your
mind, either for final projects or just more generally for projects that
you might decide to tackle after the course, these are all documented
on manual.cs50.net where the CS50 manual where we have lots of
techniques documented. &gt;&gt; And this is just shorthand notation for
saying that there exists in the world things called SMS to email
gateways, which is a fancy way of saying, there's servers in the world
that know how to convert emails to text messages. So if for your final project you want
to create some sort of mobile themed service that allows you to alert friends
or users to events on campus or what's being served in the D Hall
that night or any such alert feature, know that it's simple as sending an
email as with PHPMailer which you might have used for pset7 or we saw
briefly a week or so ago, to addresses like this. &gt;&gt; And in fact you can text this assuming
your friend has an unlimited texting plan and you don't want
to charge them $0.10. But if you send an email to your friend
who you know to have Verizon or AT&amp;T using Gmail and just sending it to
their phone number at whatever the sub domain there is, realize you
will send a text message. &gt;&gt; But this is one of those things
to be careful of. If you troll through last year's CS50
videos I think it was, a horrific, horrific, horrific bug I wrote in code
ended up sending about 20,000 text messages live to our
students in class. And only because someone noticed that
they were getting multiple text messages from me did I have the
wherewithal to hit Control C quickly and stop that process. Control C, you recall, is your friend
in instances of infinite loop. So beware the power we have just given
to you rather irresponsibly, most likely, based on my own experience. But that's on the web and has
been there for some time. &gt;&gt; All right. So textmarks.com. So this is a website. And there's bunches of others out there
as well that we've actually used as a class for years to be able
to receive text messages. Unfortunately, sending text messages is
easy as sending emails like that. Receiving's a little harder, especially
if you want to have one of those sexy short codes that's only
five or six digits long. &gt;&gt; So for instance, for years you've been
able to send a text message-- and you can try this as well-- to 41411. And that's the phone number for
this particular startup. And if you send a message to 41411-- I'll just write it up here, so 41411-- and then send them a message
like SBOY for Shuttle Boy. And then type in something
like mather quad. So you send that text message
to that phone number. Within a few seconds you should get back
a response from the CS50 Shuttle Boy service, which is the shuttle
scheduling software that we've had out there on the web for some time. And it will respond to
you via text message. &gt;&gt; Because what we have done as a class, as
a programmer, is to write software, configured our free account with text
marks to listen for text messages sent to SBOY at that number. And what they do is forward those text
messages to our PHP-based website as HTTP parameters saying, here. This user with this phone number
sent you this text message. Do with it what you want. &gt;&gt; So we wrote some software that upon
receiving a string like SBOY mather quad, we parse it. We figure out where the spaces
are between words. And we as a class decide
how to respond to that. And if you try that now, for instance,
you should see, via response within a few seconds, the next few shuttles going
from mather to the quad if any. And there's other stops. You can type in Boylston or other such
stops on campus, and it should recognize those words. &gt;&gt; So parse.com. This is another service that we've been
pointing some students at for final projects that's wonderful
in that it's free for a reasonable amount of usage. And if I go to parse.com you'll see
that this is an alternative to actually having something like
your own MySQL database. And frankly, it's just
kind of mesmerizing. This is what's inside of the
cloud even on a cloudy day. &gt;&gt; So parse.com allows you to do a
bunch of interesting things. And there's other alternatives
to this out there. For instance, you can use them
as your back end database. So you don't need to have
a web hosting company. You don't need to have
a MySQL database. You can instead use their back end. &gt;&gt; If you're doing a mobile project for
Android or iOS or the like, know that there exists things like push services
so you can push alerts to your friends or your users' home screens. And then a bunch of other
features as well. &gt;&gt; So if you have interest, check out these
websites and websites like them to just see how many other peoples'
shoulders you can stand on to make really cool software of your own. &gt;&gt; Now in terms of authentication, an FAQ,
is how do you actually guarantee that your users are people on campus,
Harvard students or faculty or staff? So CS50 has its own authentication
service called CS50 ID. Go to that URL and you can restrict your
website to anyone with a Harvard ID, for instance. So know that we can handle that. You guys should not be in the business
of saying, what's your Harvard ID? What's your Harvard PIN? Let me now do something with it. We'll do all of that. And what we'll give you back is
someone's name and email address, but not anything sensitive. &gt;&gt; An app on a mobile device, it can be
made to work on a mobile device, but it's not quite designed for that. So you'll end up spending a non trivial
amount of time doing so. So I would discourage
that route for now. This is really intended for
web based applications. &gt;&gt; So web hosting. So if you haven't seen on
the course's homepage-- and here's where we'll begin a story-- web hosting is all about paying for
usually a service, host a server owned by someone else on the web that has an
IP address, and you then put your website on it. And they usually give you email
accounts and databases and other such features. &gt;&gt; Know that if you don't want to actually
pay for such, go to that URL there and CS50 actually has a non-profit
account that you can use to actually have not http://project
inside of the appliance for your final project. If you actually want it to be something
like, isawyouharvard.com, you can buy that domain name-- although
not that particular one-- and then you can go about hosting it on a
public web server like we can offer you guys through here. &gt;&gt; And in fact if unfamiliar,
if you've never been to isawyouharvard.com, one, go there. But two, know that that was a young
woman's name by Tej To Toor Too two years ago, three years ago, who was a
CS50 alumni who happened a day or two before the CS50 fair sent out an email
to her house mailing list and voila. Two days later by the CS50 fair, she had
hundreds of users all creeping on each other on her website and
saying how they had seen her or him on campus. So that's one of CS50's favorite
success stories from a CS50 final project. &gt;&gt; So how do you go about putting a website
like that on the internet? Well, there's a few such
ingredients here. So one, you have to buy a domain name. There are bunches of places in
the world from which you can buy a domain name. And for instance, one that we recommend
only because it's popular and it's cheap is called
namecheap.com. But you can go godaddy.com and
dozens of others out there. You can read up on reviews. &gt;&gt; But for the most part it doesn't
matter from whom you buy a domain name. And they vary in price and
they vary in suffix. The suffixes like .com, .net,
.org, .io, .tv, those actually vary in price. But if we wanted to do something like
cats.com we can go to this website, click Search. Presumably this one is taken. But apparently, catsagainst.com
is available. pluscats.com is available. Lovecats, catscorner, dampcats.net. All of this hopefully pseudo
randomly generated. If you want cats.pw, $1,500 only,
which is a bit insane. So someone has really snatched up all
the cat related domain names here for varying prices. &gt;&gt; As an aside, let's see. Who has cats.com? Know that you guys have at
your disposal fairly sophisticated commands now. Like I can type literally
who is cats.com? And because of the way the internet is
structured you can actually see who has registered this. Apparently this person is [INAUDIBLE]
using a proxy service. So whoever owns cats.com doesn't want
the world to know who they are. So they've registered if through
some random privacy service. But sometimes you actually
get actual owners. &gt;&gt; And this is to say, especially if you're
pursuing some startup and you really want some domain name and you're
willing to pay someone else for it, you can figure out contact
information in that way. &gt;&gt; But also interesting is this. Let me scroll up to this portion. So this is that same output. And this is just tacky. So apparently cats.com can be
yours for the right price. But what's interesting here
is that the name servers-- this is total abuse of what a name
server's supposed to be-- your name server is not supposed to be
thisdomainforsale.com. If we actually choose something like-- let's choose something a little more
legitimate like, who is google.com, and scroll up here. So here-- what happened there? Interesting. Beyond who is-- let's keep it more low key. &gt;&gt; Who is mit.edu? OK. This is helpful. So this is what I was hoping for. Legitimate use of the DNS service. Name servers here indicate
the following. This is MIT's way of saying, whenever
someone in the world, wherever they are, types in mit.edu and hits Enter,
your laptop, whether Mac or PC, will somehow eventually figure out that the
people in the world that know what the IP address is for mit.edu or any of the
sub domains at mit.edu or any of these servers here-- and it actually
looks like MITs infrastructure is pretty robust as you would expect. They have multiple names servers
which is good for redundancy. And in fact, they seem to be globally
distributed across the world. A bunch of those seem to be in the US,
a couple in Asia, one in Europe, two in somewhere else. &gt;&gt; But the point here is that DNS that
we've been taking for granted and generally described as a big Excel table
that has IP addresses and domain names is actually fairly sophisticated
hierarchical service so that in the world there's actually a finite number
of servers that essentially know where all of the .coms are or all of
the .nets are, all of the .orgs are, and so forth. &gt;&gt; So when you go ahead and buy a domain
name from a place like Name Cheap or Go Daddy or any other website, one of
the key steps that you'll have to do you, if you do this even for your final
project, is tell the registrar from whom you're buying the domain
name, who in the world knows your website's IP addresses, who
your name servers are. &gt;&gt; So if you use, for instance CS50's
hosting account-- we happen to have this account through dreamhost.com
which is a popular web hosting company-- they will tell you that you should buy
your domain and tell the world that your domain's name server is
ns1.dreamhost.com, ns2.dreamhost.com, and ns3.dreamhost.com. &gt;&gt; But that's it. Buying a domain name means giving them
the money and getting ownership of the domain, but it's more like
a rental though. You get it for a year and then they bill
you recurringly for the rest of your life until you cancel
the domain name. And then you tell them who
the name servers are. But then you're done with
your registrar. And from there you'll interact only with
your web hosting company, which in CS50's case will be DreamHost. But again, more documentation will be
provided to you if you decide to go that route. &gt;&gt; So if you do this after the course's
end, simply googling web hosting company will turn up thousands
of options. And I would generally encourage you to
ask friends who might have used a company before if they recommend
them and had a good experience. &gt;&gt; Because there's a lot of fly by night
web hosting companies, like a guy in his basement with a server
that has an IP address. He has some extra RAM and hard disk
space and just sells web hosting accounts even though there's no way that
server could handle hundreds of users or thousands of users. So realize you will get
what you pay for. &gt;&gt; For quite a while for my personal home
page-- and this was totally acceptable because I had, like, two
visitors a month-- I was paying, like, $2.95 a month. And I'm pretty sure it was
in someone's basement. But again, you don't get necessarily
any guarantees of uptime or scalability. So again, you're typically looking
at something more than that. &gt;&gt; Well, what about SSL? So what's SSL used for? Let's now start to steer in the
directions of security and things that can harm us. Especially as you venture
out on your own. &gt;&gt; What's SSL, or what's SSL used for? Security, OK. So it's used for security. What does that mean? So it stands for Secure Sockets Layer. And it is indicated by a URL
that starts with https://. Many of us have probably never typed
https://, but you'll often find that your browser is redirected from HTTP to
HTTPS so that everything is there after encrypted. &gt;&gt; FYI, using SSL requires typically that
you have a unique IP address. And typically to get a unique IP address
you need to pay a web hosting company a few dollars more per month. So realize this is very easily
implemented these days by buying an IP address and by buying what's
called an SSL certificate. But realize that it does come
at some additional cost. And, as we'll try to scare in just a
bit, it's not even necessarily 100% protective of whatever it is
you're trying to protect. &gt;&gt; So for security, I'd thought I'd
do sort of a random segue here. As you might know from CS50's lecture
videos, our production team has been a fan as I have of taking really nice
photography of campus, and aerial photography most recently. If you ever look up and you see
something flying with a little camera, it may actually be CS50. And I just thought I'd share minute of
some of the footage the team has gathered, particularly as we look to
the spring semester and next fall. If any of you have a knack for
photography, videography, we would love to get you involved
behind the scenes. But more on those details in a week. &gt;&gt; [MUSIC] &gt;&gt; DAVID J. MALAN: Turns out there's a
miniature golf course on the top of the stadium that we never knew about. &gt;&gt; [MUSIC] &gt;&gt; DAVID J. MALAN: You can see the
outline of the drone there. &gt;&gt; [MUSIC] &gt;&gt; DAVID J. MALAN: The best part here
is, watch the jogger on the left. &gt;&gt; [MUSIC] &gt;&gt; DAVID J. MALAN: Another example of what
you can do with technology that's only tangentially, frankly,
related to security. But I thought that would be a more
fun way of just saying, security. So let's see if we can't scare you guys
now with not only a bit of a few threats, but also an underlying
understanding of what these threats are so that moving forward you can
decide how and whether to defend yourself against these things and at
least to be mindful of them as you make decisions as to whether or not to
send that email, whether or not to log into that website, whether or not to
use that cyber cafe's Wi-Fi access point so that you know what the
threats are indeed around you. &gt;&gt; So Jonathan referred to something
like this on Monday. He had a window screen shot. This one is of a Mac. How many of you have ever installed
software on your Mac or PC? Obviously everyone. How many of you have given much thought
to typing in your password when prompted? I mean, even I don't, frankly. So a couple of us are good
at being paranoid. But consider what you're
actually doing here. &gt;&gt; On a typical Mac or PC you have
an administrator account. And typically you're the only one using
a laptop at least these days. So your account, Malan or JHarvard
or whatever it is, is the administrator account. And what that means is you have
root access to your computer. You can install anything you want,
delete anything you want. &gt;&gt; And typically these days, because of
dated design decisions from years ago, the way most software gets installed
is as an administrator. And even if your Mac or PC has at least
gotten smart enough over the years with the latest incarnations of
Mac OS and Windows to not run your username by default as the
administrator, when you download some new program off the internet and try to
install it, you're probably going to be prompted for your password. But the catch is at that point, you're
literally handing the keys of your computer over to whatever random
program you just downloaded and allowing it to install
whatever it wants. &gt;&gt; And as Jonathan alluded to, realize
that it might say that it wants to install your software that you care
about, Spotify or iTunes or whatever it is you're trying to install. But you're literally trusting the author
or authors of the software to only do what the program
is supposed to do. &gt;&gt; But there is absolutely nothing
stopping most programs on most operating systems from deleting files,
from uploading them to some company's website, from trolling around,
for encrypting things. And again, we've sort of built
an entire infrastructure over the years on trust. And so realize that you've just been
trusting random people and random companies for the most part. &gt;&gt; And Jonathan alluded to too, sometimes
those companies themselves are sort of knowingly malicious, all right? Sony caught a lot of flack a few years
ago for installing what was called a rootkit kit on people's computers
without their knowledge. And the gist of this was that when you
bought a CD for instance that they didn't want you to be able to copy or
rip the music off of, the CD would install, without your knowing,
a rootkit on your computer. Rootkit just meaning software that runs
as administrator that potentially does bad things. &gt;&gt; But among the things this thing
did was it hid itself. So some of you might be pretty savvy
with your computer and know, well, I can just open the Task Manager or the
Activity Monitor and I can look at all of the arcanely named programs
that are running. And if anything looks suspicious
I'll just kill it or delete it. But that's what the rootkit did. It essentially said, if running Task
Manager, don't show yourself. &gt;&gt; So the software was there. And only if you really, really looked
hard could you even find it. And this was done in the name
of copy protection. But just imagine what could
have been done otherwise. &gt;&gt; Now in terms of protecting yourself. A lot of websites are wonderfully
gracious in that they put these padlock icons on their homepage which
means that the website is secure. This is from bankofamerica.com
this morning. So what does that little padlock icon
there mean next to the Sign In button? &gt;&gt; Absolutely nothing. It means someone knows how to use
Photoshop to make a picture of a padlock icon. Like quite literally, the fact that it's
there is meant to be a positive signal to the user like,
ooh, secure website. I should trust this website and now
type in my username and password. And this has been conventional for
years, as recently as this morning. &gt;&gt; But consider the habits that
this is getting us into. Consider the implicit message that all
of these banks in this case have been sending us for years. If you see padlock, then secure. All right? &gt;&gt; So how can you abuse that system
of trust if you're the bad guy? Put a padlock on your website, and
logically, the users have been conditioned for years to assume
padlock means secure. And it might actually be secure. You might have a wonderfully secure
SSL HTTPS connection to a fake website .com. And no one else in the world can see
that you're about to hand him or her your username and password
to your account. &gt;&gt; This though, perhaps, is a
little more reassuring. So this is a screen shot of the top
of my browser this morning at bankofamerica.com. And notice here too we
have a padlock icon. What does it mean in this context
in Chrome at least? &gt;&gt; So this is now using SSL. So this is actually a better thing. And the fact that Chrome is making it
green is meant to draw our attention to the fact that this is
not only over SSL. This is a company that someone out
there has verified is actually bankofamerica.com. And that means that Bank of America,
when buying their so-called SSL certificate, essentially big random,
somewhat random numbers that implement security for them, they have been
verified by some independent third party that says, yep. This is actually the CEO of Bank of
America trying to buy the certificate. Chrome will therefore trust that
certification authority and say in green, this is bankofamerica.com. And Bank of America just pays a few
hundred dollars for that or a few thousand as opposed to a
few tens of dollars. &gt;&gt; But here too, how many of you have ever
behaved any differently because the URL in your browser is
green instead of black? Right? So a couple of us. And that's good to be paranoid. But even then, those of you who even
notice these things, do you actually stop logging into an otherwise secure
website if the URL is not green? All right, so probably not, right? At least most of us, if it's not green,
most likely you're just going to be like, whatever. Like, I want to log into this website. That's why I'm here. I'm going to log in nonetheless. &gt;&gt; As an aside, Chrome is a little
better about this. But there's a lot of browsers like
Firefox for instance, at least for some time, where that padlock icon
is, you can actually put any icon of your own. Let me see what the latest version
of Firefox looks like. So if we go to CS50.net. &gt;&gt; OK, so they've gotten better as well. What the browsers used to do is like,
here's for instance [? SAAS's ?] crest up here. That's the so-called favorite
icon for a website. Years ago-- actually not that long ago-- that little
shield would have been right here next to the URL. Because some genius decided that it
would just look pretty classy to have your graphical logo right
next to your URL. And design wise, that actually
is pretty compelling. &gt;&gt; So what did bad guy start doing? They started changing their favorite
icons, or their default icon for a homepage to be not a crest
but a padlock, which had absolutely no meaning. Other than their favorite icon
was a padlock it had no indications of security. &gt;&gt; So the lessons here are
a couple I think. One is that there are actually some
well intentioned mechanisms for teaching us users about security even
if you weren't even aware what green meant or what even HTTPS meant. But if those mechanisms get us into
the bad habit of trusting websites when we see those positive signals,
they're very easily abused as we saw just a moment ago with something
silly like this. &gt;&gt; So session hijacking comes into
play, as we said before, with cookies for instance. And what does this actually mean? Well with session hijacking this is all
about stealing someone's cookies. So if I open up Chrome here, for
instance, and I open up the Inspector down here and I go to
the Network Tab-- and we've done this before-- and I go to something like
http://facebook.com Enter, a whole bunch of stuff goes across the screen
because of all the images and CSS and JavaScript files. &gt;&gt; But if I look at this one here notice
that Facebook is indeed planting one or more cookies on my
browser right here. So these are essentially the hand
stamps that represent me. And now hopefully my browser will
present this again and again when revisiting that website. But that only is secure, we said a
couple weeks ago, if you're using SSL. &gt;&gt; But even SSL itself can
be compromised. Consider after all the way SSL works. When your browser connects to a remote
server via https://, long story short, cryptography is involved. It's not as simple as Caesar or
Visionaire or even DES, DES from a while back in pset2. It's more sophisticated than that. It's called public key cryptography. But really big and really random
numbers are used to scramble information between point A, you,
and point B, like facebook.com. &gt;&gt; But the problem is, how many of us again
ever type in https:// to start our website connection
in that secure mode? I mean, how many of you even
type http://facebook.com? All right, if you do, like, hello. You don't need to do that
anymore, right? The browser will figure it out. &gt;&gt; But most of us do indeed
just type facebook.com. Because if we're using a browser, the
browsers have gotten smart enough by 2013 to assume if you're using a
browser, you type in an address, you probably want to access it not
via email or instant message. You mean HTTP and Port 80. Those conventions have been adopted. &gt;&gt; But how does redirection work? Well, notice what happens here. If I go back to Chrome-- and let's do this in incognito
mode so that all of my cookies are thrown away. And let me go here to,
again, facebook.com. And let's see what happens. &gt;&gt; Recall that the first request was
indeed just for facebook.com. But what was the response that I got? It wasn't a 200 OK. It was 300, or 301, which is a
redirect telling me to go to http://www.facebook.com, which is
where Facebook wants me to go. But then if we look at the next request,
and we've seen this before, notice what their second response is. Specifically that they want me now to
go to the SSL version of Facebook. &gt;&gt; So here is an opportunity. This is a wonderfully useful feature
of just the web and HTTP. If the end user like Facebook wants me
to stay on the secure version of their website, great. They will redirect me for myself. And so I don't have to even
think about that. &gt;&gt; But what if between point A and B,
between you and Facebook, there's some bad guy, there's some system
administrator at Harvard who's curious to see who your friends are. Or there's some-- years ago, this used to sound crazy-- but there's some government entity like
the NSA who's actually interested in who you're poking on Facebook. Where's the opportunity there? Well, so long as someone has enough
technical savvy and they have access to your actual network over Wi-Fi
or some physical wire, what could they do? &gt;&gt; Well, if they're on the same network as
you and they know something about TCP/IP and IP addresses and DNS and how
all of that works, what if that man in the middle, what if that National
Security Agency, whatever it may be, but what if that entity simply
responds more quickly than Facebook to your HTTP request and says,
oh, I am Facebook. Go ahead, and here's the
HTML for facebook.com. &gt;&gt; Computers are pretty darn fast. So you could write a program running on
a server like nsa.gov that when it hears a request from you for
facebook.com, very quickly behind the scenes gets the real facebook.com making
a perfectly [? esque ?] secure SSL connection between NSA and between
Facebook, getting that HTML very securely for the login page, and then
the NSA server just responds to you with a login page for facebook.com. &gt;&gt; Now how many of you would even notice
that you're using Facebook over HTTP still at that point because you've
accidentally connected to nsa.gov and not Facebook? The URLs not changing. All of this is being done
behind the scenes. But most of us, myself included,
probably wouldn't notice such a minor detail. &gt;&gt; So you might have a perfectly workable
connection between you and what you think is Facebook, but there's a
so-called man in the middle. And this is a general term for man in
the middle attack where you have some entity between you and point B that's
somehow manipulating, stealing, or watching your data. So even SSL is not surefire, especially
if you've been tricked into not turning it on because of how these
underlying mechanisms actually work. &gt;&gt; So a lesson today then too is if you
really want to be paranoid-- and even here there are threats-- you should really start getting into
the habit of typing in https://www whatever domain name you
actually care about. &gt;&gt; And as an aside too there's
yet another threat with regard to session hijacking. Very often when you first visit a
website like facebook.com, unless the server has been configured to say that
that hand stamp it put on you yesterday should be secure itself, your
browser might very well, upon visiting things like facebook.com
google.com, twitter.com, your browser might be presenting that hand stamp only
to be slapped down and said, no. Use SSL. &gt;&gt; But it's too late at that point. If you have already sent your hand
stamp, your cookie, in the clear with no SSL, you have a split second
vulnerability where someone sniffing your traffic, whether roommate or NSA,
can then use that same cookie, and with a bit of technical savvy,
present it as his or her own. &gt;&gt; Another attack you might
not have thought about. This one is really on you if you screw
this up in writing some website that somehow uses SQL. So here, for instance, is a screen
shot of Harvard's login. And this is a general example
of something with a username and password. Super common. So let's assume that SSL exists and
there's no man in the middle or anything like that. Now we're focusing on the server's
code that you might write. &gt;&gt; Well, when I type in a username and
password, suppose that the PIN service is implemented in PHP. And you might have some code
on that server like this. Get the user name from the post super
global and get the password, and then if they're using some pset7 like
code there's a query function that might do this. Select Star from users where username
equals that and password equals that. &gt;&gt; That looks, at first glance,
totally reasonable. This is syntactically valid PHP code. Logically there's nothing
wrong with this. Presumably there's some more lines that
actually do something with the result that comes back
from the database. But this is vulnerable for
the following reason. &gt;&gt; Notice that, like a good citizen,
I have put in quotes, single quotes, the user name. And I put in single quotes
the password. And that's a good thing because they're
not supposed to be numbers. Typically they're going to be text. So I'm quoting them like strings. &gt;&gt; And if I now advance further what if--
and I've removed the bullets from the PIN service temporarily-- what if I try to log in as
President [? Scroob ?] but I claim that my password is
12345' OR '1'='1, and notice what I haven't done. I did not close the other
single quote. Because I'm pretty sharp
here as the bad guy. And I'm assuming they're you're
not very good with your PHP and MySQL code. I'm guessing that you're not checking
for the presence of quotes. &gt;&gt; So what just happened is that when your
user has typed in that string, the query you're about to
create looks like this. And long story short, if you and
something together or you or something together this is going to return
a row from the database. Because it is always the
case that 1 equals 1. &gt;&gt; And just because you didn't anticipate
that your users, good or bad, might have an apostrophe in their name you
have created a SQL query that's still valid, and will return now more results
than you might have intended. And so this bad guy now has potentially
logged in to your server because your database is returning a row
even if he or she has no idea what [? Scroob's ?] actual password is. &gt;&gt; Oh, I realized a typo here. I should've said password equals
12345 like the previous example or 1 equals 1. I'll fix that online. &gt;&gt; So why did we have you using the query
function with question marks? One of the things the query function
does for you is it makes sure that when you pass in arguments after the
commas here like this that the query that's actually sent to the
database looks like this. A lot uglier to look at, but back
slashes have been automatically inserted to avoid precisely that
injection attack that I showed a moment ago. &gt;&gt; Now a fun XKCD that I thought I'd pull
up here that hopefully should now be a little more understandable
is this one here. &gt;&gt; A little bit? Maybe we need a little more
discussion on that. So this is alluding to a little kid
named Bobby who has somehow taken advantage of a website that is just
trusting that what the user has typed in is not, in fact, SQL code,
but is in fact a string. &gt;&gt; Now you may recall that drop-- you might have seen this-- drop means
delete a table, delete a database. So if you essentially claim that
your name is Robert";droptabl estudentsomething, ] you might very well trick the database
not only into checking that you're indeed Robert, but semicolon also
proceed to drop the table. &gt;&gt; And so SQL injection attacks can
actually be as threatening as this whereby you can delete someone's data,
you can select more datas than intended, you can insert
or update data. And you can actually see this upon at
home exercise, not for malicious purposes but just for instructional,
is any time you're prompted to log into website, especially some sort of
non very public, very popular website, try logging in as John O'Reilly
or someone with an apostrophe in their name. Or literally just type apostrophe,
hit Enter, and see what happens. &gt;&gt; And all too often, tragically, people
have not sanitized their inputs and made sure that things like quotes
or semicolons are escaped. Which is why in pset7 we give
you this query function. But do not under appreciate exactly
what it is doing for you. &gt;&gt; So with that said, enjoy using
the web this week. And we will see you on Monday. &gt;&gt; At the next CD50. &gt;&gt; [MUSIC]

Linear Algebra

The following content is
provided by MIT OpenCourseWare under a Creative Commons
license. Additional information about
our license and MIT OpenCourseWare in general is
available at ocw.mit.edu. PROFESSOR: So we started last
week on the big topic for the rest of the semester,
optimization. Maybe can you close that door
or just part way anyway. Thanks. Great. That's perfect, just
like that. So, since it was a few days ago,
I wanted to recap what I did in the first lecture about
optimization, which was to pick out the least squares
problem as a beautiful model problem. I followed that through -- the input is a matrix a
rectangular, a right hand side b probably measurements. We would like to get au
equal b but we can't. We got too many measurements. We do the best possible, which
is the solution u hat of this normal equation. Then I rewrote the normal
equations -- well, I also drew the picture that you see
over here on the left. The picture that shows
the two optimization problems at the same time. Here is the vector b. Here are all possible au's -- this is the column space,
this is all au's. The best au was the
projection. The error e was what we couldn't
get right, the part that's perpendicular to the
column space we can't help. It's the solution to another
projection problem that e is the same -- of course, it's
the same over here -- as projecting b onto this
perpendicular line, which is the line of all vectors,
a tranpose e says -- what that says in words is
e is perpendicular to the columns of a. I've drawn it the best
I could as a perpendicular to the columns. So that we have a plane
of columns -- this is like a 3 x 2 matrix. We have a plane with the
two columns and the perpendicular line. Then just near the end, I said
that it would be great to have -- this model would be almost
all we need except it should have one more matrix and
I call that matrix c. I want just to show you quickly
where that comes. So this is all section 7.1 of
the notes, except that these words are not yet typed. So they'll, as soon as possible,
and an updated 7.1 will include this, what
I want to do. Because I think it's the very
best introduction I could give to these pair of optimization
problems. You might think who needs
optimization. Your main activity
might be solving differential equations. So, can I just take a time out
here because I happened to see the homework upcoming. I think it's 16930, so it's a
course a little bit like this, only the first word in the
course title is "Advanced," and our first word is
"Introduction," but, of course, it's the same course
or same stuff. This is the homework that I
don't think has even been assigned, even been
handed out yet. But I just thought it's a great
example of a applied, so that you see where optimization
appears and what's involved. So differential equations are
involved, but also you got something that you
want to optimize. So I just wrote it on this board
and I simplified it a little over the homework that
they'll actually get. so here's the problem. We want a certain distribution
of heat. So I could draw a picture. We want a heat distribution for
whatever reason that maybe goes like this over the
integral zero to 1. What do we have at our
disposal to get the heat to be that way? Well, we've got sources
of heat. But we don't have a continuous
source, we only have n parameters to play with. I mean right away
you recognize an optimization problem. We're trying to get this
function here u knot of x. We're trying to match a
function, but we've only got n parameters. Those will be the
right hand side. So what we're allowed to choose,
we can put in little space heaters, and we can turn
them to the temperatures we want to -- temperatures
s1, s2, s3. That was probably a stupid
choice to put an s for down there because I don't
even know if negative heat is allowed. Anyway, we wouldn't want it
if we're aiming for that distribution. Now you understand the s's
are not supposed to match that u knot. The s's are the sources of
heat, and the u's are the outputs, the distribution, and
they are controlled by a differential equation. What we control is the right
hand side of the equation, but we only control n parameters. Then we have to solve the
equation and find out what distribution that gives. So that's all like differential
equations, we know how to do it. It's one dimensional in this
example, so straightforward. But now comes the optimization
part. We take this result
and we compare it with the desired result. So the actual result from some
source of heat might be something like that. We want this u of x, the heat
distribution that we're actually producing to be as
close as possible to u knot. Close could mean different
things, but if we measure closeness in this integral
square mean square sense, then we're going to have
nice problem. In fact, we're going
to have a linear -- everything's linear right now. Well, everything's quadratic
here, so when we minimize it's going to give us a linear
equation for the s's. I just think that's a
good model problem. A good model of what
we might do. I think actually in the problem
to be assigned -- well, it's not a big
deal for us. There is convection as well. So this is a pure diffusion
problem, just the second derivative. We know very well that if there
was a first derivative in there, the stuff's
convecting, passing through the region. So if I put on a convective
term, cdudx, how does that change the problem? Not in a big, big major way, but
one thing we can guess is that now that is not a symmetric
term, right. We've seen the difference
between first differences and second differences, first
derivatives and second derivatives. So now it's a little trickier
and it's not symmetric. So somehow there will be a
primal problem, this one, and there will be a adjoint
problem, dual problem, perpendicular problem, whatever
name you want to give it, just as there is in our very
first model over there. So that's my sort of quick look
to kind of put on the board one example that I didn't
invent, that came from applications and gives
a sort of typical of what you have to do. You control an input, you get
an output, so that's the analysis problem. Find the output. But then comes the optimization
problem -- make that output close to something
that you wish. So what's the typical algorithm
going to do? It's going to make a choice of
s, it's going to solve the analysis problem for the u,
it's going to look and see what the error is, it's going
to figure out probably the gradient somehow. What's the steepest way
to make it closer? That's going to lead us
to a change of s. We use a change of s, the new
s, solve that, and iterate. That would be a typical
algorithm. We might be able to shortcut
it in a model problem like this. But that's totally the typical
optimization idea, is an analysis problem and then figure
out a gradient to get a better source back to
the analysis problem with the new source. What the algebra, the math, has
to do is those two steps of figuring out OK, how do we
improve the error, how do we reduce the error, what's
the steepest direction? Somehow we got to compute
a derivative. Actually, that's what
this month is about. Derivatives that are not just
like the derivative of x cube or something. I often wondered how many
presidents could take the derivative of x cube
and I'm not sure. Anybody occur to you who you
could count on being able to take the derivative
of x cubed? I don't think the current
president would know what it meant. But I think Carter could have
done it, because he went to the Naval Academy. Jefferson was probably --
he knew everything. I don't know. Anybody else has another
candidate they can tell me. So there is our problem --
finding derivatives that would be definitely beyond the
capacity of the White House. Now I want to stay with this
model a little more because it's the perfect model. So this was the like model 1,
unweighted ordinary least squared, and it produced the
identity matrix in there. I mentioned last time what I
want to do, the more general model has another symmetric
positive-definite matrix in there, but not necessarily i,
and it comes from weighted least squares. So that's what I'm going
to talk about. So what are weighted
least squares. Well, you've got these
measurements and you think they maybe don't all, maybe
they're not independent, maybe they're not equally reliable. So you weight them by how
reliable they are. A more reliable one you would
put a heavier weight, w, on because you want that to
be more important. So, you change to
this problem. But it looks practically
the same. The only difference is a has
become wa, b has become wb. So the equations will be the
same, but a is now wa, b is now wb, and those are
the equations. I guess I should call, just
to make clear that u is a different u, that the best
u now depends on the choice of weights. I should really be calling that
u -- somehow indicate that it depends on the weight. Now they key nice thing is that
if I write this out as a tranpose -- can you just write this out. You get the a tranpose, and
the a is over here. But what's in the middle? What's this matrix c -- I jumped the gun and called that
matrix in the middle c, but how is it connected to w? You can see it here as c
is w tranpose w, right? It's just sitting there
in the middle. That's great that the fact
that the combination w tranpose w is all you
need to know. So we can forget w
in favor of -- I'll just put it here -- w
tranpose w is now given the name c, and this matrix is
symmetric positive-definite. So it's a great matrix and it's
exactly the one we want. This is exactly the
equation we want. So if I go back to writing it in
this -- you know, this was the equation. You remember the point that we
could go directly to one equation for the best u, or we
could keep our options open and have two equations that
led to the same one. They're totally equivalent. But the two equations will give
us not only u, but the error, b minus au, as
an other unknown. That's what we did up there. So we had two equations,
and if we eliminated e we got to this. Now I want to have two
equations, and if I eliminate e, I get to that. So let me see what
those would be. Well, here's one of them. a tranpose c -- e is zero --
you see, that's the only difference really. That the weighted normal
equation, I just took a tranpose c, and then I took the
b minus au together, and this is what I'm calling e. So one way to do it is just the
new guy is just now e plus auw is still b. But now a tranpose ce is zero. Good deal. I mean that's quite nice. It isn't absolutely perfect
though, because I've lost the symmetry. I'm putting a c in there and
I don't really want it. I want a c but I don't
want it there. So, I just make a
small change. I'm going to introduce a new
unknown that I'll call little w, apologies for the fact that
it's also a w, it just happened to fit. That'll be the ce. So I'm just calling this
a new name here. So that now my e -- of course,
if I just invert, e is c inverse w. So now I'm just going to write
these equations with w instead of e because I like it better. So this equation is now a
tranpose w equals zero. This equation e is disappeared
in favor of c inverse w plus au equals b. So that's the system
that I really like. That's the saddle point system,
the Kuhn-Tucker system, the primal dual system,
the fundamental system of the whole subject in this
linear matrix case. We haven't got functions, we
got vectors, and we've got symmetry, and we've got
linearity, and we've got a saddle point matrix that's now
the s -- well, let me just change it here. It's just changed to
this, c inverse. That's the fundamental matrix
of the whole subject. So, s is the saddle
point matrix. So I wanted to get that far. You see that the whole picture
was elementary linear algebra. Let me come back to the
elementary figure that illustrates what's
the geometry. How was the geometry affected
by introducing this guy w, this weighting matrix or the
c equal w transport w? Well, here was the picture
from last time. A right angle picture. This was a right triangle. This line was perpendicular
to that plane, but not anymore now. The second one it's a
tranpose ce is zero. That's still a line, but it's
not any longer perpendicular to the -- this is still
all the au's. This plane is still the column
space of all the au's. We have the same b, but you see
the problem is it lost its 90 degree angle, because
the projection is now a projection, it's now an oblique projection, it's slanted. This is the best au, and if I
occasionally keep up-to-date I'll put that uw there. There's still an error e, and
this is still a parallelogram but it's not a rectangle
anymore. Forgive my enthusiasm. I'm sort of happy that the
picture and the algebra both come out so neatly. I totally agree that at this
point I'm asking you to follow a model without giving you an
application, and that's one reason I threw in this mention
of a specific application that came from somewhere else. But this is the picture there. So I'll say one more word
about the picture. I said that we lost
the right angle. We lost perpendicularity,
and, of course, literally speaking we did. This is no longer -- this is
not a right angle anymore. This is not a right
angle anymore. It's not a right angle in the
usual meaning of right angles. It is a right angle in the
inner product that's associated with c. In other words, right
angles here mean x transpose y equals zero. That's the idea of a right
angle, right? x perpendicular to y, and they have different
letters here. Now over here I still have
perpendicular, but I don't -- this is not the right inner
product anymore. It should be a weighted inner
product, weighted with this c in the middle. So that's really what I
mean by c orthogonal. Maybe I'll put those
words down. So this weighted thing is -- if
I can squeeze it, I doubt if I can -- is c orthogonality,
weighted orthogonality. So let me circle the
whole thing. The c is the w tranpose w. Just to say that we aren't
giving up on dot products and perpendiculars and good
equations, we're just changing them by inserting c every
time in a product. What it means is that this is
the natural inner product for the particular problem. This is the natural inner
product for Euclid, right? But then from some specific
application like this one or a million others, they have
their own natural inner product, and the inner product
for that particular problem would be one of these guys with
some kind of a matrix c showing up. So least squares and weighted
least squares, that's my example one. Now I'd like to give a second
example, a more mechanic -- will come closer to mechanics. Because this is least squares,
statistics, algebra. But let me put on the
middle board an application out of mechanics. It will be, say, I'll make it
small and just a couple of springs with a mass between them
and fixed at both ends. So this spring extends
by some amount. This spring extends
by some amount. There's a force on this mass. So there's a force on there
mass, maybe just gravity, f. That's the external force from
the mass of that's here in between the springs. Then also acting on that
mass are spring forces. This spring is pulling
it up, right? There's a spring force, w1. Here, do you want
me to draw -- really which direction
is this? I'm going to draw it this way
just to show the w2 drawn that way would actually be negative,
because I think that this spring would get
compressed, right -- this mass is pushing it down. This spring would be
under compression. It would be pushing
the mass back up. So the w2 in that picture
would be negative. So this w1 will actually be
positive and go the way the arrow is showing. This w2 would actually be
negative and go not that way the arrow is showing. But what's the -- oh, what
equations have we got then? What's our optimization
problem? Well actually, we
have a choice. We could work with equations,
period. Actually, one of the equations
is pretty obvious. This mass is an equilibrium. So w1 is equal to w2 plus s. So it doesn't move. Or you might prefer me to write
it, I would rather write it, with w's on one side and
source terms on the other. So that's the equilibrium
equation. So what decides, we want to know
what these w's are, and these springs are extended. So that first spring is expanded
by an amount e1, and this second spring is extended
by an amount e2. Stretched or compressed. e1 is probably going to be
positive here -- that spring's going to be stretched. This spring is going to be
compressed, so that e2 is probably going to be negative. What's the mechanics here? Well, I can state it two
ways, as I said. I can state the mechanics in
terms of equations -- force and stretching elastic
constant. That's how we did it in the
first semester, 18085. It's a little clearer because
simple equation [UNINTELLIGIBLE PHRASE]. Or I can state the problem as
a minimization of energy. That's what I want to
do today in 1806. So I want to minimize 8.6. I want to minimize
the total energy. The energy in these springs. Subject to the constraint --
this constraint, equilibrium. So that's the optimization
statement of the mechanical problem. Well, I guess all that
remains is -- well, I guess what remains? First of all, I need an
expression for this energy. If the springs are governed by
Hooke's law, then it'll be pretty simple. If they're real springs that
don't quite obey Hooke's law then there'll be none -- no,
there'll be fourth degree, sixth degree , whatever,
terms in the energy. It's like the energy in the
first spring plus the energy in the second spring, anyway. e in the first spring, and the
energy in a second spring, and, of course, the two springs
could have different spring constants. So those e's -- I'll make life easy in solving,
if I choose Hooke's law, if I choose the energy
to be just a square. The constraint is linear, so
that'll be the model problem. And actually, that'll be the
kind of problem I've got here. One reason for introducing a
new example was to get some mechanics into the lecture,
but also to get a problem where we're doing a minimization
but we've got a condition on the w's. The question is how do
you find a minimum? You can't just set derivatives
of the energy to zero. You would discover w1 equal
w2 equals zero. Nothing happening. That would be the minimum. But that minimum is ruled out,
that solution is ruled out because we have this
constraint. We've got to balance
the external force. So this is the question and
you'll maybe have met this question in other courses,
but it's essential. How do you deal with minimizing
when there's a constraint? I guess in some way we
had it over here. We were minimizing something --
well, the minimum would be, take u equal u knot. But no, there was a constraint
that u had to satisfy a certain equilibrium equation. Here it was a differential
equation so that problem is a little harder than this one
where the equation is just discrete, one simple equation. So how are you going to do it? Well, actually the quickest way
would be -- that's such an easy constraint that I could
say hey, w2 is w1 minus f. So I could just, if I wanted to
really like shortcut this whole lecture, I could say
well, w2 is w1 minus f. Now I've accounted for
the constraint. I've removed w2 from
the problem. I have a minimization, an
ordinary minimization with an unknown w1. I take the derivative. I solve derivative
equals zero. I find w1, then I go back,
I get that w2. That's the fast way. Of course, gets the
right answer. But there's another way
that in the end turns out to be better. It's not necessarily better for
this simple problem, but it's better for the general
approach to constrained optimization. So I'm going to not do the
simple deal of solving for w2, but I'm going to keep the
constraint around. It's the idea of Lagrange
multipliers. You've heard those words and
probably seen it happen. So what is Lagrange
multiplier? What is Lagrange's idea? Lagrange's idea is he
constructed a function of w, which is this to work with,
which is the same energy. But he's going to include
a multiplier. Now the next question is what
letter shall I use for that multiplier, Lagrange's
multiplier. Books on optimization often
call it Lambda -- Lambda's sort of like
for Lagrange. So, Lagrange obviously wasn't
Greek, but anyway -- close. Lambda for us always
means eigenvalue -- For me always means
eigenvalue. So I'm reluctant
to use Lambda. Sometimes in books on economics,
which is we're doing this all the time, the
multiplier's called pi because it turns out to be a price. But let me use u for the
Lagrange multiplier. So that'll be the Lagrange
multiplier. So what do I do with it? I multiply this equation by u
and I build it in to this. So this thing is this part -- I'll just copy that down there
-- plus or minus, depending what I want, depending which
sign I want to end up with, u the multiplier times the
constraint -- w1 minus w2 -- let me put it like that. So the constraint is that
this should be zero. So you can say I haven't
added anything. I've added zero. But that will only be true
at the end when I have a specific w1 and w2. Right now what I've done is I've
built in the constraint into the function. Lagrange's brilliant idea was
that now I've got a function that I can -- my derivatives
I can take. I could set the derivatives to
zero. dldw1 is going to be zero. dldw2 is going
to be zero. And dldu is going to be zero. So we got now three equations,
three unknown. Instead of going from two
to one, we've gone from two up to three. But it's so much more systematic
that it's the right thing to do. What is this last equation? What's the u derivative
of an expression? Well, u doesn't appear here, the
derivative is just w1 -- this just leads us to w1 minus
w2 minus f, which is the constraint that we wanted. So the constraint is showing
up as this equation. Just the way the constraint
showed up as this equation here. I guess what I want to say is
when I wrote this down, when we did this first example,
I didn't say here's the constraint. I mean that equation kind of
came out from the normal equations here. So what we're doing new here is
the constraint equation is sort of part of the mechanics
and we're asking the question how do I deal with it. Here it came out of the
geometry, so it wasn't there at the beginning so we didn't
have to say how do we deal with this equation. It just emerged. Here it's really forced
on us right away. So anyway, we've still got to
figure out the derivatives of those, and I guess I do have
to finally now say what choice -- yeah. So this is -- what is this, the derivative
of e1 minus u. If I take the w1 derivative,
I've got the derivative of v1 with respect to w1. Then w1 doesn't appear there
but it appears here, so it's minus u. This would be the derivative of
v2 with respect to w2, that second spring minus --
oh maybe plus cu. Well, those are the
three equations. Let me move now to the linear
case, just so we see the beautiful pattern. So if I make the equations
linear, what's the energy in Hooke's law spring here if the
extension is e1 and if it produces a force of w1, I want
to know what is this e1 here. So let me just remember
Hooke's law. I think Hooke's law would say
that -- well, there's some elastic constant c1, so there's
a c1 and a c2 that tell us how hard or soft
the springs are. So these are physical
constants. If I remember right,
the energy -- so I'm going to erase a little
here just to -- well no. So what is the energy in
that first spring? You remember, there's a 1/2
of an c1 e1 squared. That's the energy in a spring
with constant c1 and the stretch e1. But now I really want it not in
terms of e1, I want it in terms of w1, just the
way I did here. I've got to do the
same thing here. I want to get to w, because
the constraint is in terms of w. What does Hooke's law say? Hooke's law says w equals ce,
that's Hooke, Hooke's law. The fourth is the elastic
constant times the stretch. So in place of the e
I have w over c. So this is 1/2, e1 squared will
be w1 squared over c1 squared, and then a c1 cancels
-- that's what it looks like. I guess I'm certainly happy
to see that I'm coming up with c inverse. This c is showing up in the
denominator, and that's exactly the -- so this is what
I mean, this is what I want for my energy is a 1/2, w1
squared over c1, and a 1/2, w2 squared over c2, and now I was
doing a minus sign because that kind of goes well with
mechanics where a plus sign would go well with all the
other applications. So now I've got explicit
energy. So now I can say what
this thing really -- the derivative with
respect to w1. OK. The derivative with respect
to w1 is just w1 over c1. Is that what I'm getting? This is zero. This says w2 over c2 minus u is
zero, and this one says w1 minus w2 equals f. Our three equations. Yes, so I will help kill these
equal signs and just look at -- oh, that's a plus. We've got our saddle
point matrix again. That's the nice thing here. This is a problem with a 3 x 3
matrix, with three unknowns, w1, w2, and u. With right hand side
zero zero and f. With what matrix? That looks like a 1 over
c1, zero and a minus 1. This looks like zero, a 1
over c2 and a plus 1. This looks like -- oh,
wait a minute. I don't want this. w1 minus w2 equal f. I can live with 1 and minus 1
there, but it's not really what I wanted. I wanted the signs to -- you
know what I want here. I want the transpose of that to
be here, and zero to be in that block. So that's what my saddle point
matrix would look like. Well, let me just say that
I could live with either. I was aiming for this one
because it's symmetric. But a lot of people would rather
have the opposite signs and have the 1 minus 1 there. I don't care which sign
f has, of course. Some people these days are
liking this form better because then it has a symmetric
part and an anti-symmetric part. I mean the thing is at some
point we're going to get problems like this with
thousands of unknowns, and we're going to think how
do we solve them and maybe some iteration. So we might want the matrix to
be symmetric but indefinite, or we might want a
positive-definite symmetric part and an anti-symmetric
part. What we can't have is positive
definite-symmetric. That's like asking for what
can't happen here. The combination of the problems
is producing a saddle point and we can play with that
sign, but we can't make that zero something -- oh,
we could actually. What I was going to say is we
can't make that zero something different, but you could. That would be a possible
way to -- it's another way people thought
of of solving these problems is artificially throw
in a big number there, or even a small number, and push things
towards positive. Anyway, my purpose today is
essentially completed, that we're getting out of physical
application after I linearized and it became a linear equation
and it had that saddle point form. So, saddle point form here,
saddle point form here, saddle point forms everywhere. I mean we'll have the saddle
point forms for differential equations, as well as for
matrix equations. So those are the examples to
sort of hang on to, and it's section 7.1 that has
a big part of it. Ah -- I always have one last
thing to say. What's the meaning of u? So, u was a Lagrange
multiplier. Lagrange just like helped us out
by saying OK, there was a constraint by using one
of my multipliers. But the point is that
the multiplier always has a real meaning. I mention prices before. Here, what's the meaning of u? What's the physical meaning of
the Lagrange multiplier. It turns out to be the
displacement of the math. It's the dual variable,
so it always has some interpretation. In this case, with mechanics
it's the amount, the mass comes down when the
force asks. What's more, it also has a
derivative interpretation. Turns out -- I'll just put turns out that
the Lagrange multiplier u turns out to be the derivative
of the minimum energy in the system with respect to
the source term. It's the sensitivity of
the problem somehow. I'll just use sensitivity. You often want to know -- it's actually this quantity
that we need over there. We want to know how much does
the answer depend on the source, and that's what the
Lagrange multiplier tells us. So, if I computed the solution
here, so the notes do this -- maybe I'll leave it
for the notes. The notes solve this
little problem. That's easy to do. They figure out what the energy
is in the springs. It depends on the right hand
side, f, which is just a number here. The energy turns out
to be quadratic. You can take its derivative and
you find out that it's u. So that Lagrange multiplier,
that's really a key message, is an important quantity
in itself. Here it happens to mean
displacement, which is obviously crucial quantity, and
in general it tells us the change in the minimum energy
with respect to a change in the input. That sensitivity's a natural
word to use for that. So that's a final word about --
well, a near to final word about Lagrange multipliers. So I'll see you Wednesday and
by that time I'll know more about the projects and we'll
be moving onward with optimization. Good. Thanks.

AI

The following content is
provided under a Creative Commons license. Your support will help
MIT OpenCourseWare continue to offer high quality
educational resources for free. To make a donation or
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. AMARTYA SHANKHA
BISWAS: So today, we're going to look at approximation
algorithms for the traveling salesman problem. So I hope everyone knows
what the traveling salesman problem is. You have a graph, you're
trying to visit every vertex. So you start at your vertex,
you visit every single one, and you end at the
starting vertex. And you want to do that in
the shortest possible time, or distance, or
whatever the metric is. So, unfortunately,
this is NP-hard. Also, the approximation
algorithms for any constant approximation
is also known to be NP-hard. So you should have gone over
approximation algorithms in lectures. So, basically, let's say
the optimal solution has some value of e, and if
you want to guarantee that your algorithm will
end up with a value which is within a constant factor
of v, so less than cv, that is an
approximation algorithm. But for the traveling
salesman problem, the constant approximation
algorithms are also NP-hard. So instead, we
modify it slightly. So, on the traveling
salesman problem, we impose something
called a metric. So the important relation
here is this one. So, first of all,
let's go through this. So you have a distance metric
for-- so xy are vertices. Your distance is always greater
than 0, which is reasonable. You're undirected,
which is this relation. And you have the
triangle inequality, which means that if
you have 3 vertices, and you have distance like
this, this distance is always smaller than the sum of
these two distances, which is like real world-ish things
that make sense, right? If this distance was
longer, it would just take this path instead. So the distance by
taking other node is always greater than or
equal to the direct distance. So, turns out, the Metric
TSP problem is also NP-hard. So nothing very
great there, but you can do a constant
approximation here. And you'll go through
two approximations today. The first, the simpler
one, is a 2 approximation, and then, we'll
improve this to 3/2. So let's start
with the first one. So before we begin, let's
define a couple of terms. Let's define c of
S. So what is S? Let X be a path. Or, rather, let's say
S is a set of edges. So if you have your graph,
here, your set of edges could be something like
this, this, this, this. So that's set of edges. And c of S is defined as the
sum of the weights of all the edges. And this, actually,
should be a multiset, because you can count
the same edge twice. So you can count this
edge three times, or how ever many times you want. So that's the definition. So, now, we want to
find a cycle which goes through all
the vertices, and we want to minimize the
cost of that cycle. So let's say your optimal--
this should remind you of the Hamiltonian
Cycle problem. So this is, essentially,
finding a Hamiltonian cycle in the graph of minimum weight. So this is worse than the
Hamiltonian cycle problem. This is find the minimum
weight Hamiltonian cycle. So let's say there exists this
beautiful Hamiltonian cycle. You know of minimum weight. Let's call that
H star G. So it's the best Hamiltonian
cycle on this graph, and that's the shortest path
that your traveling salesman can take. And so the cost of that is
[INAUDIBLE] if H star S G. So how do we go about trying
to approximate this problem? So think about what
algorithms you've seen that sort of connect
all the vertices of a graph, and minimize costs of edges. Does it remind you of
anything polynomial that you've seen in the class,
that connects all the vertices? Expands all the vertices? Exactly. So [INAUDIBLE],
minimum spanning tree. This minimum spanning
tree is polynomial time, and it connects all the vertex. So let's take some graph. Let's just go with this one. So you have some vertices,
and you make a minimum spanning tree out of them. Now, clearly, this
is not a cycle yet. But let's try to
construct a path out of this minimum spanning tree. So, first, let's root it. So let's say we are rooted
minimum spanning tree. Let's say this is the root. So you have three things
going out of there. I think that's it. So let's give these labels. So, now, what we're
going to do is, the way we're going to
traverse all the vertices, let's just do a DFS. So, in DFS traversal,
you'll first see 1, then you'll
go down and see 2, then you'll go down and see
3, then you'll go back up, see 2 again. Go back down, see 4. Go back up, see 2. 1, 5, 1, 6. So, basically, you're ignoring
the rest of the graph. You find your minimum
spanning tree, and you follow all the paths. Follow them back up, and
just do a DFS traversal. And then, you go
reach back to 1. And then, you have this-- well,
it visits all the vertices. It visits some of
them more than once, which is a problem, which
we'll deal with shortly. But it visits all the
vertices, and it's a cycle. So, now, the problem is,
the traveling salesman problem does not allow you to
visit vertices more than once. Because, if you did not
have this restriction, you could shorten
your bat length by going to a separate
vertex and coming back, or something like that. So let's make that
more concrete. So, at least in this case,
given this triangle inequality, I claim that you can just
delete the duplicate vertices. So let's look at
the duplicate ones. You have 1, 2, 3, this repeats. So you delete that. Repeats, repeats, repeats,
and that's cycling back. So how do you delete things? So, in this case, you
had a path, right? You were going 1 to 2, and
2 to 3, to 2, and so on. Let's write the 4 in. So this is [INAUDIBLE]
only at tree edges. So how would you delete this? So, let's say, you find
the first duplicate vertex, and you don't want that. That's not allowed. So all you do is, simply,
you follow the path, and you bypass it. And by the triangle inequality,
you know that bypassing it will never increase your cost. It'll decrease it, or it
will keep it the same. So you can remove the
duplicate vertex in this path just by bypassing it. But, also, remember that
this is a complete graph. So the metric is defined
on all pairs of vertices. So every edge exists
with some value. So that also follows with
the triangle inequality. So if an edge does
not exist, just make it the sum of the--
so if xz is not an edge, just make it the
sum of xy plus yz. So in any case, you construct
the initial path just by going down the tree and
doing a naive DFS traversal. And, then, you correct
that path by skipping over the duplicates. So, finally, we'll end
up with [INAUDIBLE] skipping all the duplicates
we'll get a 1, 2, 3, 4, 5, 6, 1. And that's a valid cycle. So let's call this
minimum spanning tree T. So that's your MST. And you are removing duplicate
edges, and getting a cycle, C. So now, actually, let's
take another step back. OK, let's define
our cycle, first. So let's call this
cycle, this guy is C. And then,
you're going from C, and you're deleting
the duplicates, and you're getting C dash. So now what you have
is cost of C dash. That's a [INAUDIBLE],
but anyway. The cost of C dash is less
than equal to cost of C. And what is the cost of C? If you know the weight of
your minimum spanning tree, what is the cost of C? C is just doing a [INAUDIBLE]
of this traversal. So what is the cost
of C, if you know your minimum spanning tree? STUDENT: [INAUDIBLE] AMARTYA SHANKHA BISWAS: No. So you're traversing every edge
in the minimum spanning tree twice. So you're doing a
DFS traversal, right? So you're going down, and
you're coming back up. So you're going down, coming
back up, backtracking, coming back down. So every edge is
traversed, right? So it's exactly twice T. So let's bring up
a different board. So you know that
cost of C is twice the cost of T. Does
that make sense, why the traversal implies
that you have every edge being visited twice, right? OK. So, now, our claim is that the C
dash cycle is a 2 approximation of the actual cycle. So why is that? So we've already
[INAUDIBLE], so this also implies that C of C dash is less
than equal to 2 of C T. Realize that this is not a valid cycle,
but this is a valid cycle. So, now, we need to show
that this is somehow bounded by a star G.
So how do you do that? Well, look at H star
G. What is H star G? H star G is just a cycle, which
goes through the optimal cycle, which goes through
all the vertices and comes back to
the parent vertex. So this is H star of G.
This is the optimal thing. Now, you can take an edge,
e, here, and delete it. And then you'll get
a spanning tree, because this is
your optimal cycle. Remove one edge, and
you get a spanning tree. So let's call that T dash. Does that make sense, why
that is a spanning tree? Because you had a cycle,
and you remove one edge, so it touches all the
vertices, and it's a tree. So it's a spanning
tree, but it's not the minimum spanning tree. So you know that H star G,
the cost of H star of G, is greater than equal
to the cost of H star of G minus the
edge we removed, is greater than equal to
the cost of T. Make sense? So you remove one
edge, and then that is still greater than the
minimum spanning tree. So, now, combining
this guy and this guy, you get cost of C dash is less
than equal to 2 [INAUDIBLE]. We know that cost of C is
less than cost of H of G, so you get a 2-approximation. So does that make sense? So that was a 2-approximation. That was pretty
straightforward, We just constructed a spanning tree. You did a DFS traversal
and removed duplicates, and you have a nice path. OK, let's just keep it down. But here's the thing. It seems kind of wasteful
to go through all the edges when you don't need to. So you're traveling down the
tree, you're going back up, and you're traversing
every edge twice. So it seems kind of
ridiculous that you would be doing every edge twice. So what could you do better? Well, before we introduce that,
let's prove a couple of lemmas. So, first, we started with this. So let's say S is a subset
of V. So you have a graph, and you make a subgraph. So you pick out some vertices. So you pick out this one,
and this one, and this one, and this one, and
that is your S. And, so, you get
a new graph which contains just those vertices. And, whatever, I
just connect them. So the claim is
that that graph also has a Hamiltonian cycle,
the minimum cost Hamiltonian cycle, which is also the
traveling salesman solution. So let's call that H star of
S. So it's some cycle, which looks like this, I guess, here. So H star of S. Now, the claim
is that the cost of H star of S is less than equal to
the cost of H star of G. That should make
intuitive sense, because you're taking
only some of the vertices and trying to traverse
them, and, in this case, you'll try to traverse
all the vertices. However, this is only true
because of the triangle inequality, and let's
see why that is the case. So, proof by contradiction. Say cost of H star of
S is actually greater than cost of H star of G.
OK, so, look at H star of G. It's a cycle, right? So you have something. Your cycle. And, in this cycle, you
have all the vertices of S. So pick them out. And, now, you have
a cycle which has cost less than the
optimal cycle in S, but it contains all the vertices
in S. So what you can do is, now you can use the
skipping lemma from before. So, last time, we didn't
move duplicate vertices. But, this time, what you'll
do is, instead, you'll just skip over this vertex. We'll skip over this vertex. And so, every vertex that's
not an S, skip over it. And that can only
decrease the cost. So, now, you've constructed
another cycle, which contains only the
vertices of S. So it's a Hamiltonian cycle for S, but
it has cost less than equal to H star of G, which means
that this can never be true. So the important fact
is, there, that if you have a subset of vertices
making the restricted graph, the cost of the minimum
Hamiltonian cycle is always less than equal to
the one in the original graph. So, intuitively, that
should make sense. OK, next thing is
something which might seem unfamiliar right
now, perfect matchings. So you've seen perfect
matchings in the content of bipartite graphs, right? So you find the minimum
cost perfect matching. You do this flow thing,
you send all the flow in, and then you connect the
vertices with the capacities, and whatever. So, it turns out,
in a complete graph, you can still do
perfect matching. So perfect matching is, you have
a bunch of-- so, let's say, you need to have an even
number of vertices, right, until we have
perfect matching. So, let's say, you
have these varieties. So this is a perfect matching. So every vertex has one edge
coming out of it, exactly one edge coming out of it. And it needs to be even,
because, otherwise, that doesn't work out. So that's perfect matching,
and the minimum cost perfect matching is the
minimum among all such things. And you did this for
bivariate graphs. It's finite
[INAUDIBLE] networks. So I'm not going to go into
the algorithm for this. It's kind of complicated, but
it uses linear programming, and you can find this
for a complete graph, and it's polynomial,
so that's good. So, given a complete graph, you
can find the perfect matching. [INAUDIBLE] for now. OK, one last thing we want to
introduce is Euler circuits. So who has heard of
Euler circuits before? Anyone? Sort of? OK. So the reason we
are-- so, OK, let's go back to what we did before. We had a tree, and the best
way we found to traverse it was just going down, and
going back up, and going down, and terribly messy. So what we would,
rather, want to do, is sort of traverse the thing
without repeating edges. So, I don't know,
you've probably seen this puzzle before. So you have this
graph given to you, and the task is to draw
this graph without lifting your pen off the paper. So first, for example,
in this graph, you could start here,
go here, go here, and come back, and
something, and there you got. So you can do that. But if you add on another
lobe, here, then you can't make a circuit. You can still make
a path, I believe. If you add another one,
you can't even make a path. So how does this work? Let's see. So, let's say, forget
about the graph, for now. Let's say you're just drawing. So you start somewhere. You go to a vertex. You go to another vertex. Come back, go to this
vertex, leave it. And so, observe that, whenever
you're making this drawing, you go to a vertex,
and you leave it. Every time you hit a
vertex, you leave it. Since there's the circuit,
you just loop around, and every time you
enter a vertex, you'll have to leave it. What that means is that,
even though this is not directed, if you drew out the
actual path, you would see, the number of edges
going into a vertex is equal the number
of ones leaving, which means that every
degree has to be even. So if you go and look at this
graph, which is that of lobes, this degree is not even. Neither is this, neither
is this, neither is this. They're all 5, I think, yeah. They're all 5, which
means that this can never have an Euler circuit. So a graph can only
have an Euler circuit if it has even degrees
for every vertex. And the other way is also true. If a graph has even
degrees on every vertex, then it must have
an Euler circuit. That's not hard to
prove, but there's a constructive
algorithm you can use. So, let's say, you're given
this graph, for instance. You would simply
go to the graph, just start at some random
node, and then go through, and keep following
edges until you can no longer following edges. So, let's say, you stop here. Then, you pick another
edge, and start, and so on. And, then, you can splice these
cycles together at some point. So it's kind of a
[INAUDIBLE] argument, but it should be
sort of intuitive why you can construct another
path, given an even degree. You just perform searches. You just create cycles, and
you splice them together. But, for now, just
take it as fact that a graph is
an Euler circuit, as in you can draw it
without lifting your pen off, if, and only if, every
vertex has even degree, so why is that interesting? So, let's say, we could
add some edges to our tree and turn it into one
of those nice graphs. Right now, this is not, right? This is degree 1, degree 1,
degree 3, degree 1, degree 1, degree 3. All of them are odd,
so that's not good. But, let's say, you could
add some edges in, turn it into an Euler
circuit, and then you could do a nice reversal off it,
and, maybe, that will give you a better approximation. So with that hope, let's
look at the algorithm. So what you do is, you
go back to your tree. Let's just leave it at that. So, now, let's see. So this is degree
2, that's good. This is degree 3,
that's not good. Degree 1, this is fine. This is degree 1. This also is degree 1. This is degree 3. Actually, let's get rid of this,
so it does not have degree 3. That's a lot of vertices. OK, there we go. So, in this graph,
you see that you have 1, 2, 3, 4, 5 vertices
which have degree odd. So, now, we would like to
add some edges to turn this into an Euler circuitable graph. So how do we do that? So let's call the set of odd
edges S. Odd vertices, sorry. So you take the set of
odd-degree vertices. Now, go back to
perfect matchings. So what does a
perfect matching do? It adds edges to that
graph so that everything gets degree increased by one. So if you increase the degree
of all the odd vertices by 1, everything turns even, right? So you take the set
of odd vertices. OK, another thing to observe. Realize that, how many
odd vertices can you have? Can you have an odd
number of odd vertices? Because that would screw
up the whole thing where we needed an even number of things. So why is that not possible? Why can you not have an
odd number of odd vertices? So the thing is, that, let's
say, you have some graph, and what is the sum of
the degrees of the graph? Let's move to a
different board for this. So you have a graph, G, and
you want sum of degrees. So di is the degree, for all
V. So what is this equal to? So, let's say, you
have this graph. So, now, if you count the
degrees of every vertex, you're basically counting the
number of edges coming out, which means that every
edge is counted twice, once for this end point and
once for this end point. So this edge is counted
twice, for here and here. This edge is also counted twice. So, basically, the
sum of the degrees is nothing but 2 times mod
of E. Does that make sense? So this is even. Now, let's say you take
only the odd vertices out. So the even vertices are good. This is good. This is good. This is not good. This is also not good. So these are the odd vertices. So the sum of the degrees
of the even vertices are, by definition, even. So, if you remove them,
the sum of the odd vertices should also remain even. And so, you have the sum of
odd degrees becoming even, which means that you'll need
to have an even number of them. Make sense? So, going back to this, so
there's a lot of branching off, here. But going back to
the main point, here, is that you have an
even number of odd vertices. So consider the restriction
of the original graph, G to this set, S.
So, now, we're going to the first thing
we did, there, where we considered a
restriction of the graph to a certain set of vertices. So we take the set,
so 1, 2, 3, 4, 5. So you take these five vertices. And you consider the graph that
is restricted to these five vertices. Oh, sorry, there should be six. Oh, that's interesting. Oh, there we go. That's the other one. So those are the six vertices
which have odd degrees. Now, you find the
perfect matching with your polynomial-time
algorithm, and you get something. So you can now add those
edges back in here. So these are your new
edges, and, let's say, something, this one. OK, so you get three new edges. And, now, realize that all
the degrees are now even, so now you can do
your Euler circuit. Also, let's call this matching
M. So this is a set of edges, M. Let's call the
original tree T, and the new thing that
is formed, is T union M. So you're taking all
the edges from T, and you're adding
all the edges from M. So, realize that you
can have multiple edges, but that's fine, because
Euler circuits allow you to have multiple edges. So, now, you take
this graph, and you find this Euler circuit. So that is basically this thing. So this set of edges in some
order, and that order exists. So the cost of the
Euler circuit-- let's call it C-- is equal to
the cost of T plus the cost of M, right? Because you're traversing
all the edges in your graph. So that is the cost
for the Euler circuit. Now, my claim is that-- so this
is the difference between all the nodes in the graph. And, remember, you can do the
whole duplication argument, from before. So, where's that? Oh, there. So you can do a
duplication argument, so you're visiting all
the edges in the graph, all the vertices in the graph. You remove the duplicates,
and you have a valid path. Now, let's see if this
valid path is actually a better approximation. So, again, you will go
from C. So this C, you will go to C dash. And this will give you,
as before, cost of C dash is less than equal
to the cost of C, So, now, you're only
interested in cost of C. So what are we doing there? So we know that the cost
of C is equal to cost of T plus the cost of M. So,
from the previous problem, we know that the cost of T is
less than equal to the cost of H star of G, right? So this is less than
equal to cost of H star G. So this guy is less than the
optimal Hamiltonian path. What about this guy? So let's look at the
actual H star G, again. So, actually, let's
look at H star S. So remember that S is
the set of odd vertices, and that is where
this matching is done. So H star S is nothing but
the optimal Hamiltonian cycle on that restricted graph. From our previous lemma,
which is down here, we know that H star of S
is less than H star of G. So we know that cost of is less
than equal to cost of whatever, G. So, now, let's just
look at H star of S. Now, we construct a matching. We take every other edge. We take this one, and we take
this one, and we take this one. And look at the
alternate set, also. So take this one, and
this one, and this one. So look at the blue
set and the red set. Since it's part of
a Hamiltonian cycle, they're both matchings, right? So let's call the red one
M1, and the blue one M2. So I'm not saying they're
perfect matchings. They're not the
minimum matchings, but they're definitely
perfect matchings. And because we know the
perfect matching is M, or the minimum matching
is M. Cost of M is less than equal to cost of
M1, and, also, the cost of M is less than equal
to the cost of M2. That make sense? Because M was the
minimum cost matching, you have cost of M is less
than any other matching, which is constructed from this. Again, so this implies
that the cost of M is less than equal
to half cost of-- OK, let's not write this here. Let's get a different board. So cost of n is less than
equal to half cost of M1 plus cost of M2. Since both of these guys
are larger than this, their average is also
larger than this, right? But what is cost
of M1 cost of M2? This is nothing but half
cost of H star of S, right? Because the Hamiltonian cycle
is constructed from M1 and M2. And by our lemma,
this is less than equal to half cost
of H star of G. So, now, we have all we need. This is less than equal to
half cost of H star of G. And then, add them together,
you get cost of C dash is less than equal to
cost of C, is less than equal to the sum, which is equal
to 3/2 cost of H star of G. So that's the proof. So there's a lot of
things going on, here. Let's try to go back
and see what we did. So we took a minimum
spanning tree, and then we tried to remove
all the odd degree vertices. So we took all the
odd degree vertices, and made a perfect matching, the
minimum cost perfect matching. So we added edges just to
make everything even degree. Then, we took the Euler
circuit on that graph, and we removed duplicates. So that's fine,
that's the easy part. But you do [INAUDIBLE]
circuit on that graph, then you argued that, because
all the circuits in that graph is just the sum of the
edges in the spanning tree plus the sum of the edges
in the matching, which you added later. So the spanning tree
had already bounded, in the previous argument. The matching was bounded by
taking the optimal Hamiltonian cycle, decomposing it
into two matchings, arguing that those
two matchings are not the optimal matchings--
not necessarily the optimal
matchings, but they're less than equal to the optimal. So they're even, there. And, so, the cost of
the optimal matching, which you were using in
our constructive path, constructive
Hamiltonian cycle, is less than equal to
both of these matching. You add them up, you get
that bound, and it works. So questions on
any of the steps? This is a lot of branching off
and then coming back together. Yeah. STUDENT: The last
one's 3/2, right? AMARTYA SHANKHA BISWAS: Yes. That is not 3 over 3. That would be p equal to np. Anything else? You're free to go, or ask
questions, or whatever.

Diff. Eq.

The following content is
provided under a Creative Commons license. Your support will help
MIT OpenCourseWare continue to offer high quality
educational resources for free. To make a donation, or to
view additional materials from hundreds of MIT courses,
visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: Now, today I need
to get started by finishing up what I did last time. Namely, talking about
numerical methods. And I want to just
carry out one example. And then I want to
fill in one loose end. And then we'll talk
about the unit overall. We were talking, last time,
about numerical integration. I'm going to illustrate this
just with the simplest example that I can. We're going to look at the
integral from 1 to 2 of dx / x. Which we know
perfectly well already is the log of x evaluated
between 1 and 2, which is ln 2 - ln 1. Which is just ln 2. Now, if you punch that
into your calculator, you're going to get
something like this. I hope I saved it here. Yeah. It's about 0.693147. That's more digits than we're
going to get in our discussion here. Anyway, that's about
how big this number is. And the numerical
integration methods will give you about as
much accuracy as you can get on the function itself. And, of course,
some functions we may have more trouble
approximating. But the function 1 / x, we
know pretty well how to do, because we know how to divide. So since the function that
we're integrating here is 1 / x, it's going to be not
too difficult to get some arithmetic. Nevertheless, I'm
going to do this in the simplest possible case. Namely, just with two intervals. Now, you really can't expect
things to work so well with two intervals. That's a pretty ridiculous
approximation to your function. When you have two
intervals, that means you're looking at the
graph of this hyperbola. And you have 1 here, and you
have 2 here and you have 3/2. And you're really only
keeping track of the values at these three spots. So the idea that you can
approximate the area just by knowing the values
of three places is a little bit of a
stretch of the imagination. But we're going
to try it anyway. Now, the trapezoidal rule
is the following formula. It's delta x (1/2 the first
value + the second value + 1/2 the third value). In this case, the pattern
is 1/2, 1, 1, 1, 1, 1, 1/2. And in this case, delta x =
1/2 because this interval's of length 1. The b - a, right. Let's just point that out here. Here, b = 2. a = 1. b - a = 1. And the number n is 2. And so, delta x, which
is (b - a) / n, is 1/2. So here's what we get. And let's just see
what this number is. It's 1/2 of the value at here. Well, so let's just check
what these values are. This value is 1, this
value over here is 2/3, and the last value is 1/2. Because the function, of
course, was y = 1 / x. And those were the three
values that we have. So y_0, this one is
y_0, this one is y_1, and this one is y_2. Now, here we have 1/2*
1 + 2/3 + 1/2 * 1/2. Now, on an exam,
I don't expect you to add up long messes
of numbers like this. When you have two
numbers, I expect you to add them up if they're
reasonable, or subtract them. Just as we do when we
take antiderivatives. Like, for example,
I don't want you to leave the answer to
an integration like this in this form. I want you to simplify
it at least down to here. And I of course don't
expect you to know the numerical approximation. But I certainly expect
you to be able to do that. On the other hand,
when the arithmetic gets a little bit long,
you can relax a little bit. But I did carry this
out on my calculator. Unless I'm mistaken,
it's about 0.96. It's pretty far off. So remember what it was. It's what you get when you
get these straight lines. And there are these little
extra pieces of junk there. Now, don't trust that
too much, but the point is that it's far off. So now, let's take a
look at Simpson's Rule. And I claim that Simpson's
Rule is surprisingly accurate. In this case, really,
even a little more than it deserves to be. The formula is (delta x
/ 3) (y_0 + 4 y_1 + y_2). So the pattern is
1, 4, 1, or 1, 4 and then it alternates 2's and
4's until 4, 1 at the very end. And if I just plug in the
numbers now, what I get is 1/6, because
delta x = 1/2 again. And the value for y_0 was 1. And the value for y_1 was 2/3. And the value for y_2 was 1/2. So here's the
estimate in this case. And this one I did
carry out carefully. And it came out to 0.69444. Which is actually
pretty impressive, if you think about it. Given what the logarithm is. Now, what's going on with
Simpson's Rule in general is this. If you-- Simpson's
minus the exact answer, in absolute value, is
approximately of the size of (delta x)^4. That's really the
way it behaves. Which means that if
delta x is about 1/10, so if we had divided this
up into 10 intervals, which we didn't, but if
we'd divided it up into 10 intervals, then you could
expect that delta x-- the error would
be about 10^(-4). In other words, four digits of
accuracy here for this thing. But the exact analysis of
this, a more careful analysis of this, is in your textbook. And I'm not going to do it. But I just want to point out
that it is an effective method. It really does give
you nice four-digit with manageable-- you could
even really do it by hand. It's so convenient,
the Simpson's Rule. Whereas the other rules
aren't really that impressive as far as giving fairly
accurate answers. The last little
remark to make is that the reason is
that Simpson's Rule is matching a parabola. And somehow the parabola
follows this curve better. It's giving the exact answer. So I'll mention this. Simpson's Rule is derived
using the exact answer for all degree 2 polynomials. In other words, parabolas. All parabolas. But even all the
ones of lower degree. So straight lines would
work, and constants would work as well. Whereas the other ones only
work for, say, straight lines. The trapezoidal rule only
works for straight lines. But there is a weird accident. It turns out that it
also works for cubics. Once you get the formulas,
it works for cubics. So it's also exact for cubics. And that's what explains
the fourth order validity. The last thing that
I want to point out is that this is extremely
vague, what I said there. And you should be a little
bit cautious about it. You need to watch out
for 1/x for x near 0. All bets are off if the
function is singular. And there's a lot
of area under there. And it's also true that if
the derivative messes up, you're in trouble too. You really need for the function
to be nice and smooth in order for Simpson's Rule to work. This is wath out. That's a real wath out, but
we'll try to-- Watch out. Watch out for whenever x near 0. Then this thing doesn't work. This thing really depends
on bounds on derivatives. But I'm going to be
relatively vague about that. I'm not attempting to give
you an error analysis here. OK, so if you were
doing this on an exam, how do you remember this
strange pattern of numbers? The one thing that I want to
recommend to you is, as a way of remembering it, so
the one mnemonic device, we'll call it a
mnemonic device here, for remembering what it
is that you're doing, is to remind yourself of
what happens for the simplest possible case. Which is f(x) = 1. It seems very modest, but
if it doesn't give you the exact answer
for f(x) = 1, you've got the wrong weightings. And here, if you check out what
happens in the first formula here, y_0 / 2 + y_1 +..., well,
we'll go all the way to y_(n-1) + y_n / 2. If you check that
formula out here, this is the trapezoidal rule. If you check it out for
this case, then what you get is that this is equal
to delta x times what? Well, all of these are 1's. And how many are
there in the middle? There are n - 1 of
them in the middle. So it's 1/2 + n - 1 + 1/2. At the tail end. So all told it's delta x n. And I remind you that
delta x = b - a / n. So, delta x, this thing,
is equal to b - a. And that's just as it should be. What we just calculated is an
approximation to this integral here. Which is just the area of
the rectangle of base b - a and height 1. Which of course is b - a. So this is the check that
you got your weighted average correct here. You've put the correct
weightings on everything. And you can do this same
thing with Simpson's Rule. And match up those quantities. There was a question in
the room at some point. No, OK. So now, the next thing
I want to do for you is the loose end
which I left hanging. Namely, I want to compute that
mysterious constant square root of pi / 2. This is really one of the
most famous computations in calculus. And it's a very,
very clever trick. I certainly don't expect you
to come up with this trick. I certainly wouldn't
have myself. But it's an important
thing to calculate. And it's just very useful. So I'm going to
tell you about it. And it's just on the subject
that we're dealing with in this unit; namely, slicing. Or adding up. So the first step, which is just
something that we already did, was that we found the
volume under this curve. This bell-shaped
curve, e^(-r^2). But rotated around an axis. Rotated around this axis. Around this way. So we figured that out. And that was a relatively
short computation. I'm just going to remind
you, it goes by shells. We integrate the whole
range from 0 to infinity. And we have 2 pi r
2 pi r e^(-r^2) dr. So this again is the
circumference of the shell. This is the height of
the shell, and this is the thickness of the shell. Circumference,
height, thickness. So we're just taking a little
piece here and sweeping it around. And then adding up. And then this antiderivative
is pi-- -pi e^(-r^2), evaluated at 0 and infinity. And we worked this
out last time. This is pi. It's pi (1 - 0). Which is pi. So the conclusion
is that V = pi. We already know that. Now, the problem that
we want to deal with now is the problem not of
a volume, but an area. And this looks quite different. And of course the answer
is going to be different. But let's do it. So this is this
question mark here. And I'm going to do the one
from minus infinity to infinity. And I'll relate it
to what we talked about earlier in this unit,
in just a couple of minutes when I show you the procedure
that we're going to follow. So here's the
quantity and now, what this is interpreted as is the
area under this bell curve. This time, Q is really an area. Now, what's going to turn
out to happen, is this. This is the trick. We're going to compute
V in a different way. And you'll see it laid
out in just a second. We will compute V by slices. We're going to slice it
like a piece of bread here. We're going to solve for
that same thing here. And then, amazingly, what's
going to happen is that we will discover that V = Q^2. That's going to be
what's going to come out. And that's the end of the
computation that we want. Because actually we
already know what V is. We don't want to read
this equation forward. We want to read
it the other way. We want to say Q^2 = V,
which we already know is pi. And so Q is equal to
the square root of pi. I haven't shown this yet,
this is the weird part. And I'm going to put
it in a little box so that we know that this
is what we need to check. We need to check this fact here. We haven't done that yet. Now, let me connect this with
what we did a few days ago. With what I called one of
the important functions of mathematics besides
the ones you already know. And so the function
that we were faced with, and that we discussed,
was this one. And then, we were interested
in the value at infinity. We were interested in this. Which, if you draw
a picture of it, and you draw the
same bell curve, that's the area
under half. of it. That's the area starting
from 0 and going to infinity. That's the area under half. So this chunk is F of infinity. And now I hope that this
part of the connection is not meant to be fancy. The idea here is that
Q = 2 F(infinity). This number here. And so F F(infinity) is equal
to the square root of pi over 2, if we believe what
we said on the last panel. And that was the thing that I
drew a picture of on the board. Namely, the graph of
F looked like this. And there was this
asymptote, which was the limit F(x) tends
to square root of pi over 2, as x goes to infinity. That was that limiting value. Which is F of infinity. So this is the asymptote. And now I've explained
the connection between what we claimed before,
which was quite mysterious, and what we're actually going
to be able to check now. Concretely, by making
this computation. So how in the world can you
get something like this. What's in that orange
box there, that V = Q^2. Again, the technique
is to use slices here. And I'm going to have to
draw you a 3-D picture to visualize the slice. Let's do that. I'm going to draw
three axes now, because we're now going to be
in three-dimensional space, and I want you to imagine
the x-axis as coming out of the blackboard, the
y-axis is horizontal, and there's a new
axis, which I'll call the z-axis, which is going up. So what's happening
here is that I'm thinking of this-- This is,
if you like, some kind of side view. And this is a view where I've
tilted things a little bit up to the top. Now, the distribution,
or you could think of this target
in the plane, where the most likely places
to hit were in the middle and it died off. As we went down. Now, I want to draw a
picture of this graph. I'm going to draw a
picture of e^(-r^2). And it's basically a hump. So I'm going to take the
first-- the slice along y = 0. The y = 0 slice. And I claim that that
goes up like this. And then comes back down. Let me shade this in,
so that you can see what kind of a slice this is. This is supposed to be along
this vertical plane here. Which is coming out
of the blackboard and coming towards you. And that's a slice. Now, I'm going to draw
one more slice so that you can see what's happening. I'm going to draw a
slice at another place. Along here. This will be y = b. Some other level. And now I'm going to
show you what happens. What happens is that the
hump dies down a little bit. So the bump is just
a little bit lower. And it's going to look a
little bit the same way. But it's just going
to be a bit smaller. So there's another slice here. Like that. And I want to give a
name to these slices. I'm going to call this A(b). That is, the area
of the b slice. Under the surface. OK? Yes, question. STUDENT: [INAUDIBLE] PROFESSOR: Yeah, the solid. Yeah. We're trying to figure
out this volume here, which is the one we started
out with, by slices. So first I have to think
of-- I'm going to visualize-- So here I didn't even visualize. I took a cross
section and I thought about how to spin it around
without actually doing that in three-dimensional space. But now I'm going to take
a different kind of slice. I'm going to take
that same bump, which is a three-dimensional object. I'm going to lay
it down on a plane. Which looks like this. And then it's a bump here. It's a hump. And now I'm going to try to
slice it by various planes. STUDENT: [INAUDIBLE] PROFESSOR: So one
way of defining the bump, as you just suggested,
is you take this curve and you rotate it
around this z-axis. So in other words, you make
this the axis of rotation, you spin it around. That's correct. So that shows you that the
peaks as you go down here are going to descend
the same way. But I don't want to
draw those lines. I want to imagine what
the parallel slices are. Because I don't want
to get cross slices. I want all slices parallel
to the same thing. STUDENT: [INAUDIBLE] PROFESSOR: OK. This is not particularly
easy to visualize. Now, here's the formula
for volume by slices. The formula for volume by
slices is that you add up the areas of the slices. That's how you do it. You take each slice. You add the
cross-sectional area, and then you take a
little thickness, dy, and then you add all of them up. Because this is extending
over the whole plane, we're going to have to go all
the way from minus infinity to plus infinity. And this is the formula
for volumes by slicing. And now our goal, in order
to do this calculation, we're going to just fix
y is equal to some b. We're just going to fix
one of these slices. And we're going
to calculate A(b). That's what we
need to do in order to make this procedure succeed. This is the only place
where this method works. But it's an important one. In order to make
it work, I'm going to have to again draw the plot
from a different point of view. I'm going to do the top view. So I want to look down
on this x-y plane here. This is the x-direction,
and here's the y-direction. And then again I want
to draw my slice. My slice is here. At y = b. So we're just
right on top of it. And it's coming up
at some kind of bump. Here, with a little
higher in the middle and going down on the sides. Now, the formula for
the height is this. If I take a distance r here,
the formula for the height of the bump is e^(-r^2). I'll store that over here. e^(-r^2) is the
height at this place. If this distance
to the origin is r. That's true all the way around. And in terms of b and x,
we can figure out that by this right triangle. This height is b, and
this distance is x. So r^2 = b^2 + x^2. Question. STUDENT: [INAUDIBLE] PROFESSOR: The question
is, is that the x-y plane. So the answer is that over here
I cleverly used the letter r. I avoided using y's
and z's or anything. And over here, this
is the distance r. And you like, this
is z, going up. That's the way to think of it. So that all of the
letters are consistent. So I just avoided
giving it a name. That's good, that's
exactly the point. Alright. So now, I claim I have
a formula for r^2. And so I can write this down. This e^(-b^2 + x^2). But now I'm going to use
the rule of exponents. Which is that this is the same
as e^(-b^2) times e^(-x^2). And that's going to be
the main way in which we use the particular function
that we're dealing with here. That's really the
main step, amazingly. So now I get to
compute what A(b) is. A(b) is the area under a curve. So it's going to be, let
me write it over here, A(b) is the area
under this curve here. Which is some constant
times-- so if you imagine, call this thing the name c. Under some curve, ce^(-x^2). Where the c is
equal to e^(-b^2). That's what our slice is. In fact, it looks
like one of those. It looks like one
of those bumps. Here's its formula again. It's the integral from
minus infinity to infinity of e^(-b^2) e^(-x^2) dx. We just recopied
what I had up there. And this is the height at
each value of x, with b fixed. And now, so we have
a lot of steps here. But each of them
is very elementary. The first one was just
that law of exponents. That we could split
the two into products. Now I'm going to make that
splitting even further. This is a constant. It's not varying with x. So I'm going to factor
it out of the integral. This is e^(-b^2) times the
integral from minus infinity to infinity of e^(-x^2) dx. So this might look frightening,
but actually it's just the property of an integral. All integrals have
this kind of property. You can always factor
out a constant. And now here comes
the remarkable thing. This is e^(-b^2) times a number
which is now familiar to us. What is this number? This is what we're looking for. This is our unknown, Q.
So I've computed A(b), and now I'm ready to
finish the problem off. A(b) = e^(-b^2) Q. Q is
that strange number which we don't know yet. What it is. So now I'm going to
compute the whole volume. The whole volume,
remember, it's over there, it's minus infinity
to infinity, A(y) dy. And now I'm just going to plug
in the formula that we've found for A. Now I'm doing
this for each b, so I'm doing it
varying over all b's. So I have the integral from
minus infinity to infinity. And here I have e^(-y^2). I've replaced b by y. And now I have Q. And I have dy. I just recopied what
I had over there into the formula for slicing. And now, I'm going to do
this trick of factoring out the constant a second time. This is a constant. It doesn't depend on y. It's the same for all y,
it just will factor out. So this is the same as Q
times the integral from minus infinity to infinity,
e^(-y^2) dy. And now, lo and behold,
this expression here. Of course, notice how
I defined Q. Let's go back carefully to
where Q is defined. Here's Q. This t is
a dummy variable. It doesn't matter
what I call it. I can call it x,
I can call it u, I can call it v.
In this case, I've given it two different names. At this stage, I called it x. And at this stage
I'm calling it y. But it's the same variable. And so this little chunk is
Q and altogether I have two of them, for Q^2
being the total. And that's the end
of the argument. It's a real miracle. STUDENT: [INAUDIBLE] PROFESSOR: Great question. The question is, wait a minute. As y changes, doesn't x change. And so then this
wouldn't be a constant. So that's the way in which
we've used the letters x and y in this whole course. When you get to 18.02,
you'll almost never do that. Always y and x will be
different variables. And they won't have to
depend on each other. Now, let me show you where on
this picture the x and the y are. We've got a whole x-y plane,
and here I'm fixing y = b, y isn't varying. Whereas x is changing. So, in other words, I don't have
a relationship between x and y, unless I fix it. In this case I've decided that
y is going to be constant. For all x. Over here, I made a computation. And I have a Q, which
is just a single number. No matter which b I took,
it didn't matter which. No matter which y equals b. Of course, I changed
the name to b so it wouldn't be so jarring to you. But in fact this
b was y all along. It's just that the x varied
completely independently of the y. I could fix the y and vary
the x, I could fix the x and vary the y. So it's a different
use of the letters. From what you're used to. It happens that y is
not a function of x. In this case. Yes. STUDENT: [INAUDIBLE] PROFESSOR: Yes. STUDENT: [INAUDIBLE] PROFESSOR: The question
is, because I'm rotating around the z-axis,
doesn't x change exactly as much as y does. What happens is that x and
y are symmetric variables. They can be treated equally. But if I decide to take slices
with respect to y being fixed and x varying, then of
course they're now separated, and I have a separate role
for the x and a separate role for the y. Or if I'd sliced
it the other way, I would have gotten
the same answer. I just would have reversed
the roles of x and y. So what's happening
is that x and y are on equal footing with
each other in this picture, and I could've
sliced the other way. I would have gotten
the same answer. That's more or less the
answer to your question. OK. Now I have given
you a review sheet, and I want to run
through, briefly, what's going to be on the exam. And this list of
exam questions is what's going to be on the exam. There are, sorry this is
not displayed correctly. So, exam questions,
but now I'm just going to show you what they are. There are five
questions on the exam. They are completely parallel
to what you got last year. So you should look at that test. It's worth looking at. And you'll see in the
descriptions on this sheet that what I'm describing
is what's on that test. So what's going to happen
is - and this is also posted on the Web
- is that you'll be expected to calculate
some definite integrals using the fundamental
theorem of calculus. Do a numerical approximation. There'll be a Riemann,
a trapezoidal rule and a Simpson's Rule. Calculate areas and volumes. And then some other
cumulative sum. Either an average value or
probability or perhaps work. And sketch a function
which is given in this form as an integral. So those are the
questions, and you'll see by the example of last
year's exam exactly the style. They're really going
to be very similar. Yes, question. STUDENT: [INAUDIBLE] PROFESSOR: OK, good question. So the question is,
for Riemann sums, what's the difference
between upper and lower, and right and left? So here we have a Riemann sum. And I'm going to give
you a picture which is, maybe this function y = 1 /
x, which was the one that we were discussing earlier. If you take the
function y = 1 / x and you break it up
into pieces here, however it doesn't
matter how many pieces, let's just say there
are four of them. Then the lower Riemann
sum is the staircase which fits underneath. So this one is a picture
of the lower sum. It's always less. And in the case of a decreasing
function, it's going to be, so since if you like,
since 1 / x decreases, the lower sum equals
the right sum. You can see that
visually on this picture. The values you're
going to select are going to be the right
ends of the rectangles. The upper sum is the left one. Now, if the function
wiggles up and down, then you have to pick
whichever side is appropriate. Or maybe it'll be a
point in the middle, if the maximum is
achieved in the middle. Yeah, another question. STUDENT: [INAUDIBLE] PROFESSOR: Correct. If the function is
increasing, then the lower sum is the left sum. So it just exactly
reverses what's here. So this is decreasing,
lower sum is right-hand sum. Increasing, lower
sum is left-hand sum. STUDENT: [INAUDIBLE] PROFESSOR: Yes. STUDENT: [INAUDIBLE] PROFESSOR: Good question. Suppose you're faced
with a function like this in this last problem. Which, generally, these
are the trickiest problems. And the question is,
how are you ever going to be able to decide
on an asymptote, even whether there
is an asymptote. And the answer is, you're not. It's going to be
pretty tricky to get keep track of what's happening
as it goes to infinity. We had an example
on the homework where is was
oscillating and it's very unclear what's going on. You have to do a very
long analysis for that. So in fact, just don't
worry about that now. At the very end of
the class, we'll talk a little bit
about these asymptotes. And really, the first issue
is whether they exist or not. And that's even something. That's a serious question
which we'll address at the very end of this course. STUDENT: [INAUDIBLE] PROFESSOR: That's right. It's not going to be
anything that complicated. Other questions? We we still have a
five whole minutes, and I have an example to give,
if nobody has a question. Yeah. STUDENT: [INAUDIBLE] PROFESSOR: The question,
uh, will I tell you which one of what to use? STUDENT: [INAUDIBLE]
PROFESSOR: When I tell you the numeric
approximation is, you'll see on the exam. The practice exam that you have. I will ask you for all three. I will ask you for the Riemann
sum, the trapezoidal rule, and the Simpson's rule. I'm guaranteeing you they'll
all three be on the exam. I'm guaranteeing that
every single thing which is on that piece of
paper is on the exam. And you'll see it on the
exam that you've got. It's exactly parallel
to what's there. STUDENT: [INAUDIBLE] PROFESSOR: So with
areas and volume, the question is will I tell
you which method to use. So let's discuss that. So with areas and volumes,
there's basically-- So this is always
true with areas. And it's true with
volumes of revolution. By the way you should
read this sheet. Not everything that's
on here have I said. But you should read it. Because it's all relevant. So with volumes of revolution,
you always work your way back to some 2-D diagram. So there's some 2-D
diagram which is always-- two-dimensional
diagram, which is always connected with these problems. I mean, something this hard
is really just too hard to do on an exam, right? I mean, I'm not going to ask
you something this complicated on the exam. Because this involves
a three-dimensional visualization. But once you're
down to 2-D, you're supposed to be
able to handle it. Now, what's the main issue after
you've got your 2-D diagram? The main issue is, do you
want to integrate with respect to dx or dy? And the answer is
that it will depend. And if there's one
that's going to cause you incredible difficulty,
and I feel that you're not able to dodge it, then
I might give you a hint and say you'd better use
shells, or you'd better use disks or washers
or something like that. But if I feel that you're
grown up enough to figure out which one it is, because
one of them is so ridiculous you say forget it, immediately,
after thinking about it. Then I won't tell you which one. Because I figure,
in other words, I don't want you
to waste your time. But I'm willing to waste a
minute or two of your time on a wild goose chase. So let me give you
an example of this. Suppose you're looking at the
curve y between 0 and x - x^3. So this is some kind of lump. Like this. It goes from 0 to 1, because the
right-hand side is 0 at 0 and 1 here. It's some kind of thing. And there are these
two possibilities. One of them is to do shells. And then, so this is supposed
to be rotated around the y-axis. In this case. And the same would apply,
actually, to the area problem. So I'm doing a slightly
more complicated problem. But you could ask for the area
underneath this, and so forth. OK. So we can integrate this dx,
or we can integrate this dy. This indicates that
I'm deciding that this is going to be of thickness
dx, and I'm integrating dx. So that's a choice
that I'm making. Now, the minute I
made that choice I know that these are shells. Because they sweep around this
way and that makes them shells. Cylindrical shells. And if I do that,
the setup is this. It's 2 2 pi x (x - x^3) dx. Now, I claim that when
you get to this point, you already know you've won. Because this is an easy
integral to calculate. So you're done here. You're happy. Now, if you happened to say,
oh gee, I hate to do this. I want to do
something clever, you could try to do it
with cutting this way. Let's do this. And this would be
the dy thickness. And then when you
sweep this around, you get what we call a washer. Which is really just the
difference of two disks. So the shape here is this
thing swung around this axis. And it looks like this. So it's going to be the
difference of radii. So what's the formula for this? It's some integral of
pi times the right end, which I'll call x_2,
and here the left end, which I'll call x_1. So this is pi pi
(x_2^2 - x_1^2) dy. Now, already at this stage,
you think to yourself this is more complicated
than the other method. So you've already abandoned it. But I'm just going to go one
step further into this one to see what it is
that's happening. If you try to figure out what
these values x_1 and x_2 are, that corresponds to solving
for x_1 and x_2 in terms of y. So that's the
following equation. x_1 and x_2 solve the
equation that-- the curve, x - x^3 is equal y. Now, look at this equation. That's the equation x^3--
sorry, x^3 - x + y, I guess. Let's see. Yeah, that's right,
is equal to 0. This is a cubic equation. Although there is
a formula for this. You've never been taught the
formula for this equation. So therefore, you
will never, ever be able to get a formula for
x_2 and x_1 as a function of y. And you'll never be able
to compute this one. This is more than just a dead
end, it's like crash, burn, and, you know, self-destruct. So there may be such a
thing, so do the other way. Good luck, folks.

